Configuring GraphRAG Indexing
The GraphRAG system is highly configurable. This page provides an overview of the configuration options available for the GraphRAG indexing engine.

Default Configuration Mode
The default configuration mode is the simplest way to get started with the GraphRAG system. It is designed to work out-of-the-box with minimal configuration. The primary configuration sections for the Indexing Engine pipelines are described below. The main ways to set up GraphRAG in Default Configuration mode are via:

Init command (recommended)
Purely using environment variables
Using JSON or YAML for deeper control
Custom Configuration Mode
Custom configuration mode is an advanced use-case. Most users will want to use the Default Configuration instead. The primary configuration sections for Indexing Engine pipelines are described below. Details about how to use custom configuration are available in the Custom Configuration Mode documentation.

Configuring GraphRAG Indexing
To start using GraphRAG, you need to configure the system. The init command is the easiest way to get started. It will create a .env and settings.yaml files in the specified directory with the necessary configuration settings. It will also output the default LLM prompts used by GraphRAG.

Usage
python -m graphrag.index [--init] [--root PATH]

Options
--init - Initialize the directory with the necessary configuration files.
--root PATH - The root directory to initialize. Default is the current directory.
Example
python -m graphrag.index --init --root ./ragtest

Output
The init command will create the following files in the specified directory:

settings.yaml - The configuration settings file. This file contains the configuration settings for GraphRAG.
.env - The environment variables file. These are referenced in the settings.yaml file.
prompts/ - The LLM prompts folder. This contains the default prompts used by GraphRAG, you can modify them or run the Auto Prompt Tuning command to generate new prompts adapted to your data.
Next Steps
After initializing your workspace, you can either run the Prompt Tuning command to adapt the prompts to your data or even start running the Indexing Pipeline to index your data. For more information on configuring GraphRAG, see the Configuration documentation.


Configuring GraphRAG Indexing
To start using GraphRAG, you need to configure the system. The init command is the easiest way to get started. It will create a .env and settings.yaml files in the specified directory with the necessary configuration settings. It will also output the default LLM prompts used by GraphRAG.

Usage
python -m graphrag.index [--init] [--root PATH]

Options
--init - Initialize the directory with the necessary configuration files.
--root PATH - The root directory to initialize. Default is the current directory.
Example
python -m graphrag.index --init --root ./ragtest

Output
The init command will create the following files in the specified directory:

settings.yaml - The configuration settings file. This file contains the configuration settings for GraphRAG.
.env - The environment variables file. These are referenced in the settings.yaml file.
prompts/ - The LLM prompts folder. This contains the default prompts used by GraphRAG, you can modify them or run the Auto Prompt Tuning command to generate new prompts adapted to your data.
Next Steps
After initializing your workspace, you can either run the Prompt Tuning command to adapt the prompts to your data or even start running the Indexing Pipeline to index your data. For more information on configuring GraphRAG, see the Configuration documentation.

Default Configuration Mode (using JSON/YAML)
The default configuration mode may be configured by using a config.json or config.yml file in the data project root. If a .env file is present along with this config file, then it will be loaded, and the environment variables defined therein will be available for token replacements in your configuration document using ${ENV_VAR} syntax.

For example:

# .env
API_KEY=some_api_key

# config.json
{
    "llm": {
        "api_key": "${API_KEY}"
    }
}
Config Sections
input
Fields
type file|blob - The input type to use. Default=file
file_type text|csv - The type of input data to load. Either text or csv. Default is text
file_encoding str - The encoding of the input file. Default is utf-8
file_pattern str - A regex to match input files. Default is .*\.csv$ if in csv mode and .*\.txt$ if in text mode.
source_column str - (CSV Mode Only) The source column name.
timestamp_column str - (CSV Mode Only) The timestamp column name.
timestamp_format str - (CSV Mode Only) The source format.
text_column str - (CSV Mode Only) The text column name.
title_column str - (CSV Mode Only) The title column name.
document_attribute_columns list[str] - (CSV Mode Only) The additional document attributes to include.
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to read input from, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
llm
This is the base LLM configuration section. Other steps may override this configuration with their own LLM configuration.

Fields
api_key str - The OpenAI API key to use.
type openai_chat|azure_openai_chat|openai_embedding|azure_openai_embedding - The type of LLM to use.
model str - The model name.
max_tokens int - The maximum number of output tokens.
request_timeout float - The per-request timeout.
api_base str - The API base url to use.
api_version str - The API version
organization str - The client organization.
proxy str - The proxy URL to use.
cognitive_services_endpoint str - The url endpoint for cognitive services.
deployment_name str - The deployment name to use (Azure).
model_supports_json bool - Whether the model supports JSON-mode output.
tokens_per_minute int - Set a leaky-bucket throttle on tokens-per-minute.
requests_per_minute int - Set a leaky-bucket throttle on requests-per-minute.
max_retries int - The maximum number of retries to use.
max_retry_wait float - The maximum backoff time.
sleep_on_rate_limit_recommendation bool - Whether to adhere to sleep recommendations (Azure).
concurrent_requests int The number of open requests to allow at once.
temperature float - The temperature to use.
top_p float - The top-p value to use.
n int - The number of completions to generate.
parallelization
Fields
stagger float - The threading stagger value.
num_threads int - The maximum number of work threads.
async_mode
asyncio|threaded The async mode to use. Either asyncio or `threaded.

embeddings
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
batch_size int - The maximum batch size to use.
batch_max_tokens int - The maximum batch #-tokens.
target required|all - Determines which set of embeddings to emit.
skip list[str] - Which embeddings to skip.
strategy dict - Fully override the text-embedding strategy.
chunks
Fields
size int - The max chunk size in tokens.
overlap int - The chunk overlap in tokens.
group_by_columns list[str] - group documents by fields before chunking.
strategy dict - Fully override the chunking strategy.
cache
Fields
type file|memory|none|blob - The cache type to use. Default=file
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to write cache to, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
storage
Fields
type file|memory|blob - The storage type to use. Default=file
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to write reports to, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
reporting
Fields
type file|console|blob - The reporting type to use. Default=file
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to write reports to, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
entity_extraction
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
entity_types list[str] - The entity types to identify.
max_gleanings int - The maximum number of gleaning cycles to use.
strategy dict - Fully override the entity extraction strategy.
summarize_descriptions
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
max_length int - The maximum number of output tokens per summarization.
strategy dict - Fully override the summarize description strategy.
claim_extraction
Fields
enabled bool - Whether to enable claim extraction. default=False
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
description str - Describes the types of claims we want to extract.
max_gleanings int - The maximum number of gleaning cycles to use.
strategy dict - Fully override the claim extraction strategy.
community_reports
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
max_length int - The maximum number of output tokens per report.
max_input_length int - The maximum number of input tokens to use when generating reports.
strategy dict - Fully override the community reports strategy.
cluster_graph
Fields
max_cluster_size int - The maximum cluster size to emit.
strategy dict - Fully override the cluster_graph strategy.
embed_graph
Fields
enabled bool - Whether to enable graph embeddings.
num_walks int - The node2vec number of walks.
walk_length int - The node2vec walk length.
window_size int - The node2vec window size.
iterations int - The node2vec number of iterations.
random_seed int - The node2vec random seed.
strategy dict - Fully override the embed graph strategy.
umap
Fields
enabled bool - Whether to enable UMAP layouts.
snapshots
Fields
graphml bool - Emit graphml snapshots.
raw_entities bool - Emit raw entity snapshots.
top_level_nodes bool - Emit top-level-node snapshots.
encoding_model
str - The text encoding model to use. Default is cl100k_base.

skip_workflows
list[str] - Which workflow names to skip.

Custom Configuration Mode
The primary configuration sections for Indexing Engine pipelines are described below. Each configuration section can be expressed in Python (for use in Python API mode) as well as YAML, but YAML is show here for brevity.

Using custom configuration is an advanced use-case. Most users will want to use the Default Configuration instead.

Indexing Engine Examples
The examples directory contains several examples of how to use the indexing engine with custom configuration.

Most examples include two different forms of running the pipeline, both are contained in the examples run.py

Using mostly the Python API
Using mostly the a pipeline configuration file
To run an example:

Run poetry shell to activate a virtual environment with the required dependencies.
Run PYTHONPATH="$(pwd)" python examples/path_to_example/run.py from the root directory.
For example to run the single_verb example, you would run the following commands:

poetry shell

PYTHONPATH="$(pwd)" python examples/single_verb/run.py

Configuration Sections
> extends
This configuration allows you to extend a base configuration file or files.

# single base
extends: ../base_config.yml

# multiple bases
extends:
  - ../base_config.yml
  - ../base_config2.yml

> root_dir
This configuration allows you to set the root directory for the pipeline. All data inputs and outputs are assumed to be relative to this path.

root_dir: /workspace/data_project

> storage
This configuration allows you define the output strategy for the pipeline.

type: The type of storage to use. Options are file, memory, and blob
base_dir (type: file only): The base directory to store the data in. This is relative to the config root.
connection_string (type: blob only): The connection string to use for blob storage.
container_name (type: blob only): The container to use for blob storage.
> cache
This configuration allows you define the cache strategy for the pipeline.

type: The type of cache to use. Options are file and memory, and blob.
base_dir (type: file only): The base directory to store the cache in. This is relative to the config root.
connection_string (type: blob only): The connection string to use for blob storage.
container_name (type: blob only): The container to use for blob storage.
> reporting
This configuration allows you define the reporting strategy for the pipeline. Report files are generated artifacts that summarize the performance metrics of the pipeline and emit any error messages.

type: The type of reporting to use. Options are file, memory, and blob
base_dir (type: file only): The base directory to store the reports in. This is relative to the config root.
connection_string (type: blob only): The connection string to use for blob storage.
container_name (type: blob only): The container to use for blob storage.
> workflows
This configuration section defines the workflow DAG for the pipeline. Here we define an array of workflows and express their inter-dependencies in steps:

name: The name of the workflow. This is used to reference the workflow in other parts of the config.
steps: The DataShaper steps that this workflow comprises. If a step defines an input in the form of workflow:<workflow_name>, then it is assumed to have a dependency on the output of that workflow.
workflows:
  - name: workflow1
    steps:
      - verb: derive
        args:
          column1: "col1"
          column2: "col2"
  - name: workflow2
    steps:
      - verb: derive
        args:
          column1: "col1"
          column2: "col2"
        input:
          # dependency established here
          source: workflow:workflow1

> input
type: The type of input to use. Options are file or blob.
file_type: The file type field discriminates between the different input types. Options are csv and text.
base_dir: The base directory to read the input files from. This is relative to the config file.
file_pattern: A regex to match the input files. The regex must have named groups for each of the fields in the file_filter.
post_process: A DataShaper workflow definition to apply to the input before executing the primary workflow.
source_column (type: csv only): The column containing the source/author of the data
text_column (type: csv only): The column containing the text of the data
timestamp_column (type: csv only): The column containing the timestamp of the data
timestamp_format (type: csv only): The format of the timestamp
input:
  type: file
  file_type: csv
  base_dir: ../data/csv # the directory containing the CSV files, this is relative to the config file
  file_pattern: '.*[\/](?P<source>[^\/]+)[\/](?P<year>\d{4})-(?P<month>\d{2})-(?P<day>\d{2})_(?P<author>[^_]+)_\d+\.csv$' # a regex to match the CSV files
  # An additional file filter which uses the named groups from the file_pattern to further filter the files
  # file_filter:
  #   # source: (source_filter)
  #   year: (2023)
  #   month: (06)
  #   # day: (22)
  source_column: "author" # the column containing the source/author of the data
  text_column: "message" # the column containing the text of the data
  timestamp_column: "date(yyyyMMddHHmmss)" # optional, the column containing the timestamp of the data
  timestamp_format: "%Y%m%d%H%M%S" # optional,  the format of the timestamp
  post_process: # Optional, set of steps to process the data before going into the workflow
    - verb: filter
      args:
        column: "title",
        value: "My document"

input:
  type: file
  file_type: csv
  base_dir: ../data/csv # the directory containing the CSV files, this is relative to the config file
  file_pattern: '.*[\/](?P<source>[^\/]+)[\/](?P<year>\d{4})-(?P<month>\d{2})-(?P<day>\d{2})_(?P<author>[^_]+)_\d+\.csv$' # a regex to match the CSV files
  # An additional file filter which uses the named groups from the file_pattern to further filter the files
  # file_filter:
  #   # source: (source_filter)
  #   year: (2023)
  #   month: (06)
  #   # day: (22)
  post_process: # Optional, set of steps to process the data before going into the workflow
    - verb: filter
      args:
        column: "title",
        value: "My document"

Configuration Template
The following template can be used and stored as a .env in the the directory where you're are pointing the --root parameter on your Indexing Pipeline execution.

For details about how to run the Indexing Pipeline, refer to the Index CLI documentation.

.env File Template
Required variables are uncommented. All the optional configuration can be turned on or off as needed.

Minimal Configuration
# Base LLM Settings
GRAPHRAG_API_KEY="your_api_key"
GRAPHRAG_API_BASE="http://<domain>.openai.azure.com" # For Azure OpenAI Users
GRAPHRAG_API_VERSION="api_version" # For Azure OpenAI Users

# Text Generation Settings
GRAPHRAG_LLM_TYPE="azure_openai_chat" # or openai_chat
GRAPHRAG_LLM_DEPLOYMENT_NAME="gpt-4-turbo-preview"
GRAPHRAG_LLM_MODEL_SUPPORTS_JSON=True

# Text Embedding Settings
GRAPHRAG_EMBEDDING_TYPE="azure_openai_embedding" # or openai_embedding
GRAPHRAG_LLM_DEPLOYMENT_NAME="text-embedding-3-small"

# Data Mapping Settings
GRAPHRAG_INPUT_TYPE="text"

Full Configuration

# Required LLM Config

# Input Data Configuration
GRAPHRAG_INPUT_TYPE="file"

# Plaintext Input Data Configuration
# GRAPHRAG_INPUT_FILE_PATTERN=.*\.txt

# Text Input Data Configuration
GRAPHRAG_INPUT_FILE_TYPE="text"
GRAPHRAG_INPUT_FILE_PATTERN=".*\.txt$"
GRAPHRAG_INPUT_SOURCE_COLUMN=source
# GRAPHRAG_INPUT_TIMESTAMP_COLUMN=None
# GRAPHRAG_INPUT_TIMESTAMP_FORMAT=None
# GRAPHRAG_INPUT_TEXT_COLUMN="text"
# GRAPHRAG_INPUT_ATTRIBUTE_COLUMNS=id
# GRAPHRAG_INPUT_TITLE_COLUMN="title"
# GRAPHRAG_INPUT_TYPE="file"
# GRAPHRAG_INPUT_CONNECTION_STRING=None
# GRAPHRAG_INPUT_CONTAINER_NAME=None
# GRAPHRAG_INPUT_BASE_DIR=None

# Base LLM Settings
GRAPHRAG_API_KEY="your_api_key"
GRAPHRAG_API_BASE="http://<domain>.openai.azure.com" # For Azure OpenAI Users
GRAPHRAG_API_VERSION="api_version" # For Azure OpenAI Users
# GRAPHRAG_API_ORGANIZATION=None
# GRAPHRAG_API_PROXY=None

# Text Generation Settings
# GRAPHRAG_LLM_TYPE=openai_chat
GRAPHRAG_LLM_API_KEY="your_api_key" # If GRAPHRAG_API_KEY is not set
GRAPHRAG_LLM_API_BASE="http://<domain>.openai.azure.com" # For Azure OpenAI Users and if GRAPHRAG_API_BASE is not set
GRAPHRAG_LLM_API_VERSION="api_version" # For Azure OpenAI Users and if GRAPHRAG_API_VERSION is not set
GRAPHRAG_LLM_MODEL_SUPPORTS_JSON=True # Suggested by default
# GRAPHRAG_LLM_API_ORGANIZATION=None
# GRAPHRAG_LLM_API_PROXY=None
# GRAPHRAG_LLM_DEPLOYMENT_NAME=None
# GRAPHRAG_LLM_MODEL=gpt-4-turbo-preview
# GRAPHRAG_LLM_MAX_TOKENS=4000
# GRAPHRAG_LLM_REQUEST_TIMEOUT=180
# GRAPHRAG_LLM_THREAD_COUNT=50
# GRAPHRAG_LLM_THREAD_STAGGER=0.3
# GRAPHRAG_LLM_CONCURRENT_REQUESTS=25
# GRAPHRAG_LLM_TPM=0
# GRAPHRAG_LLM_RPM=0
# GRAPHRAG_LLM_MAX_RETRIES=10
# GRAPHRAG_LLM_MAX_RETRY_WAIT=10
# GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION=True

# Text Embedding Settings
# GRAPHRAG_EMBEDDING_TYPE=openai_embedding
GRAPHRAG_EMBEDDING_API_KEY="your_api_key" # If GRAPHRAG_API_KEY is not set
GRAPHRAG_EMBEDDING_API_BASE="http://<domain>.openai.azure.com"  # For Azure OpenAI Users and if GRAPHRAG_API_BASE is not set
GRAPHRAG_EMBEDDING_API_VERSION="api_version" # For Azure OpenAI Users and if GRAPHRAG_API_VERSION is not set
# GRAPHRAG_EMBEDDING_API_ORGANIZATION=None
# GRAPHRAG_EMBEDDING_API_PROXY=None
# GRAPHRAG_EMBEDDING_DEPLOYMENT_NAME=None
# GRAPHRAG_EMBEDDING_MODEL=text-embedding-3-small
# GRAPHRAG_EMBEDDING_BATCH_SIZE=16
# GRAPHRAG_EMBEDDING_BATCH_MAX_TOKENS=8191
# GRAPHRAG_EMBEDDING_TARGET=required
# GRAPHRAG_EMBEDDING_SKIP=None
# GRAPHRAG_EMBEDDING_THREAD_COUNT=None
# GRAPHRAG_EMBEDDING_THREAD_STAGGER=50
# GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS=25
# GRAPHRAG_EMBEDDING_TPM=0
# GRAPHRAG_EMBEDDING_RPM=0
# GRAPHRAG_EMBEDDING_MAX_RETRIES=10
# GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT=10
# GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION=True

# Data Mapping Settings
# GRAPHRAG_INPUT_ENCODING=utf-8

# Data Chunking
# GRAPHRAG_CHUNK_SIZE=1200
# GRAPHRAG_CHUNK_OVERLAP=100
# GRAPHRAG_CHUNK_BY_COLUMNS=id

# Prompting Overrides
# GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE=None
# GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS=1
# GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES=organization,person,event,geo
# GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE=None
# GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH=500
# GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION="Any claims or facts that could be relevant to threat analysis."
# GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE=None
# GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS=1
# GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE=None
# GRAPHRAG_COMMUNITY_REPORT_MAX_LENGTH=1500

# Storage
# GRAPHRAG_STORAGE_TYPE=file
# GRAPHRAG_STORAGE_CONNECTION_STRING=None
# GRAPHRAG_STORAGE_CONTAINER_NAME=None
# GRAPHRAG_STORAGE_BASE_DIR=None

# Cache
# GRAPHRAG_CACHE_TYPE=file
# GRAPHRAG_CACHE_CONNECTION_STRING=None
# GRAPHRAG_CACHE_CONTAINER_NAME=None
# GRAPHRAG_CACHE_BASE_DIR=None

# Reporting
# GRAPHRAG_REPORTING_TYPE=file
# GRAPHRAG_REPORTING_CONNECTION_STRING=None
# GRAPHRAG_REPORTING_CONTAINER_NAME=None
# GRAPHRAG_REPORTING_BASE_DIR=None

# Node2Vec Parameters
# GRAPHRAG_NODE2VEC_ENABLED=False
# GRAPHRAG_NODE2VEC_NUM_WALKS=10
# GRAPHRAG_NODE2VEC_WALK_LENGTH=40
# GRAPHRAG_NODE2VEC_WINDOW_SIZE=2
# GRAPHRAG_NODE2VEC_ITERATIONS=3
# GRAPHRAG_NODE2VEC_RANDOM_SEED=597832

# Data Snapshotting
# GRAPHRAG_SNAPSHOT_GRAPHML=False
# GRAPHRAG_SNAPSHOT_RAW_ENTITIES=False
# GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES=False

# Miscellaneous Settings
# GRAPHRAG_ASYNC_MODE=asyncio
# GRAPHRAG_ENCODING_MODEL=cl100k_base
# GRAPHRAG_MAX_CLUSTER_SIZE=10
# GRAPHRAG_ENTITY_RESOLUTION_ENABLED=False
# GRAPHRAG_SKIP_WORKFLOWS=None
# GRAPHRAG_UMAP_ENABLED=False



