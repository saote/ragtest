ST231 Lecture Notes
Contents
Preface 6
1 Introduction 7
1.1 Simple linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.2 Terminology and notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.3 From simple to multiple regression . . . . . . . . . . . . . . . . . . . . . . . 15
1.4 The class of linear models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.5 Summary of Chapter 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2 Linearity 26
2.1 Anscombe’s Quartet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.2 Polynomial regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.3 A linear model with a log-transformed predictor . . . . . . . . . . . . . . . . 32
2.4 A strategy for statistical modelling . . . . . . . . . . . . . . . . . . . . . . . 35
2.5 What makes a good model? . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
2.6 Summary of Chapter 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3 Residual analysis 38
3.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.2 Model assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.3 Residual plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.4 Assessing normality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.5 Summary of Chapter 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
4 Non-linear transformations 51
4.1 The log-transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.2 The mammals dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.3 The trees dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.4 Summary of Chapter 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
5 Least squares estimation 59
5.1 The least squares estimate . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
2
CONTENTS 3
5.2 Example: simple linear regression . . . . . . . . . . . . . . . . . . . . . . . . 59
5.3 Derivation of the LSE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
5.4 Example: simple linear regression continued . . . . . . . . . . . . . . . . . . 64
5.5 Example: invertible linear transformations . . . . . . . . . . . . . . . . . . . 67
5.6 Summary of Chapter 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
6 Maximum likelihood estimation 71
6.1 The likelihood function of a normal linear model . . . . . . . . . . . . . . . 71
6.2 The MLE for the parameter vector . . . . . . . . . . . . . . . . . . . . . . . 72
6.3 Sampling distribution of the least squares estimator . . . . . . . . . . . . . 73
6.4 Unbiased estimator for the error variance . . . . . . . . . . . . . . . . . . . 76
6.5 Summary of Chapter 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
7 The hat matrix 79
7.1 Properties of the hat matrix. . . . . . . . . . . . . . . . . . . . . . . . . . . 79
7.2 Properties of the residuals and of the fitted values . . . . . . . . . . . . . . 81
7.3 Summary of Chapter 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
8 Unusual observations 84
8.1 Leverages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
8.2 Outliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
8.3 Influence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
8.4 What to do with influential data points . . . . . . . . . . . . . . . . . . . . 89
8.5 Summary of Chapter 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
9 Categorical predictor variables 91
9.1 Retail data: including brand as a predictor variable . . . . . . . . . . . . . . 91
9.2 Alternative parameterisations . . . . . . . . . . . . . . . . . . . . . . . . . . 96
9.3 A model with an interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
9.4 Summary of Chapter 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
10 The T-statistic 105
10.1 Useful results for the multivariate normal distribution . . . . . . . . . . . . 105
10.2 Distributional properties of βb and s
2
. . . . . . . . . . . . . . . . . . . . . . 105
10.3 The T-statistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
10.4 A simple example in R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
10.5 Summary of Chapter 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
11 Interval estimation 111
11.1 Confidence intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
11.2 Simple example ctd. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
11.3 Estimation and prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
11.4 Summary of Chapter 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
4 CONTENTS
12 The t-test for normal linear models 117
12.1 Hypothesis testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
12.2 A simple example in R ctd . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
12.3 Comparison with other t-tests . . . . . . . . . . . . . . . . . . . . . . . . . . 120
12.4 Limitations of the t-test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
12.5 Summary of Chapter 12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
13 The F-test and ANOVA 124
13.1 Nested linear models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
13.2 The decomposition of the total sum of squares . . . . . . . . . . . . . . . . 126
13.3 The test for the existence of regression . . . . . . . . . . . . . . . . . . . . . 129
13.4 ANOVA table for the existence of regression . . . . . . . . . . . . . . . . . . 131
13.5 Illustration in R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
13.6 The distribution of the F-statistic . . . . . . . . . . . . . . . . . . . . . . . 136
13.7 Summary of Chapter 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
14 More on F-tests and ANOVA 140
14.1 Sequential ANOVA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
14.2 Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
14.3 Test for non-linearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
14.4 Summary of Chapter 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
15 Model selection criteria and variable selection 151
15.1 Bias and Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
15.2 The model hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
15.3 Model selection statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
15.4 Variable selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
15.5 Multicollinearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
Summary of Chapter 15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
16 The General Linear Model 168
16.1 The generalised least squares estimator . . . . . . . . . . . . . . . . . . . . . 168
16.2 Weighted regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
16.3 Serially correlated errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
16.4 Summary of Chapter 16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
17 Generalised Linear Models 177
17.1 Example - Spam filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
17.2 Definition of a GLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
17.3 GLMs for binary response data . . . . . . . . . . . . . . . . . . . . . . . . . 180
17.4 Hypothesis testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
17.5 Summary of Chapter 17 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
CONTENTS 5
18 Appendix 191
Appendix A - Linear algebra and multivariable calculus . . . . . . . . . . . . . . 191
Appendix B - Generalised expectation . . . . . . . . . . . . . . . . . . . . . . . . 192
Appendix C - The distribution of s
2
. . . . . . . . . . . . . . . . . . . . . . . . . 194
Appendix D - M-estimators and robust regression . . . . . . . . . . . . . . . . . . 197
Appendix E - The distribution of the F-statistic . . . . . . . . . . . . . . . . . . 201
Appendix F - The exponential family of distributions . . . . . . . . . . . . . . . . 206
Appendix G - Poisson GLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
Appendix H - A summary of some key results . . . . . . . . . . . . . . . . . . . . 212
Preface
All rights are reserved.
To obtain a pdf version of these notes, click on the pdf icon in the top bar.
These materials are solely for your own use and you must not distribute these in any format.
Do not upload these materials to the internet or any filesharing sites nor provide them to
any third party or forum.
If you find any typos in these notes, please inform the module leader by submitting the
details to the ST231 anonymous submission form.
Please note:
As with many textbooks on the subject, much of theory we discuss in this module is quoted
as developed by European and/or Anglo-American scientists. This is also reflected in the
historical datasets we consider. However, this does not mean that there were not similar or
other important discoveries made elsewhere. This is my first version of these lecture notes
and so follows the exposition common in many textbooks. But I do hope that over time I
will be able to augment this with material from more diverse sources. If you are aware of
suitable material, then please do let me know. Thanks!
6
Chapter 1
Introduction
Normal linear models are statistical models that describe the relationship between a single
quantitative outcome variable and one or more explanatory variables. Statistical modelling
often uses normal linear models because, as we will see, they span a surprisingly large class
of useful models. The mathematical theory underpinning these models is well developed as it
is analytically tractable and we will spend a substantial amount of this module deriving this
theory. To do this we will use tools from linear algebra as is common for machine learning
algorithms in general, a fact that is alluded to in this comic by xkcd.com.
Normal linear models have many nice properties some of which we will learn about in later
chapters. And finally, numerically, it is relatively straightforward to fit these models and we
will use well-established functions in R to do so.
The modules ST117 and ST121 introduced the simplest form of a linear model, namely
a simple linear regression model. In the current chapter we will start with simple linear
regression and then explain how to extend to regression models with several explanatory
variables. The aim of the initial sections in this chapter is to give an intuitive understanding
and overview of the subject. This means that we will defer some of the more technical
details until later.
1.1 Simple linear regression
Let’s start with an example relating to data on 48 Solitaire gold rings. A Solitaire ring is a
ring mounted with a single diamond. The data were originally collected by Dr Singfat Chu1
and are available as the dataset diamond in the R library UsingR.
1Chu, S. (1996) Diamond ring pricing using linear regression. Journal of Statistics Education, 4(3).
7
8 CHAPTER 1. INTRODUCTION
Figure 1.1 shows a plot of the price of each ring in Singapore Dollars (S$) against the weight
of its diamond. The weight of a diamond is measured in carat where one carat corresponds
to 20 milligrams.
Figure 1.1: Scatterplot of the diamond data.
Scatterplots like these are helpful to illustrate the relationship between two quantitative
variables. We observe that the price of a Solitaire ring depends on the weight of its diamond:
the heavier the diamond, the more expensive tends to be the ring. No surprise there! While
the figure visually illustrates how the price of a ring depends on the weight of its diamond,
it would certainly be useful to have a more concise mathematical representation of that
relationship.
Considering the scatterplot again, we observe that the relationship between price and weight
is linear, that is, the points in the scatterplot lie close to a straight line. In Figure 1.2
the line of best fit has been added to the scatterplot, thus illustrating the linearity of the
relationship between the price of a ring and the weight of its diamond.
However, we also note that the relationship is not deterministic, that is, the points in the
plot do not lie exactly on the straight line, but instead are scattered around it. Or, in other
words, given the weight of the diamond we can use the line to produce a reasonable estimate
of the price of the corresponding Solitaire ring but we cannot predict the price exactly.
In the above, we have implicitly decomposed the relationship between the price of a Solitaire
ring and the weight of its diamond into two parts:
• a deterministic or systematic part, in this example a straight line; and
• a random error that describes how the observations scatter around the systematic
part.
1.1. SIMPLE LINEAR REGRESSION 9
Figure 1.2: Scatterplot of the diamond data with fitted regression line.
Let
h(weight) = β0 + β1 weight
define the straight line that forms the systematic component of the relationship between
price and weight. Furthermore, let ϵ denote the random error, then a model for the price of
a Solitaire ring can defined by the equation
Price = β0 + β1 weight + ϵ. (1.1)
We assume that the error has a normal distribution with mean zero and variance σ
2
. The
assumption that the mean of the error is zero implies that the average price of a ring is
determined by the systematic component and thus by the weight of its diamond ring. We
will make this definition more precise later on, but for now it serves as an illustration of how
a linear model comprises a systematic component and a random component.
The intercept β0 and the slope β1 in (1.1) are referred to as the parameters of the model.
To be able to make practical use of the model, we need to determine reasonable values for
β0 and β1. We can find these by determining the line of best fit. (We will discuss later how
this line is determined mathematically. For now, rest assured that R will do all the hard
work for you!)
To determine the line of best fit in R we use a command of the form
model <- lm(response ~ predictor, data = dataset)
Then the summary function summary(model) gives us the details of the fitted model, in
particular the estimated values for the parameters β0 and β1. The R code given below starts
by loading the package UsingR which provides the dataset diamond. (If you try this code
and get an error message, then this is likely due to not having installed the package UsingR.
Use the command install.packages("UsingR") to install it, then try again.)
10 CHAPTER 1. INTRODUCTION
library(UsingR)
SLR.diamond <- lm(price ~ carat, data = diamond)
summary(SLR.diamond)
Note that in the dataset the weight variable is called carat. So the second line of code
determines the line of best fit and the last line provides a statistical summary of the fitted
model. This produces the following output:
Call:
lm(formula = price ~ carat, data = diamond)
Residuals:
Min 1Q Median 3Q Max
-85.159 -21.448 -0.869 18.972 79.370
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) -259.63 17.32 -14.99 <2e-16 ***
carat 3721.02 81.79 45.50 <2e-16 ***
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 31.84 on 46 degrees of freedom
Multiple R-squared: 0.9783, Adjusted R-squared: 0.9778
F-statistic: 2070 on 1 and 46 DF, p-value: < 2.2e-16
We will learn in this module what all of this output means. For now just consider the column
Estimate in the middle table which lists information about the coefficients. This column
lists the intercept and slope of the line of best fit, with the intercept given as -259.63 and
the slope, that is the coefficient associated with the explanatory variable carat, as 3721.02.
So what does this tell us about the relationship between the price of a Solitaire ring and the
weight of its diamond?
A naive interpretation of the type that you may have learned in school is as follows.
• Intercept: the average price of a Solitaire ring with a diamond of zero weight is expected
to be around - 260 Singapore Dollars.
• Slope: the average price of a Solitaire ring increases by around S$ 3721,- for every
additional carat of weight of its diamond.
Note that I have rounded the coefficients to something that seemed sensible in the context.
And, before you ask, I will not give you hard and fixed rules about rounding, but instead
expect you to do something that is sensible in a given context. Also note the use of units of
1.1. SIMPLE LINEAR REGRESSION 11
measurement, here Singapore Dollars.
There are two issues with the interpretation above. Firstly, let’s consider the interpretation
of the estimated slope. The heaviest diamond in the dataset is 0.35 carat and so considering
a one carat increase seems rather extreme. Secondly consider the interpretation of the
estimated intercept. It does not seem entirely sensible to consider the price of a Solitaire
ring whose diamond has zero weight. (And even so, what does a negative average price
mean?)
First, let us consider the interpretation of the slope coefficient. I criticised that using a one
carat increase in weight requires an extrapolation, so let’s choose the more sensible increase
of 0.1 carat.
• On average, the price of a Solitaire ring increases by S$ 372,- for every additional 0.1
carat of weight of its diamond.
In the above I considered an increase of 0.1 carat, that is a tenth of a unit increase. This leads
to the predicted price of the ring changing by a tenth of the coefficient of the explanatory
variable carat. This hopefully makes intuitive sense from the model equation. While the
above is a common interpretation, note that we cannot increase the weight of a diamond. A
more careful interpretation considers a comparison between two Solitaire rings.
• Consider two diamond rings where the first ring has a diamond that is 0.1 carat heavier
than the diamond of the second ring. Then the fitted model predicts the price of the
first ring to be S$ 372,- higher than that of the second ring.
Next, let’s consider an approach that will provide a more natural interpretation of the fitted
intercept. Suppose avg_weight denotes the average weight of the diamonds in the dataset
and we specify the model as
Price = α0 + α1 (weight − avg_weight) + ϵ. (1.2)
So instead of weight we are now using the signed difference between weight and the average
weight, that is the deviations from the mean, as our predictor variable. We can implement
this in R using a command of the form
model <- lm(response ~ I(predictor - mean(predictor)), data = dataset)
The I() tells R to evaluate the expression in brackets before using it as an explanatory
variable. We then apply the command
coef(model)
to obtain the estimated coefficients. Let’s do this for our diamond data:
slr.diamond2 <- lm(price ~ I(carat - mean(carat)), data = diamond)
coef(slr.diamond2)
12 CHAPTER 1. INTRODUCTION
(Intercept) I(carat - mean(carat))
500.0833 3721.0249
Note how the estimate of the intercept has changed but the estimate of the slope has
remained the same.2
So how do we interpret this new estimate of the intercept? The predictor variable is now the
difference between the weight of the diamond under consideration and the average weight
of diamonds. So this predictor is zero when we consider a ring with a diamond of average
weight, which, in this dataset, is 0.204 carat. This leads to the following interpretation of
the estimated intercept:
• The expected price of a Solitaire ring with a diamond of average weight, that is a 0.204
carat diamond, is approximately S$ 500,-.
So here the estimated intercept represents the predicted price of a Solitaire ring with a
diamond of average weight rather than the predicted price of a ring with a diamond of
zero weight. I hope you agree that this interpretation makes much more sense in the given
application context. Note, however, that even in this new approach the predicted price of a
Solitaire ring with a diamond that has a very small weight is negative. But then considering
a Solitaire ring whose diamond has close to zero weight does not make any practical sense.
Moreover, it also means that we are extrapolating substantially beyond the range of the
data. All rings in the dataset have a diamond with a positive weight of at least 0.12 carat.
And we should not assume that the relationship described by the fitted model extends too
far beyond the range of the data.
An interesting property to note in the above approach is that if we compute the sample
mean of the prices of Solitaire rings in the dataset, then we find that this is equal to the
estimated intercept of S$ 500.08.3
Figure 1.3 shows two scatterplots of the diamond data. The left plot depicts the price of
each ring against the weight of its diamond. Added to this is the fitted regression line from
the original model formulation (1.1). The right plot shows the price of each ring against the
difference between the weight of its diamond and the average diamond weight. Added to
this is the line of best fit based on (1.2) which used the weight deviations from the mean as
the predictor variable.
We observe that the fitted regression lines are identical in relation to the data. What has
changed is simply that the coordinate system has shifted horizontally. This illustrates that
the predictions from the two fitted lines will be identical. Statistically we regard these as
2We will learn later in the module why this is the case, but you might be able to figure this out yourself!
Or have a look at Exercise 1 in the Exercise Booklet!
3This is related to the fact that in simple linear regression, the fitted line goes through the point whose
x-coordinate is the sample mean of the explanatory variable and whose y-coordinate is the sample mean of
the response variable.
1.1. SIMPLE LINEAR REGRESSION 13
Figure 1.3: Scatterplots of the diamond data. The plot on the left illustrates the fitted
model based on (1.1), while the plot on the right illustrates the fitted model based on (1.2).
the same model, just using a different parameterisation. A more formal definition of
re-parameterisations like this will be given in later on in the module.
So far we have covered material that most of you are already familiar with. However, the
above discussion exemplifies a skill that you need to have or to develop in order to to do
well in this module. And that is, you will need to be able to move confidently between the
mathematical description of a model, its implementation in R and its interpretation within
the application context.
Before we move on, below are some exercises to help you assimilate and practice the material
discussed so far. The solutions to the exercises can be found in the ST231 Exercise Booklet
on moodle.
Exercise 1: Derive the mathematical relationship between the parameters in the model
formulation in (1.1) and the one in (1.2).
Exercise 2: Use R to show numerically that the fitted model based on (1.2) leads to the
same predictions as the fitted model based on (1.1). We say that the model in (1.2) is a
reparameterisation of the model in (1.1).
Exercise 3: In the diamond example we noted that both fitted models were predicting
negative prices for Solitaire rings with very small diamonds. One way of avoiding this is to
fix the intercept to be equal to zero and thus fit the model
Price = β1 weight + ϵ.
This approach is known as regression through the origin. The model can be fitted in R
using the command lm(price ~ 0 + carat, data = diamond). Fit a regression through
the origin to the diamond data. Then compare the fit of the resulting model to the fit of the
model in (1.1) which does not restrict the intercept to be equal to zero.
14 CHAPTER 1. INTRODUCTION
1.2 Terminology and notation
A simple linear regression model describes the relationship between an outcome variable
Y and an explanatory variable X. Here the variables X and Y take on different roles. We
assume that X is useful in “explaining” the variation in Y and that Y is affected by and
thus “responds” to X. Therefore we call X the explanatory variable and Y the response
variable. But this terminology is not standardised and you may find alternative terms in the
various textbooks, journal papers and the web. Below is a list of terminology commonly
used in the context of linear models.
Y X
response variable explanatory variable
outcome variable input variable
predictor variable
regressor variable
dependent variable independent variable
In my notes and lectures I may use any of the above terms, but I will not be using the last
pair, that is, dependent and independent variables. This is because I find this particular
terminology unfortunate as the term “independent” in this context is not meant to refer to
“statistical independence” but might be misinterpreted as such.
When we fit a simple linear regression model we use data consisting of a set of n paired
observations
(x1, y1), (x2, y2), . . . , (xn, yn).
Thus there are n units of observation or cases, each yielding a pair of measurements. Here
xj denotes the measurement of the explanatory variable X for the jth unit of observation.
Similarly, yj denotes the measurement of the response variable Y for the jth unit of
observation. In our Solitaire ring example, yj was the price of the jth ring and xj the weight
of its diamond. Thus the units of observation were the individual rings.
Note that we distinguish between
• variables (or variates) which are those features in the system of interest that are
quantifiable, and
• data which are the realised values we obtain when variates are measured on specific
units of observation.
This is analogous to the distinction made in your probability module between a random
variable and its realisation. While it is important to draw such distinctions, be aware that
applied statistics has a tradition of being rather less consistent in terminology and notation
than its older sibling mathematics. This isn’t really a problem so long as we fully understand
the theory underneath. However, at an undergraduate level, while you are still learning how
1.3. FROM SIMPLE TO MULTIPLE REGRESSION 15
statistics really works, you should aim to maintain precision of language and notation as
much as possible.
Coming back to the discussion on the variables used in a linear model, note that, in contrast
to the response variable Y , we treat any explanatory variables as fixed rather than random.
So X is not a random variable. If the data come from a designed experiment, then the values
of the explanatory variables are typically chosen by the experimenter. Or alternatively, in
observational studies, the data might be simply measured/observed at the same time as the
response variable. However, in either case, we will consider the explanatory variables as
non-random and develop the model conditional on the values taken by the explanatory
variables.
Let me also take the opportunity here to highlight a notational issue that we will encounter
in the next section. In probability theory we use capital letters to denote random variables
to distinguish them from their realisations for which we use small letters. In linear algebra
we use small letters to denote vectors and capital letters to denote matrices. In this module
we will be using both random variables and their realisations as well as (random) vectors and
also matrices. So you will need to be attentive to which objects are random and which are
not, or which are scalars and which are vectors or matrices. In these notes and the lecture
slides the use of a boldface notation indicates a vector or matrix object. In handwritten
work presented in the lectures I will use an underline to indicate vectors (but please forgive
me if I occasionally forget).
1.3 From simple to multiple regression
Let’s consider another example, this time a simulated dataset on the sales of a certain
product across n = 96 retail stores.4 For each of the 96 retail stores the data comprise
information for a fixed period of time on
• sales - the number of units of the product sold (in thousands);
• price - the price at which the product was sold (in £);
• advert - the local advertising budget spent on promoting the product (in £1,000’s);
and
• brand - the brand of the retail store.
Let us first consider the relationship between the response variable sales, that is the number
of units sold, and the explanatory variable price, that is the price charged per unit. Figure
1.4 below shows the scatterplot of sales against price together with the line of best fit.
4As an artificial dataset it may not be entirely realistic, but it will serve well to illustrate various points.
16 CHAPTER 1. INTRODUCTION
Figure 1.4: Scatterplot of sales against price. The line of best fit has been added to the plot.
We observe that there is a reasonably linear relationship between sales and price, albeit a
weaker one than in the diamond ring example. To describe the relationship between the
sales volume and the price charged per unit we may use the model
Sales = β0 + β1 price + ϵ.
Reasonable values for β0 and β1 are found by determining the line of best fit. Figure 1.5
provides an illustration of how this line of best fit is computed. (We will formalise this
later, for now we are just trying to develop some intuitive understanding of the approach.)
For each datapoint I have added a red vertical line from the datapoint to the straight line,
illustrating the residuals. A residual is the difference between an observation and the point
on the line that has the same x-coordinate as the observation. The line of best fit is found by
minimising the sum of squared residuals, that is by minimising the sum of squared lengths
of the red vertical lines.
Figure 1.5: Scatterplot of sales against price together with the line of best fit. The red lines
illustrate the residuals.
1.3. FROM SIMPLE TO MULTIPLE REGRESSION 17
The coefficients of the fitted model are computed using the following code. (I used the
round() command to round the figures to 3 decimal points.)
slr.model <- lm(sales ~ price, data = Retail)
round(coef(slr.model), 3)
(Intercept) price
148.461 -0.225
From this output we deduce that with every additional pound charged for the product,
the fitted model predicts the number of units sold to reduce by 0.225 × 1, 000 = 225 units.
(Recall that sales is measured in thousands.) As mentioned earlier, the dataset contains
observations of more than just sales and price, so how can we extend the model to consider
other variables that might affect the sales volume? For example, the amount spent on local
advertising for the product (advert) may also have an influence on the sales volume. Plotting
sales against advert in Figure 1.6 we observe a linear relationship.
Figure 1.6: Scatterplot of the sales volume against the local advertising budget. The line of
best fit has been added to the plot.
As a simple extension of the previous model we might consider the following:
Sales = β0 + β1 price + β2 advert + ϵ.
Here the systematic component is given by
β0 + β1 price + β2 advert.
This is no longer a straight line in R
2 as in simple linear regression but describes a plane in
R
3
.
Let’s consider a graphical illustration. If you view this document in its html version, it shows
an interactive 3D graphical representation of the data and fitted model. In this graphic
18 CHAPTER 1. INTRODUCTION
the datapoints are displayed as yellow spheres in a 3D coordinate system with price and
advert on the x- and y-axis, respectively, and sales on the z-axis. The fitted regression
plane, that is the plane of best fit, is displayed as a lilac plane. The signed distances along
the z-axis between the datapoints and the plane of best fit, and thus is the residuals, are
illustrated as blue or pink lines, depending on whether the residual is positive or negative.
The plane of best fit was determined by minimising the sum of squared residuals. Use the
mouse to rotate the figure to see the plane at different angles.
The pdf format does not support interactive graphics, so if you are viewing this document in
pdf format, it shows a static graphic illustrating the regression plane in grey. The datapoints
are depicted as spheres with the ones that lie above the plane shown in black and those
below the plane in grey.
The plane of best fit is found by so-called least squares estimation which minimises
the sum of squared distances along the z-axis between the observed datapoints and the
corresponding plane.
Figure 1.7: 3D scatterplot of the Retail data including the plane of best fit describing
the relationship between the response (sales) and the two explanatory variables (price and
advert).
Earlier we discussed two simple linear regression examples: the diamond ring example and
a model for the Retail data which used price as the only explanatory variable. In these
examples the data used to fit the model consisted of paired observations of the response
variable and the explanatory variable. Thus the observed data can be thought of as points in
R
2 which we then summarised using a straight line. But now we consider using both price
and advert as explanatory variables. So here the observations consist of values for three
variables, that is the sales volume (sales), the price (price) and the advertising budget
(advert). Hence they are points in R
3 which we then summarise using a plane.
To determine the plane of best fit in R we use a command of the form
1.3. FROM SIMPLE TO MULTIPLE REGRESSION 19
model <- lm(response ~ predictor1 + predictor2, data = dataset)
Then, as before, the command coef(model) gives us the the estimated values for the
parameters β0, β1 and β2. The R code is given below.
mlr.model <- lm(sales ~ price + advert, data = Retail)
round(coef(mlr.model), 3)
(Intercept) price advert
157.465 -0.404 0.544
Thus the systematic component of the fitted model is given by
157.465 − 0.404 price + 0.544 advert.
Generally, we interpret the coefficient of a quantitative explanatory variable as the average
effect on the response when the value of the explanatory variable is increased by one unit.
To do this for models with more than one predictor variable we need to assume that the
values of the other predictor variables remain fixed. (As we will see in some later examples,
we cannot always make this assumption.) The intercept is interpreted analogously to the
case of simple linear regression: we set the values of all explanatory variables equal to zero
(provided this is sensible in the application context and the range of observations for the
explanatory variables covers the origin.)
So if a model is given by the equation
Y = β0 + β1X1 + β2X2 + ϵ
we may use the following interpretation (provided this is sensible in the given context).
• Intercept β0: If X1 and X2 take value zero then, on average, we expect the value of Y
to be equal to β0.
• Coefficient β1: For a given value of X2, an increase in X1 by one unit is associated
with an average change in Y of β1 units.
• Coefficient β2: Assuming the value of X1 is held constant, with a unit increase in X2
we predict a change in Y of β2 units.
Coming back to the sales example we might give the following interpretation (keeping careful
track of the units of measurements as sales and advert were measured in thousands.)
• price: Assuming a fixed spend on local advertising, we predict the sales volume to
reduce on average by 404 units for every £1,- increase in price.
• advert: For a given price of the product, an increase in the local advertising budget
by £1000,- is associated with an increase in sales by 544 units.
20 CHAPTER 1. INTRODUCTION
Our first model for the Retail data was a simple linear regression using price as the
only predictor variable. The estimated slope coefficient in the simple linear regression was
computed as -0.225, and so is different from the coefficient of price in the multiple regression
model above. In the simple linear regression the computed coefficient for price estimates
the expected change of sales regardless of (or ignoring) the effect of local advertising. In
the multiple linear regression the computed coefficient for price estimates the expected
change of sales adjusting for or controlling for the influence of local advertising on the
sales volume. To indicate that we have made adjustments for other variables, we refer to the
coefficients of the explanatory variables in a multiple regression model as partial regression
coefficients.
I have not given an interpretation for the intercept. This is because, in the dataset, price
takes values that are considerably larger than zero and so the usual interpretation of the
intercept as the average response value when the explanatory variables take value zero
requires an extrapolation. (Also, it may not make much sense within the application context
to charge £0,- for a product.)
Hopefully the above illustrates, in principle, how we might further extend the approach
to models with several explanatory variables. Linear regression with several quantitative
explanatory variables is referred to as multiple (linear) regression. To formalise the
approach we need to be able to represent objects mathematically in an n-dimensional space.
The mathematical tools needed for this are provided by linear algebra, namely vectors and
matrices. We will discuss this in the next section, but, for now, here is an exercise for you
to practice on.
Exercise 4 Reparameterise the multiple regression model for the Retail data such that
the intercept can be given an interpretation without the need for extrapolation. Fit the
model and then compare the estimated slope coefficients with the estimates for the original
parameterisation.
1.4 The class of linear models
In simple linear regression we use data consisting of a set of n paired observations
(x1, y1), (x2, y2), . . . , (xn, yn)
where, for the jth unit of observation, xj denotes the observed value of the variable X and
yj the observed value of the variable Y .
We assume that the distribution of the random variable Y depends on the value of the
variable X. The variable X is assumed to be non-random. For j = 1, . . . , n let Yj be an
independent copy of the random variable Y conditional on the value xj for the variable X.
A simple linear regression model is then given by
Yj = β0 + β1xj + ϵj , j = 1, . . . , n (1.3)
1.4. THE CLASS OF LINEAR MODELS 21
where the ϵj ’s, the so called errors, are normally distributed random variables with mean
zero and variance σ
2 and are independent of each other (and of the predictor variable X).
Here the systematic component β0 + β1xj determines the mean of Yj which thus depends
on xj . The distribution of Yj around this mean is governed by the distribution of ϵj . In
particular, the variance of Yj is equal to the variance of ϵj and thus equal to σ
2
.
The simple linear regression model thus describes how the mean of Y changes as the value
of X changes. This is determined by the systematic component. Formally,
E (Y | X = x) = β0 + β1x. (1.4)
Coming back to the set of equations in (1.3) let’s consider a more concise formulation. We
can re-express the n equations in (1.3) using vector and matrix notation as
Y = X β + ϵ. (1.5)
Here Y is the column vector
Y =



Y1
.
.
.
Yn


 ,
X is the n × 2 matrix
X =



1 x1
.
.
.
.
.
.
1 xn


 ,
and β and ϵ are the column vectors
β =
"
β0
β1
#
and ϵ =



ϵ1
.
.
.
ϵn


 .
Note that in the above we used boldface for the vector and matrix objects. In particular we
used it to distinguish the matrix X from the variable X. The matrix X is referred to as
the design matrix, or sometimes model matrix. We assume that ϵ ∼ Nn(0, σ2In), that
is, the vector of errors has a multivariate normal distribution of dimension n, with mean
equal to the n-vector of zeros and variance-covariance matrix equal to σ
2In where In is the
n × n identity matrix.
22 CHAPTER 1. INTRODUCTION
More generally, in multiple regression with k quantitative explanatory variables X1, . . . , Xk
we consider data of the form
response variable | predictor variables
y1 | x11 x12 · · · x1k
y2 | x21 x22 · · · x2k
.
.
. |
.
.
.
.
.
.
.
.
.
.
.
.
yn | xn1 xn2 · · · xnk
Similar to simple linear regression, for the jth unit of observation we have
• yj is the observed response value, while
• xjm is the value of the mth explanatory variable Xm.
A multiple regression model (with an intercept term) assumes that for j = 1, . . . , n
Yj = β0 + β1xj1 + . . . + βkxjk + ϵj , (1.6)
where the ϵj ’s are iid (independent and identically distributed) N (0, σ2
) random variables.
Thus the above yields the set of n equations
Y1 = β0 + β1x11 + . . . + βkx1k + ϵ1
Y2 = β0 + β1x21 + . . . + βkx2k + ϵ2
.
.
. =
.
.
.
Yn = β0 + β1xn1 + . . . + βkxnk + ϵn
which can be written more concisely as
Y = X β + ϵ (1.7)
with Y , β and ϵ defined as the column vectors
Y =



Y1
.
.
.
Yn


 , β =



β0
.
.
.
βk



and ϵ =



ϵ1
.
.
.
ϵn


 ,
and ϵ ∼ Nn(0, σ2In). Furthermore, X is the n × (k + 1) matrix
X =



1 x11 x12 · · · x1k
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 xn1 xn2 · · · xnk


 .
1.4. THE CLASS OF LINEAR MODELS 23
Analogous to the special case of simple linear regression, a linear model with k quantitative
explanatory variables X1, X2, . . . , Xk specifies the expectation of the response variable Y
given that X1, X2, . . . , Xk take values x1, x2, . . . , xk respectively as
E (Y | X1 = x1, . . . , Xk = xk) = β0 + β1x1 + . . . + βkxk. (1.8)
Thus the mean of the response is a function of the values of the explanatory variables and
is commonly referred to as the mean function of the response. Note that the expression
given by β0 + β1x1 + . . . + βkxk is linear in the parameters β0, β1, . . . , βk and commonly
referred to as the linear predictor.
The variance of Y given the values of X1, X2, . . . , Xk, that is the variance function of the
response, is given by
Var (Y | X1 = x1, . . . , Xk = xk) = Var (ϵ) = σ
2
(1.9)
and thus constant.
Consider again the response variables for the n units of observation, that is Y1, . . . , Yn. Given
the values of the explanatory variables, the independence of the errors ϵ1, . . . , ϵn carries over
to the responses and so Y1, . . . , Yn are independent. Furthermore, as a linear combination of
constants and normally distributed errors, Y1, . . . , Yn also have a normal distribution. Thus
using vector/matrix notation we may deduce that
Y ∼ Nn(Xβ, σ2
In),
E (Y ) = E (Y | X) = Xβ,
Var (Y ) = Var (Y | X) = σ
2
In
where In is the n × n identity matrix. (Here we use the convention that if Z is a random
vector, then Var(Z) denotes the variance-covariance matrix of Z.)
Some important comments on notation
In the above I have opted to be more explicit with notation than many textbooks are. For
example, in the above I have used notation that makes explicit the assumption of dependence
on the values of the explanatory variables, that is E(Y | X) and Var(Y | X). (Recall
that the design matrix X contains the values of the explanatory variables.) However, it
is common practice in the literature to use this dependence as a tacit assumption and so
simplify notation to E(Y ) and Var(Y ). I will adopt this practice going forwards.
I have also opted to use capital letters for the variables and small letters for their observed
values, that is the data. In contrast, it is common practice in most textbooks to use small
letters for the response variables associated with specific units of observation. So you will
find the notation y1, . . . yn being used for both the random response variables and their
observations. In contrast I use Y1, . . . Yn for the response variables and y1, . . . yn for their
observed values as this aligns with the notation used in other modules.
24 CHAPTER 1. INTRODUCTION
Exercise 5 Suppose for j = 1, . . . , n we have
Yj = β0 + β1xj1 + β2xj2 + · · · + βkxjk + ϵj ,
where ϵ1, . . . , ϵn are iid and ϵj ∼ N (0, σ2
). Let Y = X β + ϵ be the matrix form of the
above set of equations. Show that
E (Y ) = X β and Var (Y ) = σ
2
In,
where In is the n × n identity matrix.
1.5 Summary of Chapter 1
Chapter 1 introduced the class of normal linear models defined as
Y = Xβ + ϵ,
where ϵ ∼ Nn(0, σ2In). Here Y is the vector of response variables, X is the design matrix,
β is the parameter vector and ϵ is the vector of errors. The vector β is of dimension p and
the vectors Y and ϵ are of dimension n, where n is the number of units of observation and
n ≥ p. Finally, X is an n × p matrix and In is the n × n identity matrix.
For example, for a multiple linear regression model with k explanatory variables and an
intercept term, the design matrix is given by the n × (k + 1) matrix
X =



1 x11 x12 · · · x1k
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 xn1 xn2 · · · xnk


 .
which for j = 1, . . . , n produces the model equation of the form
Yj = β0 + β1xj1 + . . . + βkxjk + ϵj
where xjk is the observation of the kth explanatory variable for the jth unit of observation.
We assume that ϵ ∼ Nn(0, σ2In), thus Y ∼ Nn(Xβ, σ2In), and, in particular, the mean
function of the response random vector is given by
E(Y ) = Xβ.
Hence the mean function of the response vector Y depends on the values of the explanatory
variables as given by the design matrix X. The variance-covariance matrix of the response
vector is given by
Var(Y ) = σ
2
In
and so does not depend on the values of the explanatory variables.
1.5. SUMMARY OF CHAPTER 1 25
After working through this chapter you should
• understand how a normal linear model is defined via a systematic component and a
random error;
• be aware of the assumptions made for the random error and the implications of these
assumptions for the distributional properties of the response random vector;
• be able to describe a model both in vector/matrix notation and in terms of its model
equation for the jth unit of observation;
• be able to interpret the estimated coefficients of a model.
Below is a textbook recommendation if you wish to engage in further (optional!) reading on
the topics covered in this chapter:
• Generalized linear models with examples in R (2018) by Dunn and Smyth: Section 1.5
Chapter 2
Linearity
We will start this chapter by illustrating the importance of graphically exploring the data
and the fitted models. This helps to identify whether the systematic component of our linear
model is an adequate description of the relationship within the data. So far we have only
considered systematic components which are linear functions of the explanatory variables.
This is a rather restrictive class of models that is limited in what types of relationship can
be described. Fortunately, we can still use least squares estimation to fit models even when
the systematic component is a more general function of the explanatory variables provided
it is still linear in the parameters. In other words, using least squares estimation we can
fit models where the systematic component can be written as the product between a design
matrix and a parameter vector. In the second half of this chapter we will discuss examples
of these more general models.
2.1 Anscombe’s Quartet
Even though we may be able to fit a linear model numerically, this does not necessarily
mean that the fitted model is an adequate description of the data.
In 1973 Francis J. Anscombe1
constructed four datasets, available as anscombe in R, that
have the same summary statistics and corresponding regression line, but very different
scatterplots.
The aim was to illustrate the importance of graphical explorations of the data as part of
formal statistical analysis and modelling.
Below are the summary statistics of the four datasets consisting of 11 paired observations
of the variables X and Y together with the fitted regression line. As a little experiment,
sketch what you think the scatterplot of the data might look like.
1Anscombe, Francis J. (1973). Graphs in statistical analysis. The American Statistician, 27, 17–21. doi:
10.2307/2682899.
26
2.1. ANSCOMBE’S QUARTET 27
Number of observations 11
Sample mean of x 9
Sample variance of x 11
Sample mean of y 7.5
Sample variance of y 4.12
Correlation coefficient of x and y 0.816
Fitted regression line y = 3 + 0.50x
Next we consider the four datasets in turn.
2.1.1 Dataset 1
If asked to sketch a scatterplot of the data with the above summary statistics, most people
probably sketch something like the scatterplot of this first dataset in Figure 2.1. The x-values
are evenly spread around their mean and the y-values are scattered randomly about the
regression line.
Figure 2.1: Scatterplot of Anscombe’s Dataset 1.
2.1.2 Dataset 2
In this dataset the x-values are again evenly spread, but this time the y-values make it clear
that we have chosen a poor model. A simple linear regression model doesn’t capture the
shape of the curved relationship that is visible in the data. Probably a parabola would be a
more adequate description.
28 CHAPTER 2. LINEARITY
Figure 2.2: Scatterplot of Anscombe’s Dataset 2.
2.1.3 Dataset 3
In the third example it looks like the data might have come from a simple linear regression
model – except for one outlier. Unfortunately this outlier has a strong influence on the fitted
line and, as such, has affected the fit to the rest of the data.
Figure 2.3: Scatterplot of Anscombe’s Dataset 3.
2.1.4 Dataset 4
In this dataset all but one of the x-values are the same. The slope of the line is entirely
dependent on the single unusual datapoint that has a different x-value from the rest. Does a
simple linear model even make sense for this dataset?
2.2. POLYNOMIAL REGRESSION 29
Figure 2.4: Scatterplot of Anscombe’s Dataset 4.
For another example of datasets which have the same summary statistics but drastically
different graphics, check out Alberto Cairo’s Datasaurus Dozen.
Exercise 6 As advocated by Anscombe, statistical modelling should be preceded by an
exploratory analysis. In Practical 1 you explored numerically the diamond dataset from
the package UsingR. Perform a graphical exploration of the dataset by using appropriate
graphical tools such as histograms and scatterplots.
2.2 Polynomial regression
So far we have only considered linear models where the systematic component was a linear
function of the explanatory variables. However, for many datasets, for example Dataset 2
in Anscombe’s quartet, this does not provide an adequate description of the relationship
between the response and the explanatory variable(s). Fortunately, linear models span a
much wider class of models as we will discuss next.
Let us consider again Anscombe’s Dataset 2. We noted earlier that the observations appear
to lie along a parabola. In fact we can determine the parabola of best fit using the linear
model framework. We assume for j = 1, . . . , 11:
Yj = β0 + β1xj + β2x
2
j + ϵj (2.1)
where xj is the value of the explanatory variable for the jth unit of observation. The errors
are assumed to be iid (independent and identically distributed) N (0, σ2
). Here we simply
added a quadratic term as an additional predictor term to the systematic component of the
model. This approach is referred to as quadratic regression.
We can use least squares estimation to determine the parameters β0, β1 and β2 such that
30 CHAPTER 2. LINEARITY
the sum of squared vertical distances between the observations and the parabola given by
β0 + β1x + β2x
2 are minimised. We simply need to define the appropriate design matrix.
The model equations in (2.1) expressed in matrix notation are given by






Y1
Y2
.
.
.
Y11






=






1 x1 x
2
1
1 x2 x
2
2
.
.
.
.
.
.
.
.
.
1 x11 x
2
11









β0
β1
β2


 +






ϵ1
ϵ2
.
.
.
ϵ11






.
Thus the design matrix is
X =






1 x1 x
2
1
1 x2 x
2
2
.
.
.
.
.
.
.
.
.
1 x11 x
2
11






To fit this model in R we use a command of the form
lm(response ~ predictor + I(predictorˆ2))
Here ˆ2 is R syntax for taking the square and I() is needed to make R aware that we want
to apply the square to the predictor.
plr.model <- lm(y ~ x + I(xˆ2))
round(coef(plr.model), 4)
(Intercept) x I(x^2)
-5.9957 2.7808 -0.1267
Figure 2.5: Scatterplot of Anscombe’s Dataset 2 with the parabola of best fit.
Figure 2.5 shows the scatterplot of the data together with the quadratic curve of best fit.
The figure illustrates how well the relationship between X and Y is captured by the curve.
2.2. POLYNOMIAL REGRESSION 31
Now that we have fitted the quadratic regression model, how do we interpret its estimated
coefficients? Previously we discussed how to interpret the coefficient of a predictor variable
in a multiple regression model, namely as the change in the expected response when the
predictor value is increased by one unit assuming all other explanatory variables remain
fixed. However, I also mentioned that this is not always sensible and a quadratic regression
model is an example where this does not work. If we increase the quadratic term, that is x
2
,
then we automatically also increase the linear term, that is x, and vice versa. More formally,
we can show that (see Exercise 7)
E(Y |X = x + 1) − E(Y |X = x) = β1 + β2 + 2β2x.
Hence the expected change in the response due to a unit change in the explanatory variable
is no longer constant, but depends on the current value x of the predictor variable.
Therefore we need to interpret the polynomial as a whole. When there is only one explanatory
variable, a plot such as the one in Figure 2.5 is usually helpful.2 We may also say something
like the following:
• The expected response has a quadratic relationship with the explanatory variable.
The systematic component of the fitted model (−5.9957 + 2.7808x − 0.1267x
2
) is
an upturned parabola that takes its maximum value of about 9.26 around x = 11.
Thus the relationship between the expected response and the explanatory variable is
increasing up to about x = 11, and decreasing thereafter.
In the above model the systematic component describing the relationship of the response
variable and the predictor variable is a polynomial of order 2 and thus a non-linear function.
You may wonder why we still count this as a linear model. This is because the “linear” in
the term “linear model” refers to linearity in the parameters. As long as the systematic
component is linear in the parameters, then we can still write the linear predictor for the
given dataset as Xβ, that is the design matrix times the parameter vector. And this means
we can still use the least squares estimation algorithm to produce reasonable estimates of
the parameters.
In the following section we will consider another example of a model with a systematic
component that is non-linear in the explanatory variable. But before we do this, let us briefly
introduce a reasonably straightforward extension from quadratic regression to polynomial
regression of higher order.
In polynomial regression of order q ≥ 2 we consider model equations of the form
Yj = β0 + β1xj + β2x
2
j + · · · + βqx
q
j + ϵj , j = 1, . . . , n,
where xj is the value of the explanatory variable for the jth unit of observation. As usual,
2You will learn in the computer practicals how to produce a plot like this!
32 CHAPTER 2. LINEARITY
the errors are assumed to be iid N (0, σ2
). This can be fitted with least squares estimation
using the design matrix
X =






1 x1 x
2
1
· · · x
q
1
1 x2 x
2
2
· · · x
q
2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 xn x
2
n
· · · x
q
n






.
Exercise 7 Consider a quadratic regression model given by
Yi = β0 + β1xi + β2x
2
i + ϵi for i = 1, . . . , n
where the errors are assumed to be iid N (0, σ2
). Show that
E(Y |X = x + 1) − E(Y |X = x) = β1 + β2 + 2β2x.
2.3 A linear model with a log-transformed predictor
The dataset ufcwc from the alr4 package3
comprises information on a sample of 139 Western
cedar trees. The data were collected at the University Idaho Experimental Forest at the
Upper Flat Creek stand. For each tree we have measurements of the following variables:
• Dbh - the diameter of the tree (in mm) measured 137 cm from the ground;
• Height - the height of the tree in decimetres (dm).
Figure 2.6 is a plot of the data with the line of best fit.
3Weisberg, S. (2014). Applied Linear Regression, 4th edition. New York: Wiley.
2.3. A LINEAR MODEL WITH A LOG-TRANSFORMED PREDICTOR 33
Figure 2.6: Scatterplot of height against diameter for the Western red cedar data together.
The plot also shows the line of best fit.
The regression line provides a reasonably good fit to the data, but if you study the plot
closely you might notice that more of the observations with intermediate diameter appear
to lie above than below the regression line. Even more pronounced is the fact that the
observations with a small or very large diameter tend to lie below the regression line. This
suggests that the relationship might be curved. In contrast to Anscombe’s Dataset 2 though,
it seems that the relationship is still monotonic, that is the larger the diameter, the taller
the tree tends to be.
Because the relationship resembles a logarithmic curve we might consider the model
Heightj = β0 + β1 log2
(diameterj ) + ϵj , j = 1, . . . , 139
with design matrix
X =






1 log2
(diameter1)
1 log2
(diameter2)
.
.
.
.
.
.
1 log2
(diameter139)






.
Here log2 denotes the logarithm of base 2. As usual, the errors are assumed to be iid
N (0, σ2
).
If we plot the observed heights against the log2
-diameters, as shown in Figure 2.7, we note
that the relationship between the response and the log-transformed explanatory variable
appears fairly linear. Thus fitting a simple linear regression to this data seems sensible.
In R we fit the model using a command of the form4
4Note that unlike in the quadratic regression example, here we did not need to use the inhibitor command
I(). This is because, when used in a model formula, the operators ‘+’, ‘-’, ’*’ and ‘ˆ’ are not treated as
34 CHAPTER 2. LINEARITY
Figure 2.7: Scatterplot of height against the base 2 log-diameter for the Western red cedar
data. Shown in grey is the line of best fit.
lm(response ~ log2(predictor))
This yields the following estimated coefficients.
tree.model <- lm(Height ~ log2(Dbh), data=ufcwc)
round(coef(tree.model), 2)
(Intercept) log2(Dbh)
-463.31 82.84
You may wonder why we have chosen the base 2 logarithm rather than the natural logarithm.
This is because it makes the interpretation of the coefficients easier. Recall that log2
(x)+ 1 =
log2
(2x). Thus an increase by one unit in the log2
-diameter corresponds to taking the base
2 logarithm of twice the diameter.5 The estimated slope coefficient is close to 83 and so we
can interpret this estimate as follows:
• A doubling of the diameter of a Western red cedar tree is associated with an increase
in its height of around 83 decimeters on average.
Figure 2.8 provides a visual comparison between the model using the original explanatory
variable and the one using the log-transformed explanatory variable. We see that the latter
appears to fit the data better.
arithmetic operators but have a different role. To tell R to treat them as arithmetic operators we use the
inhibitor function I().
5Another common choice in stastical practice is the logarithm with base 10 where for interpretation we
consider a ten-fold increase in the predictor variable.
2.4. A STRATEGY FOR STATISTICAL MODELLING 35
Figure 2.8: Scatterplot of height against the diameter for the Western red cedar data. Added
is the line of best fit and the logarithmic curve of best fit.
Exercise 8 Suppose we have observations of the response variable Y and two explanatory
variables X1 and X2 for n units of observation. Let the parameter vector be given by
β = (β0, β1, β2)
T
. Which of the following define a linear model?
For i = 1, . . . , n
1. Yi = β1xi,1 + ϵi
,
2. Yi = β0 + β1 exp(xi,1) + ϵi
,
3. Yi = β0 + exp(β1xi,1) + ϵi
,
4. Yi = β0 + β1(xi,1 − xi,2)
2 + ϵi
,
5. Yi = β0 + 2β1xi,1 + β2 log(xi,2) + ϵi
.
2.4 A strategy for statistical modelling
Now that we have a little more experience in statistical modelling, it seems an opportune
moment to discuss a general modelling strategy.
As we have already seen, for a given dataset there is a number of different models we might
consider. Generally (and somewhat idealised), statistical modelling proceeds as follows.
1. Explore the data and the application context. This is to get an understanding of the
data and its features.
2. Propose a model for the data, that is a parametric formula linking the response variable
with the explanatory variable, recognising the stochastic nature of the response. Or,
more simply expressed, we specify a systematic component and a random error.
36 CHAPTER 2. LINEARITY
3. Fit the model (that is find the best set of values for the parameters).
4. Ask “Is the model adequate?” For example, is the model consistent with the data?
Is the systematic component correctly specified? Does the model allow the main
questions of the analysis to be answered?
5. Fit other plausible models, compare them and choose the most appropriate.
6. Use the chosen model to answer the questions of interest.
The above is an idealised version and often, in real life settings, we may have to cycle through
the various steps repeatedly.
Also keep in mind the quote by the eminent statistician George E. Box who said: “Essentially
all models are wrong, but some are useful.” For more on George Box, read this paper by
Wasserstein6 which was published in Significance, a journal of the Royal Statistical Society.
2.5 What makes a good model?
When evaluating a model we should keep in mind its purpose. The aim of a model may be
one or more of the following.
• Description: Here the model is used to summarise the data and thus reduce dimensionality. For example, Gapminder provides data and visualisations on various
economic indices that allow comparisons between countries. To make the comparisons
meaningful it is important that such indices are computed consistently, so a key
consideration is comparability.
• Prediction: Here the aim is to use the model for forecasting or prediction. In this
case, a key consideration is prediction accuracy and the cost of wrong predictions.
• Explanation: Here the emphasis is to shed light on the relationship between the
predictor variables and the response and so the focus is on parameter estimation and
interpretation.
Whether or not a model is adequate (for description, estimation or prediction) is partially a
matter of subjective judgement. But two criteria that should be considered are parsimony
and plausibility.
2.5.1 Plausibility
In general, the most plausible model is the one that makes most sense scientifically. However,
in practice a simpler model may be better at prediction than the “true” model, particularly if
we consider only a specific range of input values. Plausibility is therefore usually interpreted
as “does the model make practical sense?”
6Wasserstein, R. (2010). George Box: A model statistician. Significance, 7(3), 134-135.
2.6. SUMMARY OF CHAPTER 2 37
A good example of this is the acceleration due to gravity. If you are modelling the path of a
ball falling in a room, it is safe enough to assume that the acceleration due to gravity is the
same everywhere in the room. However, once you start experimenting with objects in space,
you will quickly find that this relationship is only any good locally.
2.5.2 Parsimony
Parsimony, in general scientific practice, directs us to choose the simplest explanation. This
is often referred to as Ockham’s razor.
In statistics, the most parsimonious model is the simplest one that still adequately fits the
data. In practice this means the model with the fewest parameters that still captures the
essential features of the data. If a model is unnecessarily complex, the precision of estimation
and prediction decreases. There are also other practical advantages to a less complex model
such as saving costs in data collection and ease of interpretation.
2.6 Summary of Chapter 2
The systematic component of a linear model is linear in the parameters but not necessarily
a linear function of the explanatory variables. It may include polynomials of the explanatory
variables (which is referred to as polynomial regression) or other non-linear functions such
as the logarithm.
After studying this chapter you should
• be aware that linear models are linear in the parameters but not necessarily in the
explanatory variables;
• be able to state the model equation and design matrix for a polynomial regression
model;
• be aware of linear models using transformed explanatory variables;
• be aware of a general model development strategy and the guiding principles of
plausibility and parsimony.
If you wish to engage in further (optional!) reading on the topics covered in this chapter,
then the following section of one of the textbooks recommended for this module covers
related material.
• A Modern Approach to Regression with R (2009) by Sheather: Chapter 5 pg 140 -
143.
Chapter 3
Residual analysis
When we defined normal linear models we made a number of model assumptions. After
fitting such a model, we therefore need to check that these assumptions are adequately met.
In this chapter we consider the model assumptions in more detail. We discuss how to use
residual plots and Q-Q plots to assess whether the assumptions are reasonable in a given
context. We will also discuss what we might do if we identify violations of one or more of
the assumptions.
3.1 Definitions
We start by defining relevant modelling concepts and terminology. Suppose we have n units
of observation. Consider the normal linear model with p ≤ n parameters given by
Y = X β + ϵ. (3.1)
where X is the n × p design matrix and Y , β and ϵ are the column vectors
Y =



Y1
.
.
.
Yn


 , β =



β1
.
.
.
βp



and ϵ =



ϵ1
.
.
.
ϵn


 .
3.1.1 Estimates
The parameters β1, . . . , βp are assumed unknown but fixed. In the examples considered in
the previous chapter we used R to determine reasonable values for these parameters. We
called these reasonable values estimates and denoted them using hats, that is βˆ
1, . . . , βˆ
p,
to distinguish the estimates from the unknown parameters. We can extend this notation to
38
3.1. DEFINITIONS 39
the parameter vector β by writing βb, that is
βb =




bβ1
.
.
.
bβp




denotes the estimate of the parameter vector β. In R the command coef(model) returns
the estimate of the parameter vector for the linear model object model.
3.1.2 Fitted values
Once we have derived estimates of the parameters, we can use them to compute estimates
of the expected response for given values of the predictor variables. For example, for the
diamond ring data we computed an estimate of the expected price for a Solitaire ring with a
diamond of average weight. More generally, we can compute an estimate of the expected
response for the jth unit of observation as
ybj = x
T
j βb
where x
T
j
is the jth row of the design matrix X. Here ybj is called the fitted value for the
jth unit of observation. The vector of fitted values is then given by
yb =



yb1
.
.
.
ybn


 = Xβb.
As an example consider simple linear regression. Suppose the jth observation of the
explanatory variable is zj , then we have
x
T
j = [1, zj ] and βb =
"
βˆ
0
βˆ
1
#
and so
ybj = bβ0 + bβ1zj = [1, zj ]
"
βˆ
0
βˆ
1
#
= x
T
j βb.
In R the command fitted(model) returns the fitted values of a model.
40 CHAPTER 3. RESIDUAL ANALYSIS
3.1.3 Residuals
Next, we define the jth residual as the difference between the jth observed response value
and its fitted value, that is
ϵbj = yj − ybj = yj − x
T
j βb.
Recall that as Y = Xβ + ϵ, we have Yj = x
T
j β + ϵj and so
ϵj = Yj − x
T
j β,
where j = 1, . . . , n. Note the similarity between the last expression for the jth error ϵj and
the previous definition of the jth residual ϵbj . We can therefore view the jth residual as an
estimate of the error ϵj which is why we use the notation ϵbj .
In R we can obtain the residuals using the command residuals(model).
3.1.4 Deviance or residual sum of squares (ResidSS)
Recall Figure 1.5 where we used lines to indicate the vertical distances between the response
observations and their fitted values. The signed vertical distances correspond to the residuals
we introduced above. For a well-fitting model we would like these distances to be small. More
precisely, we want to choose parameter estimates such that the sum of squared distances is
as small as possible. This sum of squared residuals given by
D =
Xn
i=1
(yi − ybi)
2 =
Xn
i=1
ϵb
2
i
.
is called the model deviance or the residual sum of squares (ResidSS).
In the next section we will formally derive how to compute the estimate for the parameter
vector β that minimises the residual sum of squares.
Exercise 9:
The fitted simple regression line for the dataset below is given by yˆ = 0.34 + 1.52z. Calculate
the fitted values, the residuals and the deviance for this model.
zi
| 1 2 2 3 8
yi
| 1 2 4 7 12
3.2 Model assumptions
As stated at the outset, linear models are concerned with the relationship between a single,
quantitative response variable and one or more explanatory variables. The response variable
3.3. RESIDUAL PLOTS 41
is assumed to be measured on a continuous scale.
1 Note that there is no such restriction
for the explanatory variables.
A normal linear model Y = Xβ + ϵ makes the following assumptions.
• Linearity: We assume that the linear predictor Xβ adequately describes the mean
function of the response variable and thus captures the systematic relationship between
the explanatory variables and the response variable. Note that this also implies that
the additive errors have a mean of zero.
• Independence: The errors ϵ1, . . . , ϵn are assumed to be independent of each other
and, hence, so are the response variables Y1, . . . , Yn.
• Homoscedasticity or constant variance: We assume that the errors ϵ1, . . . , ϵn
have constant variance (and, as a result, so do the response variables Y1, . . . , Yn). A
violation of the homoscedasticity assumption is referred to as heteroscedasticity.
• Normality: We assume that the errors have a normal distribution, and, as a result,
so do the response variables.
Note With the above assumptions, ϵ1, . . . , ϵn are independent and identially distributed.
However, while the response random variables are independent, they are not identically
distributed as their means depend on the values taken by the explanatory variables!
The data collection process should aim to ensure that the errors are independent and so
consideration of this process gives clues about potential violations of the assumption of
independence. For example, if the data are collected sequentially, then this may lead to
errors that are correlated in time. Similar issues arise with spatial data, where nearby
observations might be correlated.
As we do not observe the errors per se, we instead examine the residuals as estimates of the
errors. Using residual plots, that is scatterplots of the residuals against the fitted values
and, in the case of multiple regression, also scatterplots of the residuals against each of the
explanatory variables, we look for signs of violations of linearity or homoscedasticity. In
addition we use normal Q-Q-plots to assess the normality assumption.
To gain experience in assessing residual plots, let us look at some examples.
3.3 Residual plots
The command plot(model, which = 1) in R produces a scatterplot of the residuals of the
fitted model against its fitted values.
1Occasionally we may use linear models for a discrete response, provided the response variable is not “too
discrete”.
42 CHAPTER 3. RESIDUAL ANALYSIS
3.3.1 A null plot
First let us consider what such a residual plot should like like when the model assumptions
are satisfied. The residuals should show no distinct patterns, but instead should randomly
scatter around a horizontal line. Furthermore, the should scatter with a constant variability
as we move along the horizontal axis. A residual plot that does not show evidence of
violations of the model assumptions is called a null plot.
Let’s have a look at some example residual plots of some simple linear regression models.
The figures in this and the next sections show two plots: on the left, a scatterplot of the
response variable against the explanatory variable and, on the right, a residual plot with the
residuals plotted against the fitted values. Note that in multiple regression it is recommended
to also consider plots of the residuals against the individual explanatory variables. To help
evaluate the plots, we have added the fitted regression line to the scatterplot of the data
and we added a smoother to the residual plot. The smoother is a non-parametric estimate
of the mean of the residuals as a function of the fitted values. If the model assumptions
hold, then this should resemble a horizontal line at zero.
The first example, illustrated in Figure 3.1, shows plots of a well-fitting model. In the
scatterplot on the left, the observations are scattered relatively evenly on either side of the
regression line which thus appears to be an adequate description of the relationship between
the explanatory variable and the response. In the residual plot to the right, the smoothing
curve resembles a horizontal line and the variation of the residuals appears relatively stable
as we move along the horizontal axis.
Figure 3.1: A well-fitting model: to the left is the scatterplot of the data with the fitted
regression line and to the right is the corresponding residual plot with a smoothing curve.
3.3. RESIDUAL PLOTS 43
3.3.2 Examples of unacceptable residual plots
The next example, shown in Figure 3.2, may remind you of the Western cedar tree data
that we considered in the last chapter. From the scatterplot of the data we observe that the
response variable is increasing with the explanatory variable. However the rate of change
appears to be decreasing and so the relationship is curved rather than linear. Therefore the
fitted regression line is not an adequate description of the systematic structure visible in the
data. The linearity assumption is violated. This is also reflected in the residual plot, where
the smoother is curved rather than resembling a horizontal line. However, in both plots the
variation of the observations as we move along the horizontal axis appears constant.
Figure 3.2: Scatterplot and residual plot of an example where the relationship between the
response variable and the explanatory variable is monotonic but non-linear.
A potential remedial action is to apply a transform to the explanatory variable (as we
did for the Western cedar tree data) and then fit a simple linear regression model of the
response on the transformed explanatory variable. Common transformations include the
square root transform and the log-transform which can be applied if the explanatory variable
is non-negative, in the case of the sqrt-transform, or positive, in the case of the log-transform.
If a transformation of the explanatory variable does not remedy the problem, as a second
approach we might consider fitting a more complex model, for example a quadratic regression
model.
The next example, shown in Figure 3.3, is similar to the previous example with the relationship
between the response and the explanatory variable appearing curved rather than linear.
Thus the fitted regression line is not an adequate description of the relationship. However,
this time the trend is not monotonic. The non-linearity also translates to the residual plot
where the smoother has the shape of an upturned parabola. As in the previous example,
in both plots, the variation of observations as we move along the horizontal axis appears
constant.
44 CHAPTER 3. RESIDUAL ANALYSIS
Figure 3.3: Scatterplot and residual plot of an example where the relationship between the
response variable and the explanatory variable is non-monotonic and non-linear.
A transform will not address a non-monotonic non-linearity. Instead we might consider
using a more complex model. In the example here, a quadratic regression model is likely to
work well.
In the next example shown in Figure 3.4 the residual plot resembles a ‘right-opening
megaphone’. The variation of the residuals is increasing with fitted value. The smoothing
curve on the other hand resembles a horizontal line. Similar features can be observed in the
scatterplot of the data although there they are harder to spot. The fitted regression line
describes the general trend well, but the average distance of the observations to the fitted
line increases with the explanatory variable.
Figure 3.4: Scatterplot and residual plot of an example where the variance is increasing with
fitted values.
An approach that might correct for this is to transform the response variable. Common
3.3. RESIDUAL PLOTS 45
choices when the response variable is positive and the variance is increasing are to take the
square-root or the logarithm of the response variable. Residual plots with a megaphone
shape are very common in practice, particularly when the range of the response variable
is restricted such as for lengths or distances which, by nature, are positive (or at least
non-negative).
The final example in Figure 3.5 suffers from both heteroscedasticity and non-linearity. The
observations, both in the scatterplot of the data and the residual plot, scatter around a
convex curve rather than a straight line and the variation of the observations increases
as we move along the horizontal axis. Here we might transform the response to stabilise
the variance. Then, if the linearity assumption is still not reasonable, we may consider a
transformation of the explanatory variable or fit a more complex model. Sometimes it is not
possible to resolve both heteroscedasticity and non-linearity. In this case we would prioritise
linearity as this is the more important assumption out of the two.
Figure 3.5: Scatterplot and residual plot of an example which suffers from non-linearity and
heteroscedasticity.
Note that whenever we take remedial action such as transformations we need to check
the residual plots of the modified model to ensure that the model assumptions are now
reasonable.
Assessing residual plots requires careful examination and also some experience. It can be
challenging to judge whether a feature is systematic or simply due to random variation.
You will consider further examples of residual plots in the computer practicals. In your
coursework, when considering residual plots, you should first describe the main features that
you have identified in the plot before deciding whether the plot is acceptable or not. If it is
not acceptable, then you should suggests an appropriate remedial action and explain why
you opted for that specific action.
In the next section we discuss examples of residual plots that show distinct features but are
considered acceptable.
46 CHAPTER 3. RESIDUAL ANALYSIS
3.3.3 Further examples of acceptable residual plots
In this section we consider some further examples of data that were simulated from simple
linear regression models and thus are known to satisfy the linear model assumptions.
The most striking feature in the first example, shown in Figure 3.6, is that there are more
low x-values than high ones, so more of the observations are on the left hand side of the
scatterplot of the data. However the observations are relatively evenly distributed on either
side of the fitted regression line. As in the scatterplot of the data, in the residual plot most
observations are on the left hand side of the plot. The smoothing curve on the whole is
relatively horizontal. The skewness of the distribution of fitted values makes it more difficult
to assess the constant variance assumption. However, as this is simulated data we know that
is satisfies the modelling assumptions. This illustrates that it can be hard to judge whether
an apparent feature is a model violation or just random variation. So, play around in R with
simulated data to get more experience in judging residual plots!
Figure 3.6: Scatterplot and residual plot for an example with a skewed predictor.
In the next pair of plots in Figure 3.7, the residuals lie on diagonal lines of gradient -1. This
is due to the fact that rounding was applied to the response variable. This feature is not
uncommon in practice but often more subtle than in this example. Despite the rounding, the
fitted regression line provides a good description of the relationship between the explanatory
variable and the response variable displayed in the scatterplot. In the residual plot the
smoothing curve is fairly horizontal and the variation of the residuals also appears relatively
constant.
3.4. ASSESSING NORMALITY 47
Figure 3.7: Scatterplot and residual plot of an example where the response was rounded.
Figure 3.8 shows the final example of a pair of acceptable plots. The observations in both
plots appear arranged in vertical columns. This is simply due to the fact that we have
repeated x-values which thus have the same fitted value. The smoother in the residual
plot resembles a horizontal line and the variation of the residuals along the horizontal axis
appears constant. Thus the assumptions of linearity and constant variance seem reasonable
in this example.
Figure 3.8: Scatterplot and residual plot of an example with repeated values of the explanatory
variable.
3.4 Assessing normality
In this section we consider how to assess the validity of the assumption that the errors are
normally distributed. Fortunately most of the statistical inference that we will introduce in
later chapters is quite robust to departures from normality, particularly for large sample
sizes n. So this is the assumption we are usually least concerned about.
48 CHAPTER 3. RESIDUAL ANALYSIS
We can assess the normality assumption using a normal Quantile-Quantile plot (Q-Q
plot). The basic idea of a normal Quantile-Quantile plot is to graphically compare the
data against some reference to a normal distribution. One such reference are the expected
ordered values from the appropriate normal distribution.
Most statistical software packages create normal Q-Q plots, although not every statistical
software package creates them in the same way. Some use a probability or percentage scale
while others use a normal score scale for the vertical axis. Others put the data on the vertical
axis and the normal scale on the horizontal axis. Regardless of how the plot is created, the
basic interpretation is the same.
• If the data lie close to a straight line representing the reference normal distribution,
then the assumption that the data could have come from a normal distribution is
validated.
• If the data depart from the straight line representing the normal distribution, then the
assumption of normality is called into question.
In R, to produce a normal Q-Q plot for the standardised residuals of the fitted model we use
plot(model, which = 2).
Figure 3.9: Two Normal Q-Q plots: on the left the data are from a standard normal
distribution. On the right they are from a t-distribution on 2 degrees of freedom.
As an example consider Figure 3.9. The left plot shows a normal Q-Q plot for n = 200
datapoints simulated from a normal distribution, and we observe that the observations follow
the reference line reasonably closely. Notice that even though the data were simulated from
the normal distribution, there are some deviations due to randomness.
The right plot shows normal Q-Q plot for n = 200 datapoints simulated from a t2 distribution.
Most of the data fall surprisingly close to the line – only in the tails of the distribution is
the difference noticeable. With fewer observations than n = 200, the two plots might look
very similar.
In the right plot the sample quantiles in the left tail (on the left hand side of the plot)
3.4. ASSESSING NORMALITY 49
are smaller than expected as they lie below the reference line. This means that the data
contained more observations out in the left tail than expected under a normal distribution.
Similarly, the sample quantiles in the right tail (on the right hand side of the plot) are larger
than expected as they lie above the reference line. This means that the data contained more
observations out in the right tail than expected under a normal distribution. Below is a plot
of the standard normal density and the t2 density, further illustrating why we would expect
this tail behaviour.
Figure 3.10: Comparison of the density function of a standard normal distribution (solid
black line) with the one of the t-distribution with 2 degrees of freedom (dashed red line).
This answer to a StackExchange post has some more examples of patterns that we may
observe in normal Q-Q Plots.
Exercise 10
What departure from normality do the following normal Q-Q plots indicate?
50 CHAPTER 3. RESIDUAL ANALYSIS
3.5 Summary of Chapter 3
• A normal linear model Y = Xβ + ϵ makes the following assumptions:
1. Linearity: the linear predictor Xβ adequately describes the mean function of
the response variable.
2. Homoscedasticity or constant variance: the errors ϵ1, . . . , ϵn have constant
variance.
3. Independence: the errors ϵ1, . . . , ϵn are independent of each other.
4. Normality: the errors ϵ1, . . . , ϵn have a normal distribution.
• If the model assumptions hold, then a plot of the residuals against the fitted values
shows no distinct patterns. The residuals scatter randomly around a horizontal line
and have constant variation.
• Normal Q-Q plots of the standardised residuals are used to assess the normality
assumption. If the data lie along a straight line in a normal Q-Q plot, then normality
assumption is reasonable. If the data depart from the straight line, then this suggests
a violation of the normality assumption.
• Remedial actions to address violations of the modelling assumptions include non-linear
transformations and the use of more complex models.
After studying this chapter you should
• be aware of the normal linear model assumptions;
• know how diagnose violations of the model assumptions;
• be able to suggest potential remedial action in case of a violation.
If you wish to engage in further (optional!) reading on the topics covered in this chapter,
then the following section of one of the textbooks recommended for this module covers
related material.
• A Modern Approach to Regression with R (2009) by Sheather: Sections 3.1, 3.2.5 and
3.26
• Generalized linear models with examples in R (2018) by Dunn and Smyth: Section 3.2
Chapter 4
Non-linear transformations
Non-linear transformations are one approach to address certain model violations, so let us
briefly summarise when such transformations might be useful.
• Transformations of the response and/or the explanatory variables may be used to meet
the assumption of linearity.
• A transformation of the response is often used to address certain forms of heteroscedasticity.
• Transformations may also reduce the influence of unusual observations.
A particularly common transform is the log-transform, so in the following we consider this
transform in more detail. We already saw an example with the Western red cedar data in
Section 2.3 where we log-transformed the explanatory variable, so next we will discuss a
log-transformation of the response variable.
4.1 The log-transformation
Suppose we have two explanatory variables X1 and X2 and consider the model
log(Yi) = β0 + β1xi1 + β2xi2 + ϵi
, (4.1)
where i = 1, . . . , n and xik is the ith observation of the explanatory variable Xk for k = 1, 2.
In the original scale of the response, this model corresponds to
Yi = exp 
β0 + β1xi1 + β2xi2 + ϵi

= e
β0 e
β1xi1 e
β2xi2 exp(ϵi)
where i = 1, . . . , n. Thus, in the original scale the errors are given by exp(ϵi) and act
multiplicatively rather than additively.
51
52 CHAPTER 4. NON-LINEAR TRANSFORMATIONS
We usually express predictions for the response in the original scale. We do this by backtransforming, that is, if zb0 is the prediction for the logged response from the model in (4.1),
we predict exp(zb0) for the response in the original scale.
However, note that for any positive random variable Y that is not constant, we have
E (Y ) > exp [E (log(Y ))] .
For example, if log(Y ) has a N (µ, σ2
) distribution, then Y has a lognormal(µ, σ2
) distribution
and so
E(Y ) = exp 
µ +
σ
2
2
!
> exp(µ) = exp [E (log(Y ))] .
Furthermore, if log(Y ) ∼ N (µ, σ2
), then the geometric mean of Y , that is exp [E (log(Y ))], is
also equal to the median of Y and so this provides further justifiction for back-transforming
the predicted response.
To interpret the regression coefficients obtained after log-transforming the response, we
proceed as follows. Using model (4.1), we start with a prediction for the log-transformed
response:
log( \yA) = bβ0 + bβ1x1 + bβ2x2
which back-transformed gives the prediction
ybA = exp 
bβ0 + bβ1x1 + bβ2x2

.
Now suppose X1 increases by one unit while X2 stays fixed. Then
log( \yB) = bβ0 + bβ1(x1 + 1) + bβ2x2, and so
ybB = exp 
bβ0 + bβ1(x1 + 1) + bβ2x2

= ybA exp( bβ1).
Thus an increase in X1 by one unit with X2 staying fixed results in a change in the predicted
response (in the original scale) by a factor of exp(
bβ1). Of course, this interpretation is only
sensible if, indeed, X1 can increase while X2 stays fixed.
Log-transforms are often useful when dealing with response variables that correspond to
positive physical measurements like weights or lengths. For such variables it is not unusual
to find that the precision of measurement is higher for smaller observations. This then leads
to a variance that is increasing with the mean. In such cases log-transforming the response
may stabilise the variance. It is also common for these variables to have a right skewed
distribution and then log-transforming the variable tends to make the distribution more
symmetrical. Finally, applying a log-transformation to a positive response variable also
means that predictions from the model will always be positive.
The next sections discuss examples where log-transformations were used to build valid linear
models.
4.2. THE MAMMALS DATASET 53
4.2 The mammals dataset
In this section we consider the mammals data from the MASS package which was also discussed
in ST117/ST121. The dataset contains the average body weight in kg (BODY) and the
average brain weight in g (BRAIN) for 62 species of land mammal. Figure 4.1 is a scatterplot
of BRAIN against BODY.
Figure 4.1: Scatterplot of the mammals dataset
We observe that most observations are clumped together in the lower left hand corner.
This is due to the fact that both, body weight and brain weight, have a heavily skewed
distribution. Furthermore, three observations stand out as unusual. The African and the
Asian elephant have an unusually high average body weight as well as average brain weight.
In contrast the human has an average body weight that is not unusual but an average brain
weight that is unusually high relative to the average body weight. The scale of the plot
makes it difficult to discern the relationship between the two variables, but it looks unlikely
that the relationship is linear. Also, if we were to fit a linear model to this data, then this
would be heavily influenced by the observations for the two elephant species.
A common approach to reducing the positive skew of a distribution and thus reduce the
influence of unusual observations is to take logs. For example, Figure 4.2, shows a comparison
of the histogram of body weight and the histogram of log(body weight) for the mammals data
and we observe that the distribution of the log-transformed variable is much less skewed.
The more important issue, though, is the lack of evidence of a linear relationship between
body weight and brain weight. A more detailed examination of the observations in the lower
left corner, which is not presented here, reveals that the relationship is likely to be non-linear
so a transformation may help improve on linearity. Because the response variable and the
explanatory variable are weights and therefore the same type of measurement, applying the
same transformation to both seems reasonable.
54 CHAPTER 4. NON-LINEAR TRANSFORMATIONS
Figure 4.2: Histogram of body weight and of the logarithm of body weight.
Figure 4.3 shows a plot of log(BRAIN) against log(BODY) together with the line of best fit
and illustrates how linearity is improved by log-transforming both the explanatory variable
and the response variable.
Figure 4.3: Scatterplot of the logarithm of brain weight against the logarithm of body weight
together with the line of best fit.
Fitting the model
log(BRAINi) = β0 + β1 log(BODYi) + ϵi for i = 1 . . . , 62
we obtain as an estimate for the slope coefficient bβ1 = 0.75. But how do we interpret this
result?
4.3. THE TREES DATASET 55
Consider a species A with average body weight x. For this species the model predicts an
average brain weight of
ybA = exp( bβ0 + bβ1 log(x)).
Suppose now species B has an average body weight that is given by 1.1x. For species B the
model predicts an average brain weight of
ybB = exp( bβ0 + bβ1 log(1.1x)) = exp 
bβ0 + bβ1[log(1.1) + log(x)]
= ybA 1.1
βb1
Now, as bβ1 = 0.75, we have 1.1
βb1 = 1.1
0.75 = 1.074. Thus for a 10% larger average body
weight, the predicted average brain weight increases by 7.4%.
4.3 The trees dataset
The physical context of an application can also suggest the use of a log-transform. Here we
will consider one such example based on the trees dataset. The data comprise measurements
of 31 felled black cherry trees, namely
• Height: the height of the tree in feet;
• Girth: the diameter of the tree in inches measured at 4 ft 6 inches from the ground;
• Volume: the volume of timber that the tree produced in cubic feet.
The diameter was wrongly labelled girth in the original dataset.
The aim is to predict the volume of timber from the height and the diameter of the tree.
Here the physical context is helpful in deciding on a suitable model. If we approximate the
shape of the tree by a cylinder, say, then we have
Volume ∝ Diameter2 × Height
and so the explanatory variables Height and Diameter enter multiplicatively. Taking logs
allows us to turn this into an additive relationship that can be modelled via a linear model.
So a reasonable model to start with is
log 
Volumei

= β0 + β1 log 
Diameteri

+ β2 log 
Heighti

+ ϵi for i = 1, . . . , 31.
In the original scale this corresponds to
Volumei = exp(β0) Diameterβ1
i Heightβ2
i
exp(e
ϵi
).
But before we jump into modelling, let’s first do a very, very brief initial exploratory analysis
on the transformed variables. We can use the pairs command in R to produce a scatterplot
matrix as in Figure 4.4. I have augmented the pairs function, so that in addition to pairwise
scatterplots it also plots the histograms of the variables.1
1
If you are curious of how I implemented this, check the R help function for pairs.
56 CHAPTER 4. NON-LINEAR TRANSFORMATIONS
Figure 4.4: Scatterplot matrix for the log-transformed trees data.
We observe that the histograms of the log-transformed variables show some but not very
pronounced skew and that the scatterplots look reasonably linear. So let’s proceed with
fitting the model and considering the residual plots.
The plot of residuals versus fitted values shown in Figure 4.5 shows a smoothing curve that
resembles a horizontal line. Given this is a small dataset the variation of the residuals is
difficult to assess, but assuming homoscedasticity does not seem too unreasonable.
Figure 4.5: Residuals versus fitted values for the linear model fitted to the log-transformed
tree data.
In multiple regression it is recommended to also plot the residuals against each of the
explanatory variables. This is shown in Figure 4.6, but, as in the plot of residuals versus
fitted values, the assumption of linearity seems reasonable for the fitted model.
4.3. THE TREES DATASET 57
Figure 4.6: Residuals versus log-height and versus log-diameter for the model fitted to the
log-transformed tree data.
Finally, let’s consider the estimates produced by the fitted model. Note that the data frame
logTrees consists of the log-transformed tree data.
trees.model <- lm(logVolume ~ logDiameter + logHeight, data=logTrees)
round(coef(trees.model), 2)
(Intercept) logDiameter logHeight
-6.63 1.98 1.12
From our earlier cylinder approximation we would expect the log-diameter to have a coefficient
close to 2 and the log-height to have a coefficient close to 1. We observe that the estimated
coefficients are in fact close to these theoretical values. We can now interpret the coefficient
for log-height as follows. For a given diameter, an increase in the height of the tree (in
feet) by 10% is associated with an increase in timber volume (in cubic feet) by a factor of
1.1
1.12 ≈ 1.11, that is an 11% increase.
Exercise 11
Consider the regression of log-volume on log-diameter and log-height for the trees dataset
as presented above.
1. Give a quantitative interpretation of the coefficient for log-diameter in the original
scale of the response.
2. In the above regression, what intercept would we expect if the shape of a tree was
reasonably well approximated by a cylinder?
3. Suppose instead of a cylinder we use a cone to approximate the shape of a tree, what
intercept would this approximation suggest?
4. Compare your solutions to 2. and 3. with the estimated intercept for the regression of
log-volume on log-diameter and log-height. What do you conclude?
The above problem is also considered in Section 3.10 of Generalized linear models with
examples in R (2018) by Dunn and Smyth.
58 CHAPTER 4. NON-LINEAR TRANSFORMATIONS
4.4 Summary of Chapter 4
This chapter gave an illustration of how to develop valid linear models and interpret the
results.
Non-linear transformations may resolve violations of the model assumptions:
• Transformations of the response and/or the explanatory variables may be used to meet
the assumption of linearity.
• A transformation of the response is often used to address certain forms of heteroscedasticity.
• Transformations may also reduce the influence of unusual observations.
The chapter also explained how to interpret models with log-transformed variables.
After studying this chapter you should
• know the role of transformations in linear modelling, in particular the logtransformation;
• understand how to apply transformations to address model violations;
• be able to interpret models with log-transformed variables.
If you wish to engage in further (optional!) reading on the topics covered in this chapter,
then the following section of one of the textbooks recommended for this module covers
related material.
• A Modern Approach to Regression with R (2009) by Sheather: Section 3.3.
• Generalized linear models with examples in R (2018) by Dunn and Smyth:
Sections 3.9 and 3.10.
Chapter 5
Least squares estimation
In this chapter we introduce the mathematical underpinning of least squares estimation.
We use this to derive a closed form expression for the estimate of the parameter vector. This
also provides us with the framework to describe model re-parameterisations mathematically.
In Chapter 6 we will then show that the least squares estimate for the parameter vector in a
normal linear model is also its maximum likelihood estimate.
We will make use of vector and matrix notation to express various statements in a more
concise fashion. Appendix A collates some of the results from linear algebra and multivariable calculus that will be used within this and other chapters and that, hopefully, you
are already familiar with.
5.1 The least squares estimate
Definition 5.1 (The least squares estimate). The least squares estimate of β is the
vector βb of dimension p which minimizes the sum of squared differences
S(β) = Xn
j=1
(yj − x
T
j β)
2
.
Before we formally derive the closed form expression for the least squares estimator, let’s
first consider an example, namely simple linear regression.
5.2 Example: simple linear regression
Suppose we have n paired observations (x1, y1), (x2, y2), . . . , (xn, yn) for which we consider
the simple linear regression model given by
Yj = β0 + β1xj + ϵj , where j = 1, . . . , n.
59
60 CHAPTER 5. LEAST SQUARES ESTIMATION
The residual sum of squares function S(β), where β = (β0, β1)
T
, is given by
S(β) = Xn
j=1

yj − (β0 + β1xj )
2
.
Thus, to determine the least squares estimate, we need to determine the value of the vector
β at which S(β) takes its minimum.
We have
S(β) = Xn
j=1

yj − (β0 + β1xj )
2
=
Xn
j=1
y
2
j +
Xn
j=1
(β0 + β1xj )
2 − 2
Xn
j=1
(β0 + β1xj )yj .
Taking partial derivatives we obtain
∂S(β)
∂β0
= 2Xn
j=1
(β0 + β1xj ) − 2
Xn
j=1
yj , and
∂S(β)
∂β1
= 2Xn
j=1
xj (β0 + β1xj ) − 2
Xn
j=1
xjyj .
Setting the partial derivatives equal to zero, we obtain as solution1
bβ0 = ¯y − bβ1x¯ and bβ1 =
Pn
j=1 xjyj

− nx¯y¯
Pn
j=1 x
2
j

− nx¯
2
.
If we define Sxx =
Pn
j=1(xj − x¯)
2 and Sxy =
Pn
j=1(xj − x¯)(yj − y¯), then
(i) Sxy =
Xn
j=1
(xj − x¯)(yj − y¯)
=
Xn
j=1
xjyj

− x¯
Xn
j=1
yj

− y¯
Xn
j=1
xj

+ nx¯y¯
=
Xn
j=1
xjyj

− nx¯y¯ − ny¯x¯ + nx¯y¯ =
Xn
j=1
xjyj

− nx¯y¯
and, analogously,
(ii) Sxx =
Xn
j=1
(xj − x¯)
2 = (Xn
j=1
x
2
j
) − nx¯
2
.
1Hint: first set ∂S(β)
∂β0
to zero to work out an expression for β0, then use the solution when setting ∂S(β)
∂β1
equal to zero.
5.3. DERIVATION OF THE LSE 61
Using (i) and (ii) we can thus deduce that
bβ1 =
Pn
j=1 xjyj

− nx¯y¯
Pn
j=1 x
2
j

− nx¯
2
=
Sxy
Sxx
and bβ0 = ¯y − bβ1x¯ = ¯y −
Sxy
Sxx
x. ¯
Note that, strictly, we have shown that βb is a stationary point, but not that it minimises
S(β). We will prove that it is indeed a global minimum in Section 5.3.
Exercise 12: Using the estimates we derived for simple linear regression, show that the
fitted regression line goes through the point (¯x, y¯).
Exercise 13: Consider the following dataset.
xi
| 1 2 2 3 8
yi
| 3.5 4 6 7.5 17.25
a. Complete the following table:
(xi − x¯)
2
| (yi − y )(xi − x )
|
|
|
|
|
b. Use the above table to compute the parameter estimates for a simple linear regression
model fitted to the above data.
c. Compute the fitted values and residuals of the fitted model.
d. Finally compute the deviance of the model.
5.3 Derivation of the LSE
Suppose we have n units of observation and assume that for j = 1, . . . , n
Yj = x
T
j β + ϵj ,
where β is of dimension p ≤ n and x
T
j
is the jth row of the n × p design matrix X.
Theorem 5.1 (Least squares estimation). If X is of full rank, that is rank (X) = p, then
βb =

XTX
−1
XT y.
62 CHAPTER 5. LEAST SQUARES ESTIMATION
Remark
The above theorem provides us with a closed form expression for βb that can be easily
computed. It relies on the assumption that rank (X) = p. Normally we are able to choose
the design matrix X and thus can ensure that it is of full rank.
Proof. The proof of Theorem 5.1 is divided into three steps.
• Step 1: We compute the Jacobian matrix of S(β).
• Step 2: We give a closed form expression for the stationary point βb of S(β).
• Step 3: We show that S(β) achieves its global minimum at βb.
Step 1: The Jacobian matrix of S(β)
We start by deriving a matrix expression for S(β) before we determine its Jacobian matrix.
We will use a few facts from linear algebra which we quickly revise here. Recall that for
vectors a and b of dimension m we have
(i) a
Ta =
Xm
j=1
a
2
j and
(ii) (a + b)
T
(a + b) = a
Ta + b
T
b + 2b
Ta
(iii) (a − b)
T
(a − b) = a
Ta + b
T
b − 2b
Ta
Furthermore, for conformable matrices A and B we have
(iv) (AB)
T = BT AT
.
Now, coming back to S(β), recall that
y − Xβ =



y1 − x
T
1 β
.
.
.
yn − x
T
nβ



and so
S(β) = Xn
j=1

yj − x
T
j β
2 (i) = (y − Xβ)
T
(y − Xβ).
Thus,
S(β) = (y − Xβ)
T
(y − Xβ)
(iii)
= y
T y + (Xβ)
TXβ − 2(Xβ)
T y
(iv)
= y
T y + β
TXTXβ − 2β
TXT y. (5.1)
Next we compute the partial derivatives of S(β). Recall the following facts from multivariable
calculus. Let f : R
p −→ R be a function, let a be a vector of dimension p and let A be a
symmetric p × p matrix. Now,
5.3. DERIVATION OF THE LSE 63
(v) if f(β) = β
Ta, then ∂f(β)
∂β = a;
(vi) if f(β) = β
T Aβ, then ∂f(β)
∂β = 2Aβ.
If we apply this to the expression of S(β) derived in (5.1), that is
S(β) = y
T y + β
TXTXβ − 2β
TXT y,
noting that XTX is a symmetric matrix, then we deduce that
∂S(β)
∂β
= 2XTXβ − 2XT y. (5.2)
Step 2: A closed form expression for the stationary point of S(β)
At a stationary point the partial derivatives in (5.2) must be zero, thus a stationary point βb
satisfies
XTXβb − XT y = 0 (5.3)
We call the set of equations in (5.3) the normal equations.
As X is chosen to be of full rank, that is rank (X) = p, we have rank 
XTX

= p and so
XTX is non-singular and its inverse exists. Then, from (5.3)
βb =

XTX
−1
XT y
is the unique stationary point of S(β).
Step 3: S(β) achieves its global minimum at βb
To show that S(βb) is the global minimum we show that S(β) ≥ S(βb) for all β. We do this
by subtracting and then adding Xβb in the expression for S(β):
S(β) = (y − Xβ)
T
(y − Xβ)
= (y − Xβb + Xβb − Xβ)
T
(y − Xβb + Xβb − Xβ)
=

(y − Xβb) + (Xβb − Xβ)
T 
(y − Xβb) + (Xβb − Xβ)

(ii) = (y − Xβb)
T
(y − Xβb) + (Xβb − Xβ)
T
(Xβb − Xβ) + 2(Xβb − Xβ)
T
(y − Xβb)
= S(βb) + 
X(βb − β)
T 
X(βb − β)

+ 2
X(βb − β)
T
(y − Xβb)
(iv)
= S(βb) + (βb − β)
TXTX(βb − β) + 2(βb − β)
TXT
(y − Xβb)
= S(βb) + (βb − β)
TXTX(βb − β) + 2(βb − β)
T
(XT y − XTXβb).
As βb solves the normal equations (5.3), the term (XT y − XTXβb) is equal to 0 and so the
last term in S(β) is
2(βb − β)
T
(XT y − XTXβb) = 0.
64 CHAPTER 5. LEAST SQUARES ESTIMATION
Furthermore, the second term of S(β), that is (βb − β)
TXTX(βb − β) is of the form a
Ta
and thus a sum of squares which must be non-negative. Hence we can deduce
S(β) = S(βb) + (βb − β)
TXTX(βb − β) + 2(βb − β)
T
(XT y − XTXβb)
= S(βb) + (βb − β)
TXTX(βb − β)
| {z }
≥0
+ 0
≥ S(βb).
In conclusion, βb =

XTX
−1
XT y is the global miminiser of S(β).
In the next section we will illustrate the various terms derived above for the specific example
of simple linear regression.
5.4 Example: simple linear regression continued
In this section we demonstrate how the quantities used in the derivation of the least squares
estimate lead to the equations for simple linear regression that we considered in Section 5.2.
As shown in Section 1.4, in simple linear regression the design matrix and parameter vector
are given by
X =



1 x1
.
.
.
.
.
.
1 xn



and β =
"
β0
β1
#
and so Xβ =



β0 + β1x1
.
.
.
β0 + β1xn


 .
Now,
S(β) = (y − Xβ)
T
(y − Xβ)
=
h
y1 − (β0 + β1x1) . . . yn − (β0 + β1xn)
i



y1 − (β0 + β1x1)
.
.
.
yn − (β0 + β1xn)



=
Xn
j=1

yj − (β0 + β1xj )
2
=
Xn
j=1
y
2
j +
Xn
j=1
(β0 + β1xj )
2 − 2
Xn
j=1
(β0 + β1xj )yj
= y
T y + β
TXTXβ − 2β
TXT y.
Compare the above to equation (5.1).
5.4. EXAMPLE: SIMPLE LINEAR REGRESSION CONTINUED 65
Taking partial derivatives we obtain
∂S(β)
∂β0
= 2Xn
j=1
(β0 + β1xj ) − 2
Xn
j=1
yj
∂S(β)
∂β1
= 2Xn
j=1
xj (β0 + β1xj ) − 2
Xn
j=1
xjyj
which in matrix notation is
∂S(β)
∂β
=


2
Pn
j=1(β0 + β1xj ) − 2
Pn
j=1 yj
2
Pn
j=1 xj (β0 + β1xj ) − 2
Pn
j=1 xjyj


= 2 "
1 · · · 1
x1 · · · xn
#



β0 + β1x1
.
.
.
β0 + β1xn


 − 2
"
1 · · · 1
x1 · · · xn
#



y1
.
.
.
yn



= 2XTXβ − 2XT y.
Compare the above to equation (5.2).
In Section 5.2 we set the partial derivatives equal to zero and derived as solution:
bβ1 =
Pn
j=1 xjyj

− nx¯y¯
Pn
j=1 x
2
j

− nx¯
2
=
Sxy
Sxx
and bβ0 = ¯y − bβ1x¯ = ¯y −
Sxy
Sxx
x. ¯
Using matrix notation instead we have
XT y =
"
1 · · · 1
x1 · · · xn
#



y1
.
.
.
yn


 =


Pn
j=1 yj
Pn
j=1 xjyj


XTX =
"
1 · · · 1
x1 · · · xn
#



1 x1
.
.
.
.
.
.
1 xn


 =


n nx¯
nx¯
Pn
j=1 x
2
j

 .
66 CHAPTER 5. LEAST SQUARES ESTIMATION
and so

XTX
−1
=
1

n
Pn
j=1 x
2
j

− n2x¯
2


Pn
i=1 x
2
j −nx¯
−nx n ¯


=
1
 Pn
j=1 x
2
j

− nx¯
2


1
n
Pn
i=1 x
2
j −x¯
−x¯ 1


=
1
Sxx


1
n
Pn
i=1 x
2
j −x¯
−x¯ 1

 .
Thus
βb =

XTX
−1
XT y
=
1
Sxx


1
n
Pn
i=1 x
2
j −x¯
−x¯ 1




Pn
j=1 yj
Pn
j=1 xjyj


=
1
Sxx


y¯
Pn
i=1 x
2
j − x¯
Pn
j=1 xjyj
Pn
j=1 xjyj − x¯
Pn
j=1 yj


=
1
Sxx


y¯
Pn
i=1 x
2
j − x¯
Pn
j=1 xjyj − nx¯
2y¯ + nx¯
2y¯
Pn
j=1 xjyj − x¯
Pn
j=1 yj


=
1
Sxx



y¯

(
Pn
i=1 x
2
j
) − nx¯
2

− x¯

(
Pn
j=1 xjyj ) − ny¯x¯

Pn
j=1 xjyj − nx¯y¯



=
1
Sxx "
yS¯ xx − xS¯ xy
Sxy #
.
Exercise 14: Consider the straight line model
Yj = α0 + α1 (xj − x) + ϵj , j = 1, . . . , n.
a. Write out the design matrix X for the above model.
b. Let x¯ =
1
n
Pn
j=1 xj and Sxx =
Pn
j=1(xj − x¯)
2
. Show that
XTX =
"
n 0
0 Sxx #
.
5.5. EXAMPLE: INVERTIBLE LINEAR TRANSFORMATIONS 67
c. Let y¯ =
1
n
Pn
j=1 yj and Sxy =
Pn
j=1(xj − x¯)(yj − y¯). Show that
Sxy =
Xn
j=1
yj (xj − x¯).
d. Use the above results to derive an expression for 
XTX
−1
XT y.
e. Use d. and Theorem 5.1 to show that the least squares estimate for the model is given
by
αb =
"
αb0
αb1
#
=



y
Sxx
Sxy


 .
5.5 Example: invertible linear transformations
In Section 1.1 we considered the re-parameterisation of a simple linear regression model
for the diamond data. In the following we discuss the mathematical background for reparameterisations. Suppose that we want to rewrite the model Y = Xβ + ϵ in terms of
another parameter vector γ. Assume that γ = Aβ, where A is a p × p non-singular matrix,
and so our model becomes Y = XA−1γ + ϵ, which has design matrix Z = XA−1
.
Recall that for conformable matrices B and C we have
(i) (BC)
T = C
T BT
(ii) (BC)
−1 = C
−1B−1
and
(iii) (BT
)
−1 = (B−1
)
T
Then,
γb =

Z
TZ
−1
Z
T y
=

XA−1
T 
XA−1
−1 
XA−1
T
y
(i)
=

(A−1
)
T
(XTX)A−1
−1
(A−1
)
TXT y
(iii)
=

(AT
)
−1
(XTX)A−1
−1
(AT
)
−1XT y
(ii)
= A(XTX)
−1AT
(AT
)
−1XT y
= A(XTX)
−1XT y
= Aβb.
Thus the algebraic relationship between γ and β caries over to the estimates γb and βb.
68 CHAPTER 5. LEAST SQUARES ESTIMATION
The fitted values are
yb = Zγb = XA−1γb = XA−1Aβb = Xβb.
Thus the fitted values, and so also the residuals and the deviance, are unchanged by a
reparameterisation of the form γ = Aβ where A is an invertible matrix.
Example For j = 1, . . . , n let xj be the jth observation of the explanatory variable and x¯
the corresponding sample mean. Then the model
(1) Yj = α0 + α1 (xj − x¯) + ϵj
is a reparameterisation of the model
(2) Yj = β0 + β1xj + ϵj .
We can rewrite the last model equation as
(2a) Yj = (β0 + β1x¯) + β1(xj − x¯) + ϵj
and so deduce that
"
α0
α1
#
=
"
1 ¯x
0 1 # " β0
β1
#
.
Therefore this is a linear transformation using the non-singular matrix
A =
"
1 ¯x
0 1 #
with inverse given by
A−1 =
"
1 −x¯
0 1 #
.
Let
Xβ =



1 x1
.
.
.
.
.
.
1 xn



be the design matrix of the model parameterisation in (2). Then the design matrix of the
parameterisation in (1) is given by
Xα = XβA−1 =



1 x1
.
.
.
.
.
.
1 xn



"
1 −x¯
0 1 #
=



1 x1 − x¯
.
.
.
.
.
.
1 xn − x¯


 .
5.6. SUMMARY OF CHAPTER 5 69
Exercise 15: For j = 1, . . . , n let xj be the jth observation of the explanatory variable. Let
c ̸= 0 be a constant by which we multiply all observations of the explanatory variable.
a. Show that the model
(1) Yj = α0 + α1 (cxj ) + ϵj
is a reparameterisation of the model
(2) Yj = β0 + β1xj + ϵj .
b. Derive the algebraic relationship between the set of least squares estimates for (1),
that is αb0 and αb1, and the set of least squares estimates for (2), that is bβ0 and bβ1.
c. Maya has fitted a simple linear regression model to a set of data where the explanatory
variable was ambient temperature measured in Celsius. How do the least squares
estimates of the intercept and slope change, if the temperature were measured in
Fahrenheit instead?
5.6 Summary of Chapter 5
• The fitted values are defined as
ybj = x
T
j βb, for j = 1, . . . , n
where x
T
j
is the jth row of the design matrix X and βb is the estimate of the parameter
vector β.
• The residuals are defined as
ϵbj = yj − ybj = yj − x
T
j βb for j = 1, . . . , n.
• The deviance or the residual sum of squares (ResidSS) is defined as
D =
Xn
i=1
(yi − ybi)
2 =
Xn
i=1
ϵb
2
i
.
• The least squares estimate of β is the vector βb which minimizes the sum of squared
differences
S(β) = Xn
j=1
(yj − x
T
j β)
2
.
• Provided X is of full rank, that is rank (X) = p, we have βb =

XTX
−1
XT y.
70 CHAPTER 5. LEAST SQUARES ESTIMATION
• Invertible linear transformations produce re-parameterisations of a model. The
fitted values, residuals and deviance do not change after reparameterisation, but the
parameters may have a different interpretation.
At the end of this chapter you should
• understand how the least squares estimate for the parameter vector of a linear model
is defined;
• be able to derive a closed form expression for the least squares estimate;
• be able to compute the least squares estimate for the parameter vector of a linear
model;
• understand how model re-parameterisation can be described mathematically and be
aware of the effect of a model re-parameterisation.
If you wish to engage in further (optional!) reading, consider
• A Modern Approach to Regression with R (2009) by Sheather:
Chapter 2 pages 15 - 19 and Chapter 5 pages 130 - 133.
Chapter 6
Maximum likelihood estimation
In the previous chapter we derived the least squares estimate (LSE) βb for the parameter
vector β. Note that in the derivation of the LSE we did not make use of the fact that the
errors are iid N (0, σ2
). In this chapter we will use this assumption to derive the maximum
likelihood estimate (MLE) for β. As we will see, the MLE for β is identical to the LSE
βb from the last chapter. We finish the chapter by deriving the maximum likelihood estimate
for the error variance σ
2
.
6.1 The likelihood function of a normal linear model
Informally, the maximum likelihood estimate for the parameter vector of a statistical model
is the value in the parameter space that maximises the likelihood function for a given set
of data. Thus, to define maximum likelihood estimation for normal linear models, we first
need to determine the likelihood function of such a model.
Recall that the probability density of a multivariate normal distribution Nn(µ, Σ) is given
by
fµ,Σ(y1, . . . , yn) = 1
(2π)
n/2 p
det(Σ)
exp 
−
1
2
(y − µ)
T Σ−1
(y − µ)

. (6.1)
If we interpret this as a function of the parameters µ and Σ given the data y1, . . . , yn, then
this gives the likelihood function of a normal linear model.
Theorem 6.1 (Likelihood function). The likelihood function for the normal linear model
Y ∼ Nn(Xβ, σ2
In).
is given by
L

β, σ2
| y

=
1
(2π)
n/2
(σ
2)
n/2
exp 
−
1
2σ
2
h
(y − Xβ)
T
(y − Xβ)
i
.
71
72 CHAPTER 6. MAXIMUM LIKELIHOOD ESTIMATION
Proof. This follows from equation (6.1) by setting µ = Xβ and Σ = σ
2
In.
Exercise 16:
Suppose for j = 1, . . . , n we have
Yj = β0 + β1xj1 + β2xj2 + · · · + βkxjk + ϵj ,
where ϵ1, . . . , ϵn are iid N (0, σ2
). Show that the log-likelihood function for this model is
given by
l(β0, β1, . . . , βk, σ2
| y) = −
n
2
log (2π) −
n
2
log 
σ
2

−
1
2σ
2
Xn
j=1

yi − [β0 + β1xj1 + β2xj2 + · · · + βkxjk]
2
.
Hence, for the least squares estimates bβ0,
bβ1, . . . ,
bβk we have
l(
bβ0,
bβ1, . . . ,
bβk, σ2
| y) = −
n
2
log (2π) −
n
2
log 
σ
2

−
1
2σ
2 D,
where D is the deviance or residual sum of squares of the fitted model.
6.2 The MLE for the parameter vector
Theorem 6.2 (MLE for the parameter vector). The maximum likelihood estimate (MLE)
for the parameter vector β in a normal linear model is equal to
βb =

XTX
−1
XT y.
Proof. We have
Y ∼ Nn(Xβ, σ2
In)
and so the likelihood function of the normal linear model is given by
L

β, σ2
| y

=
1
(2π)
n/2
(σ
2)
n/2
exp 
−
1
2σ
2
h
(y − Xβ)
T
(y − Xβ)
i
Hence the log-likelihood function is given by
ℓ

β, σ2
| y

= −
n
2
log (2π) −
n
2
log σ
2 −
1
2σ
2
(y − X β)
T
(y − Xβ).
= −
n
2
log (2π) −
n
2
log σ
2 −
1
2σ
2
S(β).
Thus for a given value of σ
2
, the log-likelihood function is maximised by the vector β that
minimises the residual sum of squares function S(β). We proved in Section 5.3 that S(β) is
6.3. SAMPLING DISTRIBUTION OF THE LEAST SQUARES ESTIMATOR 73
minimised by least squares estimate βb. Hence the MLE for β must be βb irrespective of the
value of σ
2
.
More formally, to obtain the maximum likelihood estimates we must solve
∂ℓ(β, σ2
| y)
∂β
= 0 and ∂ℓ(β, σ2
| y)
∂σ2
= 0
However, as
∂ℓ(β, σ2
| y)
∂β
= −
1
2σ
2
∂S(β)
∂β
the first set of equations
∂ℓ(β, σ2
| y)
∂β
= 0
produces the normal equations (5.3) from Section 5.3 which are solved by the least squares
estimate βb.
We can also find the maximum likelihood estimate for σ
2 by solving the following equation
for σ
2
:
∂ℓ(βb, σ2
| y)
∂σ2
= −
n
2σ
2
+
1
2σ
4

y − X βb
T 
y − X βb

= 0.
The solution to this equation is given by
σb
2
MLE =
1
n

y − X βb
T 
y − X βb

.
However, as we will discuss in a later in this chapter, the corresponding estimator is biased
and so we normally use a different estimator for the error variance σ
2
.
6.3 Sampling distribution of the least squares estimator
In Chapter 5 we learned how to compute the least squares estimate βb of the parameter
vector β assuming the design matrix X is of full rank. We have now shown that βb is the
maximum likelihood estimate for the parameter vector. The estimate βb is a function of the
observed response vector y given by
βb = βb(y) = (XTX)
−1XT y.
We can also use the function that defines βb(y) to define the random vector
βb(Y ) = (XTX)
−1XTY
74 CHAPTER 6. MAXIMUM LIKELIHOOD ESTIMATION
where Y is the random vector whose entries are the response random variables Y1, . . . , Yn.
The random vector βb(Y ) is called the estimator of β. Assuming the data are, in fact, a
realisation of the linear model Y = Xβ + ϵ, then the estimate βb(y) is a realisation of the
estimator βb(Y ). As the estimator is a random vector, it has distributional properties such
as an expectation and a variance-covariance matrix.
You have covered the expectation and the variance-covariance matrix of a random vector
last term, so this is pre-requisite knowledge. But, for convenience, Appendix B provides a
brief overview of the concepts that we will be using in ST231.
Consider the linear model for n units of observation with parameter vector β of dimension p
where p ≤ n:
Y = Xβ + ϵ.
As usual, we assume ϵ ∼ Nn(0, σ2In) where In is the n × n identity matrix. This then
implies that Y ∼ Nn(Xβ, σ2In).
Theorem 6.3 (Sampling distribution of the least squares/maximum likelihood estimator).
Consider the linear model Y = Xβ + ϵ where X is of full rank. Then the estimator of β
given by
βb = βb(Y ) = 
XTX
−1
XTY
has a multivariate normal distribution Np(β, σ2
(XTX)
−1
). Thus, in particular,
• E

βb

= β, that is the least squares estimator or maximum likelihood estimator of β
is unbiased;
• Var 
βb

= σ
2
(XTX)
−1
.
Proof. The proof is going to make use of the following fact. If the random vector Z has
distribution Z ∼ Nn(µ, Σ) and A is a p×n matrix of constants, then AZ ∼ Np(Aµ, AΣAT
).
Set A =

XTX
−1
XT which is a p × n matrix of constants, then
βb(Y ) = 
XTX
−1
XTY = AY .
We have Y ∼ Nn(Xβ, σ2In) and so it follows that βb(Y ) ∼ Np(AXβ, Aσ
2InAT
).
Now
AXβ =

XTX
−1
XTXβ = β
6.3. SAMPLING DISTRIBUTION OF THE LEAST SQUARES ESTIMATOR 75
and
Aσ
2
InAT = σ
2AAT
= σ
2

XTX
−1
XT
h XTX
−1
XT
iT
= σ
2

XTX
−1
XTX

XTX
−1
T
using (BC)
T = C
T BT
= σ
2

XTX
−1
XT X

XTX
T
−1
since (B−1
)
T = (BT
)
−1
= σ
2

XTX
−1
XTX

XTX
−1
as XTX is symmetric
= σ
2

XTX
−1
.
Therefore, βb(Y ) ∼ Np(β, σ2
(XTX)
−1
).
A note on notation
Above we have used the common notational shorthand of referring to βb(Y ) as just βb.
Previously we have used the same shorthand for βb = βb(y). It is common practice in the
linear models literature not to distinguish in notation between the estimator and the estimate.
Therefore you will need to be attentive to when we refer to an estimator and when to an
estimate as this will not be necessarily indicated by the notation. Also please note that
this notational shortcut may not apply in other modules.
Exercise 17 Consider the simple linear regression model
Yj = β0 + β1xj + ϵj for j = 1, . . . , n.
a. From first principles, that is using what you know about E(Yj | X = xj ) where X
is the explanatory variable, derive the expectation of the least squares estimators
bβ0 = bβ0(Y ) and bβ1 = bβ1(Y ).
b. Let X denote the design matrix of the above model. Using the result that Var(βb) =
σ
2
(XTX)
−1 derive Var(
bβ0), Var(
bβ1) and Cov(
bβ0,
bβ1).
76 CHAPTER 6. MAXIMUM LIKELIHOOD ESTIMATION
6.4 Unbiased estimator for the error variance
Recall that earlier we derived the maximum likelihood estimate for the error variance σ
2
given by
σb
2
MLE(y) = 1
n

y − X βb(y)
T 
y − X βb(y)

=
1
n
(y − yb)
T
(y − yb).
We can define the corresponding estimator as
σb
2
MLE(Y ) = 1
n

Y − X βb(Y )
T 
Y − X βb(Y )

.
However, it can be shown that this estimator is biased as
E
h
σb
2
MLE(Y )
i
=
n − p
n
σ
2
.
We will not prove this formally in ST231, but you have seen a similar result in other modules
when estimating the variance of an iid sample z1, . . . , zn. We use σb
2 =
1
n−1
Pn
i=1(zi − z¯)
2
rather than σb
2 =
1
n
Pn
i=1(zi − z¯)
2
to ensure that the corresponding estimator is unbiased. In
this example we use one parameter estimate (the mean) in the calculation of the estimate of
the error variance. Therefore, to ensure unbiasedness of the corresponding estimator, we
divide by n − 1 rather than n.
For normal linear models we use p parameter estimates to produce an estimate of the mean
of the response vector yb and then use this in the calcuation of the estimate of the error
variance. Therefore, to ensure unbiasedness of the corresponding estimator, we divide by
n − p rather than n.
We summarise this in the following theorem:
Theorem 6.4 (Unbiased estimator for the error variance). Let βb = bβ(Y ) be the estimator
for β. Then
s
2
(Y ) = 1
n − p
(Y − Xβb)
T
(Y − Xβb)
is an unbiased estimator for σ
2
.
We will not prove this theorem in the lectures, but for those interested a proof can be found
in Appendix C. The proof makes use of properties of the so-called hat matrix which is the
subject of the next chapter.
Remark
Let
βb = βb(y) = (XTX)
−1XT y
6.5. SUMMARY OF CHAPTER 6 77
be the least squares estimate of β. Then we use
s
2 = s
2
(y) = 1
n − p
(y − Xβb)
T
(y − Xβb)
as an estimate for σ
2
. We refer to this estimate as the unbiased estimate of σ
2
, (although
strictly bias is a notion that applies to an estimator and not an estimate.) Note that
s
2
(y) = 1
n − p
Xn
i=1
(yi − ybi)
2 =
ResidSS
n − p
,
where ResidSS is the residual sum of squares or deviance of the fitted model. The quantity
ResidSS/(n − p) is also commonly known as the residual mean square and will feature
later on when we discuss ANOVA, that is the analysis of variance.
Exercise 18 Consider the linear model with
E[Y1 | X1 = x1] = β1 E[Y2 | X2 = x2] = β2 E[Y3 | X3 = x3] = β1 − β2.
(a) Determine the design matrix X and the estimator βb.
(b) Given that y = (6, 9, 3)T
, compute an estimate of β and an unbiased estimate of σ
2
.
6.5 Summary of Chapter 6
Consider the normal linear model Y = Xβ + ϵ with p parameters and design matrix X of
rank p. Assume ϵ ∼ Nn(0, σ2In).
• The least squares estimate βb = βb(y) = 
XTX
−1
XT y is also the maximum likelihood
estimate (MLE) for β.
• The least squares estimator or maximum likelihood estimator of β is defined
as
βb = βb(Y ) = (XTX)
−1XTY
and satisfies βb(Y ) ∼ Np(AXβ, Aσ
2InAT
).
• The maximum likelihood estimate for σ
2
is given by σb
2
MLE =
1
n

y − X βb(y)
T 
y − X βb(y)

but the corresponding estimator is biased.
• The unbiased estimate for σ
2
is given by s
2 =
1
n−p

y − X βb(y)
T 
y − X βb(y)

and
the corresponding estimator is unbiased.
After studying this chapter you should
• be able to derive the maximum likelihood estimate for β and σ
2
,
78 CHAPTER 6. MAXIMUM LIKELIHOOD ESTIMATION
• know the definition of the least squares/maximum likelihood estimator βb(Y );
• be able to derive the distribution including the expectation and variance of βb(Y );
• be aware that the maximum likelihood estimator for σ
2
is biased,
• be able to compute an unbiased estimate of σ
2
for a given fitted linear model.
If you wish to engage in further (optional!) reading, consider
• A Modern Approach to Regression with R (2009) by Sheather:
Chapter 2 pages 15 - 19, Chapter 5 pages 130 - 134 and Section 6.1.2 page 154. Note
that this textbook does not distinguish between estimates and estimators.
Chapter 7
The hat matrix
In linear algebra terms, least squares regression is, in fact, an orthogonal projection. In this
chapter we explore the corresponding projection matrix, the so-called hat matrix.
7.1 Properties of the hat matrix.
Recall that the fitted values (denoted by yb) are the predicted response values for each
observation. We have
yb = Xβb = X(XTX)
−1XT y = Hy,
where the matrix H = X

XTX
−1
XT
is called the hat matrix, as it “puts the hat on
y”. More formally, H is a linear map that takes the observed values y to the fitted values yb.
The residuals (denoted by bϵ) are the estimates of the errors of a statistical model.
We have
bϵ = y − yb = y − Hy = (In − H) y.
Thus the properties of both the fitted values and the residuals are linked to the properties
of the hat matrix H.
Lemma 7.1 (Properties of the hat matrix). Let H = X

XTX
−1
XT
be the hat matrix,
then
(i) H is a symmetric idempotent matrix, that is HT = H and H2 = H.
(ii) (In − H) is symmetric idempotent.
(iii) tr (In − H) = n − p = rank (In − H).
79
80 CHAPTER 7. THE HAT MATRIX
Proof.
(i) We have
HT =

X(XTX)
−1XT
T
= (XT
)
T

(XTX)
−1
T
XT
= X

(XTX)
T
−1
XT = X

XTX
−1
XT = H.
Moreover,
H2 = X

XTX
−1
XTX

XTX
−1
XT = H.
Therefore the hat matrix is symmetric and idempotent.
(ii) As both In and H are symmetric, it follows that (In − H) is a symmetric matrix.
Moreover, as H2 = H
(In − H)
2 = I
2
n − InH − HIn + H2 = In − H − H + H = In − H
and so (In − H) is idempotent.
(iii) Recall that for a square matrix A the trace tr(A) is equal to the sum of its diagonal
elements and that for conformable matrices A and B we have tr(AB) = tr(BA).
Thus,
tr(H) = tr 
X

XTX
−1
XT

= tr 
XTX
−1
XTX

= tr(Ip) = p
Therefore
tr(In − H) = tr(In) − tr(H) = n − p.
Finally note that for an idempotent matrix its rank is equal to its trace.
The hat matrix will feature in some of the theory that we derive in the coming chapters. In
particular we will find that its diagonal elements play a special role, such much so that they
deserve their own name.
Definition 7.1 (Leverage). Let H = [hij ]n×n
denote the hat matrix. Then its ith diagonal
element hii is called the leverage of the ith data point.
Lemma 7.2 (Properties of leverages). Consider a linear model with p parameters and n
units of observation. Let hii denote the leverage of the ith observation. Then
(i) 0 ≤ hii ≤ 1.
(ii) 1
n
Pn
i=1 hii =
p
n
.
7.2. PROPERTIES OF THE RESIDUALS AND OF THE FITTED VALUES 81
Proof.
Proof of (i): Left as an exercise.
Proof of (ii): Follows directly from Lemma 7.1 (iii), that is tr(H) = p.
Exercise 19 Let hii be the ith leverage. Show that 0 ⩽ hii ⩽ 1 for all i = 1, . . . , n.
Hint: Determine an expression for the diagonal elements of H2
and then use the fact that
the hat matrix is idempotent and symmetric to show that
hii = h
2
ii +
X
k̸=i
hik.
7.2 Properties of the residuals and of the fitted values
As we did with βb we can consider fitted values and residuals as random vectors by replacing
the observed response vector y with the vector of response random variables Y . This allows
us to consider distributional properties such as variances and covariances.
7.2.1 Variance of the fitted values and of the residuals
Let H be the hat matrix and define the random vector of fitted values as
Yc = HY .
Recall that, because the hat matrix is a function of the design matrix, it is a matrix of
constants. Furthermore, as it is both idempotent and symmetric we have
HHT = HH = H2 = H.
Finally note that
HX = X

XTX
−1
XTX = X.
We can now compute the expectation and variance of Yc as follows.
E

Yc

= E (HY ) = HE (Y )
= HXβ = Xβ and
Var 
Yc

= Var (HY ) = HVar (Y ) HT
= Hσ
2
InHT = σ
2HHT = σ
2H.
Thus if we consider an individual unit of observation, say i, we have
Var 
Y
b
i

= σ
2hii
82 CHAPTER 7. THE HAT MATRIX
where hii is the ith diagonal element of the hat matrix, also called the ith leverage. As
hii ≤ 1 it follows that
Var 
Y
b
i

≤ σ
2 = Var (Yi).
Similarly define
Eb = (In − H)Y
as the random vector whose observations correspond to the residual vector bϵ. Then we have
E

Eb

= E ((In − H)Y ) = (In − H)E (Y )
= (In − H)Xβ = Xβ − HXβ = Xβ − Xβ = 0.
To compute the variance of the residual random vector recall that (In − H) is idempotent
and symmetric and so we can show that
Var 
Eb

= Var
(In − H)Y

= σ
2
(In − H).
Therefore, for an individual unit of observation, say i, we have
Var 
Eb
i

= σ
2
(1 − hii).
Hence, as hii approaches 1, the variance of the ith residual tends to zero. As the expectation
of the ith residual is zero, this implies that the ith residual tends to zero. This in turn
implies that when hii approaches 1, then the fitted value yˆi will tend to yi
.
Exercise 20 Let hii be the ith leverage. Show that the variance of the ith residual is given
by
Var(Eb
i) = σ
2
[In − H]
ii = σ
2
(1 − hii).
7.2.2 Residual plots
Recall that previously we considered residual plots which were scatterplots of the residuals
ϵbi against the fitted values ybi
. We plot ϵbi against ybi and not yi because, assuming the model
assumptions hold, Cov(Eb
i
, Y
b
i) = 0, whereas Cov(Eb
i
, Yi) ̸= 0 in general.
We start by considering the covariance matrix of the vector of residuals Eb = (In − H)Y
and the vector of response variables Y .
Cov(Eb,Y ) = Cov
(In − H)Y ,Y

= (In − H)Cov(Y ,Y )
= (In − H)Var(Y ) = (In − H)σ
2
In
= σ
2
(In − H) ̸= 0n×n.
where 0n×n denotes the n × n matrix whose entries are all equal to zero.
7.3. SUMMARY OF CHAPTER 7 83
In contrast we have that
Cov(Eb,Yc) = Cov((In − H)Y , HY ) = (In − H)Cov(Y ,Y )HT
= (In − H)Var(Y )H = (In − H)σ
2
InH
= σ
2
(H − H2
) = 0n×n.
7.3 Summary of Chapter 7
Consider the linear model Y = Xβ + ϵ with p parameters and design matrix X of rank p.
• The hat matrix H is defined as
H = X

XTX
−1
XT
and is symmetric idempotent and of rank p.
The matrix (In − H) is also symmetric idempotent but of rank n − p.
• Define the random vector of fitted values as
Yc = HY
and the random vector of residuals as
Eb = (In − H)Y ,
then
Var(Yc) = σ
2H
Var(Eb) = σ
2
(In − H)
Cov(Eb,Yc) = 0n×n
Cov(Eb,Yc) ̸= 0n×n in general.
After studying this chapter you should
• know the definition of the hat matrix H;
• be able to derive the properties of H and (In − H);
• be able to derive the variances and covariances of the residuals and the fitted values.
Chapter 8
Unusual observations
A dataset may contain data points that have a much stronger influence on the fitted model
than others. If we remove these data points from the dataset, then this causes a substantial
change to the fitted model. In this chapter we begin by considering leverages, which we
introduced in the last chapter, and then discuss regression outliers, that is data points with a
large positive or negative residual. The leverage of a data point together with the magnitude
of its residual determine how influential the point is. We introduce the Cook’s distance
which is a measure of influence that can be used to identify influential data points and then
finish the chapter by discussing how to handle such data points.
8.1 Leverages
Let H =
h
hiji
n×n
be the hat matrix which we introduced in the last chapter.
Recall that yb = Xβb = Hy and so, for i = 1, . . . , n,
ybi =
Xn
j=1
hijyj = hiiyi +
Xn
j=1
j̸=i
hijyj .
Thus the amount of weight that the observed value yi has when computing the fitted value
ybi
is given by hii, that is the leverage of the ith data point. This is consistent with our
observation at the end of Section 7.2.1 where we noted that as hii approaches 1, the fitted
value yˆi tends to yi
.
Example For simple linear regression (with an intercept term), we can show that
hii =
1
n
+
(xi − x¯)
2
Pn
j=1
(xj − x¯)
2
, for i = 1, . . . , n.
84
8.1. LEVERAGES 85
Here xi
is the ith observation of the explanatory variable and x¯ is the sample mean of the
explanatory variable. This implies that the most influential observations (the ones with the
highest leverages) are those that are furthest from x¯. More generally, data points have high
leverage if the values of their explanatory variables are unusual.
Because observations with high leverage may have a large influence on the fitted model, we
should investigate any data points with relatively large leverage. In the last chapter we
showed that the average leverage is equal to p/n and so, as a rule of thumb, we tend to
examine data points with leverage greater than 2p/n. Here, as usual, p is the dimension of
the parameter vector β and n is the number of data points.
We can use the command influenceIndexPlot(model, vars="hat") from the car package
to plot the leverages for a given model. Leverages are also referred to as hat-values as they
are derived from the hat matrix. As an example consider again the mammals dataset from
the MASS package where we fitted a regression of the logarithm of average brain weight on
the logarithm of average body weight. Figure 8.1 shows an index plot of the leverages for
this example. The rule of thumb threshold is indicated by a (red) horizontal line.
Figure 8.1: Index plot of the leverages for the regression of logarithm of average brain weight
on logarithm of average body weight for the mammals dataset.
The four data points that are flagged up as having leverages greater than the rule of thumb
are the two heaviest and the two lightest mammals in the dataset.
Exercise 21 Show that for a simple linear regression we have
hii =
1
n
+
(xi − x¯)
2
Pn
j=1
(xj − x¯)
2
for i = 1, . . . , n.
Hint: Parametrise the model as Yi = β0 + β1(xi − x¯) + ϵi as this simplifies the matrix
calculations.
86 CHAPTER 8. UNUSUAL OBSERVATIONS
8.2 Outliers
Here we consider regression outliers, that is response values that do not fit the current
model or, in other words, response values yi with a large positive or negative standardised
residual ri
. A standardised or internally studentised residual ri
is derived from the
corresponding raw residual by dividing the raw residual by an estimate of its standard
deviation. Recall that the ith residual has variance Var(Eb
i) = σ
2
(1 − hii). Then, for
i = 1, . . . , n we define the ith standardised residual ri as
ri =
ϵbi
s(1 − hii)
1/2
.
where s
2
is the unbiased estimate of the error variance σ
2
. For small to moderately sized
datasets we investigate any data points with |ri
| > 2. For large datasets, this may lead to a
substantial number of data points being flagged up, so there it is common to increase the
threshold to 3.
As an example consider again the regression of the logarithm of average brain weight on the
logarithm of average body weight for the mammals dataset. Figure 8.2 shows an index plot
of the absolute values of the standardised residuals. We observe that most observations have
a standardised residual with modulus less than 2. There are, however, three observations
that have an absolute standardised residual that is larger than 2, namely the observation for
human, for water opposum and for rhesus monkey. But keep in mind that we do expect
a small number of outliers for a reasonably sized dataset even if the model fits well. Also,
none of the observations has a standardised residual with a modulus larger than 3.
Figure 8.2: Index plot of the absolute standardised residuals for the simple linear regression
of log(average brain weight) on log(average body weight) for the mammals data.
8.3. INFLUENCE 87
8.3 Influence
A data point whose removal causes a large change to the fitted model is called an influential
observation. Influential points will usually be outliers or have high leverage or both. And
so this is why one of the default diagnostic plots in R is the “Residuals vs Leverage” plot.
Note however, that an outlier is not necessarily influential and, similarly, a data point with
high leverage may not be influential.
The R command plot(model, 5) produces the “Residuals vs Leverage” plot. Figure 8.3
shows an example of such a plot for the regression of logarithm of average brain weight on
logarithm of average body weight for the mammals dataset. This flags up three data points,
namely the observations for human, water opposum and musk shrew. However, none of
the observations lie close to the 0.5 contours of the Cook’s distance which are displayed as
dashed curves. The Cook’s distance is a measure of influence which we will discuss next.
Figure 8.3: Residuals versus leverage plot for the regression of logarithm of average brain
weight on logarithm of average body weight for the mammals data
8.3.1 Cook’s distance
The Cook’s distance takes into account both the response values as well as the input variates.
It is a measure of the influence that an observation has on all of the fitted values, and is
defined as
Di =
(yb(i) − yb)
T
(yb(i) − yb)
ps2
=
(βb
(i) − βb)
TXTX(βb
(i) − βb)
ps2
where yb(i) and βb
(i) are the fitted values and parameter estimates, respectively, computed
from the dataset with the ith observation removed. Thus yb(i) and βb
(i) are estimates that
88 CHAPTER 8. UNUSUAL OBSERVATIONS
are not influenced by the ith observation. As before, s
2
is the unbiased estimate of the error
variance σ
2 based on the model fitted to the full dataset.
It is possible to compute yb(i) and βb
(i) without having to explicitly fit a model after removing
the ith observation, thus reducing greatly the computational burden of this approach.
Furthermore, one can show that
Di =
ϵbi
2
ps2
×
hii
(1 − hii)
2
=
r
2
i
p
×
hii
(1 − hii)
where ri
is the standardised residual for the ith observation. Thus the Cook’s distance Di
for the ith data point is a product of two terms. One term, r
2
i
/p, quantifies the influence
of the ith data point due to its response value while the other, hii/(1 − hii) quantifies the
influence due to its predictor values.
The removal of a data point with a large Di
leads to a substantial change in the fitted model,
but what values for Di should be considered as large?
The Cook’s distance has an approximate F-distribution on p and n − p degrees of freedom.
Hence if Di
is greater than Fp,n−p(0.5), the median of the F-distribution on p and n − p
degrees of freedom, then the ith observation is normally flagged as influential. However, even
if Di ≤ Fp,n−p(0.5) for all observations, we tend to examine any data point whose Cook’s
distance is substantially larger than for most of the remaining observations.
Figure 8.4: Index plot of the Cook’s distances for the regression of logarithm of average
brain weight on logarithm of average body weight for the mammals dataset.
8.4. WHAT TO DO WITH INFLUENTIAL DATA POINTS 89
Figure 8.4 shows an index plot of the Cook’s distances for the regression of logarithm of
average brain weight on logarithm of average body weight for the mammals dataset. We note
that the observation for Human has by far the largest Cook’s distance. Given what we know
about humans, it is not surprising that this observation is flagged up as being unusual.
8.4 What to do with influential data points
It is very tempting to remove data points that have a high influence on the fitted model.
However, in Statistics we tend to be very hesitant to remove any data points unless we have
a strong justification for doing so. Such a justification might be that the data point is clearly
a data entry error or that it is clearly from another population than the one of interest.
If we cannot remove the observations, then what are the other options? A common approach
is to present the results based on the complete dataset and, in addition, present the results
based on the dataset with the influential data points removed. This is then transparent
about the effect that these data points have on the analysis.
Note that transformations may reduce the influence of influential data points. For example
in the mammals dataset, the elephant species are highly influential when using the original
variables, but are no longer influential when we consider the model based on log-transformed
variables.
A more elegant method to dealing with influential points is robust regression which provides
a systematic approach to down-weighting influential points when fitting the model. We will
not discuss robust regression any further in this module, but if you are interested, there is a
little more information in Appendix D.
8.5 Summary of Chapter 8
• Let H =
h
hiji
n×n
be the hat matrix, then for i = 1, . . . , n,
ybi =
Xn
j=1
hijyj = hiiyi +
Xn
j=1
j̸=i
hijyj .
We call hii the leverage of the ith unit of observation.
Rule of thumb: Investigate if hii > 2p/n.
• The ith standardised or internally studentised residual ri
is defined as
ri =
ϵbi
s(1 − hii)
1/2
for i = 1, . . . , n,
where s
2
is an unbiased estimate of the error variance σ
2
.
Rule of thumb: Investigate if |ri
| > 2 or |ri
| > 3.
90 CHAPTER 8. UNUSUAL OBSERVATIONS
• The Cook’s distance is a measure of influence that is defined as
Di =
(yb(i) − yb)
T
(yb(i) − yb)
ps2
for i = 1, . . . , n,
where the yb(i)
’s are the fitted values using the dataset with the ith observation removed.
Rule of thumb: Investigate if Di > Fp,n−p(0.5).
• A data point being influential is, per se, not a sufficient justification to remove it from
the dataset. Failing such a justification we report the results of the analysis both for
the complete dataset and for the dataset with the influential data points removed. An
alternative approach not discussed in the module is robust regression.
After studying this chapter you should
• know what is meant by a high leverage data point, a regression outlier and an influential
data point;
• be able to define leverage, standardised residuals and the Cook’s distance;
• be familiar with various plots illustrating the above measures and know the associated
rules of thumb;
• be aware of options on how to handle influential data points.
If you wish to engage in further (optional) reading on the topics covered in this chapter, then
the following sections of the textbooks recommended for this module cover related material.
• Generalized linear models with examples in R (2018) by Dunn and Smyth:
Sections 3.6, 3.7 and 3.13.
• Data analysis and graphics using R: an example-based approach (2010) by Maindonald
and Braun: Section 6.3.1.
Chapter 9
Categorical predictor variables
So far we have only considered quantitative predictor variables, but it is common for
statistical models to also include qualitative explanatory variables. Qualitative variables are
called categorical predictor variables or factors. Examples of such qualitative explanatory
variables are gender or the degree course of an undergraduate student in the Department of
Statistics. The values that a categorical predictor can take are referred to as levels. For
example, the categorical variable degree course may have levels Data Science, MathStat
and MORSE.
9.1 Retail data: including brand as a predictor variable
In this section we will consider models for the Retail dataset which include the categorical
predictor variable brand. While for qualitative variables there is generally no concept of
distance between the different categories, we may still want to model that the average
response is dependent on the value that the categorical predictor takes.
For example, in the Retail data, it might be that one store brand is much more popular
than the others, and so we might expect the sales volumes of stores from this brand to be
higher, on average, than that of the other brands. Let’s investigate this for the Retail
dataset. As usual, it is best to start by visualising the relevant data. Comparative boxplots
can be used to illustrate the relationship between a categorical variable and a quantitative
variable. We use a command of the form
boxplot(response ~ catPredictor, data = data)
to produce a separate boxplot of the variable response for each level of the categorical
variable catPredictor. I have done this in Figure 9.1 for the variable sales in the Retail
dataset, where the boxplots are grouped by brand. We observe that Brand C stores tend to
have a larger sales volume than the stores of the other two brands.
91
92 CHAPTER 9. CATEGORICAL PREDICTOR VARIABLES
Figure 9.1: Comparative boxplots of sales grouped by brand
In the boxplots the median is shown as a horizontal line. However, regression models are
concerned with modelling the mean, so let’s consider the average sales volume for each
brand. Rounded to full units, the sample mean of sales for stores of Brand A is 109,679
units, for Brand B 105,728 units and for Brand C 126,680 units and so the average sales
volume of Brand C stores is substantially higher than that of the other two brands. This
suggests potential merit in including brand as an explanatory variable into a statistical
model that predicts the sales volume.
Let’s investigate this further. In Section 1.3 we considered a simple linear regression of
sales on price.
1
In Figure 9.2 I have plotted the fitted regression line onto the scatterplot
of sales against price. However, this time I have colour-coded the datapoints (and used
different plotting symbols) according to brand. If we study the plot carefully, we notice that
the observations for Brand A and Brand B tend to lie below the fitted regression line. In
contrast, Brand C observations tend to lie above the line. This systematic pattern suggests
that brand, in addition to price, is useful when predicting the sales volume of a retail store.
1We also fitted a model with both price and advert as predictor variables, but to keep the discussion as
simple as possible, we consider the simple linear regression model here.
9.1. RETAIL DATA: INCLUDING BRAND AS A PREDICTOR VARIABLE 93
Figure 9.2: Scatterplot of sales against price with coding according to brand. The plot also
shows the line of best fit.
To explain the pattern observed in Figure 9.2, we may consider the following model:
Salesj =



µA + β pricej + ϵj if the jth store is of Brand A
µB + β pricej + ϵj if the jth store is of Brand B
µC + β pricej + ϵj if the jth store is of Brand C
(9.1)
where j = 1, . . . , 96. We can write the model equations in a more compact notation that
resembles the usual model equations as
Salesj = µbrand(j) + β pricej + ϵj , j = 1, . . . , 96.
Here brand(j) denotes the brand of the jth store.
The systematic component of this model is defined as three lines, one for each store brand.
The slopes of the lines are all equal to β so the effect of price on the sales volume is assumed
to be the same for all three store brands. As the lines are parallel we will refer to this as
the “parallel lines model”. However note that the intercepts of the three lines may differ
according to brand. Also notice that in the model there is no overall intercept, that is there
is no parameter that does not correspond to one of the explanatory variables. We are able
to implement this model in R by forcing the model intercept term to be equal to zero2
. We
use the command:
brand.model1 <- lm(sales ~ 0 + brand + price, data = Retail)
The resulting output provides four estimates, one for the quantitative predictor price, and
thus an estimate for β, and furthermore one for each brand and thus estimates for µA, µB
and µC respectively:
2We have seen the same approach in Exercise 3 which considered regression through the origin.
94 CHAPTER 9. CATEGORICAL PREDICTOR VARIABLES
brandA brandB brandC price
192.246 185.941 210.873 -0.539
As a visualisation, in Figure 9.3, I have added the three fitted regression lines to a scatterplot
of sales against price.
Figure 9.3: Scatterplot of sales against price coded according to brand. The plot also shows
the three brand-specific lines of best fit from the parallel lines model. The fitted lines are
parallel but shifted in the vertical direction.
Recall that in Figure 9.2 we observed a systematic pattern, namely that most of the Brand
A and Brand B datapoints were lying below the fitted regression line while the Brand C
datapoints tended to lie above the line of best fit. This issue has improved in Figure 9.3
where datapoints for each brand appear scattered more evenly above and below the line of
best fit for the respective brand. The three fitted regression lines are parallel as they have
the same slope given by the estimate of β.
If we write out the equations for the fitted regression lines3 we obtain for j = 1, . . . , 96
sales \j =



µˆA + βˆ pricej = 192.246 − 0.539 pricej
if store j is of Brand A,
µˆB + βˆ pricej = 185.941 − 0.539 pricej
if store j is of Brand B,
µˆC + βˆ pricej = 210.873 − 0.539 pricej
if store j is of Brand C.
(9.2)
Hopefully, you have now gained a sufficient understanding of the fitted model so that we can
move to its interpretation. We will omit an interpretation of µˆA, µˆB and µˆC as this would
require setting price to zero (and thus an extrapolation). Recall that when interpreting
partial coefficients we assume that all other explanatory variables in the model are held
fixed and so we can interpret the estimate for β as follows (keeping in mind that sales is
recorded in thousands):
3Recall that we use hats to indicate that a estimates.
9.1. RETAIL DATA: INCLUDING BRAND AS A PREDICTOR VARIABLE 95
The estimated coefficient for price is -0.539 and so, according to the fitted model,
for a given store brand (i.e. keeping the store brand fixed), we expect an average
reduction in sales volume by 1000 x 0.539 = 539 units for every additional pound
charged per unit.
Recall how the design matrix defines a linear model via the equation
Y = Xβ + ϵ.
Let’s consider the design matrix X for this model. We can rewrite the model equations in
(9.1) using indicator variables. Define
xjA =
n
1 if store j is of Brand A
0 otherwise, xjB =
n
1 if store j is of Brand B
0 otherwise, xjC =
n
1 if store j is of Brand C
0 otherwise.
Note that xjA = 1−xjB −xjC, so technically we only need two indicator variables to encode
the factor brand but here we also defined xjA to make the exposition clearer. More generally
for a factor variable with d levels we need d − 1 indicator variables to encode the factor
variable. The indicator variables are also referred to as dummy variables.
Using the above indicator variables, we have for j = 1, . . . , 96
Salesj = µA xjA + µB xjB + µC xjC + β pricej + ϵj . (9.3)
Note that we have n = 96 units of observation and four parameters µA, µB, µC and β, thus
the design matrix X needs to have 96 rows and 4 columns. Furthermore, we have






Sales1
Sales2
.
.
.
Sales96






= X





µA
µB
µC
β





+






ϵ1
ϵ2
.
.
.
ϵ96






Hence, to produce (9.3) the design matrix needs to be
X =






x1A x1B x1C price1
x2A x2B x2C price2
.
.
.
.
.
.
.
.
.
.
.
.
xnA xnB xnC pricen






,
where n = 96.
We can verify that R uses the design matrix above by examining the output of the command
model.matrix(brand.model1). Below we have printed the first 10 rows of the design matrix
pre-pended by a column which gives the brand of each respective unit of observation.
We observe that the first, second and third column of the design matrix respectively have
only zero/one entries corresponding to the values taken by the dummy variables we defined
earlier.
96 CHAPTER 9. CATEGORICAL PREDICTOR VARIABLES
brand brandA brandB brandC price
A 1 0 0 148.3
B 0 1 0 154.7
C 0 0 1 156.4
B 0 1 0 139.2
A 1 0 0 159.6
A 1 0 0 158.0
B 0 1 0 143.6
C 0 0 1 164.0
B 0 1 0 147.3
B 0 1 0 133.0
9.2 Alternative parameterisations
In this section we will discuss two alternative parameterisations of the parallel lines model.
The first re-parameterisation is similar to the one for the diamond ring example where
we used the deviations from the mean rather than the original values of the quantitative
predictor. So suppose for j = 1, . . . , 96
c_pricej = pricej − price,
where price = 152.76 is the sample mean of price. We then define the model equations as
Salesj =



µA + β c_pricej + ϵj if the jth store is of Brand A
µB + β c_pricej + ϵj if the jth store is of Brand B
µC + β c_pricej + ϵj if the jth store is of Brand C
(9.4)
for j = 1, . . . , 96. This can be fitted in R as follows:
Retail$c_price <- Retail$price - mean(Retail$price)
brand.model2 <- lm(sales ~ 0 + brand + c_price, data=Retail)
round(coef(brand.model2), 3)
brandA brandB brandC c_price
109.922 103.617 128.549 -0.539
We can use the above output to write out the equations for the fitted regression lines noting
that c_pricej = pricej − price = pricej− 152.76.
sales \j =



109.922 − 0.539 (pricej − 152.76) if store j is of Brand A,
103.617 − 0.539 (pricej − 152.76) if store j is of Brand B,
128.549 − 0.539 (pricej − 152.76) if store j is of Brand C.
(9.5)
9.2. ALTERNATIVE PARAMETERISATIONS 97
Note that, modulo some rounding errors, the above three fitted regression lines are identical
to the ones in (9.2). So this is the same fitted model, we have just used a different
parameterisation to describe it. Analogous to the diamonds example, the reparameterisation
here improves interpretability of the parameters and their estimates. The estimate for β,
the parameter for price, has not changed and can be interpreted as previously. However,
with the current parameterisation we can interpret the estimated coefficients for the brand
variable without assuming that the price of the product is equal to zero and instead assume
that the price is equal to the average price:
If the product is priced at £152.76, that is at the average price within the
observed sample, then we expect, on average, a Brand A store to sell 1000 x
109.922 = 109,922 units, a Brand B store to sell 103,617 units and a Brand C
store to sell 128,549 units.
Recall that a factor variable defines groups within the data. In our example these are the
stores of the various brands. Often, rather than being interested in the mean of the response
for the various groups per se, we are interested in differences in the mean response between
these groups. For example in a clinical trial we generally are interested in the effect of a
treatment in comparison to a control group. In our retail example, imagine you are the CEO
looking after the stores of Brand A. You might be interested in how, on average, the sales
volumes of the other brands differ from Brand A whilst also accounting for the pricing of
the product. We can do this by reparameterising the parallel lines model as follows. For
j = 1, . . . , 96 let the new parameterisation be given as
Salesj =



µ + β pricej + ϵj if the jth store is of Brand A
µ + αB + β pricej + ϵj if the jth store is of Brand B
µ + αC + β pricej + ϵj if the jth store is of Brand C.
Like the original parameterisation, this parameterisation has four parameters µ, αB, αC and
β. However, these have a different role and thus a different interpretation to the parameters
in the original parameterisation. In the latter the three parameters µA, µB and µC were the
intercepts of the regression lines for the three brands respectively. Here the parameter µ is
still the intercept for the regression line of Brand A. But now αB represents the difference
between the intercept of the regression line for Brand B and that for Brand A. Similarly, αC
represents the difference in intercept between the regression line for Brand B and that for
Brand A. Note how the focus has shifted towards differences with respect to Brand A.
To determine the design matrix for this new parametrisation we take the same approach as
earlier, namely rewrite the model equations using indicator variables. As before let
xjB =
(
1 store j is of Brand B
0 otherwise and xjC =
(
1 store j is of Brand C
0 otherwise
Then, for j = 1, . . . , 96
Salesj = µ + αB xjB + αC xjC + β pricej + ϵj .
98 CHAPTER 9. CATEGORICAL PREDICTOR VARIABLES
In this parameterisation the parameter vector is [µ, αB, αC, β]
T where µA acts as an
overall intercept term of the model. The corresponding design matrix X has 96 rows and 4
columns and is given by
X =






1 x1B x1C price1
1 x2B x2C price2
.
.
.
.
.
.
.
.
.
.
.
.
1 xnB xnC pricen






,
where n = 96. In fact, this is the default parameterisation implemented in R and can be
fitted as follows.
brand.model3 <- lm(sales ~ brand + price, data = Retail)
round(coef(brand.model3), 3)
(Intercept) brandB brandC price
192.246 -6.305 18.627 -0.539
Thus we have µˆ = 192.246, αˆB = −6.305, αˆC = 18.627 and βˆ = −0.539.
We can interpret the estimates for αB and αC as follows.
For a fixed price of the product, the model predicts that, on average, the sales
volume of a Brand B store will be 6.305 x 1000 = 6,305 units lower than of
Brand A store. Furthermore, according to the fitted model, for a fixed price of
the product, the expected sales volume of a Brand C store is estimated to be
18,627 units higher than the expected sales volume of a Brand A store.
Note that if we use the above estimates to compute the three fitted regression lines we
obtain the same lines as in (9.2). As before, this reparameterisation still defines the same
model, but the parameters used have a different meaning and thus interpretation. In this
last parameterisation, the expected sales volume of Brand B or Brand C stores are compared
to the expected sales volume of Brand A stores. In technical terms, we call the level of the
categorical predictor that the other levels are being compared to (here Brand A) the baseline
or reference category. This set-up is useful, for example, in medical statistics, where we
often compare treatment groups against a control group, that is a group that receives a
placebo treatment. Therefore, this type of parameterisation is referred to as treatment
coding.
Exercise 22 Consider a linear model for the Retail data that is based on one predictor
variable only, the categorical variable brand.
a. Give an interpretation of the parameters µA, µB and µC in the model
Salesj =



µA + ϵj if store j is of Brand A
µB + ϵj if store j is of Brand B
µC + ϵj if store j is of Brand C
9.2. ALTERNATIVE PARAMETERISATIONS 99
b. Determine the design matrix for the model in a.
c. Use R to fit the model in a.
d. Verify that R used the design matrix that you determined in b.
e. Compare the parameter estimates with the sample means of sales volume for stores of
Brand A, Brand B and Brand C respectively.
f. Next consider the following re-parameterisation of the model in a.
Salesj =



µ + ϵj if store j is of Brand A
µ + αB + ϵj if store j is of Brand B
µ + αC + ϵj if store j is of Brand C
.
Give an interpretation of the parameters αB and αC.
g. Determine the design matrix for the parameterisation in f.
h. Use R to fit the model using the parameterisation in f.
i. Verify that R used the design matrix that you determined in g.
j. What sales volume does the model you fitted in h. predict for stores of Brand C? How
does this compare to the sales volume for Brand C stores predicted by the model that
you fitted in c.?
Exercise 23: Consider the model in Exercise 22a. Show that the least squares estimate for
µ = (µA, µB, µC) is given by






µbA
µbB
µbC






=







1
nA
P
i∈SA
salesi
1
nB
P
i∈SB
salesi
1
nC
P
i∈SC
salesi







Here SA, SB and SC are the sets containing the indices of the observations that are of
Brand A, of Brand B or of Brand C respectively. Similarly, nA, nB and nC are the number
of observations that are of Brand A, of Brand B or of Brand C respectively. Hence the
least squares estimates of the parameters coincide with the sample means of sales for the
respective brands.
Exercise 24 Consider the two parameterisations of the parallel lines model given by
Salesj =



µA + β pricej + ϵj if the jth store is of Brand A
µB + β pricej + ϵj if the jth store is of Brand B
µC + β pricej + ϵj if the jth store is of Brand C
and
Salesj =



µ + β pricej + ϵj if the jth store is of Brand A
µ + αB + β pricej + ϵj if the jth store is of Brand B
µ + αC + β pricej + ϵj if the jth store is of Brand C
where j = 1, . . . , 96.
100 CHAPTER 9. CATEGORICAL PREDICTOR VARIABLES
a. State the mathematical relationship between the parameters µA, µB, µC and µ, αB, αC.
b. Use R to fit the above two parameterisations to the Retail data.
c. Check numerically that the relationship in a. is also satisfied for the corresponding estimates (modulo rounding errors) and thus that the fitted model for the first
parameterisation is identical to the fitted model for the second parameterisation.
Exercise 25 Another parameterisation of the parallel lines model is produced by changing
the reference category of the brand variable.
Use the command brand <- relevel(brand, ref="C") to change the reference category
of brand to be Brand C.
a. Write out the model equations for the parallel lines model such that Brand C is the
reference category.
b. Explain what the various parameters correspond to in terms of the three brand-specific
regression lines.
c. Write out the design matrix for this parameterisation of parallel lines model.
d. Use the command brand.model <- lm(sales ~ price + brand, data=Retail) to
fit the model in R.
e. State which estimated coefficient corresponds to which parameter estimate.
9.3 A model with an interaction
Recall that we started with a simple linear regression for the Retail data with price as the
only predictor variable. When we compared the fitted line against the scatterplot, we noted
a systematic pattern in which Brand C observations tended to lie above the fitted line and
observations of the other two brands tended to lie below the fitted line. To address this we
then considered the parallel lines model defined as
Salesj = µ + αB xjB + αC xjC + β pricej + ϵj ,
where xjB is the value of the indicator variable denoting whether the jth observation is of
Brand B, xjC is the value of the indicator variable denoting whether the jth observation is
of Brand C, and j = 1, . . . , 96. We noted that this model produced a better fit to the data
compared to the single line model. Next, let’s examine the fit more closely by producing
separate scatterplots for each store brand with the corresponding fitted regression line from
the parallel lines model as shown in Figure 9.4. We observe that while for Brand C the
regression line fits the data reasonably well, for Brand B the gradient of the regression line
seems too shallow (and similarly for Brand A).
So there might be merit in fitting three separate regression lines that differ both in intercept
and slope. We can do this by introducing extra predictor terms in the model that represent
a so-called interaction between brand and price. An interaction between two explanatory
variables is used when we want to model that the effect of one explanatory variable on
the response depends on the value of the other explanatory variable. In our case, we are
9.3. A MODEL WITH AN INTERACTION 101
Figure 9.4: Three scatterplots of sales against price, one for each store brand. The corresponding regression lines from the parallel lines model have also been added.
interested in whether the effect that price has on the sales volume depends on the brand of
the store.
We fit the model
Salesj = µ + αB xjB + αC xjC + β pricej + γA (xjB × pricej
) + γB (xjC × pricej
) + ϵj ,
where xjB and xjC are the values of the indicator variables for brand as defined earlier.
Thus, as before, we are using Brand A as the reference category.
By considering the different value combinations taken by the indicator variables, we observe
that this formulation leads to three regression lines which may differ in intercept and slope.
We have
Salesj =



µ + β pricej + ϵj if store j is of brand A
(µ + αB) + (β + γB) pricej + ϵj if store j is of brand B
(µ + αC) + (β + γC) pricej + ϵj if store j is of brand C.
(9.6)
Here γB describes the difference in slope between the regression line for Brand B and the
one for Brand A. Analogously, γC describes the difference in slope between the regression
line for Brand C and the one for Brand A.
102 CHAPTER 9. CATEGORICAL PREDICTOR VARIABLES
Let us fit this model in R and plot the resulting regression lines. To implement the interaction
in R we use a colon, that is we use a command of the form
lm(response ~ predictor1 + predictor2 + predictor1:predictor2).
So in our example
brand.model4 <- lm(sales ~ price + brand + price:brand, data=Retail)
round(coef(brand.model4), 3)
(Intercept) price brandB brandC price:brandB price:brandC
210.379 -0.657 14.873 -47.501 -0.146 0.426
Here the coefficient reported for the intercept is µb, the one reported for price is bβ, the one
reported for brandB is αbB and the one reported for brandC is αbC. Compared to the parallel
lines model, there are two additional estimated coefficients, namely, one for brandB:price
which is γbB and one for brandC:price which is γbC.
So how do we interpret these estimates? Let’s consider two of the estimates as examples.
We may interpret bβ as follows:
For a store of Brand A, we expect an average reduction in sales of 657 units with
every additional pound charged for the product.
Furthermore, we may interpret γbC as follows:
For a unit increase in price we expect the average reduction in sales of a Brand
C store to be 426 units smaller than that of a Brand A store.
Hence, on average, the reduction of sales volume due to a price increase is less for a Brand
C store than for a Brand A store.
By adding the relevant parameter estimates we obtain the following equations for the fitted
model:
sales [i =



210.379 − 0.657 × pricei
if store i is of Brand A
225.252 − 0.803 × pricei
if store i is of Brand B
162.879 − 0.231 × pricei
if store i is of Brand C.
Note how the absolute values of the estimated slopes of the regression line for Brand A and
for Brand B are larger than that for Brand C. So, according to the model, on average, Brand
A and Brand B stores are more susceptible to changes in price than Brand C stores.
Figure 9.5 provides a visualisation of the fitted model. Again, we observe that the fitted
regression lines for Brand B and for Brand A now have a steeper gradient than the one for
Brand C.
9.3. A MODEL WITH AN INTERACTION 103
Figure 9.5: A scatterplot of sales against price with coding according to brand. The plot
also shows the three lines of best fit, one for each brand, which are no longer constrained to
have the same slope.
Note that this is qualitatively a different model to the parallel lines model. In the parallel
lines model the effect of an increase in price on the expected sales volume is the same for
each brand. Moreover, the difference in expected sales volume for stores of different brands
is the same for every fixed value of price. Thus the effect of any of the two predictors on the
expected response is not dependent on the value of the other predictor.
In contrast, in the last model with an interaction, the slopes of the three regression lines
were allowed to differ. Thus, the effect of an increase in price on the expected sales volume
differs between brands. Moreover, as the three regression lines are not parallel, the difference
in expected sales volume for stores of different brands will depend on which price is being
charged for the product. Thus the effect of each of the two predictors on the expected
response is dependent on the value of the other predictor. And this type of effect is called
an interaction.
Note that here we have considered an interaction between a factor and a quantitative
predictor variable. There are many other forms of interaction which are implemented by
introducing product terms in the model equations. However, in this module we will only
consider interactions between a factor and a quantitative predictor variable.
Exercise 26 What is the design matrix for the model defined in (9.6)?
104 CHAPTER 9. CATEGORICAL PREDICTOR VARIABLES
9.4 Summary of Chapter 9
Chapter 9 introduced categorical predictors, also called factors, in linear models.
Categorical predictor variables are accommodated within the linear model framework using
indicator variables, so-called dummy variables. If a categorical predictor variable has d
levels we use d − 1 indicator variables to encode the factor. The reference category is
the level of the categorical variable for which we do not include an indicator variable in the
model and thus the level that the other categories are compared against.
We considered a model with one quantitative and one factor variable as predictors. We
started with a parallel lines model and examined different parameterisations of the same
model. We then considered a model in which the regression lines were not constrained to
have the same slope. To do this we introduced product terms into the model equations.
Including product terms of explanatory variables leads to interaction where the effect
of one explanatory variable on the response depends on the value of another explanatory
variable.
After working through this chapter you should
• know how to incorporate categorical predictors into a linear model, both theoretically
and in R;
• be able to interpret the estimated coefficients of such a model;
• know how to extend a parallel lines regression model with one categorical and one
quantitative predictor to a model with unrelated regression lines by introducing an
interaction between the factor and the quantitative predictor variable.
Below is a textbook recommendation if you wish to engage in further (optional!) reading on
the topics covered in this chapter:
• Generalized linear models with examples in R (2018) by Dunn and Smyth: Section 1.4.
Chapter 10
The T-statistic
Assuming a normal distribution for the errors in a linear model allows us to make statements
about the distributional properties of the estimators. In this chapter we use these results to
define the T-statistic. This is important groundwork for more formal statistical inference in
form of interval estimation and hypothesis testing which will be covered in the next chapters.
We start with a review of some results for the multivariate normal distribution.
10.1 Useful results for the multivariate normal distribution
In this section we present some results for the multivariate normal distribution which we will
not prove but make use of in the next section where we derive distributional properties of
the maximum likelihood estimator for β and the unbiased estimator for the error variance.
Suppose Z ∼ Nn (µ, Σ) where Σ is an n × n positive definite matrix. Let A be a q × n
matrix and B be a p × n matrix. Then the following results hold.
Lemma 10.1. AZ ∼ Nq

Aµ, AΣAT

.
Lemma 10.2. (Z − µ)
T Σ−1
(Z − µ) ∼ χ
2
n where χ
2
n
is the chi-squared distribution with n
degrees of freedom.
Lemma 10.3. Let U = AZ and V = BZ. Suppose Cov (U, V ) = 0, then U and V are
independent. Furthermore U and V
TV are independent.
10.2 Distributional properties of bβ and s
2
The following theorem is an extension of Fisher’s theorem which you will have encountered
in one of the mathematical statistics modules.
105
106 CHAPTER 10. THE T-STATISTIC
 Theorem 10.1. (Distributional properties of β and s2)
 Let Y ∼NnXβ, σ2In , where β has dimension p < n. Then
 (i) β(Y ) ∼ Np β, σ2 XTX−1 ,
 (ii) 1
 σ2
 β(Y ) −β T XTXβ(Y)−β ∼ χ2 p;
 (iii) β(Y ) and s2(Y ) are independent;
 (iv) (n −p)s2(Y )
 σ2
 ∼ χ2 n−p.
 Proof. To ease notation we use the abbreviations β = β(Y ) and s2 = s2(Y ), but do keep in
 mind that the distributional statements are about the estimators (and not the estimates).
 We will prove Statements (i)- (iii), but not Statement (iv). For those interested, the proof
 is in the Appendix C.
 Proof of (i): This was shown in the proof of Theorem 6.3.
 Proof of (ii): Note that by (i) the estimator β has a multivariate normal distribution in p
 dimensions. Moreover
 (β −β)TXTX(β−β)
 σ2
 = β−E[β] T Var(β) −1 β−E[β]
 which, by Lemma 10.2, has a χ2 p distribution.
 Proof of (iii): Let
 U = β=(XTX)−1XTY
 and
 V = Y −Xβ=Y −HY =(In−H)Y
 where H =X(XTX)−1XT is the hat matrix. Then
 Cov β, Y −Xβ =Cov[U, V]
 =Cov (XTX)−1XTY, (In−H)Y
 =(XTX)−1XTCov(Y, Y)(In −H)T
 =σ2(XTX)−1XT(In −H)T
 =σ2 (XTX)−1XT −(XTX)−1XTX(XTX)−1XT
 =0p×n.
10.3. THE T-STATISTIC
 107
 As U and V are linear combinations of Y , they have a multivariate normal distribution.
 Thus, if the covariance matrix of U and V is equal to 0p×n, then U and V are independent.
 Hence by Lemma 10.3, U = β and VTV = (n−p)s2 are also independent. Therefore
 statement (iii) holds.
 Proof of (iv): Omitted here, but can be found in Appendix C.
 10.3 The T-statistic
 In this section we shall derive the T-statistic for normal linear models. In particular, we will
 prove that
 βi(Y ) −βi
 s(Y )√
 dii
 ∼ tn−p,
 where dii is the ith diagonal element of (XTX)−1 and s(Y ) = 
s2(Y ). Here tk denotes a
 t-distribution with k degrees of freedom. As we will make use of the t-distribution we start
 with a reminder of its definition.
 Definition 10.1 (The t-distribution). Let Z ∼ N(0,1) and W ∼ χ2
 k. Furthermore assume
 that Z and W are independent. Then
 Z
 W/k ∼ tk
 that is, Z
 √
 W/k 
has a t-distribution on k degrees of freedom.
 In this section we will again be using the shorthand notation β = β(Y ) and s2 = s2(Y ).
 Theorem 10.2 (The T-statistic). Let Y ∼ Nn(Xβ,σ2In), then for a non-zero p-vector a
 we have
 T = aTβ−aTβ
 s
 aT XTX−1a 
∼ tn−p.
 Remarks
 1. Note that the distribution of T does not depend on β or σ2. We call a statistic whose
 distribution is independent of any parameters a pivot.
 2. The T-statistic concerns a univariate parameter aTβ. For example,
 • if aT = (0,...,0,1,0,...,0) is a p-vector whose entries are all zero, except for
 the ith entry which takes value 1, then aTβ = βi;
108
 CHAPTER 10. THE T-STATISTIC
 • if aT =(0,...,0,−1,0,...,0,1,0,...,0) is a p-vector whose entries are all zero,
 except for the ith entry which takes value-1 and the jth entry (j= i) which
 takes value 1, then aTβ = βj −βi;
 • if aT =(xi1,...,xip) = xT i is the ith row of the design matrix X, then aTβ = Yi.
 3. As Var(aT β) = σ2aT(XTX)−1a, the estimator s
 aT XTX−1a is an estimator
 for the standard deviation of aT β which is referred to as the standard error of aT β.
 Proof. In Section 10.2 we have shown that
 β ∼ Np β,σ2 XTX−1
 and
 (n −p)s2
 σ2
 and that β and s2 are independent.
 ∼ χ2
 n−p
 Now consider aT β where a is a p-vector of known constants.
 Lemma 10.1 and equation (10.1) imply that
 aT β ∼ N(aTβ, σ2aT(XTX)−1a),
 and so
 and, from (10.2),
 Z :=
 aT β −aTβ
 σ
 aT XTX−1a 
∼ N(0,1).
 W := (n−p)s2
 σ2
 and Z and W are independent. Note that
 W
 (n −p) = s
 σ .
 Now, let
 T :=
 Z
 W/(n−p) =
 aT β −aTβ
 ∼ χ2
 n−p
 aT β −aTβ
 (10.1)
 (10.2)
 σ
 aT XTX−1a 
× σ
 s =
 s
 aT XTX−1a
 then from the definition of the t-distribution (Definition 10.1), we have T ∼ tn−p.
10.4. A SIMPLE EXAMPLE IN R
 109
 Corollary 10.1. By setting aT = (0,...,0,1,0,...,0), the unit vector with a 1 in the ith
 position, we can deduce that
 βi −βi
 s√
 dii
 ∼ tn−p,
 where dii is the ith diagonal element of (XTX)−1 and s = 
√
 s2.
 Exercise 27 Using Theorem 10.2 prove Corollary 10.1 above.
 10.4 A simple example in R
 Consider the following small dataset:
 x
 y
 4.6
 87.1
 5.1
 93.1
 4.8
 89.8
 4.4
 91.4
 5.9
 99.5
 4.7
 92.1
 5.1
 95.5
 5.2
 99.3
 4.9
 98.9
 5.1
 94.4
 Fitting a simple linear regression model
 Yi
 = β1+β2xi+ϵi
 for i = 1,...,10
 to the data produces the following summary output:
 Call:
 lm(formula = y ~ x)
 Residuals:
 Min
 1Q Median
 3Q
 Max-4.1966-1.7792-0.2677 1.3135 5.3823
 Coefficients:
 Estimate Std. Error t value Pr(>|t|)
 (Intercept)
 x--
57.240
 7.404
 12.495 4.581 0.0018 **
 2.501 2.960 0.0181 *
 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 Residual standard error: 3.1 on 8 degrees of freedom
 Multiple R-squared: 0.5227,
 Adjusted R-squared: 0.4631
 F-statistic: 8.762 on 1 and 8 DF, p-value: 0.01815
 The R output contains three sections: the preamble lists the function call and summary
 statistics of the residuals. In the middle section there is a table with information about the
110
 CHAPTER 10. THE T-STATISTIC
 estimated coefficients. The bottom part (after the explanation of the asterisk notation for
 significance codes) there are summary statistics for the fitted model.
 The bottom part of the output lists Residual standard error: 3.1 on 8 degrees of
 freedom. The residual standard error is an estimate of the standard deviation of the errors,
 more precisely, it is the square-root of the unbiased estimate s2 = s2(y) of the error variance.
 Here we have s = 3.1. We have learned that s2(Y ) has a χ2 distribution with n−p degrees
 of freedom. Here n = 10 and p = 2 and so n−p = 8 is listed as the degrees of freedom for
 the residual standard error.
 The middle portion of the table has information about the estimated coefficients which
 is presented in five columns. The first column lists which parameter is being considered.
 The second column gives an estimate for that parameter. For example, the estimate of the
 slope is β2(y) = 7.404. The third column gives the standard error for each coefficient. As
 discussed in the last section, the standard error for βi is given by s√dii where dii is the
 ith diagonal element of (XTX)−1 and X is the design matrix. In the above example the
 standard error of the estimator for the slope is reported as 2.501.
 10.5 Summary of Chapter 10
 • β(Y)∼Np β, σ2 XTX−1 .
 • (n−p)s2(Y)
 σ2
 ∼χ2 n−p.
 • β(Y) and s2(Y) are independent.
 • The T-statistic for aTβ is defined as
 T = aTβ(Y)−aTβ
 s(Y )
 aT XTX−1a
 and has a t-distribution with n − p degrees of freedom.
 After studying this chapter you should
 • know how the T-statistic is defined and be able to derive its distribution;
 • knowthedistributions of β(Y ) and s2(Y ) and be able to show that these two estimators
 are independent of each other.
Chapter 11
 Interval estimation
 So far we have considered point estimates for β. An alternative are interval estimates in
 form of confidence intervals. As in the previous chapter we will be using the shorthand
 notation β = β(Y ) and s2 = s2(Y ).
 11.1 Confidence intervals
 Definition 11.1 (Confidence Interval). Let Y be a random vector whose distribution
 depends on the parameter β ∈ Θ and let ϕ(β) be a real-valued function of β.
 Suppose we have a pair of statistics L(Y ) and U(Y ) such that L(y) ≤ U(y) for all y.
 Then the interval given by
 L(y), U(y)
 is called a 100(1 − α)% confidence interval if
 inf
 β∈Θ
 Pβ L(Y) ≤ ϕ(β) ≤ U(Y) = 1−α.
 Theorem 11.1 (Confidence intervals for normal linear models). Let β(y) be the least squares
 estimate of β. Then a 100(1−α)% confidence interval for aTβ is given by
 aT β(y) − tn−p(1− α
 2)s(y)
 aT(XTX)−1a, aTβ(y) + tn−p(1− α
 2)s(y)
 aT(XTX)−1a
 where tn−p(1 − α
 2) is the 100(1 − α
 2)th quantile of the t-distribution with n − p degrees of
 freedom.
 111
112
 CHAPTER 11. INTERVAL ESTIMATION
 Proof. In the following let β = β(Y ) denote the least squares estimator for β. Define
 T = aTβ−aTβ
 s
 aT XTX−1a
 q = q(Y)=s
 aT(XTX)−1a,
 L(Y ) = aTβ−tn−p(1− α
 2)q, and
 U(Y ) = aTβ+tn−p(1− α
 2)q.
 The T-statistic T has a tn−p distribution and so
 P−tn−p(1− α
 2) ≤ T ≤ tn−p(1− α
 2)
 = PT≤tn−p(1− α
 2) − PT <−tn−p(1− α
 2)
 = (1−α
 2)−PT >tn−p(1− α
 2)
 = (1−α
 2)−(1−(1− α
 2)) = 1−α.
 Next note that
 by symmetry
 1 −α = P−tn−p(1−α
 2) ≤ T ≤ tn−p(1− α
 2)
 = P−qtn−p(1− α
 2) ≤ aTβ−aTβ ≤ qtn−p(1− α
 2)
 = PaTβ−qtn−p(1− α
 2) ≤ aTβ ≤ aTβ+qtn−p(1− α
 2)
 = PL(Y) ≤ aTβ ≤ U(Y) .
 This equality holds for all a and β. Therefore [L(y),U(y)] is a 100(1−α) confidence interval
 for aTβ.
 Remark
 Assumes that βT = (β1 ... βp)T , t then the endpoints of the 100(1−α)% confidence interval
 for βi are given by
 βi ± tn−p(1− α
 2)s dii,
 where dii is the ith diagonal element of (XTX)−1.
 11.2 Simple example ctd.
 We continue with the example from Section 10.4. Below is an excerpt of the model summary:
11.3. ESTIMATION AND PREDICTION
 113
 Coefficients:
 Estimate Std. Error t value Pr(>|t|)
 (Intercept)
 57.240
 x
 7.404
 12.495 4.581 0.0018 **
 2.501 2.960 0.0181 *
 In the table for the coefficients, the forth column gives the observed value of the T-statistic
 for the relevant coefficient assuming the true parameter is equal to zero. Thus for the slope
 we obtain the t-value
 Tobs
 =
 β2 −0
 std.error(β2)
 = 7.404
 2.501
 = 2.96.
 As derived in the last section, a 95% confidence interval for the slope β2 is given by
 β2 ± tn−p(0.975) std.error(β2).
 We can compute this interval as follows. Since n = 10 and p = 2, we have n−p = 8 and
 so we need to determine t8(0.975). In R the command qt(q, df) computes the quantile
 function at p for a t-distribution with df degrees of freedom. In our example, the command
 qt(p=0.975, df=8)
 returns a value of 2.306. As β2 = 7.404 and std.error(β2) = 2.501 our confidence interval is
 7.404 − 2.306 ×2.501, 7.404 +2.306 ×2.501 = 1.64, 13.17 .
 Note that this interval does not include zero.
 We can compute confidence intervals in R using the command confint and specifying the
 confidence level in the level option of the command. The default is 95% confidence which
 is used if the level argument is not specified.
 confint(m, parm="x") # 95% CI
 2.5 % 97.5 %
 x 1.635831 13.17146
 confint(m, parm="x", level=0.99) # 99% CI
 0.5 % 99.5 %
 x-0.9889032 15.79619
 Note how the confidence interval gets larger when we require a higher confidence.
 11.3 Estimation and prediction
 Imagine that we have fitted a linear model and now wish to apply it to some new values of
 the explanatory variables. Let xT 0 be the row of the design matrix for the new values of the
 explanatory variables. We can
114
 CHAPTER 11. INTERVAL ESTIMATION
 1. estimate the expected value of the response given the new values of the explanatory
 variables as specified in xT 0,
 2. predict the value of a new response observation assuming the explanatory variables
 take the values specified in xT 0.
 At first glance estimation and prediction appear to be the same– in both cases we use
 xT 0 β(y) as our estimate/prediction. However, the key difference arises in the variability of
 the estimator/predictor.
 11.3.1 Estimation
 We want to estimate E[Y0 | xT 0], the expected value of the response when the explanatory
 variables take the values specified in xT 0. Our model states that E[Y0 | xT 0] = xT 0β.
 In Section 11.1, we derived confidence intervals for a parameter aTβ. Here a = x0 and so
 using the formula derived in Section 11.1 a 100(1 − α)% confidence interval for E[Y0 | xT 0] is
 given by
 xT
 0 β(y) ± tn−p(1− α
 2)s
 xT 0 (XTX)−1x0.
 Note how the width of the interval is related to the variance of the estimation error
 E[Y0 | xT 0] − xT 0 β(Y ) given by
 Var E[Y0 | xT
 0] −xT
 0 β(Y )
 11.3.2 Prediction
 = Var xT
 0β−xT
 0β(Y)
 = xT
 0Var β(Y) x0
 = σ2xT
 0(XTX)−1x0.
 This time we want to predict the actual value Y0 rather than its expectation. Our model is
 Y0 = xT 0β +ϵ0, where ϵ0 ∼ N(0,σ2) is independent of ϵ1,...,ϵn.
 Although our prediction is still xT 0 β(y), the variance in the prediction error is
 Var(Y0 −xT
 0 β(Y )) = Var(xT
 0β+ϵ0 −xT
 0β(Y))
 = Var(ϵ0)+xT
 0Var(β(Y ))x0
 = σ2+σ2xT
 0(XTX)−1x0,
 which is strictly greater than the estimation error.
 An unbiased estimator for the variance of the prediction error is given by
 s2(Y ) 1+xT
 0(XTX)−1x0 .
11.4. SUMMARY OF CHAPTER 11
 115
 Now, the prediction error Y0 − xT 0 β(Y ) has a normal distribution, and so a 100(1 − α)%
 predictive interval for Y0 is given by
 xT
 0 β(y) ± tn−p(1− α
 2)s
 1 +xT 0(XTX)−1x0.
 We can compute prediction intervals in R using the predict function. Consider a simple
 linear regression model m. To compute a 95% prediction interval when x = x0 we can use
 predict(m, list(x = x0), interval="prediction", type="response", level=0.95).
 Exercise 28
 Consider the simple linear regression model from Section 10.4. We are interested in the
 response when the explanatory variable takes value x0 = 5. Compute
 a. the estimated value of E[Y0 | x0 = 5],
 b. the predicted value of Y0 given x0 = 5,
 c. a 95% confidence interval for E[Y0 | x0 = 5],
 d. a 95% predictive interval for Y0.
 Exercise 29
 Define q(Y ) = s2(Y ) 1+xT 0(XTX)−1x0 and let Y0 be the response random variable when
 the explanatory variables take values as specified in xT 0. Show that
 PxT
 0 β(Y )−tn−p(1− α
 2)q(Y ) ≤ Y0 ≤ xT
 0β(Y )−tn−p(1− α
 2)q(Y ) = 1−α.
 (It follows that
 xT
 0 β(y) ± tn−p(1− α
 2)s
 1 +xT 0(XTX)−1x0
 are the endpoints of a 100(1 − α)% predictive interval for Y0.)
 11.4 Summary of Chapter 11
 • The endpoints of a 100(1−α)% confidence interval for aTβ are given by
 aT β(y) ± tn−p(1− α
 2)s(y)
 aT(XTX)−1a,
 where tn−p(1 − α
 2) is the 100(1 − α
 2)th quantile of the t-distribution on n − p degrees
 of freedom.
 • A100(1−α)% predictive interval for Y0 given xT 0 is given by
 xT
 0 β(y) ± tn−p(1− α
 2)s(y)
 1 +xT 0(XTX)−1x0.
116
 CHAPTER 11. INTERVAL ESTIMATION
 After studying this chapter you should
 • know what a confidence interval is and be able to compute it for scalar parameters of
 a normal linear model;
 • know how to compute a predictive interval for a new response observation.
 If you wish to engage in further (optional!) reading on the topics covered in this chapter,
 then the following section of one of the textbooks recommended for this module covers
 related material.
 • Generalized linear models with examples in R (2018) by Dunn and Smyth: Section 2.8
Chapter 12
 The t-test for normal linear models
 In this chapter we will make use of the T-statistic to define a hypothesis test for normal
 linear models called the t-test.
 12.1 Hypothesis testing
 A hypothesis test is a systematic way of deciding between a null hypothesis H0 and an
 alternative hypothesis H1, which are both statements about the parameters of a model.
 We assume that the statement in the null hypothesis holds and then examine the data for
 evidence against H0 and in favour of H1.
 A common set of hypotheses for linear models is the null hypothesis H0 : βi = 0 and the
 alternative hypothesis H1 : βi= 0. This examines whether the linear model is significantly
 improved by including the ith predictor term. Simpler models are generally preferred, so we
 tend to include additional predictor terms only if there is a justification for it.
 Under the null hypothesis, that is assuming βi = 0, the T-statistic becomes
 T = βi−0
 std.error(βi) = βi
 s√
 dii
 ∼ tn−p.
 Large positive or negative values of T (which correspond to large positive or negative values
 of the estimated coefficient relative to the corresponding standard error) constitute evidence
 against the null hypothesis that the true parameter βi is equal to zero. We calibrate this
 evidence using the p-value, which is the probability that |T| will be greater than or equal to
 the observed value |Tobs|, that is
 p = P |T|≥|Tobs| = 2 1−Ftn−p
 (|Tobs|)
 where Ftn−p 
is the cumulative distribution function of the t distribution on n − p degrees of
 freedom. If the p-value is very small, then this means that, under the assumption that the
 117
118
 CHAPTER 12. THE T-TEST FOR NORMAL LINEAR MODELS
 null hypothesis holds, the probability of observing the value Tobs or more extreme is very
 small. Therefore this provides evidence against the null hypothesis and so, if p is sufficiently
 small, we reject H0. If the p-value is not small, then there is insufficient evidence to reject
 H0. So smaller p-values indicate stronger evidence against the null hypothesis. We may
 specify a threshold against which we compare the p-value. This threshold is the significance
 level α of the hypothesis test. Common values for α are 0.05 or 0.01. In ST231, if not
 specified otherwise, we assume α = 0.05.
 Note that we normally use the absolute value of T because the alternative hypothesis
 H1 : βi= 0 is two-sided: it allows for positive and negative values of βi. (Scientific journals
 commonly insist on two-sided tests in their instructions to authors).
 12.2 A simple example in R ctd
 Consider again the example from Section 10.4 where we fitted the simple linear regression
 model
 Yi
 = β1+β2x+ϵi
 in R. Below is an excerpt of the summary output.
 for i = 1,...,10
 Coefficients:
 Estimate Std. Error t value Pr(>|t|)
 (Intercept) 57.240
 12.495 4.581 0.0018 **
 x--
7.404
 2.501 2.960 0.0181 *
 Now let’s consider the results for the slope. The slope was estimated as 7.404 but could the
 data be sufficiently compatible with the hypothesis that the true (population) slope is equal
 to zero?
 To answer this formally, we perform a t-test at a 5% significance level. We are testing
 H0 : β2 = 0 against H1:β2=0.
 We use as the test statistic
 T = β2(Y)
 std.error β2(Y )
 which under H0 has a t-distribution on n − p = 10 − 2 = 8 degrees of freedom. The
 observed value for this statistic is reported in the summary output as Tobs = 2.96. The
 corresponding p-value is reported in the last column of the summary output above, but can
 also be computed using the command
 2 * (1- pt(q=2, 96, df=8))
 which returns the value 0.0181. This means that there is evidence against the null hypothesis
 at a 5% significance level (as 0.0181 < 0.05) and so we reject H0. Thus the assumption
12.2. A SIMPLE EXAMPLE IN R CTD
 119
 that the true slope is different from zero is reasonable and so the explanatory variable is
 informative about the response.
 As another example let us consider whether a quadratic model would be more adequate for
 the data. We can use a t-test to help with this decision. Let Yi = β1 + β2xi + β3x2 i + ϵi
 be the quadratic model. We are interested in whether the coefficient of x2 i is significantly
 different from zero. We have as hypotheses H0 : β3 = 0 against H1 : β3= 0.
 The quadratic regression model fitted in R is summarised in the following output:
 Call:
 lm(formula = y ~ x + I(x^2))
 Residuals:
 Min
 1Q Median
 3Q
 Max-3.8841-2.0468-0.1909 1.9633 5.0651
 Coefficients:
 Estimate Std. Error t value Pr(>|t|)
 (Intercept)-12.157
 127.851-0.095
 x
 I(x^2)
 34.550-2.637
 49.819 0.694
 4.832-0.546
 0.927
 0.510
 0.602
 Residual standard error: 3.246 on 7 degrees of freedom
 Multiple R-squared: 0.5422,
 Adjusted R-squared: 0.4114
 F-statistic: 4.145 on 2 and 7 DF, p-value: 0.06492
 Consider the last line in the table for the coefficients. Here Tobs = −2.637
 4.832 = −0.546 and so
 |Tobs| = 0.546. Note that we now have p = 3 and so n−p = 7. Hence
 P |T7| ≥|Tobs|
 = 2 1−Ft7
 (0.546) = 2(1−0.699) = 0.602
 Therefore a value of |Tobs| = 0.546 or larger commonly occurs under the null hypothesis. Thus
 there is insufficient evidence against H0 and thus to suggest that the quadratic regression
 model is preferable to the simple linear regression model. We therefore retain the simple
 linear regression model.
 Important note: The T-statistics reported on in the R summary output are not generally
 independent of each other. In the quadratic model none of the parameters are significant,
 but once the quadratic term is removed from the model, the intercept β1 and the slope β2
 become significant (at a significance level of 5%). This is important for variable selection
 which we will discuss later on in the module.
120
 CHAPTER 12. THE T-TEST FOR NORMAL LINEAR MODELS
 12.3 Comparison with other t-tests
 A t-test is a general test for comparing population means and is used in contexts other than
 linear models. But we can view some of the t-tests that you may have encountered in other
 contexts in terms of fitting a linear model and testing hypotheses about its parameters.
 One such t-test is the two sample t-test in which we have observations from two populations
 with the same variance. We can view this as fitting the linear model
 Yi
 = βA+ϵi if i isin population A,
 βB +ϵi if i is in population B.
 If we want to compare the population means then we consider as hypotheses H0 : βA = βB
 versus H1 : βA= βB. We can be rewrite these hypotheses as
 H0 : aTβ =0 and H1: aTβ=0
 where aT = (1,−1) and βT = (βA,βB).
 Therefore, using what we have learned previously about the T-statistic, under H0
 T = aTβ(Y)−aTβ
 s(Y )
 aT XTX−1a 
∼ tn−p.
 Let’s consider a (somewhat contrived) example for illustration.
 Example:
 Consider a study to investigate the effectiveness of two treatments for blood pressure. The
 study contained three groups of patients who were randomly allocated to either one of the
 two treatments or a control group who received a placebo. The table below gives the change
 in blood pressure for 15 patients after two months of treatment.
 Control Treatment 1 Treatment 2
 −4
 −10
 +1
 −2
 +1
 −1
 −5
 −8
 −5
 −7
 −4
 −2
 −5
 −2
 −7
 We would like to show that the new treatment (Treatment 1) is significantly different to
 the existing treatment (Treatment 2). We can perform a two sample t-test to compare
 Treatment 1 and Treatment 2.
12.3. COMPARISON WITH OTHER T-TESTS
 121
 First we consider the linear model:
 Yi
 = µ1+ϵi if ireceived Treatment 1,
 µ2 +ϵi if i received Treatment 2.
 We obtain as quantities and estimates
 n = 10, µ1 = −7, µ2 = −4,
 s2 = 4.5, Tobs=−2.24.
 We would like to assess whether there is evidence against H0 : µ1 = µ2 in favour of
 H1 : µ1= µ2. The null hypothesis is equivalent to H0 : µ1 − µ2 = aTµ = 0 where
 aT =(1,−1) and µT = (µ1,µ2). Then under H0
 T = µ1(Y)−µ2(Y)
 s(Y )√0.4
 So for the two-sided test the p-value is
 ∼ t8
 p = P |T8|≥|Tobs| = 2 1−Ft8
 (2.24) = 0.055.
 However, in doing this calculation we have wasted the information from the control group.
 What happens if we use these data as well?
 So now consider the linear model:
 
 
 
 
 Yi
 =
 
 
 
 µ0 +ϵi if i in control group,
 µ1 +ϵi if i received Treatment 1,
 µ2 +ϵi if i received Treatment 2.
 Then we obtain the quantities and estimates
 n = 15, µ0 = −1, µ1 = −7,
 µ2 = −4, s2 = 4.5, Tobs = −2.24.
 Under H0 : µ1 = µ2 we have
 T = µ1(Y)− µ2(Y)
 s(Y )√0.4
 ∼ t12.
 When including the data from the control group, the p-value of the two-sided test is
 p = P |T12|≥|Tobs| = 2 1−Ft12
 (2.24) = 0.045.
122
 CHAPTER 12. THE T-TEST FOR NORMAL LINEAR MODELS
 This is not a substantial change, but note that the evidence against the null hypothesis is
 stronger under the linear model that uses all of the available data.
 Conclusion
 By including all of the available information we’ve created a more powerful test of the
 null hypothesis. Even though the control group told us nothing about the difference between
 Treatment 1 and Treatment 2, we reduced the uncertainty in our estimate of σ2. In this
 (contrived) example s2 didn’t change when we included the extra information from the
 control group. In general s2 can change, and therefore so can the T-statistic. However
 including the control group will always result in a more powerful test.
 Exercise 30
 Compute (by hand) µ1, µ2, s2 and Tobs for the first model considered in the above example
 and thus verify that these are correct.
 Exercise 31 Suppose we have n independent random samples from a normally distributed
 population and we would like to test the hypothesis that the population mean is equal to
 some value µ0. This is what we call a one-sample t-test. Set up the appropriate linear
 model to perform this test and compute an expression for the relevant T-statistic. (You
 should find that the relevant quantities are familiar to you from your mathematical statistics
 modules.)
 12.4 Limitations of the t-test
 We have already seen how useful the T-statistic can be, so useful that some T-statistics
 are reported in the R summary output for a linear model. In a t-test we are concerned
 with a single regression coefficient or with a linear combination of coefficients, so the null
 hypothesis is of the form H0 : aTβ = c where c is a constant and a is a vector of the same
 length as the vector β. The t-test therefore imposes one constraint of the form aTβ = c.
 But, in general, we may want to impose more than one constraint in our null hypothesis,
 and the F-test, which we will introduce in the next chapter, allows us to do this. To impose
 q >1 constraints we use a q ×p matrix A of rank q and a q-vector c and then consider the
 null hypothesis H0 : Aβ = c.
 Exercise 32
 1. Suppose for the normal linear model Yi = β0 +β1xi +ϵi where i = 1,...,n we would
 like to test whether the slope coefficient β1 can be assumed to be equal to 3. Find
 the vector a and scalar c such that the null hypothesis can be written in the form
 H0 : aTβ =c where β =(β0,β1)T.
 2. Suppose for the normal linear model Yi = β0 +β1xi,1 +β2xi,2 +ϵi where i = 1,...,n
 we would like to test whether β1 = β2. Find the vector a and scalar c such that the
 null hypothesis can be written in the form H0 : aTβ = c where β = (β0,β1,β2)T.
12.5. SUMMARY OF CHAPTER 12
 123
 3. Suppose for the normal linear model Yi = β0 +β1xi,1 +β2xi,2 +ϵi where i = 1,...,n
 we would like to test whether β1 = β2 = 0. Find the matrix A and vector c such that
 the null hypothesis can be written in the form H0 : Aβ = c where β = (β0,β1,β2)T.
 Exercise 33
 In Chapter 9 we fitted the following model to the Retail data:
 
 
 
 Salesj
 =
 
 
 µA +β pricej +ϵj if the jth store is of Brand A
 µB +β pricej +ϵj if the jth store is of Brand B
 µC +β pricej +ϵj if the jth store is of Brand C
 (12.1)
 where j = 1,...,96. Explain why we cannot use a t-test to decide whether brand should be
 included in the model.
 12.5 Summary of Chapter 12
 • To test H0 : βi = 0 vs H1 : βi= 0, we use the T-statistic
 T = βi(Y)
 std.error(βi(Y )) = βi(Y )
 s(Y )√
 dii
 ∼ tn−p
 where dii is the ith diagonal of the matrix (XTX)−1. The p-value of the test is given
 by
 P(|T| ≥ Tobs) = 2(1−Ftn−p
 (|Tobs|))
 where Ftn−p 
is the distribution function of the t distribution on n−p degrees of freedom.
 • More generally to test H0 : aTβ = 0 vs H1 : aTβ= 0, we use the T-statistic
 T = aTβ(Y)
 std.error(aT β(Y )) =
 aT β(Y )
 s(Y )
 aT(XTX)−1a
 ∼ tn−p.
 • The t-test specifies a null-hypothesis that places exactly one linear constraint on the
 parameter vector of the normal linear model.
 After studying this chapter you should
 • know how to perform a t-test for a normal linear model;
 • understand the quantities in the table of coefficients in the R model summary output;
 • understand that the t-test uses a null-hypothesis that places one linear constraint on
 the parameter vector of the normal linear model.
 %
Chapter 13
 The F-test and ANOVA
 In this chapter we will introduce F-tests for normal linear models. These F-tests are based
 on a partition of variation of the response and are used to compare nested models. We
 start with a definition of the concept of nested linear models, then discuss a partition of the
 variation of the response known as the decomposition of the total sum of squares. Finally
 we define the F-statistic used in F-tests.
 13.1 Nested linear models
 Suppose we have n units of observation for which we consider the linear model M2 given by
 Yi
 = β0+β1xi1+β2xi2 +β3xi3 +ϵi
 for i = 1,...,n.
 Now suppose we impose the condition that β2 = β3 = 0. Then this produces the smaller
 model M1 given by
 Yi
 = β0+β1xi1+ϵi for i=1,...,n.
 Thus M1 is a special case of M2 produced by imposing two linear constraints on the parameter
 vector βT = (β0,β1,β2,β3) of M2. More formally, if we set
 A = 0 0 1 0
 0 0 0 1
 and
 c = 0
 0 ,
 then imposing Aβ = c reduces model M2 to model M1. We say M1 is nested in M2. Below
 is a more formal definition.
 Definition 13.1 (Nested linear models). Let M1 be a linear model with a parameter vector
 of dimension p1 and M2 be a linear model with parameter vector β of dimension p2. Suppose
 p2 > p1 and set q = p2−p1. We say that M1 is nested in M2 if there exists a q×p2 matrix
 A of rank q and a q-vector c such that imposing the linear constraints Aβ = c on M2
 produces the model M1.
 124
13.1. NESTED LINEAR MODELS
 125
 We refer to M2 as the “larger” model and to M1 as the “smaller” model. The “smallest”
 model we will consider is the so-called null model.
 Definition 13.2 (Null model). Suppose we have n units of observation. The null model
 M0 is defined as
 Y = β01n×1+ϵ.
 Remarks
 • The null model is the simplest statistical model for a data set. It does not utilise any
 explanatory variables to derive a prediction for the response.
 • We encountered the null model in Exercise 31 where we considered the one sample
 t-test.
 • Note that within other statistical contexts, the term “null model” may have a more
 general meaning than the one we are considering here.
 Next we develop an approach that allows us to compare the null model against a linear
 model that has the null model nested within it. But first some exercises to consolidate the
 notion of nested linear models.
 Exercise 34
 1. By finding an appropriate matrix A and vector c show that the linear model
 Yi = β0 +2xi +ϵi
 is nested in the linear model
 Yi = β0 +β1xi +ϵi where i = 1,...,n.
 Which linear constraints are imposed on the larger model to produce the smaller
 model?
 2. By finding an appropriate matrix A and vector c show that the linear model
 Yi = β0 +β1(xi,1 +xi,2) +ϵi
 is nested in the linear model
 Yi = β0 +β1xi,1 +β2xi,2 +ϵi where i = 1,...,n.
 Which linear constraints are imposed on the larger model to produce the smaller
 model?
 3. By finding an appropriate matrix A and vector c show that the linear model
 Yi = β0 +2(xi,1 +xi,2) +ϵi
126
 CHAPTER 13. THE F-TEST AND ANOVA
 is nested in the linear model
 Yi = β0 +β1xi,1 +β2xi,2 +ϵi where i = 1,...,n.
 Which linear constraints are imposed on the larger model to produce the smaller
 model?
 Exercise 35
 Consider again the linear model from Exercise 22 fitted to the Retail dataset:
 
 
 
 Salesj
 =
 
 
 µA +ϵj if store j is of Brand A
 µB +ϵj if store j is of Brand B
 µC +ϵj if store j is of Brand C
 Show that the above model has the null model nested within it.
 13.2 The decomposition of the total sum of squares
 You may have noticed that the R summary output for a linear model reports on an F-statistic.
 This statistic compares the fitted model with the null model, that is the model containing
 an intercept term only. The corresponding test is referred to as the test for existence of
 regression.
 Suppose we have n observations and we want to compare a linear model M1 with the null
 model M0, that is the model that contains only an intercept term. Let M1 be given as
 Y = Xβ+ϵ,
 where the vector of coefficients β is of dimension p < n. The null model M0 is defined as
 Y = β01n×1+ϵ.
 We furthermore assume that the null model is nested within M1. This means that the
 model M1 contains an intercept term (or can be reparameterised to contain an intercept
 term) and thus can be written as
 Y = β01n×1+X∗β∗+ϵ,
 where X∗ is a n×(p−1) matrix and β∗ is a (p−1) dimensional vector. Thus if we set
 β∗ =0(p−1)× 1, then the resulting model is the null model. Therefore the null model M0 is a
 special case of M1.
 We will refer to the larger model M1 as the full model. The following theorem provides
 the foundation for comparing M1 with M0.
13.2. THE DECOMPOSITION OF THE TOTAL SUM OF SQUARES
 127
 Theorem 13.1 (Decomposition of the total sum of squares). If the null model is nested
 within the full model, then
 n
 i=1
 (yi − y)2 =
 n
 i=1
 n
 (yi − y)2 +
 i=1
 (yi − yi)2.
 (13.1)
 Here y is the sample mean of the response variable and yi is the ith fitted value in the full
 model. We call
 • n
 i=1(yi − y)2 the total sum of squares,
 • n
 i=1(yi − yi)2 the residual sum of squares (or deviance) of the full model, and
 • n
 i=1(yi − y)2 the regression sum of squares of the full model.
 Hence the total sum of squares is equal to the residual sum of squares plus the regression
 sum of squares.
 For the proof of the theorem above we will need the following lemma.
 Lemma 13.1. If a linear model has the null model nested within it, then the residuals must
 sum to zero.
 We leave the proof of this lemma as an exercise (Exercise 36).
 Informally, this decomposition of the total sum of squares tells us that the variation in
 the response variable can be partitioned into the variation explained by the model (the
 regression sum of squares) and an “unexplained” or “residual” variation (the residual sum of
 squares).
 Note that the total sum of squares is the deviance of the null model while the residual sum
 of squares is the deviance of the full model.
 Proof. (of Theorem 13.1)
 As the full model has the null model nested within it, from Lemma 13.1, we have
 n
 0 =
 Now
 n
 n
 (yi − y)2 =
 i=1
 ϵi
 (yi − yi + yi − y)2
 i=1
 i=1
 n
 =
 i=1
 n
 (yi − yi)2 +
 i=1
 n
 =
 i=1
 (yi − yi).
 n
 (yi − y)2 +2
 i=1
 (yi − yi)(yi − y)
128 CHAPTER13. THEF-TESTANDANOVA
 but
 n
 i=1
 (yi−yi)(yi−y)=
 n
 i=1
 (yi−yi)yi−
 n
 i=1
 (yi−yi)y
 =
 n
 i=1
 yi(yi−yi)−y
 n
 i=1
 (yi−yi)
 = n
 i=1
 ϵi=0
 (bythepreviouslemma)
 =
 n
 i=1
 yi(yi−yi)
 = yT(y−y)
 = (Xβ)T(y−Xβ)
 = βT (XTy−XTXβ)
 = βT0p×1 (fromthenormalequations)
 = 0.
 Hence
 n
 i=1
 (yi−y)2
 TotalSS
 =
 n
 i=1
 (yi−yi)2
 ResidSS
 +
 n
 i=1
 (yi−y)2
 RegrSS
 .
 Inthenextsectionwewilldiscusshowtoassemblethecomponentsofthedecompositionof
 thetotalsumofsquarestoformallycomparemodelM1withthenullmodelM0.
 Exercise36: Showthatforalinearmodel thathasthenullmodelnestedwithinitthe
 residualsmustsumtozero.Hint:Without lossofgeneralityassumethatthemodelhasan
 interceptterm, thenusethenormalequationstoshowthat
 0 = XTϵ
 whereXisthedesignmatrixandϵisthevectorofresiduals.Whatdoesthefactthatthe
 linearmodelhasanintercepttermtellusaboutX?
13.3. THE TEST FOR THE EXISTENCE OF REGRESSION
 129
 13.3 The test for the existence of regression
 Assume a normal linear model, that is Y ∼ NnXβ,σ2In , which has the null model nested
 within it. If we replace y with Y in the decomposition of the total sum of squares we can
 deduce that
 1
 σ2
 n
 i=1
 (Yi − Y )2 = 1
 σ2 
n
 i=1
 (Yi − Y )2+ 1
 σ2 
n
 i=1
 (Yi − Yi)2.
 In your mathematical statistics module you will have encountered Fisher’s theorem which
 states that
 1
 σ2
 Furthermore, one can show1
 This can be used to show that
 1
 σ2
 1
 σ2
 n
 i=1
 n
 (Yi − Y )2 ∼ χ2
 n−1.
 (Yi − Yi)2 ∼ χ2
 n−p.
 i=1
 n
 i=1
 (Yi − Y )2 ∼ χ2
 p−1
 and is independent from 1
 σ2 
n
 i=1(Yi − Yi)2.
 Now recall the definition of an F-distribution.
 Definition 13.3 (F-distribution). If U1 ∼ χ2
 d1 
and U2 ∼ χ2
 d2 
are independent, then
 F = U1/d1
 U2/d2 
∼ Fd1,d2
 ,
 that is F has an F-distribution on d1 and d2 degrees of freedom.
 With this definition and our earlier discussion it follows that
 n
 F =
 i=1(Yi − Y )2 /(p−1)
 n
 i=1(Yi − Yi)2 /(n −p) 
∼ F(p−1),(n−p)
 Let D0(Y ) = n
 i=1(Yi − Y )2 be the statistic corresponding to the deviance of the null
 model and let D1(Y ) = n
 i=1(Yi− Yi)2 be the statistic corresponding to the full model, then
 we observe that the F−statistic is defined as
 F = [D0(Y)−D1(Y)]/(p−1)
 D1(Y )/(n−p)
 1see Appendix C- Proof of Theorem 10.1 (iv)) if you are interested
130
 CHAPTER 13. THE F-TEST AND ANOVA
 and so encapsulates a comparison of deviances. More precisely, it considers the reduction in
 deviance as we move from the null model to the full model relative to the deviance of the
 full model D1(Y ), both adjusted by the relevant degrees of freedom. If the full model is a
 substantial improvement over the null model, then we expect the F-statistic to be large.
 Thus, in a test for existence of regression we compare the normal linear models
 M0 : Y = β01n×1+ϵ, and
 M1 : Y = Xβ+ϵ,
 where M0 is nested within M1. If we write β = (β0, β∗)T, where β∗ are the p−1 non-intercept
 parameters, then the hypotheses for the test for the existence of regression are:
 H0 : β∗ =0 against H1 : at least one of the coefficients in β∗ is non-zero.
 Under the null hypothesis we have
 F = [D0(Y)−D1(Y)]/(p−1)
 D1(Y )/(n−p)
 The observed value of the F-statistic is computed as
 Fobs = (D0(y)−D1(y))/(p−1)
 D1(y)/(n −p)
 n
 =
 i=1
 (yi − y)2 − n
 i=1
 (yi − yi)2 /(p−1)
 n
 i=1
 n
 (yi − yi)2/(n − p)
 (yi − y)2/(p −1)
 =
 i=1
 n
 i=1
 (yi − yi)2/(n − p)
 = RegrSS/(p−1)
 ResidSS/(n − p)
 ∼ F(p−1),(n−p).
 (by the decomposition of the total sum of squares)
 Large values of the F-statistic provide evidence against H0. The p-value of the test for
 existence of regression is thus
 p = PF≥Fobs
 where F ∼ Fp−1,n−p.
13.4. ANOVA TABLE FOR THE EXISTENCE OF REGRESSION
 131
 13.4 ANOVA table for the existence of regression
 ANalysis Of VAriance (ANOVA) involves attributing part of the variation in the response
 variable to the model and the rest to the errors. The total variation in the response variable
 is given by the total sum of squares. The variation explained by the full model is the
 regression sum of squares and the variation due to the errors is the residual sum of squares.
 An ANOVA table is a convenient way of presenting this information ready for testing for
 the existence of regression.
 An ANOVAtable of the test for the existence of regression contains the following information.
 ANOVA table
 Source
 d.f.
 SS
 MS
 F
 n
 Regression
 Residual
 p −1
 n−p
 i=1
 n
 (yi − y)2
 (yi − yi)2
 i=1
 n
 Total
 n−1
 i=1
 (yi − y)2
 RegrSS
 p−1
 ResidSS
 n−p
 RegrMS
 ResidMS
 Thus, the above ANOVA table lists the decomposition of the total sum of squares in the
 column named SS and reports the corresponding degrees of freedom in the column named
 d.f. The sum of the degrees of freedom of the regression sum of squares and that of the
 residual sum of squares is equal to the degrees of freedom of the total sum of squares. The
 abbreviation MS stands for mean square which is given by a sum of squares divided by its
 degrees of freedom. The regression mean square (RegrMS) and the residual mean square
 (ResidMS) are used to compute the F-statistic. We have
 Fobs = RegrSS/(p−1)
 ResidSS/(n − p) = RegrMS
 ResidMS
 Under the normality assumption and the null hypothesis we have F ∼ Fp−1,n−p. To
 perform the test for the existence of regression we can now compute the p-value for Fobs
 as explained in the previous section and compare it against the desired significance level.
 Alternatively we can determine the appropriate critical value. We reject H0 at the 100α%
 level if Fobs ≥ Fp−1,n−p(1 − α) where Fp−1,n−p(1 − α) is the (1 − α)the quantile that is the
 value such that P(F ≤ Fp−1,n−p(1 −α)) = 1−α with F ∼ Fp−1,n−p.
132
 CHAPTER 13. THE F-TEST AND ANOVA
 Exercise 37 Fill out the missing values in the following ANOVA table for the test of
 existence of regression.
 Source
 Regression
 Residual
 Total
 d.f.
 4
 SS MS F
 200
 54 368
 13.5 Illustration in R
 Consider the following data set.
 x
 1
 2
 3
 4
 5
 6
 7
 9
 y 1.62 1.67 1.91 1.90 2.02 2.03 1.86 1.84 1.57
 8
 This small data set is a toy example to illustrate some of the ideas we have encountered so
 far and to demonstrate how to perform an ANOVA in R.
 We would like to decide on the appropriate model for this data and consider the following
 models as potential candidates.
 1. Null model: Y = β01+ϵ.
 2. Simple linear regression model: Y = β01 +β1x+ϵ.
 3. Quadratic regression model: Y = β01+β1x+β2x2 +ϵ.
 Below is the R-code used to fit the models:
 # Null model
 m0 <- lm(y ~ 1)
 # Simple linear regression model
 m1 <- lm(y ~ x)
 # Quadratic regression model
 m2 <- lm(y ~ x + I(xˆ2))
 Figure 13.1 illustrates the data and fitted models.
 In the plot we observe that the fitted null model and the simple linear regression model
 are fairly similar. Also, judging from the plot, the quadratic model appears to be the most
 appropriate model.
 To compare the model m1 to the null model m0 we can simply consult the summary output
 of m1 given below.
13.5. ILLUSTRATION IN R
 133
 Figure 13.1: Comparison of fitted models for the toy example dataset.
 Call:
 lm(formula = y ~ x)
 Residuals:
 Min
 1Q Median
 3Q
 Max-0.27711-0.13744 0.02422 0.09689 0.19989
 Coefficients:
 Estimate Std. Error t value Pr(>|t|)
 (Intercept) 1.796111 0.129811 13.836 2.43e-06 ***
 x
 0.005667 0.023068 0.246--
0.813
 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 Residual standard error: 0.1787 on 7 degrees of freedom
 Multiple R-squared: 0.008547, Adjusted R-squared:-0.1331
 F-statistic: 0.06034 on 1 and 7 DF, p-value: 0.813
 The p-value for the slope coefficient is large (0.813) and so there is insufficient evidence to
 conclude that the slope coefficient is different from zero.
 Next, let us consider the summary output for the fitted quadratic regression model.
134
 CHAPTER 13. THE F-TEST AND ANOVA
 Call:
 lm(formula = y ~ x + I(x^2))
 Residuals:
 Min
 1Q Median
 Max-0.07815-0.04354 0.02615 0.05539 0.05785
 Coefficients:
 3Q
 Estimate Std. Error t value Pr(>|t|)
 (Intercept) 1.330238 0.081491 16.324 3.37e-06 ***
 x
 0.259779 0.037418 6.943 0.000443 ***
 I(x^2)-0.025411 0.003649-6.963 0.000436 ***--
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 Residual standard error: 0.06404 on 6 degrees of freedom
 Multiple R-squared: 0.8908,
 Adjusted R-squared: 0.8544
 F-statistic: 24.48 on 2 and 6 DF, p-value: 0.001301
 Both the coefficient for x and the one for x2 have very small p-values (less than 0.0005).
 The small p-value for the coefficient for x2 means that there is strong evidence against the
 null hypothesis that that this coefficient is equal to zero. Or, in other words, the quadratic
 model is a significant improvement over the simple linear regression. Given the results of
 the previous simple linear regression model, you might be surprised that the p-value for
 the coefficient for x is also very small. However, keep in mind that the T-statistics are
 not independent and so the results will depend on what other terms are included in the
 model. Hence, when reporting such results, you should always state clearly which terms
 were included in the model. Furthermore, later on in the module, we will encounter the
 so-called marginality principle. According to this principle we ignore the p-values for a
 lower order term when the model contains a higher order term, even if the p-value suggest
 removing the lower order term.2
 While we can compare the quadratic model with the simple linear regression model using
 a t-test, we cannot compare the quadratic model with the null model in this way as this
 requires us to fix two parameters to be zero and thus imposes two linear constraints. Instead
 we compare the quadratic and null model using an ANOVA table and the corresponding
 F-test or test for the existence of regression.
 2This is unless we have used orthogonal polynomials which will not be covered as part of this module.
13.5. ILLUSTRATION IN R
 135
 To produce the corresponding ANOVA table in R, we use the command:
 anova(m0, m2)
 Analysis of Variance Table
 Model 1: y ~ 1
 Model 2: y ~ x + I(x^2)
 Res.Df
 RSS Df Sum of Sq
 1
 2
 8 0.22542
 F Pr(>F)
 6 0.02461 2 0.20081 24.479 0.001301 **--
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 The information produced by R is laid out differently to an ANOVA table for the test of
 existence of regression. The first two lines state which models are being compared. (It is
 advisable to check that these are in fact the models that you wish to compare and that the
 f
 irst model is nested within the second.) Then the output provides a table with headers
 Res.Df, RSS, Df, Sum of Sq, F and Pr(>F). The first line in this table relates to quantities
 for Model 1 and the second to quantities for Model 2. Here, Model 1 is the null model,
 so the entry under RSS, the residual sum of squares, is in fact the total sum of squares
 which, in our example, equals 0.225 (rounded). The degrees of freedom of the total sum of
 squares are reported under Res.Df and, for our example, are equal to 8 (recall the data has
 9 data points). The second line of the table considers the quantities related to Model 2, in
 our example the quadratic regression model. Its residual sum of squares is reported in the
 column RSS which, in our example, equals 0.025 (rounded). The degrees of freedom for this
 residual sum of squares are reported in the column Res.Df and are equal to 6. The entry in
 column Sum of Sq gives the regression sum of squares which, in our example, equals 0.2
 and has 2 degrees of freedom (as reported in the column Df). The mean squares are not
 reported (although can be easily computed from the information provided). However, the
 observed value of the F-statistic and the corresponding p-value are listed. In our example,
 the observed F-statistic is equal to 24.48 and has a p-value of 0.0013. Compare these
 quantities against the ones reported in the last line of the earlier summary output for the
 quadratic model!
 Below we have collated the information provided by the R output in an ANOVA table:
 Source
 Regression
 Residual
 Total
 d.f.
 2
 6
 8
 SS
 MS
 F
 0.201 0.1004 24.48
 0.025 0.0041
 0.225
 Now let us proceed with the test for the existence of regression. This test compares the full
136
 CHAPTER 13. THE F-TEST AND ANOVA
 model (here the quadratic regression model) against the null model. The hypotheses are
 thus H0 : β1 = β2 = 0 versus H1 : at least one of β1 and β2 is not equal to zero. Then, under
 H0, the F-statistic has an F-distribution with p − 1 = 2 and n−p = 6 degrees of freedom.
 As large values of the F-statistic correspond to evidence against the null-hypothesis, to
 determine the critical value of the F-test we need to consider the right tail of the distribution.
 Thus, to proceed with the test for the existence of regression at, say, a significance level of 1%
 we need to determine the critical value c0.01 satisfying P(F ≥ c0.01) = 0.01 or, equivalently,
 P(F ≤c0.01) = 0.99, where, in our example, F ∼ F2,6.
 We can compute the critical value c0.01 in R as using the command
 qf(0.01, df1=2, df2=6, lower.tail = FALSE)
 or, equivalently, the command qf(0.99, df1=2, df2=6). We obtain c0.01 = 10.92 and so,
 as the observed F-statistic given by 24.48 and thus is larger than 10.92, we reject H0 at
 the 1% significance level. We therefore conclude that the quadratic model is a significant
 improvement on the null model.
 It is common practice to report the p-value rather than just the result of a hypothesis test
 which is restricted to a given significance level. We can compute the p-value of the observed
 F-statistic, that is P(F ≥ 24.48) where F ∼ F2,6 using the pf function in R. Again, with
 this function we need to be careful to consider the right tail of the F-distribution.
 # p-value for test of existence of regression in toy example
 1- pf(24.48, df1=2, df2=6)
 # or equivalently
 pf(24.48, df1=2, df2=6, lower.tail = FALSE)
 The result is 0.0013 as reported in the earlier ANOVA.
 13.6 The distribution of the F-statistic
 The test for the existence of regression compares the model of interest with the null model.
 But we may also be interested in comparing a model M2 with a model M1 where M1 is
 nested in M2 but not the null model. We can proceed in a manner that is analogous to the
 test for the existence of regression.
 More formally, let the normal linear model M2 be given as Y = Xβ +ϵ. Let A be a q ×p
 matrix of rank q < p and let c be a q-vector. In the following we consider how to test
 H0 : Aβ =c (the so called general linear hypothesis) versus H1 : Aβ= c.
 Recall that if we impose the linear constraints Aβ = c on the full model M2, then we
 produce a reduced model M1 that is nested in M2. And so the above test compares M2 with
 M1.
 The test for the existence of regression relied on a comparison of the deviances of the models
 under consideration. Similarly, the F-test discussed here relies on a comparison of deviances,
13.6. THE DISTRIBUTION OF THE F-STATISTIC
 137
 so let
 • D2=D2(Y)=(Y −Xβ)T(Y −Xβ) be the statistic corresponding to the deviance
 of M2 and
 • D1=D1(Y)=(Y −XβH)T(Y −XβH) the statistic corresponding to the deviance
 of M1. Here βH is the least squares estimator for the parameter vector βH in the
 reduced model M1. As we are imposing q linear constraints, the dimension of βH is
 p −q.
 We know that 1
 σ2
 D2(Y ) ∼ χ2 n−p and 1
 σ2
 D1(Y ) ∼ χ2
 n−(p−q). We can further show that
 D1(Y )−D2(Y) is a sum of squares which is independent of D2(Y ). From this we can then
 deduce that 1
 σ2
 (D1(Y ) − D2(Y )) has a χ2 q-distribution. But then it follows that
 F = (D1(Y)−D2(Y))/q
 D2(Y )/(n−p)
 We summarise the result in a theorem:
 ∼ Fq,n−p.
 Theorem 13.2 (The distribution of the F-statistic). Consider the normal linear model M2
 given by Y = Xβ+ϵ, where X is an n×p design matrix of rank p and ϵ ∼ Nn 0,σ2In .
 Suppose A is a q ×p matrix of rank q and c a q-vector. Let M1 be the normal linear model
 derived from M2 by imposing the q linear constraints Aβ = c.
 Let
 • D2=(Y −Xβ)T(Y −Xβ) be the statistic corresponding to the deviance of M2 and
 • D1=(Y −XβH)T(Y −XβH) the statistic corresponding to the deviance of M1.
 Then, assuming Aβ = c, we have
 F = (D1−D2)/q
 D2/(n −p) ∼ Fq,n−p.
 Proof. We omit the proof here, but for those who are interested in the proof, this is provided
 in Appendix E.
138
 CHAPTER 13. THE F-TEST AND ANOVA
 Comments:
 • The F-test with statistic
 F = (D1−D2)/q
 D2/(n −p)
 where D1 is the deviance of the smaller model M1 and D2 is the deviance of the larger
 model M2 is only appropriate for comparing nested models. M1 is nested in M2 if
 imposing linear constraints of the form Aβ = c on the parameters of M2 results in the
 model M1. Beware that R will not check whether the two models under consideration
 are nested.
 • If the null hypothesis specifies that a subset of the parameters are equal to zero, this
 test is referred to as a partial F-test.
 • When q =1 then the F-test is equivalent to the two-sided t-test: both tests give the
 same p-value. It turns out that in this special case the F-statistic is equal to the
 square of the T-statistic.
 13.7 Summary of Chapter 13
 Let
 M2 :
 Y = Xβ+ϵ
 be a normal linear model where X is an n×p design matrix of rank p and ϵ ∼ Nn 0,σ2In .
 • The null model is the linear model that has only an intercept term, that is
 Y = β01n×1+ϵ.
 • Let A be a q×p matrix of rank q < p and c a q-vector. The linear model M1 is
 nested in M2 if imposing the constraint Aβ = c on M2 results in the model M1.
 • Let D1 be the deviance of the normal linear model M1 and D2 the deviance of the
 normal linear model M2 where M1 is nested within M2, then we have
 F = (D1−D2)/q
 D2/(n −p) ∼ Fq,n−p
 where
 n = number of units of observation,
 p = dimension of parameter vector in M2,
 q = number of parameters fixed to reduce M2 to M1.
 • Suppose A is a q ×p matrix of rank q and c a q-vector. The F-test based on the
 F-statistic above tests H0 : Aβ = c against H1 : Aβ= c. If imposing Aβ = c turns
 M2 into the null model then this is the test for the existence of regression.
13.7. SUMMARY OF CHAPTER 13
 139
 • Decomposition of the total sum of squares If the null model is nested within a
 model M, then
 n
 i=1
 where
 n
 (yi − y)2 =
 i=1
 n
 (yi − y)2 +
 i=1
 (yi − yi)2.– n
 i=1(yi − y)2 is the total sum of squares,– n
 i=1(yi − yi)2 is the residual sum of squares (or deviance of M), and– n
 i=1(yi − y)2 is the regression sum of squares of the full model.
 • Wecan use an ANOVA table to perform a test for the existence of regression.
 (13.2)
 Source
 d.f.
 SS
 MS
 F
 n
 Regression
 Residual
 p −1
 n−p
 i=1
 n
 (yi − y)2
 (yi − yi)2
 i=1
 n
 Total
 n−1
 i=1
 (yi − y)2
 RegrSS
 p−1
 ResidSS
 n−p
 RegrMS
 ResidMS
 After studying this chapter you should
 • be able to prove the decomposition of the total sum of squares;
 • know what is meant by the test for existence of regression and be able to perform the
 test;
 • be able to complete an ANOVA table for the test for existence of regression;
 • be able to perform an F-test to compare two normal linear models M1 and M2 where
 M1 is nested in M2;
 • be able to motivate the distribution of the F-statistic.
 If you wish to engage in further (optional) reading on the topic covered in this chapter, then
 the following sections of the textbooks recommended for this module cover related material.
 • Generalized linear models with examples in R (2018) by Dunn and Smyth: Sections
 2.9, 2.10.1
Chapter 14
 More on F-tests and ANOVA
 In this chapter we explore further examples of ANOVA and F-tests.
 14.1 Sequential ANOVA
 Recall that the test for existence of regression was based on a partition of the total sum
 of squares into the regression sum of squares and the residual sum of squares. But this is
 not the only partition we may consider. In a so-called sequential ANOVA we further
 partition the regression sum of squares into the contribution from each predictor term. Note
 that this requires some natural ordering in which we introduce the variables one by one into
 the model.
 Consider the toy example from Section 13.5 but this time we consider four alternative models:
 the null model, a simple linear regression model, a quadratic regression model and a cubic
 regression model. Suppose now we enter the fitted cubic model as the sole input to the
 anova function. This produces the following output.
 m3 <- lm(y ~ x + I(xˆ2) + I(xˆ3)); anova(m3)
 Analysis of Variance Table
 Response: y
 Df Sum Sq Mean Sq F value Pr(>F)
 x
 1 0.001927 0.001927 0.514 0.5055135
 I(x^2)
 I(x^3)
 1 0.198885 0.198885 53.055 0.0007631 ***
 1 0.005867 0.005867 1.565 0.2662774
 Residuals 5 0.018743 0.003749--
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 140
14.1. SEQUENTIAL ANOVA
 141
 R automatically separates the regression sum of squares of the cubic model into the contribu
tion from each predictor term as they are added one-by-one in sequence. (To change
 the order in which the predictor terms are considered, we need to change the order in the
 original lm command.) So the sum of squares reported against x is the extra sum of squares
 explained by the model (or equivalently, the reduction in residual sum of squares) when
 moving from the null model to the simple linear regression model, that is, when we include x
 as a predictor term. Similarly, the sum of squares reported against I(xˆ2) is the extra sum
 of squares explained by the model (or equivalently, the reduction in residual sum of squares)
 when moving from the simple linear regression model to the quadratic model and thus
 including x2 as an additional predictor term. Finally, the sum of squares reported against
 I(xˆ3) is the extra sum of squares explained by the model when moving from the quadratic
 regression model to the cubic model and thus including x3 as an additional predictor term.
 The regression sum of squares of the cubic model is given by the contributions from x, x2
 and x3 and so given by 0.001927 + 0.198885 + 0.005867 = 0.206679. The regression sum of
 squares of the quadratic model is given by the contributions from x and x2 and so given
 by 0.001927 + 0.198885 = 0.200812. Compare this last result to the results we obtained in
 Section 13.5!
 The above ANOVA table produced by R tests multiple hypotheses. If the formula for the
 cubic model is represented algebraically as
 Y =β0+β1x+β2x2+β3x3+ϵ
 then we can define the other models as special cases with restricted parameter values as
 follows:
 Hypothesis
 β0
 β1
 β2
 β3
 Description
 H0
 H1
 H2
 H3
 arbitrary
 arbitrary
 arbitrary
 arbitrary
 0
 arbitrary
 arbitrary
 arbitrary
 0
 0
 arbitrary
 arbitrary
 0
 0
 0
 arbitrary
 null
 model
 linear
 model
 quadratic
 model
 cubic
 model
 The rows in the table returned by anova(), that is the sequential ANOVA table, tests the
 following hypotheses:
 Label
 Test
 x
 H0 vs H1
142
 CHAPTER 14. MORE ON F-TESTS AND ANOVA
 Label
 Test
 I(xˆ2) H1 vs H2
 I(xˆ3) H2 vs H3
 From the ANOVA table returned by R we deduce that adding the first two terms leads to a
 significant improvement, however the x3 term does not, so this suggests that we should opt
 for the quadratic model. Note that this works because we added the polynomial terms in
 order of increasing complexity.
 Exercise 38: In Section 12.3 we considered a study to investigate the effectiveness of two
 treatments for blood pressure compared with a control treatment.
 Control Treatment 1 Treatment 2-4-10
 +1-2
 +1-1-5-8-5-7-4-2-5-2-7
 We fitted the following model to the data
 
 
 
 
 Yi
 =
 
 
 
 µ0 +ϵi i in control group,
 µ1 +ϵi i received Treatment 1,
 µ2 +ϵi i received Treatment 2.
 Following the steps below, use an F-test to examine the hypothesis that neither the Control
 nor Treatment 2 have an effect, that is
 H0 : µ0 =µ2 =0 against H1 : µ0=0 or µ2=0 or both are non-zero.
 a. Write out the null hypothesis in matrix form.
 b. Calculate the observed F-statistic and determine the corresponding p-value.
 c. What conclusion can you draw?
 14.2 Factors
 A common scenario in which we consider a null hypothesis specifying more than one linear
 constraint is when we would like to decide whether to include a categorical predictor (or
 factor) with more than two categories in our model.
14.2. FACTORS
 143
 For illustration we will consider the following artificial data set. We are interested in
 predicting the price of a second-hand product using its age and brand as predictors. There
 are three possible brands: A, B and C.
 The data was generated using the following linear model. (Of course, with real data we
 would not know how it was generated.)
 
 
 
 Pricei
 =
 
 
 500 −10×agei +ϵi if product i is of Brand A
 550 −10×agei +ϵi if product i is of Brand B
 555 −15×agei +ϵi if product i is of Brand C
 where ϵi 
iid
 ∼ N(0,50).
 Figure 14.1 illustrates the data using colour-coding and symbols to distinguish the observa
tions belonging to different brands.
 Figure 14.1: Plot of the artificial data set: a scatterplot of price against age, coded by
 colour/symbol according to brand.
 14.2.1 Including both age and brand as predictors
 Suppose we include both age and brand as explanatory variables in a model to predict the
 second-hand price of a product. We use Brand A as reference category so the model is
 represented algebraically as
 
 
 
 Pricei
 =
 
 
 µ
 + β ×agei + ϵi if product i is of Brand A
 µ+αB + β × agei + ϵi if product i is of Brand B
 µ+αC + β × agei + ϵi if product i is of Brand C
 Then, the parameter αB denotes the difference in intercept between the regression line for
 Brand B and the one for Brand A (and analogously for αC).
 So let us fit the model and consider the resulting R summary output.
144
 CHAPTER 14. MORE ON F-TESTS AND ANOVA
 Call:
 lm(formula = price ~ age + brand)
 Residuals:
 Min
 1Q Median-110.736-29.094
 3Q
 Max
 0.381 28.526 115.972
 Coefficients:
 Estimate Std. Error t value Pr(>|t|)
 (Intercept) 519.624
 15.002 34.638 < 2e-16 ***
 age
 brandB
 brandC----11.105
 53.188-5.562
 1.011-10.980 < 2e-16 ***
 11.421 4.657 1.08e-05 ***
 11.421-0.487
 0.627
 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 Residual standard error: 45.68 on 92 degrees of freedom
 Multiple R-squared: 0.6242,
 Adjusted R-squared: 0.6119
 F-statistic: 50.94 on 3 and 92 DF, p-value: < 2.2e-16
 Here the estimated coefficient for, say, brandB is αB = 53.19. The parameter αB represents
 the expected difference in price between a product of Brand B and a product of Brand A
 assuming they are of the same age. And so we predict the second hand price of a
 product of Brand B to be roughly £53 higher than one of Brand A of the same age. Similarly,
 the t-test reported for this coefficient considers whether there is evidence for a difference in
 expected price between a product of Brand B and one of Brand A assuming they are of the
 same age.
 From the above discussion we note that the coefficients and corresponding statistics will
 thus depend on the reference category. If we declare Brand B as the reference category using
 the R-function relevel, we obtain the following results.
 Coefficients:
 Estimate Std. Error t value Pr(>|t|)
 (Intercept) 572.811
 15.002 38.183 < 2e-16 ***
 age
 brandA
 brandC-11.105-53.187-58.750
 1.011-10.980 < 2e-16 ***
 11.421-4.657 1.08e-05 ***
 11.421-5.144 1.51e-06 ***
 We note that while earlier the coefficient for Brand C was not significant even at 5%, it now
 is. This is because while there is strong evidence for a difference in expected price between
 a product of Brand C and a product of Brand B assuming they are of the same age, there is
 insufficient evidence for a difference in expected price between a product of Brand C and a
14.2. FACTORS
 145
 product of Brand A of the same age. This illustrates that we cannot use t-tests to decide
 whether to include brand as a predictor variable in the model.
 While the estimated coefficients and corresponding t-tests differ, note that the model is still
 the same model as the one which uses Brand A as a reference category, we have just used a
 different parameterisation. If you work out the model equation for each brand in the model
 using Brand B as reference category, then you will obtain the same equations as for the
 model that uses Brand A as reference category.
 To compare the parallel lines model with the single linear regression model that contains
 only age as a predictor we need to impose more than one linear constraint and hence we
 need to use an F-test. We consider
 H0 : αB =αC =0 vs H1: at least one of αB and αC is not equal to zero.
 We are imposing q = 2 linear constraints. There are n = 96 units of observation and the
 model under the alternative hypothesis has p = 4 parameters. Hence the F-statistic has
 q = 2 and n−p = 92 degrees of freedom. We can perform the F-test in R by fitting a
 simple linear regression of price on age, leading to the fitted model m1 and then using the
 command anova(m1,m2) where m2 is the regression of price on age and brand.
 We obtain the following output which provides strong evidence against the null hypothesis
 at any of the standard significance levels. We conclude that including brand in the model is
 a significant improvement over the simple linear regression model.
 Analysis of Variance Table
 Model 1: price ~ age
 Model 2: price ~ age + brand
 Res.Df
 RSS Df Sum of Sq
 1
 2
 94 259312
 92 191990 2
 F
 Pr(>F)
 67322 16.13 9.886e-07 ***--
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 14.2.2 Interaction between brand and age
 In the previous section we introduced a model that fitted three parallel regression lines to the
 data. As a more complex model we may fit three separate regression lines that differ both in
 intercept and slope. We do this by introducing an interaction between brand and age. As
 discussed in Section 9.3, an interaction is used when we want to model that the effect of an
 explanatory variable on the response depends on the value of another explanatory variable.
 (Note we are referring to the effect of an explanatory variable on the response and thus the
 relationship between this variable and the response. We are not saying that the explanatory
 variable itself depends on the values of the other predictors.) In our case, we are interested
146
 CHAPTER 14. MORE ON F-TESTS AND ANOVA
 in whether the effect that age has on the second hand sale price of a product will depend on
 its brand.
 We may represent the model algebraically as
 
 
 
 Pricei
 =
 
 
 α
 +
 β ×agei +ϵi if product i is of brand A
 (α +αB)+(βB +β)×agei +ϵi if product i is of brand B
 (α +αC)+(βC +β)×agei +ϵi if product i is of brand C
 Note that we are using Brand A as reference category and that this formulation leads to
 three regression lines which may differ in intercept or slope.
 Let us fit this model in R.
 m3 <- lm(price ~ age + brand + age:brand)
 summary(m3)
 Call:
 lm(formula = price ~ age + brand + age:brand)
 Residuals:
 Min
 1Q Median-109.978-24.296
 3Q
 Max
 1.759 24.309 104.993
 Coefficients:
 Estimate Std. Error t value Pr(>|t|)
 (Intercept) 521.346
 23.082 22.587 < 2e-16 ***
 age
 brandB
 brandC
 age:brandB
 age:brandC----11.243
 19.768
 22.691
 2.674-2.260
 1.732-6.489 4.58e-09 ***
 32.643 0.606
 32.643 0.695
 2.450 1.091
 2.450-0.923
 0.546
 0.489
 0.278
 0.359
 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 Residual standard error: 45.18 on 90 degrees of freedom
 Multiple R-squared: 0.6404,
 Adjusted R-squared: 0.6205
 F-statistic: 32.06 on 5 and 90 DF, p-value: < 2.2e-16
 We note that neither of the coefficients for the interaction terms (in R denoted as age:brandB
 and age:brandC) is significant. However, as we have seen in the previous section, we cannot
 deduce from this that the interaction terms are not needed in the model. To formally decide
 whether we should be including the interaction in the model we need to perform an F-test in
 which we compare the model without interaction against the model with interaction. From
 this we obtain the following output:
14.2. FACTORS
 147
 anova(m2,m3)
 Analysis of Variance Table
 Model 1: price ~ age + brand
 Model 2: price ~ age + brand + age:brand
 Res.Df
 RSS Df Sum of Sq
 1
 2
 92 191990
 F Pr(>F)
 90 183695 2 8295.8 2.0322 0.137
 The resulting p-value is large enough for us not to reject the null hypothesis even at a
 10% significance level. Therefore there is insufficient evidence to support the need for an
 interaction in the model. We thus conclude that the effect of age on price is not significantly
 different between the various brands. Hence we retain the parallel lines model.
 Given our aim of fitting separate regression lines, you may wonder why we used a three
 line model that was fitted to the whole data set rather than splitting the data into subsets
 according to brand and then fitting a completely separate model for each subset. An
 advantage of the approach above is that it provides a framework to test for differences
 between the intercepts/slopes of the regression lines compared to the reference category.
 More importantly, it uses the whole data set to estimate the error variance σ2 which,
 assuming homoscedasticity, will yield a more accurate estimate and thus more powerful tests.
 We have already seen an example of this in Section 12.3 when we considered the study on
 treatment effects.
 Exercise 39
 Above we performed an F-test to decide whether to include interaction terms in the model.
 Write out the null hypothesis and the alternative hypothesis for this test. Furthermore
 determine the degrees of freedom of the corresponding F-statistic. (Hint: the number of
 units of observation in this example is n=96.)
 Exercise 40
 Consider the Retail dataset. For j = 1,...,n let
 xj1 = 1 ifstore j is of Brand B
 0 otherwise
 1. Use R to fit the model
 Salesj
 and xj2 = 1 ifstore j is of Brand C
 0 otherwise
 = µ+αBxj1+αC xj2+β pricej + ϵj
 and perform a hypothesis test to decide whether to include brand in addition to price
 in the model.
148
 CHAPTER 14. MORE ON F-TESTS AND ANOVA
 2. Use R to fit the model
 Salesj
 = µ+αBxj1+αC xj2+β pricej+γB (xj1×pricej)+γC (xj2×pricej) + ϵj,
 and perform a hypothesis test to decide whether to include an interaction between
 brand and price in the model.
 14.3 Test for non-linearity
 Here we consider a final version of an F-test, namely the test for the non-linearity of
 regression.
 Suppose we observe the response Y several times for each value of the explanatory variable
 X. For example, in the following toy example, we have two or three replications of each
 x-value.
 x-1-1 0 0 0 1 1 3 3 3
 y 1.1 1.2 1.3 1.4 1.2 1.5 1.4 2.9 3.2 3.0
 More formally, consider data (xi, yij),j = 1,...,ni, i = 1,...,m where some x’s are
 replicated, i.e. there are m distinct x’s with ni replications of xi. A representation of the
 above data set in this format is given by
 i
 1
 2
 3
 4
 xi
 yi,1-1
 0
 1
 3
 1.1 1.3 1.5 2.9
 yi,2
 yi,3
 1.2 1.4 1.4 3.2– 1.2– 3.0
 A natural starting point is to assume a simple linear regression model in which the expected
 value of the response is a linear function in x, that is we consider the straight line model
 M0 : Yij = α+βxi+ϵij where
 j =1,...,ni
 i =1,...,m.
 Figure 14.2 shows a scatterplot of the data together with the fitted straight-line model.
 We observe that the straight-line model does not fit terribly well, so in the following we
 will examine more formally whether the assumption that the expected response is a linear
 function of the explanatory variable is justified.
14.3. TEST FOR NON-LINEARITY
 149
 Figure 14.2: Scatterplot of the data with fitted regression line
 Consider an alternative model, namely
 M1 :
 Yij = µi +ϵij where
 j =1,...,ni
 i =1,...,m.
 This is a very flexible model that treats X as a factor variable. Note that if we constrain
 the µi’s to lie on a line, that is
 µi
 = α+βxi
 then this produces a straight-line model as given by M0. Hence M0 is nested in M1 and so
 we can perform an F-test to determine if model M1 is a significant improvement on model
 M0. This test is called the test for non-linearity of regression. We have
 F = (D0−D1)/(m−2)
 D1/(n −m) ,
 m
 where n =
 i=1
 ni and, D0 and D1 are the deviances of models M0 and M1 respectively.
 Under H0, that is, the straight line model M0, we have F ∼ Fm−2,n−m. Therefore we can
 compute the p-value as P(F ≥ Fobs), where Fobs is the observed F-statistic. We reject H0
 at significance level α if P(F ≥ Fobs) < α.
 Note that if there is no replication in the data set, then M1 has as many parameters as there
 are data points and so the residual degrees of freedom is n − m = 0. In this case we are
 unable to perform the test.
 For the above data set we can use R to compute the following quantities.
 Fobs = (0.68−0.077)/2
 0.077/6
 = 23.6.
150
 CHAPTER 14. MORE ON F-TESTS AND ANOVA
 Since the appropriate critical value at a 1% significance level is F2,6(0.99) = 10.92 we have
 strong evidence to reject the hypothesis H0 which assumes that a straight-line model suffices.
 Therefore, a more complex model is needed. For example, we might consider a quadratic
 model which is more parsimonious than the model that treats the explanatory variable as a
 factor.
 Exercise 41
 Data are available in the form (xi,yij),(i = 1,...,m,j = 1,...,ni). A scatterplot suggests
 that the relationship between y and x may be non-linear.
 Carry out the test for the non-linearity of regression using the following data:
 m
 m=5,
 i=1
 m
 ni = 14,
 i=1
 ni
 j=1
 m
 (yij − ¯ yi·)2 = 3.57,
 i=1
 ni
 j=1
 [yij − (β0 + β1xi)]2 = 4.98.
 (where β0, β1 are the least-squares estimates of the intercept and slope when the straight-line
 model is fitted and ¯ yi· = 1
 ni 
ni
 j=1 yij.)
 14.4 Summary of Chapter 14
 • Asequential ANOVA splits the regression sum of squares into the contributions
 made by the individual predictor terms.
 • To decide whether factors with more than 2 levels should be included in a model, we
 cannot use a t-test but instead must use an F-test.
 • The test for non-linearity of regression is an F-test that compares a simple linear
 regression model against a model in which the predictor variable is implemented as a
 factor variable.
 After studying this chapter you should
 • know how to perform a sequential ANOVA;
 • be able to apply an F-test in a variety of contexts, for example to determine whether
 including a factor is a significant improvement to the model or to perform a test for
 non-linearity.
 If you wish to engage in further (optional) reading on the topic covered in this chapter, then
 the following sections of the textbooks recommended for this module cover related material.
 • Generalized linear models with examples in R (2018) by Dunn and Smyth: Sections
 2.9, 2.10.1- 2.10.3.
 • Data analysis and graphics using R: an example-based approach (2010) by Maindonald
 and Braun: Sections 6.1, 7.1 and 7.3
Chapter 15
 Model selection criteria and
 variable selection
 In Chapters 3 and 4 we examined the local fit of a model by studying residual plots and
 making improvements via transformations to ensure that the model assumptions are met. In
 the following we will assume that these assumptions are being met by the models considered.
 In the last two chapters we considered the overall/global fit of a model by studying the
 residual sum of squares. In particular, we discovered how the F-statistic balances the
 reduction of the residual sum of squares due to adding variables to the model against the
 additional degrees of freedom being determined by the model.
 In this chapter we will continue exploring the question of how to decide which predictor
 terms to include in a normal linear model.
 15.1 Bias and Variance
 Statistically speaking, there is a trade-off between using a model with too many predictor
 terms and a model with too few predictor terms. In particular,
 i. omitting relevant predictor terms may lead to bias in estimation;
 ii. choosing a model that uses non-informative predictor terms unnecessarily increases
 the variance of the parameter estimators.
 To illustrate i. consider the following scenario.
 Scenario 1: Assume the data were generated by the model
 Yi
 = α0+α1xi+α2zi+ϵi for i=1,...,n, where ϵ ∼N(0,σ2In).
 151
152 CHAPTER 15. MODEL SELECTION CRITERIA AND VARIABLE SELECTION
 However, we model the data as
 E(Yi) = β0+β1xi for i=1,...,n.
 We will be using the following shorthand notation: Svw = n
 i=1(vi − ¯ v)(wi − ¯ w).
 It can be shown that in Scenario 1 the expectation of the least squares estimator β1 = β1(Y )
 satisfies
 E(β1) = α1+α2 
Sxz
 Sxx
 .
 Therefore β1 is only an unbiased estimator for α1, the coefficient for the predictor variable
 X, if either α2 = 0 or the sample covariance between x and z is zero.
 To illustrate ii. consider the following set-up.
 Scenario 2: Suppose the data were generated by the model1
 Yi
 = α0+α1(xi−¯ x)+ϵi for i=1,...,n, where ϵ ∼ N(0,σ2In),
 but we model this data as
 E(Yi) = β0+β1(xi−¯ x)+β2(zi − ¯ z) for i = 1,...,n.
 Then
 Var(β1) = σ2
 Szz
 SxxSzz −S2 xz 
=
 σ2
 Sxx −S2 xz/Szz 
> σ2
 Sxx 
= Var(α1).
 Therefore adding the predictor variable Z to the model has increased the variance of the
 estimator for the coefficient of X.
 Exercise 42 Assume Scenario 1 as described above and show that the expectation of the
 least squares estimator β1 is given by
 E(β1) = α1+α2 
Sxz
 Sxx
 .
 Exercise 43 Assume Scenario 2 as described above and show that the variance of the least
 squares estimator β1 is given by
 Var(β1) =
 σ2
 Sxx −S2 xz/Szz 
> σ2
 Sxx 
= Var(α1).
 1Mean-centering the predictor variables in this scenario is a re-parametrisation of the model that simplifies
 the expressions in the necessary computations.
15.2. THE MODEL HIERARCHY
 153
 15.2 The model hierarchy
 15.2.1 Definitions
 • The minimal model is the simplest model consistent with known features of the
 experiment, the data and any underlying theory.
 • The maximal model is the most complex model worth considering in the analysis.
 • The saturated model is the model with n parameters where n is number of units of
 observation.
 Note that the saturated model is of little statistical merit as it will fit the observed data
 points exactly. Similarly the null model is limited as all of its fitted values are equal. But
 both are included as boundary models.
 In judging the adequacy of the current model, that is the model under consideration, we
 must be aware of the model hierarchy:
 • null,
 • minimal,
 • current,
 • maximal,
 • saturated.
 Example
 Consider a collection of polynomial regression models for a response Y given input x. If we
 observe a relationship between the input variable and the response variable, then we should
 not consider the null model. Let us say that theory from the application context tells us
 that a cubic model is the most complex model worth considering and that we have n = 6
 units of observation. Thus, the model hierarchy is:
 • null:
 E(Yi) = β0,
 • minimal: E(Yi) = β0 +β1xi,
 • maximal: E(Yi) = β0 +β1xi +β2x2 i +β3x3 i,
 • saturated: E(Yi) = β0 +β1xi +β2x2 i +β3x3 i +β4x4 i +β5x5 i, where i = 1,...,6.
 Therefore we only consider polynomial regression models of order 1, 2 and 3 and thus with
 2, 3 or 4 parameters.
 Exercise 44
 Show that the saturated model fits the data exactly. (Hint: Show that the inverse of the
 design matrix exists and thus derive a simplified expression for the estimate β(y) and then
 use this expression to compute the fitted values y.)
154 CHAPTER 15. MODEL SELECTION CRITERIA AND VARIABLE SELECTION
 15.2.2 Principle of marginality
 Consider the polynomial regression models from the previous section. Note that we included
 all lower order terms in all polynomial expressions. For example the cubic model included all
 lower order terms, that is the linear term x and the quadratic term x2. This is recommended
 practice when using powers of numerical predictors. To illustrate why this is recommended,
 consider the model
 E(Y ) = β0+β2x2
 that is a quadratic model in which the linear term has been omitted. Note that this
 model assumes that the global extremum of the mean function is at x = 0, a rather strong
 assumption which in practice is often difficult to justify.
 Now suppose that instead of x we use z = x−c for some constant c to define the mean
 function of Y . We have learned that this is a simple re-parameterisation of the model. Then,
 E(Y ) = β0+β2x2 =β0+β2(z+c)2 = (β0+β2c2)+(2β2c)z +β2z2.
 We observe that the additive scale change in x has forced the linear term z into the model
 equation. Moreover, an additive change in scale may also change the T-statistic of all but
 the highest order term. Therefore it is common practice to retain all lower order terms in a
 polynomial model, even if they are not statistically significant.
 Similar issues arise if we include interaction terms without the main effects. Suppose X is a
 numerical predictor variable and G is a categorical predictor variable with levels 0 and 1.
 Consider the model
 E(Y ) = β0+β1xg.
 where x is the value taken by X and g is the value taken by G. This models an interaction
 between X and G but omits the main effects of the predictors, that is, the model does not
 have a predictor term that depends on the value of X only nor is there a term that depends
 on the value of G only. Now suppose we subtract a constant c from the numerical predictor
 value x. Let z = x−c, then
 E(Y ) = β0+β1xg =β0+β1(z+c)g =β0+(β1c)g+β1zg.
 So a main effect for G, that is a predictor term that depends on the value of G but not the
 value for X, appears in the resulting model equation.
 Alternatively, suppose we change the reference category for G, so let H = 1 − G, then
 E(Y ) = β0+β1xg =β0+β1x(1−h) =β0+β1x−β1xh,
 and so the term β1x and thus a main effect for X appears in the resulting model equation.
 As with polynomial regression, it is common practice to retain terms that would re-appear
 if we made inconsequential changes such as changing the reference category or adding a
 constant to a numerical predictor variable.
15.3. MODEL SELECTION STATISTICS
 155
 Thus the general practice is to apply the principle of marginality:
 • if we include higher powers of a numerical predictor variable in the model, then we
 also include the lower order terms of that variable;
 • if we include an interaction between two predictor variables in the model, then we also
 include the individual predictor variables, that is the corresponding main effects.
 15.3 Model selection statistics
 In this section we consider global fit statistics or model selection statistics. These are used
 to compare different models and thus help us with the decision of which model to choose.
 15.3.1 The coefficient of determination R2
 We could consider the model deviance as a measure of model fit. However, the deviance
 depends on the scale of the response. If we measure the response in different units, say
 meters instead of feet, then we obtain a different value for the deviance of the model. This
 is undesirable.
 A measure that is based on the deviance but that is standardised, is the coefficient of
 determination R2. We refer to the linear model under consideration as the current model.
 Definition 15.1 (Coefficient of Determination). For a current model that has the null
 model nested within it, the coefficient of determination is defined as
 R2 = 1− Deviance(current model)
 Deviance(null model) .
 Here we are using the deviance of the null model as a comparison. However, not all models
 can be compared with the null model in this way. This comparison only makes sense when
 the null model is nested within the model under consideration, the current model. This
 is satisfied if the current model contains an intercept term or can be reparameterised to
 contain an intercept term.
 For example, the model E(Y) = βx cannot be compared to the null model E(Y) = α, but
 the model E(Y) = α+βx can.
156 CHAPTER 15. MODEL SELECTION CRITERIA AND VARIABLE SELECTION
 Theorem 15.1 (Properties of the coefficient of determination).
 n
 (1)
 R2 =
 i=1
 (yi − y)2
 n
 i=1
 (2)
 (yi − y)2
 0 ⩽ R2 ⩽ 1.
 = Regression sum of squares
 Total sum of squares ,
 Proof. Firstly note that, if the null model is nested within the current model, then, by the
 decomposition of the total sum of squares,
 n
 i=1
 see Theorem 13.1.
 n
 (yi − y)2 =
 i=1
 n
 (yi − yi)2 +
 i=1
 (yi − y)2
 TotalSS = ResidSS + RegrSS
 Next note that the deviance of the null model is equal to the total sum of squares and the
 deviance of the current model is given by the residual sum of squares, that is
 n
 Deviance(null model) = TotalSS =
 Deviance(current model) = ResidSS =
 i=1
 n
 (yi − y)2,
 (yi − yi)2.
 i=1
 By applying the decomposition of the total sum of squares to the definition of R2 we can
 deduce (1).
 To prove (2) note that sums of squares must be non-negative. Hence, by definition, R2 ⩾ 0
 and, furthermore, as the residual sum of squares is bounded above by the total sum of
 squares, we have R2 ⩽ 1.
 Remarks
 • WecanuseR2 tocompare the fit of two models where neither is nested within the other,
 as long as the null model is nested within them both. For example, E(Y) = α+βx
 can be compared with E(Y) = α+βex.
 • R2canbeinterpreted as the proportion of the variation in the response that is absorbed
 by the model. By the decomposition of the total sum of squares, the “left-over” variation
 in the response is represented in the residual sum of squares.
15.3. MODEL SELECTION STATISTICS
 157
 Exercise 45 Consider the following toy example.
 E[Y1] = α,
 and
 E[Y2] = β,
 E[Y3] = α−β
 y =(6,9,3)T and ˆ y = (8,7,1)T.
 a. Compute the deviance D of the model above as well as the deviance D0 of the null model.
 b. Compute the coefficient of determination as R2 = 1− D
 D0
 .
 c. Compute the coefficient of determination as R2 = RegrSS
 TotalSS.
 Why do the results in b. and c. not agree?
 15.3.2 Adjusted R2
 The deviance of a model usually decreases (and never increases) when we add additional
 predictor terms. Therefore the coefficient of determination R2 also tends to increase as we
 include additional predictor variables in the model. However, by the principle of parsimony,
 an improvement in global fit should be balanced against the complexity of the model, that
 is the dimension p of the parameter vector. Hence alternative measures of global fit have
 been suggested that include a penalty for model complexity. One such measure is R2
 adj, the
 adjusted R2 statistic.
 Definition 15.2 (The adjusted coefficient of determination). The adjusted R2 statistic is
 defined as
 n
 R2
 adj
 = 1−s2(current model)
 s2(null model)
 = 1−
 i=1 (yi − yi)2 ⧸(n −p)
 n
 i=1 (yi − y)2 ⧸(n−1) .
 In the above definition s2(current model) is the unbiased estimate of the error variance σ2
 in the model of interest and s2(null model) is the unbiased estimate of the error variance
 σ2 under the null model. As the denominator of s2(current model) is equal to n − p, this
 expression takes account of the number of parameters p in the current model and thus the
 model complexity.
 Comparing this expression to the definition of R2 we find that
 R2
 adj
 = 1− n−1
 n−p 1−R2 .
 Both R2 and R2
 adj are reported at the bottom of the model summary in R.
 Exercise 46 For the leafburn dataset in the faraway package use R to fit the model
 log(Y ) = α+βx+ϵ where x is the nitrogen % of the leaf.
158 CHAPTER 15. MODEL SELECTION CRITERIA AND VARIABLE SELECTION
 a. In R determine the deviance of the model and the unbiased estimate s2(current model)
 of the error variance σ2.
 b. Next determine the deviance of the null model and the unbiased estimate s2(null model)
 of the error variance σ2 under the null model.
 c. Use these quantities to compute R2 and R2
 adj from first principles and compare your
 results with the results in the summary output of the model in a.
 d. Next simulate 30 independent observations from a U(0,1) distribution and use these
 to define a new input variate w.
 e. Fit the model log(Y ) = α+βx+γw+ϵ.
 f. Identify R2 and R2
 adj in the summary output of the model in e. and compare it to
 your results in c. What do you conclude?
 15.3.3 Mallow’s Cp
 Next we discuss a global fit statistic that is specifically related to variable selection. Like
 R2
 adj, it is based on the deviance of a model together with a penalty for model complexity.
 We assume we have a maximal model with pmax parameters. We then consider a less complex
 model with p < pmax parameters that is nested within the maximal model. We will refer
 to this second model as the current model. As usual n denotes the number of units of
 observation.
 Definition 15.3 (Mallow’s Cp). Mallow’s Cp is defined as
 Cp = ResidSS(current model)
 s2max
 where s2max = ResidSS(max model)
 n−pmax
 +2p−n,
 is the unbiased estimate of the error variance under the
 maximal model.
 The first term in Cp, namely ResidSS(current model)/s2max, is a measure of model fit, while
 the second term 2p −n is a measure of model complexity. Thus by identifying models with
 small Cp we are aiming to strike a balance between fit and complexity. This is in alignment
 with the principle of parsimony which aims for the simplest model that still describes the
 relevant features of the data.
 We can show that
 Cp(maximal model) = pmax = number of parameters in the maximal model.
 The aim is to identify models with a small Mallow’s Cp, preferably with Cp ⩽ p. By the
 principle of parsimony, if we have several candidate models with the same Cp, the simplest
 one is preferable. But as discussed previously, there are competing aspects of what makes a
 good model which need to be balanced against each other.
15.3. MODEL SELECTION STATISTICS
 159
 Cp considers the fit of the current model relative to the maximal model (assuming the
 current model is nested within the maximal model). This is in contrast to the previous
 model selection statistics R2 and R2
 adj which consider the fit of the current model relative to
 the null model (assuming the current model has the null model nested within it).
 Mallow’s Cp can be used to compare any two models that are nested within the maximal
 model, even if neither of these two models is nested within the other. Hence, like R2 and
 R2
 adj, Mallow’s Cp applies to models that cannot be compared with the F-test.
 15.3.4 Akaike’s information criterion
 In contrast to the previous model selection criteria, Akaike’s information criterion, or short
 the AIC, is based on the likelihood function of the model under consideration. As it is
 defined via a likelihood function, the AIC can be extended to models beyond the normal
 linear model, for example to Generalised Linear Models (GLMs).
 Definition 15.4 (AIC for a normal linear model). Let l(β,σ2) be the log-likelihood function
 of the normal linear model. Then Akaike’s information criterion or AIC for the normal
 linear model is defined as
 AIC = −2l(β,ˆ σ2
 MLE) + 2p,
 where ˆ σ2
 MLE = ResidSS
 n
 ˆ
 is the maximum likelihood estimate of the error variance σ2. (Thus
 σ2
 MLE is different from the unbiased estimate s2 which we have been using so far.)
 Recall that in the normal linear model we assume
 Y ∼ Nn(Xβ, σ2In).
 Hence, the likelihood function of β and σ2 is given by
 L(β,σ) ∝ 1
 σn exp − 1
 2σ2 
n
 i=1
 (yi −xT
 iβ)2 .
 (Note that likelihood functions are defined up to a constant of proportionality and, analo
gously, log-likelihood functions are defined up to an additive constant.)
 The log-likelihood function of β and σ2 is thus
 l(β,σ2) = − 1
 2σ2 
n
 i=1
 (yi −xT
 iβ)2 − n
 2 log(σ2)+c
 for some constant c ∈ R.
160 CHAPTER 15. MODEL SELECTION CRITERIA AND VARIABLE SELECTION
 Hence, with ResidSS = n
 i=1(yi −xT i β)2 we have that
 n
 l(β, ˆ σ2
 MLE)
 = − 1
 2ˆ σ2
 MLE
 i=1
 = −nResidSS
 (yi − xT
 i β)2 − n
 2 log(ˆ σ2
 MLE) + c
 2 ResidSS − n
 2 log(ResidSS/n) + c
 = −n
 2−n
 2 log(ResidSS/n)+c
 Therefore
 since ˆ σ2
 MLE = ResidSS
 n
 AIC = −2l(β,ˆ σ2
 MLE) + 2p = n+nlog(ResidSS/n)−2c+2p
 = nlog(ResidSS/n)+2p+constant
 where the additive constant does not depend on the residual sum of squares or p and thus is
 the same for any two models being compared. In R the AIC is simply computed as
 AIC = nlog(ResidSS/n)+2p.
 Like the other model selection criteria, the AIC aims to balance between model fit as
 measured by the residual sum of squares and model complexity given by p, the dimension of
 the parameter vector. Thus we aim for a model with small AIC.
 The AIC does not assume any type of nestedness and so can be used to compare non-nested
 models.
 So in summary, we look for candidate models with low AIC, low Cp, high R2
 adj and for models
 where R2 does not increase much when further parameters/predictor terms are added.
 Exercise 47
 Consider the artificial data Yi = x3 i+x2 i−2xi+3+ϵi, where ϵi ∼ N(0,0.4) iid, i = 1,...,20,
 as generated by the R code below.
 set.seed(30)
 x <- runif(20,-2,2)
 y <- rnorm(20,xˆ3+xˆ2-2*x+3,0.4)
 Fit polynomial regression models from degree 1 up to degree 5 to the data. Compute R2
 adj,
 Mallow’s Cp and the AIC, for each and decide which model is best.
 To compute the AIC use the function extractAIC(model) which returns the number of
 regression parameters and the AIC of model.
15.4. VARIABLE SELECTION
 161
 15.4 Variable selection
 We often wish to consider the best model which includes only the most important predictors
 (input variates). Selecting the best predictors is called “variable selection”. Variable selection
 (or more generally, model selection) is a major challenge in practice, especially when there
 are many predictors to choose from.
 The most important criterion for including predictors in a model is the ana
lyst’s/experimenter’s expert knowledge of the area under study as well as of the
 variables (including expected sign and magnitude of coefficients). This will ensure that the
 model developed is plausible.
 15.4.1 Best subsets regression
 Best subsets regression is a method of variable selection in which all possible regressions
 are performed and the best models for each number of parameters are suggested. The
 statistician then chooses the most appropriate model (or models) from the subset presented,
 based on the values of measures like R2, R2
 adj and Mallow’s Cp.
 Let us have a look at an example, the dataset fuel from the alr4 package. This comprises
 data on fuel consumption from 50 US states plus the District of Columbia. We will make
 use of 4 predictors: TAX (on fuel in cents per gallon), DLIC (proportion of licenced drivers),
 INC (per capita personal income) and ROAD (highway miles in the state). The response is
 the per capita fuel sales in thousands of gallons. For illustration purposes we supplement the
 data set with an extra (non-informative) input variate RAND, consisting of 51 independent
 observations from a U(0,1) random variable. Will the variable selection methods detect this
 input variate as non-informative?
 The regsubsets function in the R-package leaps performs best subset regression. The
 option nvmax determines the maximum number of predictor terms to consider (excluding
 the intercept) and nbest specifies the number of models for each number of parameter to
 report on. The first input of the function specifies the maximal model. In this example
 we used the formula FUEL ~ . to indicate that the maximal model contains all predictor
 variables in the data set.
 # Load required packages
 library(leaps)
 library(alr4)
 # Manipulate the data into the appropriate format
 library(dplyr)
 fuel2001 <- transform(fuel2001, TAX=Tax, DLIC= Drivers/Pop,
 FUEL= FuelC/Pop, INC=Income, ROAD=Miles)
 fuel <- select(fuel2001, c("TAX","DLIC","INC","ROAD","FUEL"))
162 CHAPTER15. MODELSELECTIONCRITERIAANDVARIABLESELECTION
 #Addthenon-informativeinputvariableRAND
 set.seed(1452)
 fuel$RAND<-runif(51)
 #Performbestsubsetsregression
 bs<-regsubsets(FUEL~.,nbest=1,nvmax=5,data=fuel,method="exhaustiv")
 sbs<- summary(bs)
 p (Intercept) TAX DLIC INC ROAD RAND R^2 R^2.adj Mallow’sCp
 2 1 0 1 0 0 0 0.22 0.20 21.51
 3 1 0 1 1 0 0 0.37 0.34 10.30
 4 1 0 1 1 1 0 0.43 0.40 6.85
 5 1 1 1 1 1 0 0.48 0.43 5.04
 6 1 1 1 1 1 1 0.49 0.43 6.00
 Thetableabovegivesthebestmodel foreachnumberofparameters. Thefirstcolumn
 specifiesthenumberofparameterspofthemodel.Thenextfivecolumnsindicatewhich
 of the inputvariablesare includedthemodel. ThenextcolumnsprovidetheR2,R2
 adj
 andMallow’sCp foreachofthemodels.Weseethatthestatisticsinitiallyimprove,but,
 movingfromthebestfive-parametermodel tothemaximalmodel, the improvement is
 onlymarginal forR2andnon-existentforR2
 adj.Furthermore, forthefive-parametermodel,
 Mallow’sCp is justmarginallyabovethenumberofparameters, that isfive. Hencethis
 output suggests thatweusethemodel that includesTAX,DLIC, INCandROADbut
 excludesthenon-informativevariableRAND.
 15.4.2 Stepwiseregression
 Ifthenumberofexplanatoryvariables isverylarge, thenthenumberofpotentialcandidate
 modelsisalsolargeandsoitcantakeaverylongtimetocalculateallpossibleregressions.
 Inthisscenariooneapproachistousestepwiseregressioninsteadofbestsubsetsregression.
 Itshouldbenotedthatthereissubstantialcriticismagainsttheuseofstepwiseregression
 methodsinthestatistical literature,butforcompletenesswedescribethemethodhere.
 Instepwiseregression,westartfromaninitialmodelandthenaddorremovepredictors
 basedonamodel selectioncriterion,oftentheAIC.Theadvantageofthisapproachisthat
 it iscomputationallylessdemandingthanbestsubsetsregression.Thedisadvantageisthat
 wearenotconsideringallpossiblemodels, forexampleiftwoexplanatoryvariablesareonly
 informativeincombination, thenneitherwilleverbeaddedtothemodel.
 Thespecialcaseofstepwiseregressionwherewestartfromtheminimalmodelandconsider
 addingasinglepredictoratatimeiscalledforwardstepwiseregression.Thespecial
 casewherewestartfromthemaximalmodelandconsiderremovingasinglepredictorata
 timeiscalledbackwardstepwiseregression.Alternativelywecanuseabidirectionalor
15.4. VARIABLE SELECTION
 163
 hybrid approach where in each step we consider adding or removing a single predictor.
 In the following example we consider artificial data where only the first three of the five
 potential explanatory variables are informative about the response. We use the step function
 in R for which the AIC is the default model selection statistic. The option direction specifies
 whether we consider forward stepwise regression (forward), backward stepwise regression
 (backward) or the hybrid approach (both). The option scope defines the range of models
 to be examined in the stepwise search.
 As the output produced by the stepwise regression method can be rather lengthy, we only
 present the output from the hybrid approach. We have chosen to start with the true model
 containing the first three explanatory variables as predictors.
 # Specifying the minimal and maximal model
 min.model <- lm(y ~ 1, data=data)
 max.model <- lm(y ~ ., data=data)
 m <- lm(y~x1+x2+x3, data=data)
 auto.both <- step(m, direction="both", scope=list(lower=min.model, upper=max.model))
 Start: AIC=26.84
 y ~ x1 + x2 + x3
 Df Sum of Sq RSS
 AIC
 + x5 1 4.99 115.74 24.616
 <none>
 120.73 26.838
 + x4 1 0.03 120.70 28.814- x2 1 33.96 154.69 49.626- x1 1 115.45 236.18 91.940- x3 1 530.62 651.35 193.388
 Step: AIC=24.62
 y ~ x1 + x2 + x3 + x5
 Df Sum of Sq RSS
 <none>
 AIC
 115.74 24.616
 + x4 1 0.20 115.54 26.443- x5 1 4.99 120.73 26.838- x2 1 33.88 149.62 48.290- x1 1 107.28 223.02 88.210- x3 1 534.26 649.99 195.179
 The output tells us that the initial model has an AIC of 26.84. It then computes the ResidSS
 and the AIC for each model produced by either adding one of the variables not currently
 included or removing a variable currently in the model. It then ranks the resulting models
164 CHAPTER 15. MODEL SELECTION CRITERIA AND VARIABLE SELECTION
 according to AIC. We see that adding x5 reduces the AIC by about 2.2 while adding x4 will
 increase the AIC by about 1.8. (Recall that we prefer models with low AIC!) Removing any
 of the variables currently in the model, that is any one of x1,x2 or x3, will also increase the
 AIC. The line with rowname <none> simply reports on the current model. So to improve
 the AIC, the variable x5 is added to the model. The algorithm then continues considering
 adding or removing individual predictors, but finds that any of these moves will increase
 the AIC. The algorithm therefore terminates and so the model suggested by the algorithm
 is the model that contains the first three and the fifth explanatory variable. (Best subsets
 regression applied to this example suggests the same final model).
 In the above example, a non-informative variable was included in the final model. Automatic
 variable selection procedures may include variables that just happen to explain the data
 by chance. For example, if we generated 10 columns of random numbers and used them as
 predictors, automatic variable selection methods are likely find that one or more of them
 “explain” some of the variation in the response, even though each column is equally useful
 (that is completely useless) for prediction.
 Automatic variable selection methods are no substitute for considering which predictors
 are important given the application context!!! Furthermore, inference results obtained after
 variable selection are biased. For example, the standard errors of estimates as well p-values
 from F- and t-tests selection are generally too small.
 Exercise 48
 The cement data in the library MASS contains observations of four predictor variables
 (x1,x2,x3 and x4) and the response variable y. Use automatic variable selection to decide
 which predictors to include in a regression model for the response y.
 15.5 Multicollinearity
 15.5.1 Definition and symptoms
 Two predictors are exactly collinear if c1X1 + c2X2 = c0 for some constants c1, c2 and c0,
 for all cases, that is
 Corr(X1,X2)2 = 1.
 If Corr(X1,X2)2 is close to 1, then X1 and X2 are approximately collinear.
 Suppose we have p−1 predictors. If
 c1X1 +c2X2 +···+cp−1Xp−1 = c0,
 then X1,X2,...,Xp−1 are collinear. If
 c1X1 +c2X2 +···+cp−1Xp−1 ≈ c0
 then the predictors are approximately collinear.
15.5. MULTICOLLINEARITY
 165
 Predictor variables that are (approximately) collinear mean that, basically, the same infor
mation is being measured in more than one way, i.e. some variables are redundant and can
 be dropped. If a subset of the predictors are exactly collinear, then the design matrix X
 will not be of full rank, and so (XTX)−1 will not exist.
 Multicollinearity causes β to be unstable with high variances. This may result in important
 variables appearing unimportant. It also makes the interpretation of inference results difficult.
 However, prediction is not really affected as the fitted values remain reasonable.
 In the case of multi-collinearity we may encounter one or more of the following possible
 symptoms.
 • High sample correlations between the Xi’s but this will not show up if more than
 two variables are collinear.
 • Anoverall F-test (i.e. test for existence of regression) that is highly significant (i.e. has
 a small p-value) while many of the βi’s have large standard errors and their individual
 t-tests are not significant (i.e. have large p-values).
 • Some β′ is have unreasonable values or known important variables have too small βi.
 15.5.2 Variance inflation factors
 Variance inflation factors can be used to detect multi-collinearity. Let R2 j be the coefficient
 of determination from a regression of Xj on the other predictors. It can be shown that
 Var βj
 n
 where SXjXj 
=
 i=1
 = σ2 1
 1 −R2 j
 1
 SXjXj
 Xij − ¯ Xj
 2 is the total sum of squares in the regression of Xj on the
 other predictors. The quantity
 VIFj = 1
 1 −R2 j
 is called the jth variance inflation factor, for j = 1,...,p − 1.
 If Xj is approximately collinear with the other predictors, then
 R2
 j ≈ 1 and VIFj is large.
 If Xj is approximately orthogonal to the other predictors, then
 R2
 j ≈ 0 and VIFj ≈ 1.
 As a rule of thumb, we consider a variance inflation factor V IFj > 10 as large.
166 CHAPTER 15. MODEL SELECTION CRITERIA AND VARIABLE SELECTION
 15.5.3 How to deal with multicollinearity?
 We may combine several predictors with high VIF into an new variable, for example by
 taking the average (if appropriate in the application context). Alternatively we may delete
 one or more of the predictors with high VIF. Here it is useful to consult with domain
 experts who understand the application context before making a decision.
 β
 Instead of making changes to the predictor variables we may use a ridge regression estimate
 rr = (XTX +λIp)−1XTy which “shrinks” the coefficients towards zero. Ridge regression
 is not further discussed in ST221 but you may encounter it in relevant third year modules.
 Redefining variables is not only useful to resolve multicollinearity but also a more general
 method to improve models. For example HEIGHT and WEIGHT, two separate predictor
 variables, might be replaced by INDEX = WEIGHT / HEIGHT, that is a single predictor
 variable.
 Exercise 49
 Explore the cement data set from the package MASS for signs of multi-collinearity.
 Summary of Chapter 15
 • Chapter 15 introduced the principle of marginality that applies to polynomial
 regression and models with interaction terms.
 • We demonstrated the statistical underpinning of the principle of parsimony by dis
cussing the bias-variance trade-off in linear models.
 • The chapter covered global fit statistics or model selection criteria: R2, R2
 adj,
 Mallow’s Cp and the AIC. Except for R2, these are based on quantities that try to
 balance between model fit and model complexity.
 R2 = 1− Deviance(current model)
 Deviance(null model) ,
 R2
 adj
 = 1−s2(current model)
 s2(null model)
 Cp = ResidSS(current model)
 s2max
 AIC = −2l(β,ˆ σ2
 MLE) + 2p,
 = 1−
 +2p−n,
 n
 i=1 (yi − yi)2 ⧸(n −p)
 n
 i=1 (yi − y)2 ⧸(n−1) ,
 • The chapter introduced approaches to variable selection, namely best subsets
 regression and stepwise methods. The former uses an exhaustive search, while
 the latter exploits a greedy algorithm that performs either a forward search, a
 backward search or a bi-directional approach.
15.5. MULTICOLLINEARITY
 167
 • The final section we considered the issue of multicollinearity, how to diagnose it
 and potential remedies.
 After studying this chapter you should
 • be aware of the principle of marginality;
 • know of the bias-variance trade-off in linear models;
 • know the definition and understand the use of the model selection criteria R2, R2
 adj,
 Mallow’s Cp and the AIC;
 • be aware of and able to implement different approaches to automatic variable selection.
 If you wish to engage in further (optional) reading on the topics covered in this chapter, then
 the following sections of the textbooks recommended for this module cover related material.
 • Generalized linear models with examples in R (2018) by Dunn and Smyth:
 Sections 1.6- 1.10, 2.10.4, 2.11, 2.12, 2.13 for an example case study as well as 3.14.
 • Data analysis and graphics using R: an example-based approach (2010) by Maindonald
 and Braun: Sections 6.3.2, 6.4, 6.5 and 6.6.
Chapter 16
 The General Linear Model
 This chapter will be covered in ST231 only if time permits.
 The General Linear Model is an extension of the ordinary linear model where we relax
 the assumption that the errors have a variance-covariance matrix given by Var(ϵ) = σ2In.
 Instead we consider the more general model in which Var(ϵ) = σ2V where V is a known
 n ×n positive definite matrix. We derive the mathematical properties of the model and
 then discuss two examples, weighted regression and models with auto-regressive errors.
 16.1 The generalised least squares estimator
 If Var(ϵ) = σ2V where V is an n×n positive definite matrix, then V can be uniquely
 decomposed as V = LLT where L is a lower triangular1 n×n matrix with positive diagonal
 entries. This is the Cholesky decomposition.
 Consider the model
 Y = Xβ+ϵ,
 where ϵ ∼ Nn(0,σ2V ). If we pre-multiply the model equation by L−1 we have
 L−1Y = L−1Xβ + L−1ϵ
 that is we reparameterise the model as
 Y′ = X′β+ϵ′,
 where
 Y′ = L−1Y, X′ = L−1X and ϵ′=L−1ϵ.
 1A lower triangular matrix is one where all entries above the diagonal are zero.
 168
16.1. THE GENERALISED LEAST SQUARES ESTIMATOR
 169
 By Lemma 10.1 we have
 ϵ′
 ∼ NnL−10, L−1σ2V(L−1)T .
 Now note that L−10 = 0. Furthermore, with V = LLT we have
 σ2L−1V (L−1)T
 = σ2L−1LLT(LT)−1
 = σ2In.
 Thus ϵ′ ∼ Nn(0,σ2In) and therefore the model Y ′ = X′β +ϵ′ is an ordinary linear model
 satisfying the usual assumptions on the errors. Hence, the least squares estimator for β is β
 given by
 β = β(Y) = X′TX′ −1X′TY′
 = L−1XTL−1X−1
 L−1XTL−1Y
 = XT(LT)−1L−1X−1XT(LT)−1L−1Y
 = XT LLT −1X−1
 XT LLT −1Y
 = XTV−1X−1XTV−1Y
 This estimator is called the generalised least squares estimator for β in the general
 linear model Y = Xβ+ϵ.
 Theorem 16.1 (Properties of the generalised least squares estimator). Let
 β = β(Y)=XTV−1X−1XTV−1Y
 be the generalised least squares estimator. Then
 β ∼ Np β, σ2 XTV−1X−1 .
 Proof. By 10.1 it follows that β(Y ) has a multivariate normal distribution of p dimensions.
 We thus only need to compute its mean and variance-covariance matrix.
 E[β] = XTV−1X−1XTV−1E[Y]
 = XTV−1X−1XTV−1Xβ
 = β.
170 CHAPTER16. THEGENERALLINEARMODEL
 Itfollowsthatβ(Y) isanunbiasedestimator.Furthermore
 Var(β) = σ2 X′TX′ −1
 = σ2 (L−1X)TL−1X−1
 = σ2 XT(LT)−1L−1X−1
 = σ2 XT(LLT)−1X−1
 = σ2 XTV−1X−1 .
 Exercise50Consideranexperiment inwhichit isknownthatthevarianceoftheerrorsof
 thefirsttwoobservations isfourtimesas largeasthevarianceof theerrorsforthenext
 threeobservations.Theerrorsareassumedtobeuncorrelated.Youaregiventhefollowing
 model:
 Yi =
 
   
   
 µ+ϵi i=1 or i=4,
 µ+λ+ϵi i=2 or i=5,
 µ+2λ+ϵ3 i=3.
 Findthedesignmatrix for thismodel andthencalculate thegeneralised least squares
 estimategiventhatyT=(125,62.5,12.5,100,50).
 16.2 Weightedregression
 ConsiderY =Xβ+ϵwhereϵ∼Nn(0,σ2V)with
 V =
 
     
 1
 w1
 0 0 ··· 0
 0 1
 w2
 0 ··· 0
 . . . . . . . . .
 0 ... 0 ... 1
 wn
 
     
 .
 Thustheerrorsareindependentbutdonothaveconstantvariance.Theerrorϵi oftheith
 unitofobservationhasalargevarianceifwi issmallandasmallvarianceifwi islarge.
 TheCholeskydecompositionofV gives
 L =
 
      
 1 √w1
 0 0 ··· 0
 0 1 √w2
 0 ··· 0
 . . . . . . . . .
 0 ... 0 ... 1 √wn
 
      
 .
16.3. SERIALLY CORRELATED ERRORS
 171
 and so
 
 
 L−1 =
 
 
 
 
 
 
 √w1 0 0 ··· 0
 0
 √w2 0 ··· 0
 .
 .
 .
 0
 .
 . . .
 . . .
 0 ... √
 .
 .
 wn
 
 
 
 
 .
 Then the generalised least squares estimate for the above model is given by the ordinary
 least squares estimate for the transformed model Y ′ = Zβ +ϵ′, where
 Y′ = L−1Y, Z = L−1X and ϵ′=L−1ϵ.
 Thus this estimate β(y) will minimise
 S(β) =
 =
 =
 n
 j=1
 n
 j=1
 n
 j=1
 (y′
 j − zT
 jβ)2
 (√wjyj −√
 wj xT
 jβ)2
 wj(yj −xT
 jβ)2,
 where zT j and xT j are the jth row of the design matrices Z and X respectively.
 Therefore, observations with relatively small variance (large wj) are given more weight when
 determining the estimate of β than observations with relatively large variance (small wj).
 Weighted regression can be performed in R by providing the lm command with the argument
 weights set equal to the vector of weights (w1,...,wn).
 16.3 Serially correlated errors
 Correlated errors are commonly found in data that arises or is collected sequentially in time.
 Thus in the following we use t to denote the index indicating the sequential order of the
 data.
 Consider Figure 16.1 which shows time series data that has no correlation in time. The plot
 on the left hand side is a traceplot of the data zt for t = 1,...,100. The plot on the right
 shows the corresponding lag 1 plot, that is a scatterplot of zt against zt−1 for t = 2,...,100.
 Contrast this with the examples in Figure 16.2. As before, the plots on the left hand side
 are traceplots of two examples of data zt for t = 1,...,100. The plots on the right show the
 corresponding lag 1 plots, that is the scatterplots of zt against zt−1 for t = 2,...,100.
 In the first example displayed in Figure 16.2 (top row) there is a positive correlation between
 successive values of the time series. In the second example (bottom row) there is a negative
172
 CHAPTER 16. THE GENERAL LINEAR MODEL
 Figure 16.1: Traceplot (left) and lag 1 plot (right) of time series data that is not serially
 correlated.
 Figure 16.2: Traceplots (left) and lag 1 plots (right) of two examples of time series with
 serial correlation.
16.3. SERIALLY CORRELATED ERRORS
 173
 correlation between zt and zt−1 for t = 2,...,100. So both are examples of time series data
 with serial correlation.
 In the following we consider the example of a timeseries model for serially correlated errors.
 16.3.1 AR(1) errors
 Suppose |α| < 1 and consider the model
 Yt
 where for t = 1,2,..., we have
 ϵt
 = xT
 tβ+ϵt
 = αϵt−1+νt
 and νt ∼ N(0,σ2 ν) iid, independent of ϵt−1,ϵt−2,.... We assume E(ϵ1) = 0 and Var(ϵt) = σ2 ϵ
 for all t.
 We say the errors ϵt follow an autoregressive process of order one (AR(1) process).
 For t > 1 we have
 E(ϵt) = E(αϵt−1+νt)
 = αE(ϵt−1)+0
 = αE(ϵt−1) = 0
 Furthermore,
 Var(ϵt) = E(ϵ2
 t) = E(α2ϵ2
 t−1 +2αϵt−1νt +ν2
 t)
 = α2E(ϵ2
 t−1)+E(ν2
 t)+2α E(ϵt−1) E(νt) = 0 
= α2Var(ϵt−1)+Var(νt)+0
 = α2σ2
 ϵ+σ2
 ν
 since E(νt) = 0
 since E(ϵ1) = 0
 since νt is indpt of ϵt−1
 since E(νt) = E(ϵt−1) = 0
 As by assumption Var(ϵt) = σ2 ϵ for all t, the above equation implies that
 σ2
 ϵ
 Moreover,
 = σ2 ν
 1 −α2 .
 Cov(ϵt,ϵt−1) = E(ϵtϵt−1)
 = E((αϵt−1 +νt) ϵt−1)
 = αE(ϵ2
 t−1)+E(νt) E(ϵt−1)
 = ασ2
 ϵ+0 = ασ2
 ϵ
 since νt is indpt of ϵt−1
 since E(νt) = 0
174
 CHAPTER 16. THE GENERAL LINEAR MODEL
 Thus
 Corr(ϵt,ϵt−1) =
 Cov(ϵt,ϵt−1)
 Var(ϵt)Var(ϵt−1)
 = α.
 More generally one can show that Corr(ϵt,ϵt+k) = αk for k ∈ N, see Exercise 51.
 If α is known, then Var(ϵ) = σ2 ϵ V = σ2 ν
 1−α2 
V with
 
 
 
 V =
 
 
 
 1
 α
 .
 .
 .
 α ··· αn−1
 1 ··· αn−2
 . . . . . .
 αn−1 ... α 1
 
 
 
 .
 .
 .
 
 
 
 .
 Provided we know α, we can now estimate the parameters of this model using the generalised
 least squares estimator.
 Exercise 51: Suppose that for t ∈ N we have
 ϵt
 = αϵt−1+νt
 where α2 < 1 and νt ∼ N(0,σν) iid and independent of ϵt−1,ϵt−2,.... Furthermore assume
 E(ϵ1) = 0 and Var(ϵt) = σ2 ϵ for all t. Show that Corr(ϵt,ϵt+k) = αk for k ∈ N0 and t ∈ N.
 16.3.2 Ignoring serial autocorrelation
 If the errors are serially correlated in time, then the OLS estimator is no longer optimal and
 leads to an inefficient estimation procedure. More importantly, the reported standard errors
 from an OLS analysis are incorrect and often too small. This leads to spurious precision in
 the resulting confidence and prediction intervals.
 16.3.3 Identifying serially correlated errors
 Suppose it is reasonable to assume that the errors have constant mean zero and that the
 correlation structure of the errors does not change over time. Then Corr(ϵt,ϵt+k) depends on
 k but not on t. Exercise 51 shows that if the errors follow an AR(1) process with parameter
 α such that α2 < 1 then Corr(ϵt,ϵt+k) = αk for all t and k ∈ N0. We call ρk = Corr(ϵt,ϵt+k)
 the auto-correlation function of the error process.
 We use sample auto-correlations rk for k ∈ N0 as estimates of the auto-correlations ρk. A
 plot of the sample auto-correlations rk against k is called a correlogram. As the rk’s
 are estimates, there will be some fluctuation from the expected behaviour even if all of
 the model assumptions are correct. To assess whether an auto-correlation at a given lag
 √
 k can be assumed different from zero we use that the approximate standard error of rk is
 given by sd.error(rk) ≈ 1
 n
 √
 . Thus it is common to use |rk| > 2
 n as a rule of thumb for
 assessing whether the auto-correlation at lag k can be assumed to be different from zero. A
16.3. SERIALLY CORRELATED ERRORS
 175
 correlogram produced in R using the command acf displays the rule of thumb thresholds as
 blue dashed lines.
 The figures below show the correlograms for the serially uncorrelated data from Figure 16.1
 and the correlated data from Figure 16.2 where the latter were produced using an AR(1)
 process. In Figure 16.4 the parameter α of the AR(1) process was α = 0.8 while for Figure
 16.5 it was α = 0.8.
 Figure 16.3: Correlogram for an example of serially uncorrelated timeseries data.
 For an AR(1) process we have ρk = αk, so we expect to see the magnitude of the auto
correlation at lag k to decay geometrically with k. If −1 < α < 0, then we also expect an
 alternating pattern of positive and negative correlations.
 Figure 16.4: Correlogram for a realisation of an AR(1) model with alpha=0.8.
176
 CHAPTER 16. THE GENERAL LINEAR MODEL
 Figure 16.5: Correlogram for a realisation of an AR(1) model with alpha=-0.8.
 16.4 Summary of Chapter 16
 • The general linear model is defined as
 Y = Xβ+ϵ,
 where ϵ ∼ Nn(0,σ2V ) for known V .
 • The generalised least squares estimator is given by
 β
 (V ) (Y ) = XTV−1X−1XTV−1Y.
 The estimator is unbiased and has variance-covariance matrix σ2 XTV −1X−1 .
 • The generalised least squares estimator is used for weighted regression and models
 with serially correlated errors where the correlation structure is known.
 After studying this chapter you should
 • know the definition of the general linear model;
 • be able to derive the generalised least squares estimator and its properties;
 • understand how the general linear model is applied in weighted regression and in linear
 models where the errors follow an AR(1) process;
 If you wish to engage in further (optional) reading on the topics covered in this chapter, then
 the following sections of the textbooks recommended for this module cover related material.
 • AModern Approach to Regression with R (2009) by Sheather: Chapter 4 and Chapter
 9.
Chapter 17
 Generalised Linear Models
 Recall that in a normal linear model we assumed that the response was continuous and that
 the errors were normally distributed. In order to consider more general statistical models
 we would like to relax these assumptions. Also recall that the normal linear model was
 characterised by a linear predictor of the form:
 E[Y ] = Xβ.
 While Generalised Linear Models, or GLMs in short, relax some of the assumptions of the
 normal linear model they still rely on a model structure that is defined via a linear predictor.
 Please note the difference in a terminology: generalised linear model, as discussed in this
 chapter, versus general linear model as discussed in the last chapter.
 17.1 Example- Spam filter
 For illustration consider the following artificial data set. Suppose that for a sample of 1000
 e-mails we recorded the rate of typos such as spelling mistakes (Typos in %) and whether
 the subject line contained enticing words such as “prize” (Enticing). The observations of
 the binary response variable Outcome were recorded as 1 if the e-mail was spam and 0 if
 the e-mail was not spam. Thus the expectation of the outcome variable is equal to the
 probability of an e-mail being spam and we would like to model this probability as a function
 of the explanatory variables.
 The output below gives a few lines of the data.
 Outcome
 1
 0
 1
 0
 1 ···
 Typos (in %) 18.87 1.85 7.97 17.19 27.25 ···
 177
178 CHAPTER17. GENERALISEDLINEARMODELS
 Outcome 1 0 1 0 1 ···
 Enticing yes yes yes no yes ···
 ExaminingthedotplotinFigure17.1weobservethatspame-mailstendtohavealarger
 percentageoftypos.However,fittingasimplelinearregressionmodelof OutcomeonTypos
 doesnotrepresentwell therelationshipbetweenthepercentageoftyposandtheprobability
 ofthee-mailbeingspam. Inparticular,weobservethatthefittedmodelwillpredictnegative
 valueswhenthepercentageoftyposissmallandvaluesgreaterthanonewhenthepercentage
 oftyposislarge.
 Figure17.1:Dotplotof therateof typos inthee-maildatasetgroupedbyspamversus
 non-spam.Alsoshownisthesimplelinearregressionmodelfittedtothedata.
 Amuchmorenatural approachis tomodel therelationshipasanS-shapedcurvewith
 horizontalasymptotesat0and1asshowninFigure17.2.
17.2. DEFINITIONOFAGLM 179
 Figure17.2: IllustrationofthefittedBinomialGLMforthespame-maildata.
 17.2 DefinitionofaGLM
 GeneralisedLinearModels(GLMs)arestatisticaldependencemodels inwhichtheresponse
 randomvariablefollowsadistributionfromtheexponential familyofdistributions. For
 example, inthespame-mail exampletheresponsevariablewasbinaryandthushada
 Bernoullidistribution.GLMsstillmakeuseofalinearpredictor,butaso-calledresponse
 functionisusedtoestablishtherelationshipbetweenthelinearpredictorandtheexpectation
 oftheresponserandomvariable.Theresponsefunctionischosentobeinvertibleandits
 inverseiscalledthelinkfunction.ThisthendefinesthethreecomponentsofaGLM.
 Definition17.1(Generalisedlinearmodel).Ageneralisedlinearmodel(GLM)consists
 ofthreecomponents.Weassumethat
 1. theresponsefollowsadistributionfromtheexponential familyofdistributions;
 2. a linear combinationof predictors, that is a linearpredictorof the formXβ,
 influencestheresponse,
 3. themeanoftheresponseisrelatedtothepredictorsviaalinkfunctiong,namely,
 g E[Y] = Xβ.
 WiththeabovedefinitionwehavethatGLMsnolonger insistontheresponsehavinga
 normaldistribution,but insteadassumethattheresponsedistributionisanExponential
 DispersionModel(EDM), that is,belongstotheexponential family. SeeAppendixFfor
180
 CHAPTER 17. GENERALISED LINEAR MODELS
 more details on the exponential family of distributions. The exponential family includes
 discrete distributions, hence GLMs can model a response that is discrete and so they also
 relax the assumption that the response is measured on a continuous scale.
 Note also that a GLM is based on g(E[Y ]) which one might think of as a transformation
 of the expectation of the response. This in contrast to the approach taken in for example
 Section 4.1 where we transformed the response and then modelled the expectation of the
 transformed response.
 In general the link function g can be any monotonic differentiable function, however in practice
 only a small number of candidates are used, generally ones for which E[Y ] = g−1(Xβ) can
 be easily computed. The inverse of the link function h = g−1 is called the response function
 and so
 E[Y ] = hXβ .
 In the following we will consider only GLMs for binary response data which belong to the
 class of Binomial GLMs. Other response distributions can be accommodated, for example we
 may model count data using a Poisson GLM. If you are interested to learn more, Appendix
 G gives a brief introduction to Poisson GLMs.
 17.3 GLMs for binary response data
 Suppose we are interested in data where the response variable has only two possible outcomes,
 eg. Success/Failure or Survival/Death. We would like to develop a model that describes the
 way in which other variables influence these two outcomes, for example age, gender, drug
 treatment in medical studies or temperature, reaction conditions in lab experiments, and so
 on. Thus, in a GLM for binary data, we would like to model the relationship between the
 binary response Y and a set of explanatory variables using a linear predictor η(x) = xTβ,
 such that
 EY η = PY=1 η = ρ(η).
 Here xT is a row of the design matrix X. (Note that we refer to both Xβ and xTβ as a
 linear predictor. It should be clear from the context when we refer to the vector Xβ and
 when to an element xTβ of the vector Xβ.)
 In our earlier spam e-mail example, the response variable was binary with the two possible
 outcomes: the e-mail being spam or the e-mail being not spam. We noted that the probability
 of an e-mail being spam seemed to be associated with the proportion of typos in the e-mail
 and so are interested in a model that predicts the probability that an e-mail is spam based
 on its proportion of typos.
 Because the response variable is binary, its distribution is fully determined by ρ(η) as a
 Bernoulli distribution with success probability ρ(η). The Bernoulli distribution is a special
 case of the Binomial distribution, and so the GLMs for binary response data presented here
 belong to the class of Binomial GLMs.
17.3. GLMS FOR BINARY RESPONSE DATA
 181
 As a next step we need to determine how exactly the success probability ρ(η) depends on
 the linear predictor η(x) = xTβ. The simplest approach would be to assume the identity
 function and so ρ(η) = xTβ. However, the linear predictor may take any value on the real
 line and, as we have seen in Section 17.1, this may lead to fitted values, and thus predicted
 probabilities, that lie outside of [0,1]. To avoid this we need a function ρ that maps the
 values of the linear predictor to the unit interval, that is we need a function ρ : R → (0,1).
 We will call this function the response function as it relates the linear predictor to the
 expectation of the response.
 17.3.1 Candidate link functions
 The logistic function
 ρ(η) = exp(η)
 1 +exp(η) =
 1
 1 +exp(−η) ,
 η ∈R
 has an S-shaped curve that has two horizontal asymptotes, namely at 0 and at 1. It is
 therefore well suited for modelling probabilities and so is the most common choice for the
 response function in GLMs for binary data.
 We can derive the corresponding link function as the inverse of the response function:
 eη
 1 +eη = ρ ⇐⇒ η = log ρ
 1 −ρ .
 The inverse of the logistic function, which is called the logit function, is given by
 g(ρ) = log ρ
 1 −ρ .
 The choice of the logit function as a link function has the advantage that it is symmetrical
 about 1/2, so if we swap round our definition of “success” and “failure” then we still get the
 same answers.
 Note that in the above ρ = eη
 1+eη 
is the success probability of the binary response Y , that is
 the probability that Y = 1. Therefore exp(η) = ρ
 1−ρ is the odds in favour of a success,
 that is in favour of the event Y = 1 occurring. If, for example, the probability of Y = 1 is
 0.25, then the odds in favour of a success are given by 0.25/(1−0.25) = 1/3. In other words,
 we expect to observe one success to every three failures. Suppose now the probability of
 success is 0.75, then the odds of success are 0.75/0.25 = 3, so we expect three successes to
 each failure.
 AGLMthat assumes binary outcomes and uses the logistic function as the response function
 is also referred to as a logistic regression model. As an aside, there are other potential
 link functions whose choice is usually motivated by particular interpretations resulting from
182
 CHAPTER 17. GENERALISED LINEAR MODELS
 Figure 17.3: A plot of the logit function and of the probit function.
 that alternative link function. For example, the probit link function is an alternative choice
 given by
 g(p) = probit(ρ) = Φ−1(ρ),
 where Φ is the cdf of the standard normal distribution. If Y = Xβ +ϵ is a normal linear
 model, and Zi = 1{Yi>0} then the MLE for β given Z is obtained from the GLM with the
 probit link function.
 A visual comparison between the two link functions is provided in the Figure 17.3.
 Exercise 52
 Consider a student who takes ST231. Let x = 1 if the student revises for the module
 regularly, and x = 0 otherwise. Let Y be the event that the student obtains a first class
 mark in the ST221 exam. Assume the distribution of Y is given by a logistic regression
 model with linear predictor η = β0 +β1x.
 Suppose the student revises regularly.
 • In terms of the parameters β0 and β1, what are the odds that the student receives a
 f
 irst class mark in the exam?
 • What is the probability that they obtain a first class mark in the exam?
 Now suppose that the student does not revise regularly.
 • In terms of the parameters β0 and β1, what are the odds that the student receives a
 f
 irst class mark in the exam?
 • What is the probability that they obtain a first class mark in the exam?
 17.3.2 Spam filter example continued
 Let’s return to our spam filter example. We fit a logistic regression model of Outcome on
 Typos using the glm command:
17.3. GLMSFORBINARYRESPONSEDATA 183
 ex.glm<- glm(Outcome~Typos,family=binomial(link="logit"))
 Thefirstargumentoftheglmcommandisamodel formula, justasinnormal linearmodels.
 Thefamilyargument intheglmspecifies the responsedistribution. As theBernoulli
 distributionisaspecialcaseoftheBinomialdistribution,wespecifybinomialinthefamily
 argument.Notethatifwedonotspecifyfamily,thentheglmcommandwillfitaGaussian
 GLMwhichisequivalenttoanormal linearmodel.Wealsospecifythelinkfunctioninthe
 familyargument,herethelogit linkfunction. Ifwewishtouseaprobit linkweneedto
 specifyfamily=binomial(link="probit").
 WewilldiscussinSection17.3.4howtheparametersofthismodelareestimated,butfor
 nowletusexaminetheresults.Figure17.4illustratesthedataandfittedlogisticregression
 model.
 Figure17.4: IllustrationofthefittedBinomialGLMforthespame-maildata.
 Theregressioncurveisincreasingwhichmeansthatthepredictedprobabilityofane-mail
 beingspamincreaseswithitspercentageoftypos.Toplotthiscurveweusedthecommand
 predict. ForGLMsthepredictfunctioninRprovidesapredictioneitheronthescale
 ofthelinearpredictorηandthusalog-odds,thatis log(ρ/(1−ρ)),oronthescaleofthe
 responseandthustheprobabilityρ.Thedefaultistoprovidepredictionsonthescaleof
 thelinearpredictor, sotoplottheregressioncurveinFigure17.4weusedtheargument
 type="response". (SeetheRhelpforpredict.glmforfurtherdetails.)
 Thedatasetcontainsanadditionalpotentialpredictorvariable,namelyEnticingwhich
 indicateswhetherthee-mailunderconsiderationcontainsanenticingwordinthesubject
 title.WecanexpandtheabovelogisticregressionmodeltoincludeEnticingasanadditional
 explanatoryvariableasfollows:
 ex.glm<- glm(Outcome~Typos+Enticing,family="binomial")
 NotethatintheabovewedidnotspecifythelinkfunctionasRassumesthelogitlinkas
184 CHAPTER17. GENERALISEDLINEARMODELS
 defaultforaBinomialGLM.AsEnticingisafactorvariablewithtwolevelsthisproduces
 twofittedregressioncurveswhichareillustratedinFigure17.5.
 Figure17.5: Illustrationof thefittedBinomialGLMforthespame-mail exampleusing
 bothTyposandEnticingaspredictorvariables.
 17.3.3 Interpretationofalogisticregressionmodel
 Supposeη=β0+β1x, theninlogisticregression
 E(Y) = ρ(η) = exp(η)
 1+exp(η) = 1
 1+exp(−η) = 1
 1+exp(−β0−β1x) .
 Thustheestimatedsuccessprobabilityρ(η)ofthebinaryresponseY isacomplexnon-linear
 functionofηwhichmakesaninterpretationattheresponsescaledifficult. Insteadit is
 commonpracticetoprovideaninterpretationfortheoddsinfavourofY=1, thatisin
 termsof ρ(η)
 1−ρ(η) ,whichbyourearlierderivationisequal toexp(η).
 Considertheeffectofanincreaseofxbyoneunitontheoddsinfavourofasuccess, thatis
 theoddsinfavouroftheeventY=1.Letη=β0+β1xandη′=β0+β1(x+1), then
 ρ(η′)
 1−ρ(η′) =exp β0+β1(x+1) =exp β0+β1x exp(β1)= ρ(η)
 1−ρ(η) exp(β1).
 Thuswithaunit increaseinxwepredictachangeof theodds infavourofasuccessby
 afactorofexp(β1). It isrelativelystraight-forwardtoshowthatthis interpretationalso
 extendstothecasewhenwehaveseveralpredictorvariablesbutareholdingallbuttheone
 predictorof interestfixed.
 Asanexample, let’sconsiderthefittedmodel forthespame-maildataset.Wecanextract
 theestimatedcoefficientsfromthefittedmodel inthesamewayasforlinearmodels,namely
 byapplyingthecommandcoeftothemodelobject:
17.3. GLMS FOR BINARY RESPONSE DATA
 185
 round(coef(ex.glm), 2)
 (Intercept)
 Typos Enticingyes-4.63
 0.32
 0.74
 The estimated coefficient for Enticing is roughly 0.74 and we may interpret this estimate
 as follows. For a fixed percentage of typos, the estimated odds of being spam for an e-mail
 that has an enticing subject line is exp(0.74) = 2.1 times the odds for an e-mail that has no
 enticing words in its subject line.
 17.3.4 Parameter estimation in logistic regression
 To fit a GLM we use maximum likelihood estimation, that is we choose the parameters which
 maximise the (log-)likelihood function of the model. As an illustration of this approach, in
 this section we will derive the log-likelihood function for a logistic regression model with a
 single quantitative predictor. Recall that a logistic regression model is a GLM for binary
 response data that uses a logit link (or equivalently, a logistic response function).
 Suppose we have ηi = β0 +β1xi for i = 1,...,n, then
 g(ρ(ηi))
 = log ρ(ηi)
 1 −ρ(ηi)
 = ηi.
 As Y1,...,Yn are independent the likelihood function for this GLM is proportional to
 n
 L =
 Taking logs we obtain
 i=1
 log(L) =
 =
 =
 n
 P(Yi = yi | ηi) =
 n
 i=1
 ρ(ηi)yi 1 − ρ(ηi) 1−yi .
 yi log ρ(ηi) +(1−yi)log 1−ρ(ηi)
 i=1
 n
 i=1
 n
 i=1
 yi log
 ρ(ηi)
 1 −ρ(ηi
 +log 1−ρ(ηi)
 yi β0 +β1xi −log 1+exp(β0 +β1xi) ,
 where the last equation uses the fact that 1 − ρ(ηi) =
 1
 1+exp(ηi) .
 In GLMs there is no closed form expression for the maximum likelihood estimator for the
 parameter vector β. Instead an iterative numerical optimisation algorithm is used to obtain
 the maximum likelihood estimates of the parameters.
186
 CHAPTER 17. GENERALISED LINEAR MODELS
 17.3.5 Model summary for GLMs
 Next let us look at the output of the summary function for the fitted model.
 Call:
 glm(formula = Outcome ~ Typos + Enticing, family = "binomial")
 Coefficients:
 Estimate Std. Error z value Pr(>|z|)
 (Intercept)-4.62935
 0.31525-14.685 < 2e-16 ***
 Typos
 0.32432
 Enticingyes 0.74034
 0.01972 16.449 < 2e-16 ***
 0.20335 3.641 0.000272 ***--
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 (Dispersion parameter for binomial family taken to be 1)
 Null deviance: 1369.87 on 999 degrees of freedom
 Residual deviance: 657.69 on 997 degrees of freedom
 AIC: 663.69
 Number of Fisher Scoring iterations: 6
 The output above is similar to that of a normal linear model, with a header that contains
 information about the distribution of the residuals, a main body containing a table with
 information on the estimated coefficients and a footer containing some additional information
 about the model.
 Next to each parameter estimate the summary output lists the standard error, the observed
 value of the Z-statistic and the corresponding p-value. These take the equivalent role to the
 information provided for a normal linear model and will be discussed in Section 17.4.
 The footer in the summary output contains further information about the fitted model. It
 reports the dispersion parameter ϕ which for the models that we consider in this chapter is
 equal to 1. It also gives the deviance of the null model as well as the fitted model. We will
 not formally define deviances1 for GLMs in this module, but note that these take the role of
 the residual sum of squares that we encountered for linear models.
 Furthermore, the output reports the AIC of the fitted model and the number of iterations
 needed in the algorithm that computes the parameter estimates. Note that the AIC is defined
 analogously to the AIC for a normal linear model, we simply replace the log-likelihood
 function of the normal linear model with the one of the GLM under consideration.
 1For those that are interested, the deviance of a model M is 2 ϕ times the difference between the maximised
 log-likelihood for the saturated model and the maximised log-likelihood for the model under consideration.
17.4. HYPOTHESIS TESTING
 187
 Exercise 53
 In January 1986, the space shuttle challenger exploded 73 seconds seconds after take-off.
 An o-ring seal had failed allowing hot gas to escape. Prior to the disaster, data had been
 collected on the performance of the o-rings. Note that in each launch there are a total of 6
 o-rings that can potentially fail.
 The data can be found as Challeng in the alr42 package. Use a logistic regression to
 examine the relationship between the odds of at least one of the six o-rings failing and
 temperature. Give a quantitative interpretation of the coefficient for temperature.
 17.3.6 Binomial GLMs
 Responses assumed to have a more general Binomial distribution can be handled in the
 same way as responses assumed to have a Bernoulli distribution. An observation from a
 Bin(N,ρ) distribution is equivalent to N observations from a Bernoulli(ρ) distribution. This
 is also why, as mentioned previously, the GLMs for binary data presented here belong to the
 class of Binomial GLMs.
 17.4 Hypothesis testing
 In the following we discuss hypothesis tests for GLMs which replace the t-tests and F-tests
 that we encountered for normal linear models.
 17.4.1 Wald’s test
 In GLMs, the Wald’s test takes the role of a t-test for a normal linear model. It uses the
 fact that maximum likelihood estimators have an asymptotic Normal distribution. Unlike
 the t-test, the null distribution for Wald’s tests is therefore only approximate for finite n.
 Suppose we would like to test the hypotheses H0 : βi = b versus H1 : βi= b.
 Under H0, we have
 Z = βi−b
 std.error(βi)
 asympt
 ∼ N(0,1).
 A large absolute observed value of the above Z-statistic provides evidence against null
 hypothesis. Approximate p-values are computed as
 p = P |Z|>|Zobs| = 2(1−Φ(|Zobs|),
 were Z is a random variable with a standard normal distribution, Zobs is the observed value
 of the Z-statistic and Φ() is the cdf of a standard normal distribution.
 2Weisberg, S. (2014). Applied Linear Regression, 4th edition. Hoboken N: Wiley.
188
 CHAPTER 17. GENERALISED LINEAR MODELS
 We can also use asymptotic normality to compute critical values, for example we reject H0
 at the 5% level if |zobs| > qnorm(0.975) = 1.96.
 In R the p-values for the Wald’s tests with hypotheses H0 : βi = 0 versus H1 : βi= 0 are
 reported for each coefficient and flagged with the usual asterisks indicating the relevant
 levels of significance.
 17.4.2 Likelihood ratio test
 To impose more than one linear constraint in the null hypothesis we need an analogue of
 the F-test in normal linear models.
 Suppose model M1 is nested within model M2 and we would like to test the null hypothesis
 that the data are adequately modelled by M1 against the alternative hypothesis that the
 more complex model M2 is needed. This can be done using the likelihood ratio test. The
 test statistic is the ratio of the maximised likelihood functions:
 Λ = L(β1 |y,X)
 L(β2 | y,X),
 where β1 is the maximum likelihood estimator for β under the null hypothesis H0 and β2 is
 the maximum likelihood estimator under the alternative hypothesis HA.
 Wilks’ theorem shows that, under H0, the distribution of −2log(Λ) can be approximated
 by a χ2
 d distribution, where d is the difference between the number of parameters in model
 M2 and the number of parameters in model M1.
 Note: This approximation is only valid when comparing nested models and assumes that
 the dispersion parameter ϕ of the response distribution is equal to 1. For the two types of
 GLMs considered in this module, the GLM for binary response data and the Poisson GLM,
 the dispersion parameter is ϕ is equal to 1.
 As for F-tests in normal linear models, we can use the anova function in R to perform the
 likelihood ratio test to compare nested models. However we need to specify the argument
 test="LRT". As an example, suppose we would like to compare the following two models:3
 m1 <- glm(Outcome ~ Typos, family="binomial")
 m2 <- glm(Outcome ~ Typos + Enticing, family="binomial")
 Note that in the above m1 is nested in m2. We fit both models and then use the anova
 command as follows:
 anova(m1, m2, test="LRT")
 3You may see in the literature the use of anova(m1, m2, test="Chisq") which is equivalent to anova(m1,
 m2, test="LRT").
17.4. HYPOTHESIS TESTING
 189
 Analysis of Deviance Table
 Model 1: Outcome ~ Typos
 Model 2: Outcome ~ Typos + Enticing
 Resid. Df Resid. Dev Df Deviance Pr(>Chi)
 1
 2--
998
 997
 671.32
 657.69 1 13.621 0.0002237 ***
 Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 The approximate p-value of the likelihood ratio test is given by 0.00022 and so there is very
 strong evidence against the null hypothesis that the coefficient for Enticing is equal to zero.
 We conclude that including Enticing significantly improves the model.
 The output is referred to as an Analysis of Deviance which is the GLM analogue of the
 Analysis of Variance in normal linear models.
 Note that likelihood ratio test like the Wald’s tests for GLMs are approximate as they are
 based on asymptotic distributions.
 For normal linear models, when we introduced the F-test we stated that if the null hypothesis
 imposes just one linear constraint then the F-test is equivalent to the corresponding t-test.
 This means that both will produce the same p-value. This is not the case for the likelihood
 ratio test and the corresponding Wald’s test. For example, for Enticing the summary
 output for the GLM fitted to the spam model reported a p-value of 0.00027 for the Wald’s
 test. The likelihood ratio test examining whether Enticing should be included in the model
 reported a p-value of 0.00022. Usually the p-values of the two tests are fairly similar, but
 occasionally they may fall on either side of the significance level threshold. In this case, keep
 in mind that this threshold is somewhat arbitrary and small changes in the data tend to
 result in small changes of a p-value.
 Furthermore, like the F-test which is more general than the t-test, the likelihood ratio test
 is more general than the Wald’s test as it can be used to compare models with more than
 one coefficient set to zero.
 Unlike for normal linear models, for GLMs there is usually no explicit expression for the
 maximum likelihood estimator for β. Instead GLMs are fitted using a numerical algorithm
 called iteratively weighted least squares. In R this is done via the command glm.
190
 CHAPTER 17. GENERALISED LINEAR MODELS
 17.5 Summary of Chapter 17
 GLMs have 3 main components:
 1. a response distribution that is an Exponential Dispersion Model,
 2. a linear predictor of the form Xβ influencing the response,
 3. the mean of the response related to the predictors via a link function g, namely,
 g E[Y] = Xβ.
 GLMs for binary data using a logit link are defined as
 Yi
 ∼ Bernoulli(ρi),
 E(Yi) = P(Yi =1) = ρi,
 g(ρi)
 = log ρi
 1 −ρi
 = xT
 iβ,
 where i = 1,...,n.
 GLMs for binary data belong to the class of Binomial GLMs. If using a logit link function,
 they may also be referred to as logistic regression models.
 We derived the log-likelihood function for a Binomial GLM and also discussed how to
 interpret the parameters of the GLMs.
 Hypothesis testing in GLMs rely on Wald’s tests and likelihood ratio tests to replace
 t-tests and F-tests used in normal linear models. Both, the Wald’s test and the likelihood
 ratio test are based on asymptotic distributional theory.
 If you wish to engage in further (optional) reading on the topics covered in this chapter, then
 the following sections of the textbooks recommended for this module cover related material.
 • Generalized linear models with examples in R (2018) by Dunn and Smyth. This is
 a book devoted to GLMs so it goes much further than what we discussed here. The
 most relevant sections are 7.2.1, 7.2.4, 7.2.5, 9.2, 9.3, 9.5, 9.11 and 10.1- 10.3.
 • Data analysis and graphics using R: an example-based approach (2010) by Maindonald
 and Braun. This book provides a more concise discussion of GLMs. The relevant
 sections are 8.1- 8.4.
Chapter 18
 Appendix
 Appendix A covers useful results from linear algebra and multivariable calculus.
 Appendix B summarises results related to the expectation of random vectors/ matrices.
 Appendix C derives the distribution of s2, the unbiased estimator of the error variance.
 Appendix D gives a very brief introduction to robust regression.
 Appendix E gives a proof for the null distribution of the F-statistic.
 Appendix F provides some further examples relating to the exponential family of distributions.
 Appendix G is a brief introduction to Poisson GLMs Appendix H gives a summary of some
 key results.
 Appendix A- Linear algebra and multivariable calculus
 Useful facts from linear algebra
 1. For column vector a of dimension n: aTa = n
 j=1a2 j is a scalar.
 2. Matrices are not commutative, so in general AB= BA.
 3. Given arbitrary matrices A and B with dimensions (n1,n2) and (m1,m2) the product
 AB exists if and only if n2 = m1. A and B are then said to be conformable.
 4. Transpose of a product: (AB)T = BTAT for any conformable matrices A and B.
 5. Inverse of a 2×2 matrix:
 if X = a b
 c d anddet(X)=ad−bc=0, then X−1 = 1
 6. Inverse of a diagonal matrix:
 [diag(d1, ...,dn)]−1 = diag(d−1
 1 ,...,d−1
 191
 d −b
 ad −bc
 n ).
 −c a .
192
 CHAPTER 18. APPENDIX
 7. Inverse of a product: (AB)−1 = B−1A−1.
 8. Inverse and transpose: (XT)−1 = (X−1)T.
 9. The rank of an n × m matrix X is the dimension of the image of the linear map
 X:Rm→Rn, where X(v) is given by Xv.
 Equivalently rank(X) is the dimension of the linear span of the columns (or rows) of
 X. Hence rank(X) = rank(XT) and rank(X) ≤ min{n,m}.
 A matrix is said to be of “full rank” if rank(X) = min{n,m}.
 10. A square matrix X is invertible if det(X)= 0 and then X−1X = In = XX−1.
 A square matrix of full rank is invertible.
 11. For column vectors a and b of the same dimension:
 (a +b)T(a+b) = aTa+bTb+2bTa.
 (a −b)T(a−b) = aTa+bTb−2bTa.
 12. If A is an n×p matrix such that n ≥ p and rank(A) = p, then ATA is a symmetric
 p ×p matrix of rank p and so its inverse (ATA)−1 exists.
 Useful facts from multivariable calculus
 Suppose f : Rp −→ R and β =(β1,...,βp)T ∈ Rp.
 1. Recall that
 
 
 ∂f(β)
 ∂β
 2. If a ∈ Rp and f(β) = βTa, then ∂f(β)
 ∂β =a.
 
 
 =
 
 
 
 
 ∂f(β)
 ∂β1
 .
 ∂f(β)
 ∂βp
 3. If A is a p×p matrix and f(β) = βTAβ, then
 ∂f(β)
 ∂β
 =(A+AT)β
 
 
 .
 .
 .
 .
 .
 
 
 
 
 
 
 .
 =2Aβ whenAissymmetric.
 Appendix B- Generalised expectation
 Expectation of a random matrix
 Let {Zij : i = 1,...,k;j = 1,...,m} be a set of random variables. We can arrange the
 random variables in a k ×m matrix as
 Z = Zij (k× m)
 .
193
 The expectation of this random matrix is then defined as the matrix of expected values of
 the individual random variables, that is
 E[Z] := E(Zij)
 (k× m)
 .
 Analogous to the expectation in the univariate setting, this generalised expectation is a
 linear operator, that is the following identities hold.
 • Let A, B andC bematrices of constants. Then, provided all matrices are conformable,
 E[AZB+C]=AE[Z]B+C.
 • If W is another random matrix, then provided all matrices are conformable,
 E[AW +BZ]=AE[W]+BE[Z].
 Covariance
 Recall that in the univariate case, the covariance of two random variables X and Y is defined
 as
 Cov(X, Y) := E (X−E[X])(Y −E[Y])
 The generalised expectation that we introduced earlier allows us to define the covariance
 of two random vectors. Let W be a p-dimensional random vector and Z a q-dimensional
 random vector. Then the covariance of the random vectors W and Z is defined as the
 matrix
 Cov(W,Z) := Cov(Wi,Zj)
 (p× q)
 As before let
 • Aand B bematrices of constants, and
 • aand b constants of dimension p and q respectively.
 Then the following identities hold:
 • Cov(W,Z) = E (W−E[W])(Z−E[Z])T = EWZT −E[W]E[Z]T .
 Note that this is a matrix (as the transpose is on the second vector) rather than the
 scalar that we produce when we take a dot product aTb).
 • Cov(W −a, Z−b) = Cov(W,Z).
 • Cov(AW, BZ) = ACov(W, Z)BT provided A and B are conformable matrices.
194
 CHAPTER 18. APPENDIX
 Variance-covariance matrix
 A particularly important covariance is that of the random vector Z and itself as this defines
 the variance-covariance matrix of Z. Suppose the random vector Z has dimension p, then
 we define
 Var(Z) := Cov(Z, Z)
 and so
 
 
 
 Var(Z) =
 
 
 
 Var(Z1)
 Cov(Z2,Z1)
 .
 .
 .
 Cov(Z1,Z2) ... Cov(Z1,Zp)
 Var(Z2)
 .
 .
 .
 . . . Cov(Z2,Zp)
 . . .
 Cov(Zp,Z1) Cov(Zp,Z2) ...
 
 
 
 .
 .
 .
 Var(Zp)
 
 
 
 .
 (p×p)
 This matrix is called the variance-covariance matrix of Z and its properties are derived from
 the properties of the covariance introduced earlier. In particular, if
 • ais a p-vector of constants and
 • Ais an m×p matrix of constants
 then we have the following properties.
 • Var(Z) is a symmetric matrix.
 • Var(Z −a)=Var(Z).
 • Var(AZ) =AVar(Z)AT.
 Appendix C- The distribution of s2
 To show that s2(Y ) is an unbiased estimator we first need to consider properties of so-called
 quadratic forms of random vectors.
 Expectation of a quadratic form
 Lemma 18.1 (Expectation of a quadratic form). Let Y be an (n×1) vector of random
 variables and let E[Y ] = µ and Var(Y ) = Σ. Let A be an n×n symmetric matrix. Then,
 EYTAY = tr(AΣ)+µTAµ.
 Proof.
 (Y −µ)T A(Y −µ)=YTAY −2µTAY +µTAµ
195
 LetΣ=(σij)n×nandA=(aij)n×n, then
 EYTAY = E (Y−µ)TA(Y−µ) +2EµTAY −µTAµ
 = E
 
 
 n
 i=1
 n
 j=1
 (Yi−µi)aij(Yj−µj)
 
 +2µTAE[Y]−µTAµ
 =
 
 
 n
 i=1
 n
 j=1
 aijE[(Yi−µi)(Yj−µj)]
 
 +µTAµ
 =
 n
 i=1
 n
 j=1
 aijσij+µTAµ
 = tr(AΣ)+µTAµ.
 Expectationofs2
 Theorem6.4statesthat
 s2(Y) = 1
 n−p (Y−Xβ)T(Y−Xβ)
 isanunbiasedestimatorforσ2.Toderivetheexpectationofs2(Y)weusetheLemma18.1.
 Proof.Recall that
 Y−Xβ = Y−Y = (Y−HY) = (In−H)Y
 whereH=XXTX−1XT isthehatmatrix.Then
 Y−Xβ T Y−Xβ = YT(In−H)T(In−H)Y
 = YT(In−H)Y
 wherethelaststepfollowsfromthefactthat(In−H) isanidempotentsymmetricmatrix.
 BythemodelassumptionswehaveE(Y)=XβandVar(Y)=σ2Inandsowecannow
 applyLemma18.1toYT(In−H)Y andobtain
 EYT(In−H)Y = tr (In−H)σ2In +(Xβ)T(In−H)(Xβ)
 = σ2tr(In−H)+βTXT(In−H)Xβ
 NownotethatHX=XXTX−1XTX=Xandso
 XT(In−H)X = XTX−XTHX
 = XTX−XTX = 0p×p.
196
 CHAPTER 18. APPENDIX
 Thus with tr(In −H) = n−p (from Lemma 7.1) we have
 EYT(In−H)Y = σ2(n−p)+βT0p× pβ
 = σ2(n−p)+0
 = (n−p)σ2.
 Therefore
 E s2(Y) = 1
 n−pE (Y −Xβ)T(Y −Xβ) = 1
 n−pEYT(In−H)Y = σ2
 which means that s2(Y ) is an unbiased estimator for σ2.
 The distribution of s2
 To derive the distribution of s2(Y ), the unbiased estimator of the error variance, we will
 make use of the following lemmas.
 Suppose Z ∼ Nn(µ, Σ) where Σ is an n×n positive definite matrix. Let A be a q ×n
 matrix and B be a p×n matrix.
 Lemma 18.2. Define U = AZ and V =BZ. Suppose Cov(U, V) = 0, then U and V
 are independent. Furthermore UTU and V TV are independent.
 Lemma 18.3. Let Q1 = (Z −µ)T A(Z −µ) and Q2 =(Z −µ)T B(Z−µ).
 If Qi ∼ χ2 ri 
where r1 > r2, and if Q = Q1 −Q2 and Q2 are independent, then Q ∼ χ2 r1−r2
 .
 We will use the above lemmas to show that
 Theorem 18.1.
 (n −p)s2(Y )
 σ2
 ∼ χ2
 n−p.
 Proof. Recall that in Section 5.3, during the proof that β(y) minimizes the sum of squares
 S(β), we showed that:
 (y −Xβ)T(y−Xβ)= y−Xβ(y) T
 y−Xβ(y) + β(y)−β T
 XTXβ(y)−β .
 The same statement holds if we replace y by Y and so if we define
 Q1 = 1
 σ2(Y −Xβ)T(Y −Xβ),
 Q2 = 1
 σ2(β −β)TXTX(β−β),
 Q= 1
 σ2(Y −XβT (Y −Xβ)
197
 where β = β(Y), then
 Q1 =Q+Q2, or, equivalently, Q1 −Q2 = Q.
 Next note that
 Q1 = Y −E[Y] T
 Var(Y)−1 Y −E[Y]
 is a quadratic form of Y and so, by 10.2, Q1 ∼ χ2 n. Furthermore Q2 ∼ χ2 p by (ii) and
 Q2 = 1
 σ2(β −β)TXTX(β−β)
 = 1
 σ2 Xβ−Xβ T
 Xβ−β
 = 1
 σ2 Y −E(Y) T
 Y −E(Y)
 = 1
 σ2 H(Y −E(Y)) T
 H(Y −E(Y))
 = Y −E(Y) T 1
 σ2HTHY −E(Y)
 [since Y = HY]
 [as Cov(Y ,Y ) = σ2In]
 = Y −E(Y) T 1
 σ2HY −E(Y)
 and so is a quadratic form of Y . All that remains is to show that Q and Q2 are independent,
 and then we can apply 18.3.
 Let U =X(β−β) and V =Y −Xβ, then Q= 1
 σ2
 VTV and Q2 = 1
 σ2
 UTU. We have
 Cov[U, V ] = CovXβ−Xβ, Y −Xβ
 =CovXβ, Y −Xβ
 =Cov[HY, (In −H)Y]
 [since Xβ = Y =HY]
 =HCov[Y, Y](In−H)T
 =σ2H(In−H)
 =σ2 H−H2
 =0n×n
 where the last equation follows from the fact that H is idempotent. Thus, by Lemma 18.2,
 Q2 = UTU and Q=VTV are independent. Hence we can apply Lemma 18.3 to obtain
 Q∼χ2 n−p. The result follows as soon as we notice that Q = (n−p)s2/σ2.
 Appendix D- M-estimators and robust regression
 M-estimators (or maximum likelihood-type estimators) are a generalisation of the maximum
 likelihood approach that can bring benefits in terms of robustness.
198
 CHAPTER 18. APPENDIX
 The likelihood function of the normal linear model is given by
 L(β,σ) ∝ 1
 σn exp − 1
 2σ2 
n
 i=1
 (yi −xT
 iβ)2 ,
 and so maximising the likelihood function with respect to β is equivalent to minimising the
 sum of squared differences
 S(β) =
 n
 i=1
 (yi − xT
 iβ)2 = (y −Xβ)T(y −Xβ).
 Notice that here our measure of distance between yi and yi = xT iβ is the squared difference.
 In M-estimation we no longer restrict ourselves to the squared difference but may choose an
 arbitrary distance function ρ(x) and we then aim to minimise n
 i=1 ρ(yi − yi). So, the MLE
 is a special case of an M-estimator where we choose as distance function ρ(x) = x2.
 Next we consider common options for the distance function or, as it often is called, the loss
 function.
 Squared deviations
 Squared deviations use the distance function ρ(x) = x2, which is illustrated in the figure
 below. This function is our ordinary least squares choice and corresponds to the maximum
 likelihood estimator. Squared deviations give the best estimator when the errors are actually
 normally distributed.
 Figure 18.1: Squared deviations.
 Absolute deviations
 Absolute deviations make use of the distance function ρ(x) = |x|.
199
 This distance function does not penalise large residuals as harshly as the squared deviation
 function, and so is more robust to outlying values of the response. But notice that the
 solution to the minimisation problem is no longer necessarily unique.
 Figure 18.2: Absolute deviations.
 Huber’s loss function
 This distance function is given by
 ρ(x) = x2
 |x| ≤ δ
 δ(2|x| − δ) |x| > δ
 Huber’s loss function combines the two previous functions– it is a quadratic for small
 residuals and proportional to the absolute value for larger ones. This allows it to combine the
 efficiency from the squared deviations for normal errors with the robustness of the absolute
 deviations.
 Tukey’s bisquare function
 The final distance measure we introduce is given by
 
 
 ρ(x) =
 
 δ2
 6 1−(1−x2
 δ2 
)3 |x| ≤ δ
 δ2
 6
 |x| > δ
 Tukey’s bisquare function is even more robust to large residuals as it weights all residuals
 larger than δ the same. Usually the tuning parameters are chosen to give 95% of the efficiency
 of the squared deviations function with normal errors.
200
 CHAPTER 18. APPENDIX
 Figure 18.3: Huber loss function.
 Figure 18.4: Tukey bisquare function.
201
 Further extensions
 As well as using robust methods to estimate the model parameters, we may also wish to use
 the same ideas when estimating the variance of the errors, for example using the Median
 Absolute Deviation (MAD), given by
 median(|Xi − M|), where M = median(Xi).
 Example
 We can fit robust regression models in R using the command rlm from the package MASS.
 For example, to fit a regression of y on x using Huber’s loss function we use the command
 rlm(y ~ x, psi = psi.huber). Below is an illustration of a toy example which has two
 outliers. It shows the fitted ordinary least squares line but also the fitted robust regression
 lines using absolute deviation, Huber’s loss and Tukey’s bisquare function respectively. The
 three fitted robust regression lines do not differ by much but are noticeably less influenced
 by the two outliers on the right hand side of the plot.
 Figure 18.5: Robust regression for a toy data set.
 Appendix E- The distribution of the F-statistic
 We start by deriving an expression for the least squares estimator for β subject to the
 constraints Aβ = c.
 Lemma 18.4. Let β = β(y) be the least squares estimate of β. Under the null hypothesis
 H0 : Aβ =c the least squares estimate of β is given by
 βH = β−XTX−1AT AXTX−1AT
 −1
 Aβ−c .
202
 CHAPTER 18. APPENDIX
 In the following we present a proof that uses Lagrange multipliers, a common approach used
 for optimisation under linear constraints. If you have not seen Lagrange multipliers before,
 then these Khan Academy videos provide an introduction.
 Proof. The objective is to minimise (y −Xβ)T (y −Xβ) with respect to β subject to
 Aβ =c. Thus, using Lagrange multipliers λ = (λ1,λ2,...,λq)T, we need to minimise g(β)
 with respect to β where g(β) is defined as
 g (β)
 = (y−Xβ)T(y−Xβ) + (Aβ−c)T λ.
 Here the first term on the right hand side is the residual sum of squares or deviance of the
 unconstrained model and the second term relates to the imposed constraints. The Jacobian
 matrix of g is given by
 ∂g(β)
 ∂β
 = −2XTy + 2XTXβ + ATλ.
 As, by definition, βH is the least squares estimate subject to Aβ = c, at the point β = βH,
 we have ∂g(β)
 ∂β =0. Solving this equation gives
 βH
 =
 XTX−1XTy − 1
 2 XTX−1ATλ
 where the first term corresponds to β, the “unconstrained” least squares estimate for β, and
 the second term relates to the imposed constraints. Therefore,
 βH
 = β− 1
 2 XTX−1ATλ.
 Multiplying this equation from the left by A and noting that AβH = c we obtain
 c
 = Aβ− 1
 2AXTX−1ATλ.
 As A is of rank q and XTX is a p×p matrix of full rank, we can re-arrange this equation
 to give
 λ = 2AXTX−1AT
 −1
 Aβ−c .
 Substituting λ into the equation for βH completes the proof of the lemma.
 Next we derive another lemma that we will be using in the proof of Theorem 13.2.
203
 Lemma 18.5. Consider
 (Y −XβH) = (Y−Xβ)+XXTX−1AT AXTX−1AT
 −1
 Aβ−c . (18.1)
 Then
 (Y −XβH)T(Y −XβH)=
 (Y −Xβ)T(Y −Xβ)+Aβ−c T AXTX−1AT
 −1
 Aβ−c .
 Proof. To ease notation we introduce a number of variables. Let
 z = Y−XβH
 v = Y−Xβ
 B = AXTX−1AT
 −1
 w = XXTX−1ATBu
 u = Aβ−c
 So this means that equation (18.1) can be written as
 z = v+w.
 (18.2)
 Thus the inner product of the vector z with itself is equal to the inner product of the
 vector v + w with itself. So we need to show that the right hand side of (18.2) equals
 (v + w)T(v + w).
 We have
 Now,
 (v + w)T(v + w) = vTv+wTw+2vTw.
 vTv = (Y −Xβ)T(Y −Xβ)
 which is the first term on the right hand side of (18.2). Further,
 wTw = uTBTAT(XTX)−1XTX(XTX)−1ABu
 = uTBTAT(XTX)−1ABu
 = uTBTB−1Bu
 = uTBTu.
204
 CHAPTER 18. APPENDIX
 which is the second term on the right hand side of (18.2) noting that BT = B. Finally,
 vTw = (Y −Xβ)TX(XTX)−1ABu.
 (18.3)
 Now note the initial factor (Y −Xβ)TX in the expression on the right hand side of (18.3).
 We have
 (Y −Xβ)TX = XT(Y −Xβ) T
 which by the normal equations is a vector containing zero entries only. Therefore the term
 in (18.3) is zero, so this is why we have no further terms in (18.2).
 Next we will derive the distribution of the F-statistics as specified in Theorem 13.2. We will
 be using the results from Section 10.1.
 We furthermore make use of part (ii) and (iv) in Theorem 10.1 from Section 10.2 which
 state the following.
 In a normal linear model with n cases and p < n parameters, the least squares estimator
 β(Y ) and s2(Y ), that is the unbiased estimator of the error variance σ2, are independent
 and, furthermore, (n − p)s2(Y )
 σ2
 ∼χ2 n−p.
 Theorem- The distribution of the F-statistic
 Consider the normal linear model M2 given by Y = Xβ +ϵ, where X is an n×p design
 matrix of rank p and ϵ ∼ Nn 0,σ2In . Suppose A is a q ×p matrix of rank q and c a
 q-vector. Let M1 be the normal linear model derived from M2 by imposing the q linear
 constraints Aβ = c.
 We define the following random variables/estimators
 • β=β(Y), the least squares estimator of β in M2;
 • βH = βH(Y), the least squares estimator of β under the constraint that Aβ = c
 which is thus the least squares estimator of the parameter vector of M1;
 • D2=D2(Y)=(Y −Xβ)T(Y −Xβ) be the statistic corresponding to the deviance
 of M2 and
 • D1=D1(Y)=(Y −XβH)T(Y −XβH) the statistic corresponding to the deviance
 of M1.
 Then,
 (1)
 (2)
 D2 −D1 = Aβ−c T AXTX−1AT
 −1
 Aβ−c , and
 assuming Aβ = c, we have F = (D1−D2)/q
 D2/(n −p) ∼ Fq,n−p.
205
 Proof. Proof of (1):
 Let B =AXTX−1AT
 −1
 , then, by the Lemma 18.4,
 βH = β−XTX−1ATBAβ−c
 and so
 (Y −XβH) = (Y −Xβ) + XXTX−1ATBAβ−c .
 Lemma 18.5 then states that
 (Y −XβH)T(Y −XβH) = (Y −Xβ)T(Y −Xβ)+Aβ−c TBAβ−c .
 Now note that D1 = (Y −XβH)T(Y −XβH) and D2 =(Y −Xβ)T(Y −Xβ) and so
 D1 −D2 = Aβ−c TBAβ−c = Aβ−c T AXTX−1AT
 −1
 Aβ−c .
 and so the result in (1) holds.
 Proof of (2):
 Recall that Var(β) = σ2(XTX)−1 and so with (1)
 D1 −D2
 σ2
 = Aβ−c T AVar(β)AT −1 Aβ−c .
 As β ∼Nn β, Var(β) , by 10.1
 Aβ ∼ Nq Aβ, AVar(β)AT .
 and so under H0 : Aβ = c
 Thus with 10.2
 Aβ ∼ Nq c, AVar(β)AT .
 D1 −D2
 σ2
 ∼ χ2
 q.
 Note that D2 = (n−p)s2 where s2 = s2(Y ) is the unbiased estimator for the error variance
 σ2 in model M2. Then by Theorem 10.1
 D2
 σ2 = (n−p)s2
 σ2
 ∼ χ2
 n−p.
206
 CHAPTER 18. APPENDIX
 Furthermore s2 and β are independent. As D2 = (n−p)s2 and we have shown that D1−D2
 is a linear function of β, it follows that D1 − D2 and D2 are independent.
 Hence by the definition of an F-distribution, under H0 : Aβ = c, we have
 (D1 −D2)/(σ2q)
 D2/[σ2(n −p)]
 = (D1−D2)/q
 D2/(n −p) ∼ Fq,n−p.
 This concludes the proof of the theorem.
 Appendix F- The exponential family of distributions
 Definition
 An Exponential Dispersion Model (EDM) or, in other words, a distribution that
 belongs to the exponential family, has a probability density function (or probability mass
 function for discrete distributions) that can be written in the following canonical form
 f(y | θ,ϕ) = c(y,ϕ) exp yθ−b(θ)
 ϕ .
 Here
 • θ is the canonical parameter of the distribution;
 • b(θ) is the cumulant function and assumed to be twice differentiable;
 • ϕ>0is the dispersion parameter; and, finally,
 • c(y,ϕ) is a normalising constant that ensures that f(y | θ,ϕ) integrates to one (or
 sums to one in case of a probability mass function).
 Example distributions from the exponential family include the Normal, Log-Normal, Expo
nential, Gamma, Beta, χ2, Bernoulli, Poisson, ...
207
 Example: Binomial distribution
 Consider the Binomial Bin(N,ρ) distribution where N is assumed known.
 If Y ∼Bin(N,ρ), then for y = 0,...,N,
 P(Y =y) = N
 y ρy(1−ρ)N−y
 = N
 y exp log(ρy(1−ρ)N−y)
 = N
 y exp ylog(ρ)+(N −y)log(1−ρ)
 = N
 y exp y(log(ρ)−log(1−ρ))+Nlog(1−ρ)
 = N
 y exp ylog( ρ
 1−ρ) +N log(1−ρ)
 Now set
 Then
 Hence, if we set
 then
 ϕ =1,
 c(y, ϕ) = N
 y and θ=log ρ
 ρ = exp(θ)
 1 +exp(θ) and so 1−p =
 b(θ) = Nlog 1+exp(θ) = Nlog(1+ ρ
 P(Y =y) = N
 y exp ylog( ρ
 = c(y,ϕ)exp yθ−b(θ)
 1 −ρ .
 1
 1 +exp(θ) .
 1 −ρ ) = −Nlog(1−ρ),
 1−ρ) +N log(1−ρ)
 ϕ .
 Therefore the Binomial distribution belongs to the exponential family of distributions.
 Properties
 If the distribution of Y belongs to the exponential family, then we can show that
 EY = b′(θ) and Var Y = ϕb′′(θ).
208
 CHAPTER 18. APPENDIX
 Thus for a distribution from the exponential family there is a functional relationship between
 the expectation and the variance.
 Example- Binomial distribution
 If Y has a Bin(N,ρ) distribution, then EY = Nρ and Var Y = Nρ. We have shown in
 the last section that for a Binomial Bin(N,ρ) distribution ϕ = 1 and
 b(θ)
 Hence
 = Nlog 1+exp(θ)
 EY = b′(θ) = N exp(θ)
 and
 Var Y
 1 +exp(θ) = N ρ
 = ϕb′′(θ) = N exp(θ)
 (1 +exp(θ))2 = N ρ(1−ρ).
 Exercise 54 Show that the Normal distribution belongs to the exponential family of
 distributions. Derive the expectation and variance of the Normal distribution by computing
 b′(θ) and ϕb′′(θ).
 Appendix G- Poisson GLM
 Suppose that we have collected some count data. For example,
 • the number of times a machine broke down each week,
 • the number of people who were infected in each town, or
 • the number of a particular bacteria in each sample.
 If the number of events were recorded in a fixed amount of time/space, then such count data
 is usually modelled using a Poisson distribution.
 18.0.1 Definition of a Poisson GLM
 Suppose that for i = 1,...,n
 Yi
 ∼ Poisson(µi)
 where
 g(µi) = xT
 iβ.
 As for Binomial GLMs we need to define a link function g. This time we need a function that
 maps (0,∞) to R, so we use the (natural) log function. The log-link function g(µ) = log(µ)
 is the most commonly chosen link function for Poisson GLMs and we will be using the
 log-link for all Poisson GLMs that we consider here.
209
 18.0.2 ParameterestimationforaPoissonGLM
 ThelikelihoodfunctionofaPoissonGLMisproportional to
 n
 i=1
 µyi
 i
 yi! exp(−µi).
 Notethatwemayignorethefactors 1
 yi! astheyonlydependonthedataandnotonthe
 meansµi.Takingthelogweobtainaslog-likelihoodfunction
 l(µi) =
 n
 i=1
 yilog(µi)−µi .
 SupposeourlinearpredictorsxT iβaregivenby
 xT
 iβ = β0+
 p−1
 j=1
 βjxij fori=1,...,n.
 Herexij is the ithobservationof thejthpredictor. Asweassumea log-link,wehave
 g(µi)= log(µi)=β0+ p−1
 j=1βjxij,andsothelog-likelihoodfunctionofthePoissonGLM
 isgivenby
 l(β0,...,βp−1) =
 n
 i=1
 yi β0+
 p−1
 j=1
 βjxij −exp β0+
 p−1
 j=1
 βjxij .
 Todetermine themaximumlikelihoodestimate forβweusean iterative least squares
 algorithm.Giventhemaximumlikelihoodestimateβwecanthencomputethefittedvalues
 µi forthemeansµi.Asweareusingalog-linkfunctionwehavefori=1,...,n
 µi = exp β0+
 p−1
 j=1
 βjxij .
 ThefittedvaluescanbecomputedinRusingthepredictcommandwithargumenttype=
 "response".
 18.0.3 InterpretationofaPoissonGLM
 Suppose
 log(µ)=β0+β1x+β2z, then µ=exp(β0) exp(β1x) exp(β2z).
 Thusanincreaseinxbyoneunitwhilekeepingzfixed, leadstothecorrespondingmean
 exp(β0) exp(β1(x+1)) exp(β2z)= exp(β0) exp(β1x) exp(β2z) exp(β1).
210
 CHAPTER 18. APPENDIX
 Thus an increase in x by one unit while keeping z fixed changes the mean µ by a factor of
 exp(β1). Note that this interpretation assumes that we can change the value of the first
 predictor while keeping the value of the second predictor fixed. This interpretation can be
 extended to models with more than two predictors.
 18.0.4 Poisson GLM with offset
 With count data we may have to take into account differences in the size of the samples or a
 measure of exposure. For example, if we count the number of people who are infected in
 each town, then we need to take into account the fact that the towns are of different sizes.
 In this case, all else being equal, we expect a town that is twice as large to have twice as
 many cases. We can accommodate this effect using a so-called offset.
 More formally, suppose that for i = 1,...,n we define Yi to be the number of people infected
 in the ith town and mi the number of people living in the town.
 Suppose E(Yi) = µi, then E(Yi/mi) = µi/mi. If we use as model log(µi/mi) = xT iβ, then
 we have log(µi) = xT iβ + log(mi). Thus log(mi) is an offset in the Poisson GLM for Y
 which allows us to model rates given by Y1/m1,...,Yn/mn.
 18.0.5 Example- Poisson GLM
 Suppose we are interested in the variables that influence the concentration of Campylobacter
 in the samples of river water. The first few lines of the data are shown below.
 campy 20 86 43 38 ···
 vol
 100 250 250 100 ···
 dist
 13
 15
 8
 21 ···
 The variable campy provides the number of the bacteria of interest in the given sample, vol
 gives the volume of the sample of river water in ml and dist reports the distance in km of
 the location at which the sample was taken from the source of the bacteria contamination.
 Note that the concentration of Campylobacter is given by the number of Campylobacter in
 a sample divided by the volume of the sample and thus defined as a rate. Therefore is is
 appropriate to use log(vol) as an offset.
 In addition to the offset we will use the distance from source as a predictor variable. We
 thus fit a Poisson model as follows:
 m <- glm(campy ~ offset(log(vol)) + dist, family="poisson", data=campy)
 Weset the family argument to "poisson" to fit a Poisson GLM and add offset(log(vol))
 as a predictor to the model formula to indicate that the logarithm of volume should be
211
 treated as an offset. This produces the following output. Note that log(vol) is not listed in
 the output as it is an offset.
 Call:
 glm(formula = campy ~ offset(log(vol)) + dist, family = "poisson",
 data = campy)
 Coefficients:
 Estimate Std. Error z value Pr(>|z|)
 (Intercept)-5.030951 0.126273-39.842 < 2e-16 ***
 dist
 0.023696 0.008168 2.901 0.00372 **--
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 (Dispersion parameter for poisson family taken to be 1)
 Null deviance: 235.65 on 199 degrees of freedom
 Residual deviance: 227.18 on 198 degrees of freedom
 AIC: 604.42
 Number of Fisher Scoring iterations: 5
 The estimated coefficient for the distance from source is 0.024, thus we expect the average
 concentration of Campylobacter to increase by a factor of exp(0.024 × 10) = 1.27 for every
 additional 10 km of distance from the source.
 Exercise 55
 Consider the following scenarios. Determine the appropriate model to use in each case.
 • Acohort of 18 year-old school leavers were surveyed regarding their future plans, with
 specific interest in higher education. Several variables were measured, including age,
 gender, smoking status, as well as whether they had a place at university or not. We
 wish to investigate the variables which are associated with university attendance.
 • Sixty kittens have been divided into 2 groups of 30. One group was fed a control diet,
 whilst the other was fed a diet supplemented with vitamin B12. After 6 weeks, the
 increase in weight (in g) of each kitten was calculated. We wish to use the variables
 diet and colouring to predict the quantity of weight gained.
 • Acar manufacturer conducted a study to investigate the reliability of their cars. They
 measured the number of times that each of 50 vehicles had broken down in the 10
 years since it was made. They also measured the number of miles that each car had
 been driven as well as the model of each vehicle and the number of times that it had
212
 CHAPTER 18. APPENDIX
 been serviced.
 Exercise 56
 In this exercise you will fit a Poisson GLM to the ships data set from the MASS package.
 a. Load the package MASS and the data set ships. Use the help function in R to obtain
 an overview on the data.
 b. Fit a Poisson regression model with service as an offset. Hint: Don’t forget to remove
 the observations where service is zero.
 c. Explore which of the variables have a significant effect on the response.
 d. Which ship type, year of construction and period of operation is at highest risk of
 damage?
 e. Calculate the expected number of damage incidents in twelve months for this kind of
 shipping. (Hint: To get a prediction on the scale of the response you need to add the
 argument type="response" to the function predict.)
 Exercise 57
 Download the file Campy.csv from the moodle. By choosing an appropriate model, investigate
 which of the variables influence the concentration of Campylobacter in the samples of river
 water. Compare the p-values generated from the Wald test and the likelihood ratio test. Do
 they lead you to choosing a different set of significant predictors?
 Appendix H- A summary of some key results
 A normal linear model is a statistical model of the form
 Y = X β + ϵ
 n×1
 n×p p×1
 n×1
 where X is of rank p < n and the errors have a multivariate normal distribution with
 ϵ ∼Nn(0,σ2In). Note that p denotes the number of columns in the design matrix which
 includes, possibly, an intercept term. Equivalently we can write: Y ∼ Nn(Xβ,σ2In).
 Under the normal linear model the following properties hold.
 • The Least Squares Estimator (LSE) of β is given by
 β(Y ) =XTX−1XTY.
 • Wehave
 β ∼ Np β,σ2 XTX−1 .
 • The LSE for β is also the Maximum Likelihood Estimator (MLE) for β.
213
 • The unbiased estimator for σ2 is given by
 s2(Y ) = 1
 n−p Y −Xβ(Y) T Y −Xβ(Y) .
 • The T-statistic for the ith parameter βi is defined as
 βi(Y ) −βi
 s2(Y )dii
 ∼ tn−p,
 where dii is the ith diagonal element of XTX−1 .
 • A100(1−α)% confidence interval (C.I.) for βi is given by
 βi(y) ± tn−p(1−α/2) s(y) dii.
 • If two normal linear models M0 and M1 with deviances D0 and D1 respectively, are
 such that M0 is nested within M1, then we have
 F = (D0−D1)/q
 D1/(n −p) ∼ Fq,n−p
 where
 n = number of units of observation,
 p = dimension of parameter vector in M1,
 q = number of parameters fixed to reduce M1 to M0

