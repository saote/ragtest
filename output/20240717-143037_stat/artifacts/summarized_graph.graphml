<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d6" for="edge" attr.name="source_id" attr.type="string" />
  <key id="d5" for="edge" attr.name="description" attr.type="string" />
  <key id="d4" for="edge" attr.name="weight" attr.type="double" />
  <key id="d3" for="node" attr.name="entity_type" attr.type="string" />
  <key id="d2" for="node" attr.name="source_id" attr.type="string" />
  <key id="d1" for="node" attr.name="description" attr.type="string" />
  <key id="d0" for="node" attr.name="type" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="ST231">
      <data key="d0">LECTURE NOTES</data>
      <data key="d1">ST231 is a set of lecture notes that cover various topics in linear regression and related statistical concepts</data>
      <data key="d2">54206a4a813f5be515f41653e9422eeb</data>
    </node>
    <node id="SIMPLE LINEAR REGRESSION">
      <data key="d0">STATISTICAL CONCEPT</data>
      <data key="d1">Simple linear regression is a statistical method used to model the relationship between a dependent variable and one independent variable</data>
      <data key="d2">54206a4a813f5be515f41653e9422eeb</data>
    </node>
    <node id="TERMINOLOGY AND NOTATION">
      <data key="d0">STATISTICAL CONCEPT</data>
      <data key="d1">Terminology and notation are the language and symbols used in the field of statistics, including terms like dependent variable, independent variable, and regression coefficients</data>
      <data key="d2">54206a4a813f5be515f41653e9422eeb</data>
    </node>
    <node id="MULTIPLE REGRESSION">
      <data key="d0">STATISTICAL CONCEPT</data>
      <data key="d1">Multiple regression is a statistical method used to model the relationship between a dependent variable and two or more independent variables</data>
      <data key="d2">54206a4a813f5be515f41653e9422eeb</data>
    </node>
    <node id="LINEAR MODELS">
      <data key="d0">STATISTICAL CONCEPT</data>
      <data key="d1">Linear models are a class of statistical models that assume a linear relationship between the dependent variable and the independent variables</data>
      <data key="d2">54206a4a813f5be515f41653e9422eeb</data>
    </node>
    <node id="ANSCOMBE'S QUARTET">
      <data key="d0">STATISTICAL CONCEPT</data>
      <data key="d1">Anscombe's Quartet is a group of four datasets that have nearly identical simple descriptive statistics but appear very different when graphed</data>
      <data key="d2">54206a4a813f5be515f41653e9422eeb</data>
    </node>
    <node id="POLYNOMIAL REGRESSION">
      <data key="d0">STATISTICAL CONCEPT</data>
      <data key="d1">Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial</data>
      <data key="d2">54206a4a813f5be515f41653e9422eeb</data>
    </node>
    <node id="LINEARITY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Linearity, a pivotal concept in statistical modelling, embodies the assumption that the relationship between variables is characterized by a proportional change in one variable relative to another. This fundamental principle is a cornerstone in linear regression models, where it is postulated that the relationship between explanatory variables and the response variable is linear. The linear predictor, X&#946;, is considered to adequately represent the mean function of the response variable, adhering to the linearity assumption. It is acknowledged that transformations of variables can facilitate the fulfillment of this assumption, enhancing model accuracy. However, linearity restricts the model to a class where the systematic component is a linear function of the explanatory variables, limiting its descriptive capacity to certain relationship types. Notably, the relationship between the response and the log-transformed explanatory variable is observed to be fairly linear, further enriching the understanding of linearity in statistical analysis.</data>
      <data key="d2">25fce1af816975003128126b5cfea73b,35bac6a2c3eb466ca9fc7b31bf2cc42c,3cbe71f7649e84cd67cb3fa0d3e632cf,6616e10c85e86291147e72776854b8a2,7cd6069e88e81548a237fa937adfecc6,c03eb12d07d48f9e94260f08dae10cdf</data>
    </node>
    <node id="ANSCOMBES_QUARTET">
      <data key="d0">EXAMPLE</data>
      <data key="d1">Anscombe's Quartet is a group of four datasets that have nearly identical simple descriptive statistics, yet appear very different when graphed. Each dataset consists of eleven (x,y) points. They were constructed in 1973 by the statistician Francis Anscombe to demonstrate both the importance of graphing data before analyzing it and the effect of outliers on statistical properties.&gt;</data>
      <data key="d2">35bac6a2c3eb466ca9fc7b31bf2cc42c</data>
    </node>
    <node id="POLYNOMIAL_REGRESSION">
      <data key="d0">MODEL</data>
      <data key="d1">Polynomial regression, a sophisticated variant of regression analysis, is employed to model the relationship between a dependent variable, y, and one or more independent variables, x, using a polynomial function. This method is particularly useful when the relationship between the variables is not linear, allowing for a more accurate representation of complex data patterns. The regression function, E(y | x), which denotes the conditional mean of y given x, is modeled as an nth degree polynomial in x. Despite the nonlinear model it fits to the data, polynomial regression is considered a linear model in the statistical estimation context, as the regression function is linear in the unknown parameters that are estimated from the data. This approach includes polynomials of the explanatory variables, enhancing the model's ability to capture the nuances of the data. Specifically, in the context provided, the polynomial regression model is of order 2, signifying a quadratic relationship between the response variable and the predictor variable. This non-linear function in terms of the predictor variable still maintains linearity in the parameters, a characteristic that simplifies the estimation process.</data>
      <data key="d2">0328e428a30c44572676dd571dd1e9bd,084dadebfca8bcb6377c205c45bee295,11452a08471d93959558de2ece9a69af,188219b9e5b6b6368360840921877de9,35bac6a2c3eb466ca9fc7b31bf2cc42c,87ba4f416a28aabc3b396908f5913b54,b8ec334f8c87bf1d9cb6043fa1a64214,b9af17718641389ba07f53be13f31f8c</data>
    </node>
    <node id="LOG_TRANSFORMED_PREDICTOR">
      <data key="d0">MODEL</data>
      <data key="d1">The LOG_TRANSFORMED_PREDICTOR refers to the logarithmic transformation applied to a specific predictor variable, in this case, the diameter, within a linear model. This transformation is employed to rectify situations where the original relationship between the predictor (diameter) and the response variable is non-linear, making it linear after the logarithmic transformation. This approach enhances the model's ability to accurately predict the response variable by ensuring that the relationship between the predictor and response adheres to the assumptions of linearity required for linear regression analysis.</data>
      <data key="d2">35bac6a2c3eb466ca9fc7b31bf2cc42c,656dce234514b9db38b5b5616557c1e9</data>
    </node>
    <node id="STRATEGY_FOR_STATISTICAL_MODELLING">
      <data key="d0">GUIDELINE</data>
      <data key="d1">A strategy for statistical modelling involves a series of steps and considerations to build, validate, and interpret statistical models. This includes defining the research question, choosing the appropriate model, checking model assumptions, interpreting results, and validating the model.&gt;</data>
      <data key="d2">35bac6a2c3eb466ca9fc7b31bf2cc42c</data>
    </node>
    <node id="GOOD_MODEL">
      <data key="d0">CRITERIA</data>
      <data key="d1">A good model, referred to here as "GOOD_MODEL", is characterized by its multifaceted criteria that ensure its effectiveness in statistical analysis. Primarily, its purpose is pivotal, serving either to describe, predict, or infer from the data. It must accurately represent the data and adeptly answer the questions of interest. Additionally, "GOOD_MODEL" is distinguished by its predictive accuracy, simplicity, and the relevance of the variables it incorporates. It is essential that this model meets the assumptions of the statistical test being employed, further enhancing its reliability and validity. The simplicity of "GOOD_MODEL" is not at the cost of its comprehensiveness, as it balances the need for straightforwardness with the necessity of capturing the essential features of the data. Overall, "GOOD_MODEL" is a well-rounded statistical model that excels in both predictive power and analytical depth, making it a robust tool for data analysis.</data>
      <data key="d2">35bac6a2c3eb466ca9fc7b31bf2cc42c,bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="SUMMARY_OF_CHAPTER_2">
      <data key="d0">SUMMARY</data>
      <data key="d1">The summary of Chapter 2 likely includes a recap of the topics covered in the chapter, which are related to linearity in statistical models, including Anscombe's Quartet, polynomial regression, models with log-transformed predictors, a strategy for statistical modelling, and criteria for a good model.&gt;</data>
      <data key="d2">35bac6a2c3eb466ca9fc7b31bf2cc42c</data>
    </node>
    <node id="RESIDUAL_ANALYSIS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Residual analysis, a pivotal component in the realm of regression analysis, serves as a robust method for evaluating the adequacy and quality of a model. Specifically, it focuses on the examination of residuals, which are defined as the discrepancies between the observed values and those predicted by the model. This analysis is instrumental in verifying the model's adherence to key assumptions, including linearity, homoscedasticity (constant variance of errors), independence of errors, and the normal distribution of residuals. By scrutinizing these residuals for any discernible patterns, data scientists can identify potential violations of these assumptions, thereby ensuring the model's reliability and validity. Residual analysis plays a critical role in statistical inference, enabling researchers to make informed decisions about the model's fit and the appropriateness of its predictions.</data>
      <data key="d2">0328e428a30c44572676dd571dd1e9bd,35bac6a2c3eb466ca9fc7b31bf2cc42c,3bfc9b92571973e54c8095302acc1aaa,aa13c33a7e61206e6021e2736002ca9a</data>
    </node>
    <node id="CHAPTER 2">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 2 of a book or document, which is summarized on page 37</data>
      <data key="d2">752d1285c8b1e15a2e175515f77ddd5a</data>
    </node>
    <node id="RESIDUAL ANALYSIS">
      <data key="d0">SECTION</data>
      <data key="d1">Section 3 of the document, which discusses residual analysis starting on page 38</data>
      <data key="d2">752d1285c8b1e15a2e175515f77ddd5a</data>
    </node>
    <node id="DEFINITIONS">
      <data key="d0">SUBSECTION</data>
      <data key="d1">Subsection 3.1 of the document, which provides definitions related to residual analysis on page 38</data>
      <data key="d2">752d1285c8b1e15a2e175515f77ddd5a</data>
    </node>
    <node id="MODEL ASSUMPTIONS">
      <data key="d0">SUBSECTION</data>
      <data key="d1">Subsection 3.2 of the document, which discusses the assumptions of the model on page 40</data>
      <data key="d2">752d1285c8b1e15a2e175515f77ddd5a</data>
    </node>
    <node id="RESIDUAL PLOTS">
      <data key="d0">SUBSECTION</data>
      <data key="d1">Subsection 3.3 of the document, which explains residual plots on page 41</data>
      <data key="d2">752d1285c8b1e15a2e175515f77ddd5a</data>
    </node>
    <node id="ASSESSING NORMALITY">
      <data key="d0">SUBSECTION</data>
      <data key="d1">Subsection 3.4 of the document, which discusses methods for assessing normality on page 47</data>
      <data key="d2">752d1285c8b1e15a2e175515f77ddd5a</data>
    </node>
    <node id="SUMMARY OF CHAPTER 3">
      <data key="d0">SUBSECTION</data>
      <data key="d1">Subsection 3.5 of the document, which summarizes Chapter 3 on page 50</data>
      <data key="d2">752d1285c8b1e15a2e175515f77ddd5a</data>
    </node>
    <node id="NON-LINEAR TRANSFORMATIONS">
      <data key="d0">SECTION</data>
      <data key="d1">Section 4 of the document, which discusses non-linear transformations starting on page 51</data>
      <data key="d2">752d1285c8b1e15a2e175515f77ddd5a</data>
    </node>
    <node id="THE LOG-TRANSFORMATION">
      <data key="d0">SUBSECTION</data>
      <data key="d1">Subsection 4.1 of the document, which discusses the log-transformation</data>
      <data key="d2">752d1285c8b1e15a2e175515f77ddd5a</data>
    </node>
    <node id="CHAPTER_3">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 3 is a section in a document that likely discusses a specific topic or set of topics related to the document's subject matter</data>
      <data key="d2">9846990771550ccdb865e49ecb96e2a3</data>
    </node>
    <node id="NON_LINEAR_TRANSFORMATIONS">
      <data key="d0">TOPIC</data>
      <data key="d1">Non-linear transformations, a topic elaborated in Chapter 4, are essential mathematical operations applied to variables in a statistical model. These transformations serve as remedial actions to address violations of model assumptions, particularly focusing on resolving issues of non-linearity, heteroscedasticity, and the impact of unusual observations. By adjusting the response and/or explanatory variables, non-linear transformations help to meet the assumptions of linearity and homoscedasticity, ensuring that the data adheres more closely to the requirements of statistical models. This process is crucial for enhancing the accuracy and reliability of model predictions by mitigating the effects of outliers and ensuring that the data conforms to the necessary statistical properties for valid analysis.</data>
      <data key="d2">07951ffe6787af44aa60c90c69e62f83,9846990771550ccdb865e49ecb96e2a3,c03eb12d07d48f9e94260f08dae10cdf,d71b402ab9edbb4347e09c7af3257cf5,e361ac139c268d5c3f3623f920e68af2</data>
    </node>
    <node id="LOG_TRANSFORMATION">
      <data key="d0">TRANSFORMATION</data>
      <data key="d1">Log transformation, a specific type of non-linear transformation, is a powerful technique discussed in Chapter 4 that is widely used in data analysis to address various statistical issues. It is particularly effective in stabilizing the variance of the data, making the distribution more symmetrical, and linearizing relationships between variables. This transformation is often applied to address heteroscedasticity, a condition where the variance of the error terms is not constant across all levels of the independent variables. Additionally, log transformation helps in reducing the influence of outliers, making it a versatile tool for data preprocessing in statistical modeling.</data>
      <data key="d2">07951ffe6787af44aa60c90c69e62f83,66f7fae9d896ff2b3fd40186cc833503,9846990771550ccdb865e49ecb96e2a3</data>
    </node>
    <node id="MAMMALS_DATASET">
      <data key="d0">DATASET</data>
      <data key="d1">The MAMMALS_DATASET is a comprehensive collection of data from the MASS package in R, specifically designed to provide insights into the average brain and body weights of various land mammal species. This dataset, featuring information on 62 species, serves as a valuable resource for researchers and data scientists interested in understanding the biological attributes of mammals. It is particularly noted for containing influential data points, such as those for elephants, which can significantly impact statistical analyses. The MAMMALS_DATASET is not only a repository of quantitative data but also plays a significant role as an illustrative case study in Chapter 4, where it is used to demonstrate the application of non-linear transformations, highlighting its educational and practical utility in the field of statistical analysis.</data>
      <data key="d2">66f7fae9d896ff2b3fd40186cc833503,83bb91cf725e5116ca2f5748fddccfae,9846990771550ccdb865e49ecb96e2a3,9e2ebbb113c00fa43f0af3c0696baf95,bd05fe6a05f9a13d33c4f1b5a771ada5,c47968226557bc2eb5aec5bb7994fd0e,f0b1289a0623b82a686b3b7dfb3a6ee8,f9d6c3504b8f8b5c25550076e45f8270</data>
    </node>
    <node id="TREES_DATASET">
      <data key="d0">DATASET</data>
      <data key="d1">The TREES_DATASET is a comprehensive collection of data centered around the dimensions and measurements of 31 felled black cherry trees. This dataset serves as a valuable resource for understanding tree metrics, featuring detailed records of tree volume, diameter, height, and girth. Its rich content makes it an ideal case study for demonstrating the practical application of non-linear transformations, likely featured in Chapter 4 of a relevant text. The dataset's focus on black cherry trees provides a specific context for analyzing tree growth and structure, making it a useful tool for researchers and data scientists interested in forestry and ecological studies.</data>
      <data key="d2">9846990771550ccdb865e49ecb96e2a3,a60af43e42c72a41fa90da06beb29d1b,d71b402ab9edbb4347e09c7af3257cf5,efeeb664622c1ee594e6a08a8322ffe3</data>
    </node>
    <node id="CHAPTER_4">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 4 is a section in a document that likely discusses non-linear transformations, including specific examples such as the log-transformation and the use of the mammals and trees datasets</data>
      <data key="d2">9846990771550ccdb865e49ecb96e2a3</data>
    </node>
    <node id="LEAST_SQUARES_ESTIMATION">
      <data key="d0">TOPIC</data>
      <data key="d1">Least squares estimation, a pivotal statistical method in the realm of data analysis, is extensively utilized for estimating the parameters of a model by minimizing the sum of the squares of the residuals. This method is particularly significant in regression analysis, where it facilitates the identification of the line or plane of best fit for a given set of data points. LEAST_SQUARES_ESTIMATION provides a closed-form expression for the parameter estimates, making it a cornerstone for various statistical analyses, especially in linear regression models.

The technique is not only fundamental for re-parameterizations of models but also crucial for deriving maximum likelihood estimates in normal linear models. It enables the fitting of models where the systematic component can be represented as the product between a design matrix and a parameter vector, accommodating more complex and general models. LEAST_SQUARES_ESTIMATION is discussed in Chapter 5, which delves into methods for estimating model parameters by minimizing the sum of the squares of the residuals, offering a comprehensive understanding of its mathematical underpinnings and practical applications.

In essence, LEAST_SQUARES_ESTIMATION is a robust algorithm used to estimate the parameters of a linear model, minimizing the sum of the squared differences between the observed values and the values predicted by the model. This method is widely recognized for its ability to provide a best-fit line or curve, making it an indispensable tool in the field of statistical analysis and regression modeling.</data>
      <data key="d2">07951ffe6787af44aa60c90c69e62f83,6616e10c85e86291147e72776854b8a2,87ba4f416a28aabc3b396908f5913b54,8a9b984c146f59b2af83d1c2f373d376,924be3e598ffeabd1fbd9b57f033b917,9846990771550ccdb865e49ecb96e2a3,9f335f1ecb85a1427df926df8bb1e89f,b1690cb1a67892245c0665e5099e322d,b9af17718641389ba07f53be13f31f8c,cf6e59c3746d399dc8baf5064f78ac57</data>
    </node>
    <node id="LEAST_SQUARES_ESTIMATE">
      <data key="d0">ESTIMATION_METHOD</data>
      <data key="d1">The LEAST_SQUARES_ESTIMATE, a statistical method for estimating model parameters, is prominently discussed in Chapter 5. This method is specifically designed to minimize the sum of the squares of the residuals, making it a powerful tool for parameter estimation. In the context of the given data, the LEAST_SQUARES_ESTIMATE is applied to calculate the values of \u00b5 = (\u00b5A, \u00b5B, \u00b5C), which are associated with the sales of three distinct brands, A, B, and C. This application of the LEAST_SQUARES_ESTIMATE involves a formula that takes into account the sales figures of each brand, providing a comprehensive and precise estimate of the parameters \u00b5A, \u00b5B, and \u00b5C.</data>
      <data key="d2">096afa471635bc59c3bfa9af4d04d625,9846990771550ccdb865e49ecb96e2a3</data>
    </node>
    <node id="SIMPLE_LINEAR_REGRESSION">
      <data key="d0">REGRESSION_METHOD</data>
      <data key="d1">Simple linear regression, a specific case of linear regression, is a statistical method employed to analyze and model the relationship between a dependent variable (response variable) and a single independent variable (explanatory variable). This model assumes a linear relationship between the two variables and aims to find the line of best fit that minimizes the sum of the squared residuals. The relationship is described by the equation Yj = &#946;0 + &#946;1xj + &#949;j, where &#946;0 is the intercept, &#946;1 is the slope, xj is the observed value of the independent variable X, and &#949;j is the error term, assumed to be normally distributed with mean zero and variance &#963;^2.

In the context of least squares estimation, simple linear regression provides an illustrative example of how the least squares estimate of the parameter vector can be derived and used to make predictions. The method is a special case of multiple linear regression, where there is only one predictor. The simple linear regression model uses only one predictor variable, in this case, price, to predict the dependent variable Y.

Simple linear regression is used to summarize and study relationships between two continuous (real-valued) variables: One variable, denoted x, is regarded as the predictor, explanatory, or independent variable. The other variable, denoted y, is regarded as the response, outcome, or dependent variable. It is the simplest form of a linear model, which involves one explanatory variable and one outcome variable, and it is used to fit the data, assuming a linear relationship between the response and the explanatory variable.</data>
      <data key="d2">15c7b5750483a382ce59751008e86751,25fce1af816975003128126b5cfea73b,28eee75e95bbbaf143368c3289585670,426434b67f6a287852ab66b82ca873cf,4683e58cf41e5f5d415a63ddb2fe0cac,512d9ffebe309a6f944ebce1ae2ff2a3,74a5a0e8ae0f846240c782cc1a30f82f,7b32c106246576bb451a5a3985914351,86ece4718d27d1a6c6a1f448cc850e2b,8a9b984c146f59b2af83d1c2f373d376,924be3e598ffeabd1fbd9b57f033b917,9846990771550ccdb865e49ecb96e2a3,a4a817bb79d6ae8812c808ca41d47f43,aa13c33a7e61206e6021e2736002ca9a,cf6e59c3746d399dc8baf5064f78ac57,e079b7c92d5c0b009ff02040eb652bc6,e47d573a10e64a657e58218df64d8920,f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </node>
    <node id="CHAPTER_5">
      <data key="d0" />
      <data key="d1">CHAPTER_5, also known as Chapter 5, is a significant section within a larger document. This chapter is expected to delve into a specific topic that is intricately connected to the overarching subject matter of the text. As a pivotal part of the document, Chapter 5 is likely to provide in-depth analysis, insights, or information that contributes to the comprehensive understanding of the text's theme. Despite the lack of detailed information, it is clear that Chapter 5 holds a substantial role in the context of the document, serving as a dedicated segment for exploring a particular aspect of the subject.</data>
      <data key="d2">9133320d451c1c1eddf1438064663b17,9846990771550ccdb865e49ecb96e2a3</data>
    </node>
    <node id="DERIVATION_OF_LSE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Derivation of the least squares estimate (LSE) involves the process of finding the parameters of a model that minimize the sum of the squares of the residuals. This process often involves calculus and linear algebra.&gt;</data>
      <data key="d2">cf6e59c3746d399dc8baf5064f78ac57</data>
    </node>
    <node id="INVERTIBLE_LINEAR_TRANSFORMATIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Invertible linear transformations, a critical concept in linear algebra, are operations that can be reversed, meaning there exists an inverse transformation for them. These transformations are pivotal in various mathematical and statistical contexts, playing a significant role in the transformation of variables within regression analysis. They are particularly useful in re-parameterising a model, which impacts the interpretation of the model's parameters. However, it's noteworthy that invertible linear transformations do not alter the fitted values, residuals, or deviance of the model, thus preserving the core statistical properties while allowing for different perspectives on the model's parameters. This dual functionality makes invertible linear transformations a versatile tool in the arsenal of data scientists and statisticians, enabling them to manipulate and interpret data in multiple ways without compromising the integrity of the statistical analysis.</data>
      <data key="d2">255685e281cc5a9edf073c700f425a6b,cf6e59c3746d399dc8baf5064f78ac57</data>
    </node>
    <node id="SUMMARY_OF_CHAPTER_5">
      <data key="d0">CONCEPT</data>
      <data key="d1">The SUMMARY_OF_CHAPTER_5 offers a concise recapitulation of the pivotal aspects discussed in Chapter 5. This chapter delves into the realm of least squares estimation and associated subjects, presenting a critical analysis and review of the methodologies and theories central to statistical modeling. The SUMMARY_OF_CHAPTER_5 serves as a handy reference, encapsulating the essence of the chapter's content, making it easier for readers to grasp the fundamental concepts and key takeaways from the detailed discussion on least squares estimators and related statistical techniques.</data>
      <data key="d2">9133320d451c1c1eddf1438064663b17,cf6e59c3746d399dc8baf5064f78ac57</data>
    </node>
    <node id="MAXIMUM_LIKELIHOOD_ESTIMATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Maximum likelihood estimation (MLE) is a method of estimating the parameters of a statistical model, given observations. The method chooses the parameter values that maximize the likelihood function, which is a function of the parameters that gives the probability of the observed data.&gt;</data>
      <data key="d2">cf6e59c3746d399dc8baf5064f78ac57</data>
    </node>
    <node id="LIKELIHOOD_FUNCTION_OF_NORMAL_LINEAR_MODEL">
      <data key="d0">CONCEPT</data>
      <data key="d1">The likelihood function of a normal linear model is the function that describes the probability of observing the given data under the assumption that the data follows a normal distribution with a linear relationship between the variables.&gt;</data>
      <data key="d2">cf6e59c3746d399dc8baf5064f78ac57</data>
    </node>
    <node id="CHAPTER_6">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 6 is a section in a document that likely discusses a specific topic related to the text's subject matter, focusing on Maximum likelihood estimation</data>
      <data key="d2">9133320d451c1c1eddf1438064663b17</data>
    </node>
    <node id="LIKELIHOOD_FUNCTION">
      <data key="d0">FUNCTION</data>
      <data key="d1">The likelihood function, central to statistical inference, serves as a pivotal tool in estimating the parameters of a model. Specifically, it quantifies the probability of observing the given data under a specified model and its parameters. In the context of a normal linear model, the likelihood function is employed to estimate the parameters by assessing the likelihood of the observed data. This function plays a crucial role in determining the best fit of the model to the data, making it an indispensable component in the realm of statistical analysis and model fitting.</data>
      <data key="d2">9133320d451c1c1eddf1438064663b17,d738df7d83784c8a41b3948271c537b6,f483798b15ef305e7826fd7142379e03</data>
    </node>
    <node id="MLE_PARAMETER_VECTOR">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">The MLE for the parameter vector is the maximum likelihood estimator for the vector of parameters in a statistical model</data>
      <data key="d2">9133320d451c1c1eddf1438064663b17</data>
    </node>
    <node id="SAMPLING_DISTRIBUTION_LEAST_SQUARES">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">Sampling distribution of the least squares estimator is the distribution of the least squares estimator over many samples</data>
      <data key="d2">9133320d451c1c1eddf1438064663b17</data>
    </node>
    <node id="UNBIASED_ESTIMATOR_ERROR_VARIANCE">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">Unbiased estimator for the error variance is an estimator of the error variance that is not systematically biased</data>
      <data key="d2">9133320d451c1c1eddf1438064663b17</data>
    </node>
    <node id="SUMMARY_OF_CHAPTER_6">
      <data key="d0">SUMMARY</data>
      <data key="d1">Summary of Chapter 6 provides a brief overview or recap of the main points covered in Chapter 6</data>
      <data key="d2">9133320d451c1c1eddf1438064663b17</data>
    </node>
    <node id="CHAPTER_7">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 7 is a section in a document that likely discusses a specific topic related to the text's subject matter, focusing on the hat matrix</data>
      <data key="d2">9133320d451c1c1eddf1438064663b17</data>
    </node>
    <node id="HAT_MATRIX_PROPERTIES">
      <data key="d0">PROPERTIES</data>
      <data key="d1">Properties of the hat matrix are characteristics or features of the hat matrix in a statistical context</data>
      <data key="d2">9133320d451c1c1eddf1438064663b17</data>
    </node>
    <node id="RESIDUALS_FITTED_VALUES_PROPERTIES">
      <data key="d0">PROPERTIES</data>
      <data key="d1">Properties of the residuals and of the fitted values are characteristics or features of the residuals and fitted values in a statistical context</data>
      <data key="d2">9133320d451c1c1eddf1438064663b17</data>
    </node>
    <node id="HAT_MATRIX">
      <data key="d0">MATRIX</data>
      <data key="d1">The Hat Matrix, a pivotal concept in regression analysis, is an n x n projection matrix denoted by H. It plays a crucial role in mapping observed values (y) to their corresponding fitted values (yb) in the context of least squares regression. Defined as H = X(XTX)^-1XT, where X is the design matrix, the Hat Matrix facilitates the calculation of leverages, represented by hii, which quantifies the leverage of the ith unit of observation. This matrix is central to understanding the influence of each data point on the fitted regression model, as it projects the observed response values onto the space of the fitted values. Its introduction and detailed analysis are featured in the last chapter, with the next chapter dedicated to its role in the calculation of the least squares estimator. Through the Hat Matrix, one can gain insights into how each observed value impacts the overall regression model, making it an indispensable tool in statistical inference and regression analysis.</data>
      <data key="d2">1d52aaeb960f9787b6229e57738f8e47,3fb977ccba63e267d2e7dd4de6479ce1,6ee02b38ae842fd5eac9a11c4fd6659f,7e05f1b457a496c8b3630e7044fc5981,83bb91cf725e5116ca2f5748fddccfae,9923e77ac6b3de95cb5026bc5e7fe8c0,bd05fe6a05f9a13d33c4f1b5a771ada5,f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </node>
    <node id="PROPERTIES_HAT_MATRIX">
      <data key="d0">PROPERTIES</data>
      <data key="d1">Properties of the hat matrix include its role in determining the leverage of data points and its symmetry and idempotence</data>
      <data key="d2">1d52aaeb960f9787b6229e57738f8e47</data>
    </node>
    <node id="RESIDUALS">
      <data key="d0">STATISTICAL_CONCEPT</data>
      <data key="d1">In the context of statistical analysis, RESIDUALS are a critical component for assessing the accuracy and fit of a model. They are defined as the differences between the observed values (y) and the fitted or predicted values (yb) generated by the model. Mathematically, this can be expressed as b\u03f5 = y - yb = (In - H)y, where (In - H) represents the residual maker matrix. RESIDUALS serve as estimates of the errors in the model, which are not directly observable. These errors are the discrepancies between the actual data points and the values predicted by the model, specifically in the context of linear regression and other statistical models.

RESIDUALS are crucial for evaluating the model's performance and identifying any patterns or trends that might indicate issues such as bias, heteroscedasticity, or outliers. Their variances and covariances can be derived, providing insights into the model's precision and the nature of the errors. In practical applications, such as sales forecasting, RESIDUALS help in understanding the differences between the observed sales values and the predictions made by the line of best fit, allowing for adjustments and improvements in the model.

Overall, RESIDUALS are the signed vertical distances between the observed data points and the fitted model, highlighting the model's errors or discrepancies in its predictions. They are used to assess the fit of the model, ensuring that it accurately represents the underlying data and can be relied upon for making informed decisions and predictions.</data>
      <data key="d2">0b650eb2f1dcd603b64fec3c4b5cd24b,0da640a09a395a50b6e16e047fa8d0d6,1d52aaeb960f9787b6229e57738f8e47,22093a562f5f05dc9891b45ab9bcbea8,312309b45c59e1c84695ac3c7e202742,3fb977ccba63e267d2e7dd4de6479ce1,5cc49d301d9cd1f8e20b92ab9d8346b0,60cc94e681863c9fcc6f9be1e500f840,674b8d5bb1f830d0fb944942514d1a16,7cd6069e88e81548a237fa937adfecc6,8a9b984c146f59b2af83d1c2f373d376,a60af43e42c72a41fa90da06beb29d1b,d1b6fcd55d937c5fe2d6add69e0bcf05,e361ac139c268d5c3f3623f920e68af2,e593096f3805c2686423cb91ea276fe6,f16299fc00a7a69bdf983dce826b4918</data>
    </node>
    <node id="FITTED_VALUES">
      <data key="d0">STATISTICAL_CONCEPT</data>
      <data key="d1">The entity "FITTED_VALUES" refers to the predicted values of the response variable in a linear regression model. These values, also known as yb, are calculated by substituting the values of the predictor variables into the model equation, which is estimated using the observed data. The fitted values are the estimates of the expected response for given values of the predictor variables and are calculated as yb = Hy, where H is the hat matrix. They represent the values predicted by the model for the response variable and are used to assess the fit of the model. The variances and covariances of these fitted values can be derived, providing insights into the precision of the predictions. In essence, fitted values are the predicted response values for each observation in the linear model, based on the simple regression line or more complex regression models, and they play a crucial role in understanding the relationship between the predictor and response variables.</data>
      <data key="d2">01d5ee79489582b4135fc96f676b24a0,0b650eb2f1dcd603b64fec3c4b5cd24b,1d52aaeb960f9787b6229e57738f8e47,22093a562f5f05dc9891b45ab9bcbea8,3fb977ccba63e267d2e7dd4de6479ce1,60cc94e681863c9fcc6f9be1e500f840,674b8d5bb1f830d0fb944942514d1a16,a60af43e42c72a41fa90da06beb29d1b,aa13c33a7e61206e6021e2736002ca9a,e361ac139c268d5c3f3623f920e68af2,e593096f3805c2686423cb91ea276fe6</data>
    </node>
    <node id="SUMMARY_CHAPTER_7">
      <data key="d0">SUMMARY</data>
      <data key="d1">Summary of Chapter 7 includes a review of the hat matrix and its properties, as well as the properties of residuals and fitted values</data>
      <data key="d2">1d52aaeb960f9787b6229e57738f8e47</data>
    </node>
    <node id="UNUSUAL_OBSERVATIONS">
      <data key="d0">STATISTICAL_CONCEPT</data>
      <data key="d1">Unusual observations, often referred to as outliers, are data points that significantly deviate from the general pattern of the data within the dataset. These points can be high leverage, outliers, or influential, meaning they have a disproportionate impact on the results of statistical analyses and the fitted model. Their presence can cause substantial changes to the model, and their removal may lead to a different model outcome. Unusual observations can indicate errors in data collection or unusual events, and transformations of the data can be applied to reduce their influence on the model. It is crucial to identify and understand these points as they can skew the results and lead to misinterpretations of the data.</data>
      <data key="d2">07951ffe6787af44aa60c90c69e62f83,1d52aaeb960f9787b6229e57738f8e47,c03eb12d07d48f9e94260f08dae10cdf,d71b402ab9edbb4347e09c7af3257cf5,e593096f3805c2686423cb91ea276fe6</data>
    </node>
    <node id="LEVERAGES">
      <data key="d0">STATISTICAL_CONCEPT</data>
      <data key="d1">Leverages, also referred to as hat-values, are pivotal measures in statistical analysis, particularly within the context of regression analysis. They serve as indicators of how far an independent variable value of a data point is from the mean of that variable, highlighting the degree to which the fitted values depend on the observed values. This dependency is crucial for identifying influential data points that can significantly impact the regression model. High leverage points, those that are substantially distant from the mean, have the potential to exert a substantial influence on the fitted values, thereby affecting the overall model fit. Derived from the hat matrix in regression analysis, leverages provide a quantitative assessment of an observation's deviation from the center of the fitted values in the regression space, making them essential for diagnosing potential issues in regression models.</data>
      <data key="d2">1d52aaeb960f9787b6229e57738f8e47,6ee02b38ae842fd5eac9a11c4fd6659f,e593096f3805c2686423cb91ea276fe6,f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </node>
    <node id="OUTLIERS">
      <data key="d0">STATISTICAL_CONCEPT</data>
      <data key="d1">Outliers, in the context of statistical analysis, are data points that deviate significantly from the other observations within a dataset. These points can be identified due to their substantial difference from the rest of the data, potentially arising from variability in measurement or experimental error. In regression analysis, outliers are particularly noteworthy as they can be response values that do not conform to the current model, thereby affecting the accuracy and reliability of the regression analysis. The presence of outliers can skew the results and lead to misleading conclusions if not properly accounted for in the analysis.</data>
      <data key="d2">1d52aaeb960f9787b6229e57738f8e47,428db872e71a17a2cf7868b03a52def0,f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </node>
    <node id="INFLUENCE">
      <data key="d0">STATISTICAL_CONCEPT</data>
      <data key="d1">"Influence, a pivotal statistical concept, quantifies the impact of individual data points on the outcomes of statistical analyses, particularly focusing on regression coefficients and the overall model fit. This measure is crucial for understanding how specific observations can sway the results, aiding in the identification of potential outliers or influential points that may disproportionately affect the conclusions drawn from the data."</data>
      <data key="d2">1d52aaeb960f9787b6229e57738f8e47,428db872e71a17a2cf7868b03a52def0,617760dc9b9682075b10899cc8473dd5</data>
    </node>
    <node id="INFLUENTIAL_DATA_POINTS">
      <data key="d0">DATA_POINTS</data>
      <data key="d1">In the context of statistical analysis, INFLUENENTIAL_DATA_POINTS are observations that wield considerable influence on the fitted model. These data points are critical as they can significantly impact the regression model, potentially skewing the results. Due to their high influence, INFLUENENTIAL_DATA_POINTS require careful consideration to ensure the accuracy and reliability of the statistical analysis. Their presence can dramatically affect the model's fit, potentially leading to misleading conclusions if not properly accounted for. Therefore, identifying and understanding INFLUENENTIAL_DATA_POINTS is essential for conducting a thorough and accurate analysis.</data>
      <data key="d2">1d52aaeb960f9787b6229e57738f8e47,323899f01972255cd3278bccee20d5d8,83bb91cf725e5116ca2f5748fddccfae</data>
    </node>
    <node id="CHAPTER 8">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 8 covers the topic of influence in statistical models, discussing influential data points and summarizing key concepts related to their impact on model outcomes</data>
      <data key="d2">617760dc9b9682075b10899cc8473dd5</data>
    </node>
    <node id="CHAPTER 9">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 9 focuses on categorical predictor variables, including examples such as brand in retail data, alternative parameterizations, and models with interactions, concluding with a summary of the chapter's main points</data>
      <data key="d2">617760dc9b9682075b10899cc8473dd5</data>
    </node>
    <node id="RETAIL DATA">
      <data key="d0">DATA</data>
      <data key="d1">Retail data is used as an example in Chapter 9 to illustrate the inclusion of brand as a predictor variable in statistical models</data>
      <data key="d2">617760dc9b9682075b10899cc8473dd5</data>
    </node>
    <node id="BRAND">
      <data key="d0">PREDICTOR_VARIABLE</data>
      <data key="d1">BRAND is a pivotal categorical predictor variable featured prominently in the context of retail data analysis, specifically discussed in Chapter 9. It serves as a key component in the regression models analyzed, representing the diverse product brands available in the retail setting. The variable BRAND categorizes stores or products into three distinct categories, denoted as A, B, and C, which correspond to different store or product brands. These categories are integral to the parallel lines model, where the impact of various brands (A, B, C) on sales and other retail metrics is assessed. As a categorical variable, BRAND facilitates the differentiation and classification of stores or products, enabling a nuanced understanding of brand influence on consumer behavior and sales outcomes within the retail dataset.</data>
      <data key="d2">06d5666e6bfdda828b48adba883b4a61,336546bc73cbe1828a0cc1a45faf8f5a,39aef0392258a09378ce45d8b03a268a,617760dc9b9682075b10899cc8473dd5,86ece4718d27d1a6c6a1f448cc850e2b,b0ca3e6c22c4cf884d03b1f6f82be5df,b2c33cb151a8e7724ebfb7b2d88bc45f,b6870535f3975c49d45e62fbe475f198,e800735d6b2a244875f5e0d292de1527,ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </node>
    <node id="ALTERNATIVE PARAMETERISATIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Alternative parameterisations are methods for representing categorical predictor variables in statistical models, discussed in Chapter 9</data>
      <data key="d2">617760dc9b9682075b10899cc8473dd5</data>
    </node>
    <node id="MODEL WITH INTERACTION">
      <data key="d0">MODEL</data>
      <data key="d1">A model with an interaction is a statistical model that includes interaction terms between predictor variables, discussed in Chapter 9</data>
      <data key="d2">617760dc9b9682075b10899cc8473dd5</data>
    </node>
    <node id="SUMMARY OF CHAPTER 8">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">617760dc9b9682075b10899cc8473dd5</data>
    </node>
    <node id="CATEGORICAL PREDICTOR VARIABLES">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">617760dc9b9682075b10899cc8473dd5</data>
    </node>
    <node id="SUMMARY OF CHAPTER 9">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">617760dc9b9682075b10899cc8473dd5</data>
    </node>
    <node id="PARAMETERISATIONS">
      <data key="d0">SECTION</data>
      <data key="d1">Parameterisations is a section in the document, likely discussing different ways to represent parameters in statistical models</data>
      <data key="d2">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </node>
    <node id="MODEL_WITH_INTERACTION">
      <data key="d0">SECTION</data>
      <data key="d1">A model with an interaction is a section in the document, probably discussing a statistical model that includes interaction terms between variables</data>
      <data key="d2">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </node>
    <node id="SUMMARY_OF_CHAPTER_9">
      <data key="d0">SECTION</data>
      <data key="d1">The SUMMARY_OF_CHAPTER_9 is a section within the document that encapsulates the pivotal insights from Chapter 9. This summary emphasizes the introduction of categorical predictors in linear models, a significant advancement in statistical modeling. It elucidates the technique of employing indicator variables as a means to integrate categorical data into linear regression analyses, thereby enhancing the model's ability to handle diverse types of predictors effectively. This section serves as a concise yet comprehensive guide to the core concepts and methodologies discussed in Chapter 9, making it an invaluable resource for readers seeking to grasp the intricacies of incorporating categorical variables in statistical models.</data>
      <data key="d2">b0ca3e6c22c4cf884d03b1f6f82be5df,ef68dd0317c62e3cdde00395f7a21bd7</data>
    </node>
    <node id="THE_T_STATISTIC">
      <data key="d0">SECTION</data>
      <data key="d1">The T-statistic is a section in the document, likely discussing the T-statistic in the context of hypothesis testing</data>
      <data key="d2">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </node>
    <node id="USEFUL_RESULTS_FOR_MULTIVARIATE_NORMAL">
      <data key="d0">SECTION</data>
      <data key="d1">Useful results for the multivariate normal distribution is a section in the document, probably discussing properties and results related to the multivariate normal distribution</data>
      <data key="d2">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </node>
    <node id="DISTRIBUTIONAL_PROPERTIES_OF_BETA_HAT_AND_S_SQUARED">
      <data key="d0">SECTION</data>
      <data key="d1">Distributional properties of &#946;b and s^2 is a section in the document, likely discussing the distributional properties of the least squares estimator and the sample variance</data>
      <data key="d2">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </node>
    <node id="SIMPLE_EXAMPLE_IN_R">
      <data key="d0">SECTION</data>
      <data key="d1">A simple example in R is a section in the document, probably providing an example of how to calculate or use the T-statistic in the R programming language</data>
      <data key="d2">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </node>
    <node id="SUMMARY_OF_CHAPTER_10">
      <data key="d0">SECTION</data>
      <data key="d1">Summary of Chapter 10 is a section in the document, summarizing the key points of Chapter 10</data>
      <data key="d2">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </node>
    <node id="CHAPTER_10">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 10 contains a simple example in R and a summary of the chapter's content</data>
      <data key="d2">0ba6a4dc0ac5f8b86cc2a22fd51b9517</data>
    </node>
    <node id="CHAPTER_11">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 11, a pivotal segment in the context of statistical analysis and modeling, delves into the intricate topic of prediction. This chapter specifically highlights interval estimation, a crucial aspect of statistical inference. It comprehensively covers confidence intervals, providing readers with a clear understanding of how to estimate population parameters within a specified range of values. The chapter includes a simple, yet illustrative example that is continued throughout, aiding in the comprehension of estimation and prediction techniques. Additionally, a concise summary is provided at the end of the chapter, encapsulating the key concepts and methodologies discussed, making it an essential resource for anyone seeking to deepen their understanding of statistical analysis.</data>
      <data key="d2">0ba6a4dc0ac5f8b86cc2a22fd51b9517,f087dce67c830cc3152c8d9cbb76cdb8</data>
    </node>
    <node id="CHAPTER_12">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 12, titled "The t-test for Normal Linear Models: An In-depth Analysis", is a comprehensive section dedicated to the exploration of the t-test within the context of normal linear models. This chapter delves into the intricacies of hypothesis testing, providing readers with a clear understanding of how to formulate and test hypotheses using the t-test. To enhance comprehension, a simple yet illustrative example is presented using the R programming language, a popular tool among data scientists and statisticians for data analysis.

Moreover, the chapter offers a comparative analysis, highlighting the differences between the t-test and other similar statistical tests. This comparison aids in clarifying the specific scenarios in which the t-test is most appropriate and effective. Additionally, the limitations of the t-test are discussed, ensuring that readers are aware of its boundaries and potential shortcomings.

In summary, Chapter 12 serves as a detailed guide to the t-test for normal linear models, encompassing hypothesis testing, practical examples, comparative insights, and an acknowledgment of limitations, making it a valuable resource for students and professionals alike in the field of statistical analysis.</data>
      <data key="d2">0ba6a4dc0ac5f8b86cc2a22fd51b9517,f087dce67c830cc3152c8d9cbb76cdb8</data>
    </node>
    <node id="CHAPTER_13">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 13 discusses the F-test and ANOVA (Analysis of Variance), likely covering their application in statistical testing and analysis</data>
      <data key="d2">f087dce67c830cc3152c8d9cbb76cdb8</data>
    </node>
    <node id="T_TEST">
      <data key="d0">TEST</data>
      <data key="d1">The t-test, a pivotal statistical hypothesis test, is employed to ascertain whether there exists a significant difference between the means of two groups. This test is particularly relevant when analyzing data that might be associated with normal linear models, a concept elaborated upon in Chapter 12. Despite its utility, the t-test has limitations in specific contexts, as also highlighted in the same chapter, which may affect its applicability and interpretation in certain scenarios.</data>
      <data key="d2">59ad428bf172e7866861ea44cbe198e2,f087dce67c830cc3152c8d9cbb76cdb8</data>
    </node>
    <node id="F_TEST">
      <data key="d0">TEST</data>
      <data key="d1">The F-test, a pivotal statistical tool in the realm of data analysis, is employed to compare variances between two or more groups. This test is particularly significant in the context of the analysis of variance (ANOVA), a topic comprehensively covered in Chapter 13 of relevant literature. The F-test's utility extends beyond variance comparison; it also serves to ascertain whether two populations share the same variance. Additionally, it plays a crucial role in ANOVA, where it is utilized to evaluate the differences in means among multiple groups. Through its application, insights into the structure and relationships within the community of interest can be gleaned, making it an indispensable component in statistical inference and algorithmic analysis.</data>
      <data key="d2">59ad428bf172e7866861ea44cbe198e2,f087dce67c830cc3152c8d9cbb76cdb8</data>
    </node>
    <node id="ANOVA">
      <data key="d0">ANALYSIS</data>
      <data key="d1">ANOVA, or Analysis of Variance, is a pivotal statistical method utilized to analyze the differences among group means in a sample. This technique is an extension of the t-test, enabling the comparison of more than two groups, thereby testing differences between two or more means. ANOVA is a topic of significant interest, often covered in depth in Chapter 13 of statistical textbooks, highlighting its importance in understanding and interpreting complex data structures and relationships within groups.</data>
      <data key="d2">59ad428bf172e7866861ea44cbe198e2,f087dce67c830cc3152c8d9cbb76cdb8,fc5b725f3c662c5471af20efdcc2dbff</data>
    </node>
    <node id="R">
      <data key="d0">SOFTWARE</data>
      <data key="d1">R is a versatile programming language and software environment primarily designed for statistical computing and graphics. It is widely utilized by statisticians and data miners for developing statistical software and conducting data analysis. R offers a rich set of functions for fitting normal linear models, estimating parameters in linear models, and determining residuals from such models. It is particularly noted for its application in the simple example of the t-test, as detailed in Chapter 12 of a referenced text. R's capabilities extend to data visualization, making it a comprehensive tool for graphical representation and data analysis. Its usage spans various domains, including the fitting of linear models for Retail data, where it plays a crucial role in model implementation, output generation, and design matrix verification. R's robust functionalities make it an indispensable tool for statistical inference, data modeling, and graphical analysis in the field of data science.</data>
      <data key="d2">01d5ee79489582b4135fc96f676b24a0,06199787dd7f75f7338dd24d4f3dc26e,084dadebfca8bcb6377c205c45bee295,119bc73ddf8eebadfb8eae272fa323a7,25fce1af816975003128126b5cfea73b,28eee75e95bbbaf143368c3289585670,312309b45c59e1c84695ac3c7e202742,48971100deb5bb374a41c1f2b7b2a86a,5cc49d301d9cd1f8e20b92ab9d8346b0,6a6f85d0a6e46196ab3a901fcc82a720,7b32c106246576bb451a5a3985914351,825b600cbab3535ce67e9f561ddcb84b,a1fc936df848a0fbc791e4bcc9b527b6,b99ecc2f79f56198a8c2adbdff95d576,c48bd062d5f6cc20c5df5758d6285562,d7f3a28534ffe830fe6f4cef8c41a9b4,e6f79ceb0df54119a4dc71b2162ac50b,e800735d6b2a244875f5e0d292de1527,eafa2cc6cc64d8bca1c080bdd2ad7654,ee295ebc4c2d6a2a5c738796fcc2ab71,f087dce67c830cc3152c8d9cbb76cdb8,f18bacb0a2fbfea44dd6326224184216</data>
    </node>
    <node id="NESTED_LINEAR_MODELS">
      <data key="d0">MATHEMATICAL_CONCEPT</data>
      <data key="d1">Nested linear models are a type of statistical model where one model is a special case of another model. They are used in the context of the F-test and ANOVA.</data>
      <data key="d2">59ad428bf172e7866861ea44cbe198e2</data>
    </node>
    <node id="TOTAL_SUM_OF_SQUARES">
      <data key="d0">MATHEMATICAL_CONCEPT</data>
      <data key="d1">The total sum of squares is a measure of the total variability in a dataset. It is decomposed into explained and unexplained variability in the context of the F-test and ANOVA.</data>
      <data key="d2">59ad428bf172e7866861ea44cbe198e2</data>
    </node>
    <node id="EXISTENCE_OF_REGRESSION">
      <data key="d0">MATHEMATICAL_CONCEPT</data>
      <data key="d1">The existence of regression refers to the presence of a relationship between a dependent variable and one or more independent variables. The F-test is used to test for this existence.</data>
      <data key="d2">59ad428bf172e7866861ea44cbe198e2</data>
    </node>
    <node id="ANOVA_TABLE">
      <data key="d0">MATHEMATICAL_CONCEPT</data>
      <data key="d1">The ANOVA_TABLE is a critical tool in statistical analysis, specifically designed to encapsulate the outcomes of an analysis of variance. This table meticulously organizes information regarding the sources of variation within a dataset, delineating between factors such as treatment and error. It provides key metrics including degrees of freedom, sums of squares, mean squares, and F-statistics, which are essential for understanding the structure and relationships within the data. Contrary to the second description, the primary function of the ANOVA table is not to determine the existence of regression but rather to assess the significance of differences among group means in a dataset. By doing so, it offers a structured approach to analyze the variance, enabling researchers and data scientists to make informed decisions about the impact of independent variables on the dependent variable.</data>
      <data key="d2">59ad428bf172e7866861ea44cbe198e2,638b52a0671088c9aa208790411ab898</data>
    </node>
    <node id="ILLUSTRATION_IN_R">
      <data key="d0">SOFTWARE_DEMONSTRATION</data>
      <data key="d1">"Illustration in R is a practical approach to elucidating statistical concepts by leveraging the capabilities of the R programming language, a preferred tool in the domain of data analysis. This method is specifically highlighted for its ability to provide tangible examples of complex statistical tests such as the F-test and ANOVA, making abstract theories more accessible and understandable. Through Illustration in R, statistical inference and algorithmic analysis become more tangible, enabling a deeper understanding of the relations and structure within the community of interest."</data>
      <data key="d2">59ad428bf172e7866861ea44cbe198e2,638b52a0671088c9aa208790411ab898</data>
    </node>
    <node id="DISTRIBUTION_OF_F_STATISTIC">
      <data key="d0">MATHEMATICAL_CONCEPT</data>
      <data key="d1">The distribution of the F-statistic is a probability distribution used in hypothesis testing. It is used to determine the significance of the F-test results.</data>
      <data key="d2">59ad428bf172e7866861ea44cbe198e2</data>
    </node>
    <node id="F_STATISTIC_DISTRIBUTION">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">The distribution of the F-statistic is a probability distribution used in hypothesis testing, particularly in ANOVA, to determine if there are significant differences between group means</data>
      <data key="d2">638b52a0671088c9aa208790411ab898</data>
    </node>
    <node id="SUMMARY_OF_CHAPTER_13">
      <data key="d0">SUMMARY</data>
      <data key="d1">Summary of Chapter 13 provides an overview of the key points covered in the chapter, which likely discusses ANOVA and regression analysis</data>
      <data key="d2">638b52a0671088c9aa208790411ab898</data>
    </node>
    <node id="SEQUENTIAL_ANOVA">
      <data key="d0">STATISTICAL_METHOD</data>
      <data key="d1">Sequential ANOVA is a method of analysis of variance where the factors are added to the model one at a time, allowing for the assessment of the incremental effect of each factor</data>
      <data key="d2">638b52a0671088c9aa208790411ab898</data>
    </node>
    <node id="FACTORS">
      <data key="d0">VARIABLES</data>
      <data key="d1">In the context of statistical analysis, FACTORS are pivotal elements discussed in Chapter 14. They can be understood as categorical variables within a statistical model, playing a significant role in the comparison of means across different groups, a method often utilized in Analysis of Variance (ANOVA). These factors, by their categorical nature, allow for the examination of how various categories influence the outcomes, making them crucial for understanding the relations and structure within the community of interest. FACTORS, therefore, are not merely considered as influences but are integral components of statistical models, particularly in scenarios where the impact of different groups needs to be assessed.</data>
      <data key="d2">322cf1f37d33953af834b695b7f08b72,638b52a0671088c9aa208790411ab898</data>
    </node>
    <node id="TEST_FOR_NON-LINEARITY">
      <data key="d0">HYPOTHESIS_TEST</data>
      <data key="d1">Test for non-linearity is a statistical test used to determine if the relationship between variables is not linear, which can affect the assumptions of regression analysis</data>
      <data key="d2">638b52a0671088c9aa208790411ab898</data>
    </node>
    <node id="CHAPTER 14">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 14 covers topics including factors, a test for non-linearity, and a summary of the chapter's content</data>
      <data key="d2">322cf1f37d33953af834b695b7f08b72</data>
    </node>
    <node id="CHAPTER 15">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 15 is a comprehensive section that delves into critical aspects of statistical modeling. This chapter focuses on model selection statistics, providing insights into the criteria used for choosing the most appropriate model from a set of candidates. It also covers variable selection, a process aimed at identifying the most relevant predictors in a model to enhance its performance and interpretability. Additionally, Chapter 15 addresses the issue of multicollinearity, a situation where independent variables in a model are highly correlated, potentially leading to unreliable and unstable estimates of regression coefficients. The discussion includes an examination of bias and variance, key concepts in understanding model performance, and the model hierarchy, which helps in understanding the relationship between different models. Through these topics, Chapter 15 offers a detailed exploration of the complexities involved in model selection and evaluation.</data>
      <data key="d2">322cf1f37d33953af834b695b7f08b72,ad35c05a2497a4e16a014d64483842a8</data>
    </node>
    <node id="TEST FOR NON-LINEARITY">
      <data key="d0">CONCEPT</data>
      <data key="d1">The test for non-linearity is a procedure described in Chapter 14 to assess the linearity of a model or data</data>
      <data key="d2">322cf1f37d33953af834b695b7f08b72</data>
    </node>
    <node id="SUMMARY OF CHAPTER 14">
      <data key="d0">CONCEPT</data>
      <data key="d1">The summary of Chapter 14 provides an overview of the topics covered in the chapter</data>
      <data key="d2">322cf1f37d33953af834b695b7f08b72</data>
    </node>
    <node id="BIAS AND VARIANCE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Bias and variance are concepts in Chapter 15 that relate to model performance and error</data>
      <data key="d2">322cf1f37d33953af834b695b7f08b72</data>
    </node>
    <node id="THE MODEL HIERARCHY">
      <data key="d0">CONCEPT</data>
      <data key="d1">The model hierarchy is a structure or classification of models discussed in Chapter 15</data>
      <data key="d2">322cf1f37d33953af834b695b7f08b72</data>
    </node>
    <node id="MODEL SELECTION STATISTICS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Model selection statistics, a pivotal concept discussed in Chapter 15, serve as essential metrics or criteria for evaluating and comparing various models. These statistics enable the identification of the most suitable model based on predefined standards, facilitating a data-driven decision-making process in the realm of statistical analysis. By quantifying model performance and fit, they assist in determining which model best represents the underlying data structure and relationships, ensuring optimal predictive accuracy and reliability.</data>
      <data key="d2">322cf1f37d33953af834b695b7f08b72,ad35c05a2497a4e16a014d64483842a8</data>
    </node>
    <node id="VARIABLE SELECTION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Variable selection, a critical process discussed in Chapter 15, involves the meticulous identification and choice of the most pertinent variables to incorporate into a statistical model. This process is essential for ensuring that the model is not only efficient but also effective in capturing the underlying relationships within the data. By focusing on the most relevant variables, variable selection helps in reducing model complexity, improving predictive accuracy, and enhancing the interpretability of the model.</data>
      <data key="d2">322cf1f37d33953af834b695b7f08b72,ad35c05a2497a4e16a014d64483842a8</data>
    </node>
    <node id="MULTICOLLINEARITY">
      <data key="d0">CONCEPT</data>
      <data key="d1">Multicollinearity refers to a situation in which two or more predictor variables in a multiple regression model are highly correlated</data>
      <data key="d2">ad35c05a2497a4e16a014d64483842a8</data>
    </node>
    <node id="SUMMARY OF CHAPTER 15">
      <data key="d0">SUMMARY</data>
      <data key="d1">The summary of Chapter 15 provides a brief overview of the key points covered in the chapter</data>
      <data key="d2">ad35c05a2497a4e16a014d64483842a8</data>
    </node>
    <node id="CHAPTER 16">
      <data key="d0">CHAPTER</data>
      <data key="d1">Chapter 16 discusses the General Linear Model, including the generalised least squares estimator and weighted regression</data>
      <data key="d2">ad35c05a2497a4e16a014d64483842a8</data>
    </node>
    <node id="GENERAL LINEAR MODEL">
      <data key="d0">CONCEPT</data>
      <data key="d1">The General Linear Model is a statistical model that includes a wide range of models, including linear regression, ANOVA, and ANCOVA</data>
      <data key="d2">ad35c05a2497a4e16a014d64483842a8</data>
    </node>
    <node id="GENERALISED LEAST SQUARES ESTIMATOR">
      <data key="d0">CONCEPT</data>
      <data key="d1">The generalised least squares estimator is a method for estimating the parameters of a linear regression model when the errors are not independent</data>
      <data key="d2">ad35c05a2497a4e16a014d64483842a8</data>
    </node>
    <node id="WEIGHTED REGRESSION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Weighted regression is a type of regression analysis where the weights are assigned to each data point based on the precision of its measurements</data>
      <data key="d2">ad35c05a2497a4e16a014d64483842a8</data>
    </node>
    <node id="SERIALLY CORRELATED ERRORS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Serially correlated errors refer to a situation where the errors in a regression model are correlated over time</data>
      <data key="d2">ad35c05a2497a4e16a014d64483842a8</data>
    </node>
    <node id="GLM">
      <data key="d0">MODEL</data>
      <data key="d1">GLM stands for Generalized Linear Model, a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution</data>
      <data key="d2">54768ad5bc5877b6bf042aac07fb64d9</data>
    </node>
    <node id="BINARY_RESPONSE_DATA">
      <data key="d0">DATA_TYPE</data>
      <data key="d1">Binary response data refers to data where the response variable can take on only two possible outcomes, often coded as 0 and 1</data>
      <data key="d2">54768ad5bc5877b6bf042aac07fb64d9</data>
    </node>
    <node id="HYPOTHESIS_TESTING">
      <data key="d0">STATISTICAL_METHOD</data>
      <data key="d1">Hypothesis testing is a statistical method used to make decisions or judgments about the probability of the observed data given a null hypothesis</data>
      <data key="d2">54768ad5bc5877b6bf042aac07fb64d9</data>
    </node>
    <node id="SUMMARY_OF_CHAPTER_17">
      <data key="d0">SUMMARY</data>
      <data key="d1">Summary of Chapter 17 provides an overview of the key concepts and topics covered in Chapter 17 of the text</data>
      <data key="d2">54768ad5bc5877b6bf042aac07fb64d9</data>
    </node>
    <node id="APPENDIX_A">
      <data key="d0">APPENDIX</data>
      <data key="d1">Appendix A covers linear algebra and multivariable calculus, providing additional mathematical background for the text</data>
      <data key="d2">54768ad5bc5877b6bf042aac07fb64d9</data>
    </node>
    <node id="APPENDIX_B">
      <data key="d0">APPENDIX</data>
      <data key="d1">Appendix B, a critical component of the document, delves into the statistical concept of Generalised Expectation. This concept is particularly significant as it is intricately linked to the theory of Generalized Linear Models (GLMs). The discussion in Appendix B provides a comprehensive overview of Generalised Expectation, offering insights that are essential for a deeper understanding of GLMs and their applications in statistical analysis.</data>
      <data key="d2">54768ad5bc5877b6bf042aac07fb64d9,acead09befa8eb465dd2e8c2d93a43c5</data>
    </node>
    <node id="APPENDIX_C">
      <data key="d0">APPENDIX</data>
      <data key="d1">Appendix C, a critical component of the statistical analysis, centers on the distribution of s^2, a pivotal measure representing the sample variance. This focus underscores the significance of understanding the variability within the data, as s^2 provides insights into the dispersion of data points around the mean. As a measure of variance, s^2 is essential for evaluating the reliability and consistency of the dataset, thereby facilitating a deeper comprehension of the underlying statistical properties and patterns.</data>
      <data key="d2">54768ad5bc5877b6bf042aac07fb64d9,acead09befa8eb465dd2e8c2d93a43c5</data>
    </node>
    <node id="APPENDIX_D">
      <data key="d0">APPENDIX</data>
      <data key="d1">Appendix D, a comprehensive section dedicated to advanced statistical methodologies, delves into the intricacies of M-estimators and robust regression. This section is particularly valuable for data scientists and statisticians dealing with datasets that contain outliers, as it provides robust methods for estimating model parameters. Unlike traditional least squares estimators, the techniques covered in Appendix D are designed to be less sensitive to the presence of outliers, ensuring more reliable and accurate model parameter estimations. By incorporating these robust methods, analysts can mitigate the impact of extreme values on their statistical models, leading to enhanced model performance and more trustworthy results.</data>
      <data key="d2">54768ad5bc5877b6bf042aac07fb64d9,acead09befa8eb465dd2e8c2d93a43c5</data>
    </node>
    <node id="APPENDIX_E">
      <data key="d0">APPENDIX</data>
      <data key="d1">Appendix E explores the distribution of the F-statistic, a test for the significance of model parameters</data>
      <data key="d2">acead09befa8eb465dd2e8c2d93a43c5</data>
    </node>
    <node id="APPENDIX_F">
      <data key="d0">APPENDIX</data>
      <data key="d1">Appendix F delves into the exponential family of distributions, a class of probability distributions</data>
      <data key="d2">acead09befa8eb465dd2e8c2d93a43c5</data>
    </node>
    <node id="APPENDIX_G">
      <data key="d0">APPENDIX</data>
      <data key="d1">Appendix G discusses Poisson GLM (Generalized Linear Model), a model for count data</data>
      <data key="d2">acead09befa8eb465dd2e8c2d93a43c5</data>
    </node>
    <node id="APPENDIX_H">
      <data key="d0">APPENDIX</data>
      <data key="d1">Appendix H provides a summary of key results from the statistical concepts covered in the appendices</data>
      <data key="d2">acead09befa8eb465dd2e8c2d93a43c5</data>
    </node>
    <node id="PDF_VERSION">
      <data key="d0">DOCUMENT</data>
      <data key="d1">The PDF version of the lecture notes is available for personal use and can be accessed by clicking on the PDF icon in the top bar</data>
      <data key="d2">d1cc8e172b83ff69b921cef864fc09f5</data>
    </node>
    <node id="RIGHTS">
      <data key="d0">LEGAL</data>
      <data key="d1">All rights to the lecture notes are reserved, and they should not be distributed in any format or uploaded to the internet, filesharing sites, or provided to third parties</data>
      <data key="d2">d1cc8e172b83ff69b921cef864fc09f5</data>
    </node>
    <node id="TYPO_REPORTING">
      <data key="d0">PROCEDURE</data>
      <data key="d1">There is a procedure for reporting typos in the lecture notes by submitting details to the ST231 anonymous submission form</data>
      <data key="d2">d1cc8e172b83ff69b921cef864fc09f5</data>
    </node>
    <node id="DIVERSITY_NOTE">
      <data key="d0">STATEMENT</data>
      <data key="d1">The author acknowledges the predominance of European and Anglo-American scientific contributions in the lecture notes and expresses a desire to include more diverse sources over time</data>
      <data key="d2">d1cc8e172b83ff69b921cef864fc09f5</data>
    </node>
    <node id="NORMAL_LINEAR_MODELS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Normal linear models are statistical models that describe the relationship between a single quantitative outcome variable and one or more explanatory variables</data>
      <data key="d2">7b32c106246576bb451a5a3985914351,d1cc8e172b83ff69b921cef864fc09f5</data>
    </node>
    <node id="LECTURE_NOTES">
      <data key="d0">DOCUMENT</data>
      <data key="d1">These are the first version of lecture notes that follow the exposition common in many textbooks, with the intention to augment over time with material from more diverse sources</data>
      <data key="d2">7b32c106246576bb451a5a3985914351</data>
    </node>
    <node id="EXPLANATORY_VARIABLES">
      <data key="d0">VARIABLE</data>
      <data key="d1">The EXPLANATORY_VARIABLES, also referred to as predictor variables or independent variables, play a crucial role in statistical models, particularly in regression analysis. These variables are non-random and considered fixed, allowing the model to be developed conditional on their values. They are used to explain or predict the response variable, also known as the dependent variable, by elucidating the variation in the outcome or response variable. In the context of a multiple linear regression model, specific EXPLANATORY_VARIABLES such as price and local advertising are employed to elucidate variations in the dependent variable. The values of these EXPLANATORY_VARIABLES determine the means of the response variables, making them essential components in understanding and predicting outcomes within the model.</data>
      <data key="d2">0da640a09a395a50b6e16e047fa8d0d6,22478e53f29f16e3eab9d167fea52b22,2ced3e26eaed2dfcd8e4caf49737cab4,426434b67f6a287852ab66b82ca873cf,7b32c106246576bb451a5a3985914351,a60af43e42c72a41fa90da06beb29d1b</data>
    </node>
    <node id="OUTCOME_VARIABLE">
      <data key="d0">VARIABLE</data>
      <data key="d1">The outcome variable is the single quantitative variable that is being predicted or explained by the explanatory variables in normal linear models</data>
      <data key="d2">7b32c106246576bb451a5a3985914351</data>
    </node>
    <node id="ST117">
      <data key="d0">COURSE</data>
      <data key="d1">ST117 is an educational module that focuses on the foundational aspects of statistical modeling. Specifically, it introduces the simplest form of a linear model, known as the simple linear regression model. This module is designed to provide a comprehensive understanding of how to model the relationship between a single independent variable and a dependent variable using linear regression techniques. Through ST117, learners gain insights into the principles of linear regression, including how to estimate parameters using least squares estimators and how to analyze residuals to assess the model's fit and assumptions. The module serves as a crucial stepping stone for those interested in advancing their knowledge in statistical analysis and data science, providing a solid base for more complex models and analyses.</data>
      <data key="d2">28eee75e95bbbaf143368c3289585670,7b32c106246576bb451a5a3985914351</data>
    </node>
    <node id="ST121">
      <data key="d0">COURSE</data>
      <data key="d1">ST121 is an educational module that focuses on introducing the foundational concepts of linear modeling. Specifically, it delves into the simplest form of linear models, the simple linear regression model. This module provides a comprehensive understanding of how simple linear regression models work, enabling learners to grasp the basic principles of statistical modeling and analysis. Through ST121, students are equipped with the knowledge to analyze and interpret data using simple linear regression, a crucial skill in the field of statistical analysis and data science.</data>
      <data key="d2">28eee75e95bbbaf143368c3289585670,7b32c106246576bb451a5a3985914351</data>
    </node>
    <node id="LINEAR_ALGEBRA">
      <data key="d0">MATHEMATICAL_TOOL</data>
      <data key="d1">Linear Algebra, a fundamental branch of mathematics, serves as a crucial tool in the theory that underpins normal linear models, which are widely utilized in machine learning algorithms. It equips practitioners with the necessary mathematical apparatus, including vectors and matrices, to accurately represent objects in an n-dimensional space, enabling a comprehensive understanding and manipulation of data in various dimensions. However, Linear Algebra's notation can present challenges, as it employs small letters for vectors and capital letters for matrices, potentially causing confusion when handling both random variables and their realizations within the same context. Despite these notational issues, Linear Algebra remains an indispensable component in the mathematical foundation of machine learning, providing the means to interpret and analyze complex data structures effectively.</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22,2ced3e26eaed2dfcd8e4caf49737cab4,7b32c106246576bb451a5a3985914351</data>
    </node>
    <node id="LINEAR_MODEL">
      <data key="d0">STATISTICAL_MODEL</data>
      <data key="d1">A linear model, in the context of statistical analysis, is a type of model that assumes a linear relationship between the dependent or response variable and one or more independent or explanatory variables. This model is widely used in regression analysis to predict the value of the dependent variable based on the values of the independent variables. The term "linear model" specifically refers to the linearity in the parameters, meaning that as long as the systematic component is linear in the parameters, the model can still be considered a linear model, even if it's not a linear function of the explanatory variables. This allows for the use of the least squares estimation algorithm to produce reasonable estimates of the parameters. The linear model is not only a tool for analyzing the relationship between one or more independent variables and a dependent variable but also a method for predicting the response variable based on the values of the explanatory variables. An example of its application is in the analysis of the relationship between the height and diameter of Western red cedar trees, where the linear model is used to predict the volume of timber from these measurements. The linear model, represented by a linear equation, is a fundamental tool in statistical inference, providing a structured approach to understanding and predicting the behavior of variables in various contexts.</data>
      <data key="d2">188219b9e5b6b6368360840921877de9,28eee75e95bbbaf143368c3289585670,3bfc9b92571973e54c8095302acc1aaa,3fb977ccba63e267d2e7dd4de6479ce1,656dce234514b9db38b5b5616557c1e9,7594eee7e77beb023d1cd64aec64920d,9611ea31ff53888971694cdefe806f64,9f335f1ecb85a1427df926df8bb1e89f,ae1e66f5b64284090abc285c1d4389f5,b9af17718641389ba07f53be13f31f8c,c9a01b92d11585f6549f62e8bd78d652,d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </node>
    <node id="MULTIPLE_LINEAR_REGRESSION">
      <data key="d0">STATISTICAL_MODEL</data>
      <data key="d1">The entity in focus is "MULTIPLE_LINEAR_REGRESSION", a sophisticated statistical technique employed in the analysis of data. This method is particularly useful for understanding the relationship between one continuous dependent variable and two or more independent variables. In the context provided, the model employs price and local advertising (also referred to as 'advert') as predictor variables to forecast the dependent variable, Y. Multiple linear regression serves as a quantitative tool that leverages several explanatory variables to predict the response variable, making it a versatile approach in statistical modeling. It is a type of linear model that comprehensively describes the relationship between a dependent variable and multiple independent variables, enhancing the accuracy and depth of predictive analysis.</data>
      <data key="d2">28eee75e95bbbaf143368c3289585670,2ced3e26eaed2dfcd8e4caf49737cab4,426434b67f6a287852ab66b82ca873cf,512d9ffebe309a6f944ebce1ae2ff2a3</data>
    </node>
    <node id="DIAMOND_DATA">
      <data key="d0">DATASET</data>
      <data key="d1">The Diamond_data dataset is a comprehensive collection of information pertaining to 48 Solitaire gold rings, featuring detailed data on the diamonds embedded within. This dataset is prominently utilized in the realm of regression analysis, serving as a pivotal tool for statistical exploration. It meticulously records the carat weight and price of the diamonds, providing a rich resource for understanding the relationship between these variables. Moreover, the dataset is employed to vividly illustrate the fitted models derived from equations (1.1) and (1.2), with scatterplots vividly depicting the interplay between the explanatory and response variables, thereby offering a visual representation of the underlying statistical models. This dataset is not only a repository of factual data but also a practical application in statistical analysis, making it a valuable asset for researchers and data scientists alike.</data>
      <data key="d2">13e6c81b642252522460f4abadaab1cc,28eee75e95bbbaf143368c3289585670,e47d573a10e64a657e58218df64d8920</data>
    </node>
    <node id="SINGAPORE_DOLLARS">
      <data key="d0">CURRENCY</data>
      <data key="d1">The Singapore Dollars (S$) is the official currency utilized in Singapore. It serves as the unit of measurement for the price of diamond rings, specifically in the context of Solitaire rings, as observed in a recent study. This currency is pivotal in quantifying the economic value of such jewelry items within the Singaporean market.</data>
      <data key="d2">28eee75e95bbbaf143368c3289585670,3e7eef51f3109d60697f3299b541b726,f0e0c5b2deaaf9fc2bc8b63d9ab989b1</data>
    </node>
    <node id="CARAT">
      <data key="d0">UNIT_OF_MEASUREMENT</data>
      <data key="d1">Carat, a crucial variable in the diamond dataset, serves as the predictor or explanatory variable in the linear regression model. It represents the weight of diamonds, measured in carats, a unit of measurement where one carat is equivalent to 20 milligrams. The dataset includes specific weights, such as 0.35 carat, illustrating the range of diamond sizes considered in the analysis. Carat's role in the regression model highlights its significance in understanding the relationships and structure within the diamond market, providing insights into how diamond weight influences other variables in the dataset.</data>
      <data key="d2">28eee75e95bbbaf143368c3289585670,2b01334fb633566ba368a764ad579fce,35e06960dba699ce0d56fc1e98bdbe96,9fc9b618723695c0c593043162a4084b,b99ecc2f79f56198a8c2adbdff95d576,e1ad57124a08c0e123deda212ea03c32</data>
    </node>
    <node id="SCATTERPLOT">
      <data key="d0">GRAPHICAL_REPRESENTATION</data>
      <data key="d1">A scatterplot is a versatile graphical representation that visualizes the relationship between two quantitative variables, typically an explanatory variable and a response variable. It displays data as a collection of points, with the value of one variable determining the position on the horizontal (x) axis and the value of the other variable determining the position on the vertical (y) axis. This type of plot is used to show the distribution of data points and can reveal patterns, trends, or the joint variation of the variables. Scatterplots are particularly useful in statistical analysis, where they can depict the relationship between variables such as the weight of diamonds and the price of Solitaire rings, or brain weight and body weight. They are also employed to visualize the data and the line of best fit in statistical models, aiding in the identification of any underlying structure or correlation within the data. Additionally, scatterplots can illustrate the relationship between sales and price for each brand, providing insights into market dynamics. Overall, scatterplots serve as a powerful tool for understanding the nature of the relationship between two variables in various contexts.</data>
      <data key="d2">11452a08471d93959558de2ece9a69af,15c7b5750483a382ce59751008e86751,23fc620f1238c6a1b5c5e3a08e149c53,28eee75e95bbbaf143368c3289585670,2a5997c641e47fc6c32ebf81101c54e0,2e5e1bdaa9fcc7b3391d277fd6bb247a,312309b45c59e1c84695ac3c7e202742,3e7eef51f3109d60697f3299b541b726,521acf88540d5897188c9ec65b17e6a6,66f7fae9d896ff2b3fd40186cc833503,674b8d5bb1f830d0fb944942514d1a16,82cfcd5865cffe55e965a50745656e60,84ffe1b8496dc660c47248c9f7b5bdea,9f335f1ecb85a1427df926df8bb1e89f,aa13c33a7e61206e6021e2736002ca9a,ac15b639b0849006471dfe102376c2c0,ae1e66f5b64284090abc285c1d4389f5,b9eb75001a4f68f7240b2ca9e0d79eb8,b9ec8a6c7960cc6196ec94fd976f05b0,c5b269ff5c94db7ebd2cb9f7be16f171,ef24ca5edd06893b737e6a1c8a9825f6,f9d6c3504b8f8b5c25550076e45f8270</data>
    </node>
    <node id="DIAMOND_RING_PRICING">
      <data key="d0">RESEARCH_TOPIC</data>
      <data key="d1">Diamond ring pricing is the subject of a study using linear regression to analyze the relationship between the weight of a diamond and the price of a solitaire ring.</data>
      <data key="d2">3e7eef51f3109d60697f3299b541b726</data>
    </node>
    <node id="LINEAR_REGRESSION">
      <data key="d0">STATISTICAL_METHOD</data>
      <data key="d1">Linear regression, a fundamental statistical method within the realm of algorithmic analysis, is employed to model and analyze the relationship between a dependent variable and one or more independent variables. This technique enables the identification of how changes in the independent variables are associated with changes in the dependent variable, providing insights into the structure and relations within the community of interest. Utilizing tools like R for data analysis, linear regression facilitates statistical inference, helping to understand the underlying patterns and associations in the data. Through the application of linear regression, least squares estimators, and residual analysis, data scientists can make informed decisions and predictions based on the observed relationships.</data>
      <data key="d2">3e7eef51f3109d60697f3299b541b726,84ffe1b8496dc660c47248c9f7b5bdea</data>
    </node>
    <node id="JOURNAL_OF_STATISTICS_EDUCATION">
      <data key="d0">ACADEMIC_JOURNAL</data>
      <data key="d1">The Journal of Statistics Education is an academic journal that published a study on diamond ring pricing using linear regression.</data>
      <data key="d2">3e7eef51f3109d60697f3299b541b726</data>
    </node>
    <node id="DIAMOND_WEIGHT">
      <data key="d0">INDEPENDENT_VARIABLE</data>
      <data key="d1">The entity in focus is "DIAMOND_WEIGHT", which pertains to the weight of a diamond, a crucial characteristic measured in carats. This measurement is not only a significant factor in determining the price of a Solitaire ring but also plays a pivotal role as the independent variable in a linear regression model designed to analyze and understand the pricing dynamics of diamond rings. The weight of the diamond, therefore, is central to both the commercial valuation of Solitaire rings and the statistical modeling of their pricing, highlighting its importance in both market transactions and academic research.</data>
      <data key="d2">3e7eef51f3109d60697f3299b541b726,f0e0c5b2deaaf9fc2bc8b63d9ab989b1</data>
    </node>
    <node id="RING_PRICE">
      <data key="d0">DEPENDENT_VARIABLE</data>
      <data key="d1">Ring price, measured in Singapore Dollars (S$), is the dependent variable in the linear regression model used to study diamond ring pricing.</data>
      <data key="d2">3e7eef51f3109d60697f3299b541b726</data>
    </node>
    <node id="LINE_OF_BEST_FIT">
      <data key="d0">MATHEMATICAL_MODEL</data>
      <data key="d1">The line of best fit, a fundamental concept in statistical analysis, is a mathematical model that best represents the linear relationship between two variables, typically a response variable and a predictor variable. This line is often calculated using methods like least squares, which minimizes the sum of squared residuals, ensuring that the line best fits the data points in a scatterplot. In the context of the diamond and ring market, the line of best fit specifically models the relationship between the weight of a diamond and the price of a Solitaire ring, accounting for weight deviations from the mean. This model is also applicable to other scenarios, such as the relationship between sales and price. Mathematically, the line of best fit can be defined by the equation Price = &#946;0 + &#946;1 weight + &#951;, where &#946;0 and &#946;1 are coefficients, and &#951; represents the error term. This model captures the systematic component of the relationship between price and weight, providing a clear and concise representation of the linear association between these variables.</data>
      <data key="d2">2e5e1bdaa9fcc7b3391d277fd6bb247a,3e7eef51f3109d60697f3299b541b726,6f10cac870c690419e5351e8a6aeae9e,ae1e66f5b64284090abc285c1d4389f5,b99ecc2f79f56198a8c2adbdff95d576,c5b269ff5c94db7ebd2cb9f7be16f171,d4bbb6beb0dd5c40d2941af71b7c1776,f16299fc00a7a69bdf983dce826b4918</data>
    </node>
    <node id="WEIGHT">
      <data key="d0">VARIABLE</data>
      <data key="d1">The entity "WEIGHT" in the context of the dataset refers to the carat weight of diamonds set in Solitaire rings. It is a crucial variable in the simple linear regression model, indicating the physical weight of the diamond, which significantly influences its price. A positive weight of at least 0.12 carats is considered for the diamonds in this dataset, with an average weight of 0.204 carats. This measurement is essential for understanding the structure and relations within the community of interest, as it provides a quantitative basis for assessing the value and characteristics of the diamonds.</data>
      <data key="d2">6f10cac870c690419e5351e8a6aeae9e,c5b269ff5c94db7ebd2cb9f7be16f171,d4bbb6beb0dd5c40d2941af71b7c1776,f18bacb0a2fbfea44dd6326224184216,f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </node>
    <node id="PRICE">
      <data key="d0">VARIABLE</data>
      <data key="d1">Price (pricej) is a crucial quantitative predictor variable in the context of sales volume prediction, representing the price of the product in the dataset, particularly in the jth store. It is an independent variable in the linear regression model, indicating the price charged for the product. Price significantly influences sales volumes, and its estimated slope coefficient in the simple linear regression model was computed as -0.225, suggesting the expected change in sales for a unit change in price. This variable takes values considerably larger than zero, making the interpretation of the intercept in a regression model require extrapolation. In the regression analysis of Solitaire rings, measured in Singapore dollars (S$), Price is the cost of the rings, analyzed in relation to the weight of the diamonds, with an estimated intercept suggesting an expected price of approximately S$ 500 for a ring with a 0.204 carat diamond. Price is also the only predictor variable used in the initial simple linear regression model, and it is one of the explanatory variables in the regression analysis, representing the price of the product. The coefficient of Price as an explanatory variable in the linear regression model indicates the average effect on the response variable when its value is increased by one unit. It is worth noting that Price is not the response variable in the linear regression model, as it represents the price of diamonds, but rather a predictor variable. The original price variable, with a sample mean of 152.76, is also referred to as Price in the diamond dataset. Overall, Price is an economic variable that represents the amount of money that must be paid for a product or service, and it is an essential component in the regression models used to predict sales volume.</data>
      <data key="d2">06d5666e6bfdda828b48adba883b4a61,0cb40986e6c2bb439e1ffcaae2df96ac,1820d10ee0f23f34b3ea88ba475bc52d,1b523d1edabe381403fc470a9b8d47fa,1d141ab04db553f78a313e430e54abb5,248924760a2bfbc82501fd6b11cfa0aa,250ee5d766c64e7975bcc427b4bf9074,2b01334fb633566ba368a764ad579fce,2ced3e26eaed2dfcd8e4caf49737cab4,2e5e1bdaa9fcc7b3391d277fd6bb247a,336546bc73cbe1828a0cc1a45faf8f5a,3dd24a54028976ba54304ec7169bb74b,426434b67f6a287852ab66b82ca873cf,6f10cac870c690419e5351e8a6aeae9e,7037e0369bfdaad5a730cabb2b44831c,7a605c3b689bec7ab2c46df9c123e3f3,8326c645426789920a99ed373725fa0e,86ece4718d27d1a6c6a1f448cc850e2b,8a9b984c146f59b2af83d1c2f373d376,906eb7d6b49fa360e7e5b65c56cd4d76,93da9813e10a119798de6982977f1239,9854704301b8df256ca1013b8d53dfac,a1fc936df848a0fbc791e4bcc9b527b6,a828fd17fc38e902484872c88a6b242c,ac15b639b0849006471dfe102376c2c0,adbc52b340a69a8633c919c4fd2cd3f6,b0ca3e6c22c4cf884d03b1f6f82be5df,b1690cb1a67892245c0665e5099e322d,b2c33cb151a8e7724ebfb7b2d88bc45f,b6870535f3975c49d45e62fbe475f198,b99ecc2f79f56198a8c2adbdff95d576,c103c6d096d52868eda26d991194b5f2,c48bd062d5f6cc20c5df5758d6285562,c5b269ff5c94db7ebd2cb9f7be16f171,d4bbb6beb0dd5c40d2941af71b7c1776,e079b7c92d5c0b009ff02040eb652bc6,e1ad57124a08c0e123deda212ea03c32,e800735d6b2a244875f5e0d292de1527,ee295ebc4c2d6a2a5c738796fcc2ab71,f16299fc00a7a69bdf983dce826b4918,f18bacb0a2fbfea44dd6326224184216,f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </node>
    <node id="BETA_0">
      <data key="d0">PARAMETER</data>
      <data key="d1">BETA_0, also denoted as &#946;0, is a crucial parameter in various regression models, primarily serving as the intercept. In the context of simple linear regression, BETA_0 represents the expected value of the response variable when the predictor variable is zero. This parameter is pivotal in determining the point where the regression line crosses the Y-axis. In more complex models, such as quadratic regression and extended linear regression, BETA_0 still maintains its role as the intercept, providing the baseline value of the response variable when all explanatory variables are at their minimum, typically zero. It is noteworthy that in log-transformed regression models, BETA_0 retains its significance as the intercept, albeit in a transformed space. This parameter is not only fundamental in linear models but also in multiple regression models, where it acts as the intercept term. BETA_0's role is consistent across different regression analyses, serving as the anchor point for the regression line or the expected value of the response variable under specific conditions. In essence, BETA_0 is the backbone of regression models, providing a starting point for understanding the relationship between variables.</data>
      <data key="d2">00e186a86624c01ce873dd577df68d17,0fbc9037ca9a440e79e9ac05664b9b3d,10ac76f99674a01ca0f4a55586dea07e,21e429490eeefe7d9c245058fd48ca68,250ee5d766c64e7975bcc427b4bf9074,2d5cdecc342ddacd2c090f1838430cee,3fdeeb7593174f5e8a9cff55a7cd92e3,416494d940a9f505da9853caca26fe63,50a56c34050fb7f7709300a51399b150,512d9ffebe309a6f944ebce1ae2ff2a3,656dce234514b9db38b5b5616557c1e9,67e4c1866b0c6e162e6e3317949e8da9,69ffba28a61d98d8d18f91c24b74dd4a,6a47154bf457c25f22c3cf9f649c5db0,6f10cac870c690419e5351e8a6aeae9e,74a5a0e8ae0f846240c782cc1a30f82f,75dc4d8cb195a1f969d9e9496631086b,7ad4ccec4c7bb3702aed71c17dc6b96f,8f1d95acff56e1633dceb775fa713174,8f7a05b6d231105a6194eebdb2df372e,90b7e0427699cc1bb461e37939935138,9611ea31ff53888971694cdefe806f64,995fb26a0261f824952fa7b2fac3382e,9a27580975988e83f6e3a0d9010893b5,a828fd17fc38e902484872c88a6b242c,b99ecc2f79f56198a8c2adbdff95d576,d14413709de2897231aaa83be3aa346f,d1b6fcd55d937c5fe2d6add69e0bcf05,d4bbb6beb0dd5c40d2941af71b7c1776,e2422d8b80004aab4ea74d5209587861,e5131a1158e58f1b7b44b21ced7b6f60,efeeb664622c1ee594e6a08a8322ffe3,f16299fc00a7a69bdf983dce826b4918</data>
    </node>
    <node id="BETA_1">
      <data key="d0">PARAMETER</data>
      <data key="d1">BETA_1 (&#946;1) is a crucial parameter in the linear regression model, often referred to as the slope parameter. It represents the coefficient of the first explanatory variable, X1, indicating the change in the response variable for a one-unit increase in the predictor variable, while holding other variables constant. In the context of specific models, BETA_1 specifically denotes the change in the response variable (such as Height or Sales) for a one-unit increase in the predictor variable (like log2(diameter) or price), given a fixed value of other variables (like X2). It is also the coefficient for the log of Diameter in models where both the response and explanatory variables are log-transformed. BETA_1 is pivotal in understanding the linear relationship between the explanatory variable and the response variable, and it is a key component in both simple and quadratic regression models, where it represents the coefficient of the linear term. In the log-transformed regression model, BETA_1 (&#946;1) is associated with the predictor X1, highlighting its versatility across different model specifications.</data>
      <data key="d2">00e186a86624c01ce873dd577df68d17,0fbc9037ca9a440e79e9ac05664b9b3d,10ac76f99674a01ca0f4a55586dea07e,21e429490eeefe7d9c245058fd48ca68,250ee5d766c64e7975bcc427b4bf9074,2d5cdecc342ddacd2c090f1838430cee,3fdeeb7593174f5e8a9cff55a7cd92e3,416494d940a9f505da9853caca26fe63,50a56c34050fb7f7709300a51399b150,512d9ffebe309a6f944ebce1ae2ff2a3,656dce234514b9db38b5b5616557c1e9,67e4c1866b0c6e162e6e3317949e8da9,69ffba28a61d98d8d18f91c24b74dd4a,6a47154bf457c25f22c3cf9f649c5db0,6f10cac870c690419e5351e8a6aeae9e,74a5a0e8ae0f846240c782cc1a30f82f,7ad4ccec4c7bb3702aed71c17dc6b96f,8f1d95acff56e1633dceb775fa713174,8f7a05b6d231105a6194eebdb2df372e,90b7e0427699cc1bb461e37939935138,9611ea31ff53888971694cdefe806f64,995fb26a0261f824952fa7b2fac3382e,9a27580975988e83f6e3a0d9010893b5,a828fd17fc38e902484872c88a6b242c,b99ecc2f79f56198a8c2adbdff95d576,d14413709de2897231aaa83be3aa346f,d1b6fcd55d937c5fe2d6add69e0bcf05,d4bbb6beb0dd5c40d2941af71b7c1776,e2422d8b80004aab4ea74d5209587861,e5131a1158e58f1b7b44b21ced7b6f60,efeeb664622c1ee594e6a08a8322ffe3,f16299fc00a7a69bdf983dce826b4918</data>
    </node>
    <node id="EPSILON">
      <data key="d0">ERROR</data>
      <data key="d1">EPSILON (&#949;) is the error term in various regression models, primarily appearing in linear regression contexts. It represents the unexplained variation in the dependent variable, such as sales, and is often used to describe the deviation of observed data from the model's predictions. In the simple linear regression model, &#949; is the random error term that accounts for the variability in the response variable not explained by the systematic component of the model. It is assumed to be normally distributed with a mean of 0 and a constant variance &#963;^2, and is independent and identically distributed (iid) across observations.

In more complex models, such as quadratic regression and parallel lines models, EPSILON still serves as the error term, capturing the unexplained variation specific to those models. The error term is crucial for understanding the fit of the model and the reliability of predictions. It is often represented as a vector (&#949;1, ..., &#949;n) for each of the n observations, where each element corresponds to the error term for a single observation. This vector is assumed to follow a multivariate normal distribution with a mean vector of zeros and a covariance matrix proportional to &#963;^2.

In the context of retail data, &#949;j is the error term for the jth store or the jth unit of observation, representing the deviation of the observed sales from the expected sales. This error term is also assumed to be normally distributed with a mean of 0 and variance &#963;^2, and is independent of each other and of the predictor variable X.

Overall, EPSILON (&#949;) is a fundamental component in regression analysis, crucial for assessing model fit and understanding the unexplained variation in the data. Its properties, such as normal distribution and independence, are key assumptions in statistical inference and hypothesis testing within regression models.</data>
      <data key="d2">00e186a86624c01ce873dd577df68d17,01d5ee79489582b4135fc96f676b24a0,06199787dd7f75f7338dd24d4f3dc26e,06d5666e6bfdda828b48adba883b4a61,0cb40986e6c2bb439e1ffcaae2df96ac,0da640a09a395a50b6e16e047fa8d0d6,119bc73ddf8eebadfb8eae272fa323a7,1b523d1edabe381403fc470a9b8d47fa,1d141ab04db553f78a313e430e54abb5,248924760a2bfbc82501fd6b11cfa0aa,250ee5d766c64e7975bcc427b4bf9074,2d5cdecc342ddacd2c090f1838430cee,3cbe71f7649e84cd67cb3fa0d3e632cf,48971100deb5bb374a41c1f2b7b2a86a,50a56c34050fb7f7709300a51399b150,512d9ffebe309a6f944ebce1ae2ff2a3,5609007c6229060ffc85d8056a7fefde,67e4c1866b0c6e162e6e3317949e8da9,69ffba28a61d98d8d18f91c24b74dd4a,6c1684ed2a4840576c6b0f4d1a3a482f,6c66e9414880964ee899ceb0f16d22e9,6f10cac870c690419e5351e8a6aeae9e,74a5a0e8ae0f846240c782cc1a30f82f,74d190f10bf6e6936242ca3cdfc4a09f,75dc4d8cb195a1f969d9e9496631086b,7ad4ccec4c7bb3702aed71c17dc6b96f,7d074208b1259e7d84f9f870d3828bb6,7fc5b8303ab530821bf2140ba6a8a889,82932abd152e0b84a1c26a2daa4c08df,8326c645426789920a99ed373725fa0e,8f1d95acff56e1633dceb775fa713174,906eb7d6b49fa360e7e5b65c56cd4d76,9611ea31ff53888971694cdefe806f64,98d6982108f2d42fe0437bff8c666e17,9a27580975988e83f6e3a0d9010893b5,a1fc936df848a0fbc791e4bcc9b527b6,a828fd17fc38e902484872c88a6b242c,ac15b639b0849006471dfe102376c2c0,aeddef300427d211c74c6008b5b6b328,b5d0a103e1f34a00aef67fedd0e8c693,b6870535f3975c49d45e62fbe475f198,b70a75a6412b2e5c44af50734844f4be,d4bbb6beb0dd5c40d2941af71b7c1776,d94760a5f9f6ea115fcc18024035a627,dd7e7d54883ca0f687568a738b95d4d0,e2422d8b80004aab4ea74d5209587861,e41cc40f061f487b1ea0f256d4a963e4,e4f14e6785c6d7b7469e695aaeb170d0,e7494d6cfc3e38a4d2f3f6b21ef6445d,f16299fc00a7a69bdf983dce826b4918,f18bacb0a2fbfea44dd6326224184216,f2300d613896880cbb7c255a4d858315,f5716ce115458c0652124734ca344806,f9b615b879f72501f338f8983d4cac3d</data>
    </node>
    <node id="DETERMINISTIC_PART">
      <data key="d0">CONCEPT</data>
      <data key="d1">The deterministic or systematic part of the relationship between price and weight is represented by the line of best fit</data>
      <data key="d2">d4bbb6beb0dd5c40d2941af71b7c1776</data>
    </node>
    <node id="RANDOM_ERROR">
      <data key="d0">CONCEPT</data>
      <data key="d1">Random error, a crucial component in statistical models, particularly in the context of algorithmic analysis, is characterized as the unpredictable variability that remains unexplained by the systematic factors within the model. This variability essentially captures the residual differences between the observed data points and the values predicted by the model, specifically in the relationship between price and weight. It quantifies the scatter of observations around the systematic part of the relationship, providing a measure of the model's inability to account for all the variability in the data. This concept is pivotal in understanding the limitations of a model and in assessing the accuracy of predictions. In essence, random error reflects the inherent uncertainty and unpredictability in the data that cannot be attributed to the known systematic components of the model.</data>
      <data key="d2">bb31f1c77bbba73300f735a100086a67,d4bbb6beb0dd5c40d2941af71b7c1776</data>
    </node>
    <node id="SIGMA_SQUARED">
      <data key="d0">VARIANCE</data>
      <data key="d1">SIGMA_SQUARED (&#963;^2) is a pivotal statistical measure representing the variance of the error term in various regression models, including linear, multiple, quadratic, and normal linear models. It quantifies the dispersion of the observed values of the dependent variable around their expected values, serving as a crucial parameter in the log-likelihood function. In the context of the multivariate normal distribution, SIGMA_SQUARED is the variance component of the variance-covariance matrix. It is also the true error variance in the regression model, reflecting the inherent variability not explained by the model. An unbiased estimate of SIGMA_SQUARED can be computed for a given fitted linear model, aiding in the assessment of model fit and the reliability of predictions. In the simple linear regression model, SIGMA_SQUARED (&#963;^2) is specifically associated with the error term &#949;, denoted as &#949;j, which encapsulates the random fluctuations in the data. This variance parameter is fundamental in understanding the structure and behavior of the community of interest within the statistical model.</data>
      <data key="d2">0443ab5e20a4f6b2f243c989ef6c723a,09caa54ca1372d152e47051be4d44ede,1da117a2f92b2db00290d2a0bfc06beb,2673d078d29f2af78fab9b6eacd15e37,2d5cdecc342ddacd2c090f1838430cee,2de7a36b32bf79c8f32612c8aaa9daa8,34fceaaf7d835828b5ee2327325c37f8,3d357cfa3ef0d00f49cf4acaeac1c9d1,3fb977ccba63e267d2e7dd4de6479ce1,45f31b040576e9f3b4def6d0466cc016,542f546c5a131196e4701fb33c9b1dee,5a0d392715f06d5e873f45ae06aa729a,6648f0d6deed51fb4fb25e6992a71ddf,679722cf8ce5ce5aee4e379528470efe,67e4c1866b0c6e162e6e3317949e8da9,69ffba28a61d98d8d18f91c24b74dd4a,6c1684ed2a4840576c6b0f4d1a3a482f,6f10cac870c690419e5351e8a6aeae9e,74a5a0e8ae0f846240c782cc1a30f82f,74d190f10bf6e6936242ca3cdfc4a09f,75dc4d8cb195a1f969d9e9496631086b,7ad4ccec4c7bb3702aed71c17dc6b96f,7d074208b1259e7d84f9f870d3828bb6,7fc5b8303ab530821bf2140ba6a8a889,87b717ba065d6d7c7431af284137eb12,8f1d95acff56e1633dceb775fa713174,9005147593b2f27b9e2a5eede3601bdc,90b7e0427699cc1bb461e37939935138,9923e77ac6b3de95cb5026bc5e7fe8c0,9a27580975988e83f6e3a0d9010893b5,9dddcd96af7b557e578b3f5f36efacd7,ad799500572246a07f983a3b92c0e61f,b5d0a103e1f34a00aef67fedd0e8c693,b70a75a6412b2e5c44af50734844f4be,c47968226557bc2eb5aec5bb7994fd0e,d14413709de2897231aaa83be3aa346f,d738df7d83784c8a41b3948271c537b6,dd7e7d54883ca0f687568a738b95d4d0,e41cc40f061f487b1ea0f256d4a963e4,e4f14e6785c6d7b7469e695aaeb170d0,e7edd8b2874a350779ae20f1ecdf4733,f483798b15ef305e7826fd7142379e03,f632f01188d2c6e3091a965580cb4600,fc5b725f3c662c5471af20efdcc2dbff</data>
    </node>
    <node id="LM_FUNCTION">
      <data key="d0">FUNCTION</data>
      <data key="d1">The LM_FUNCTION, specifically referred to as the lm() function in the R programming language, is a versatile tool employed for fitting linear models. This function is central to conducting statistical analyses, particularly when dealing with relationships between variables in a linear context. It enables users to model the association between a dependent variable and one or more independent variables, facilitating predictions and understanding of the underlying structure within the data. The lm() function in R is renowned for its efficiency and flexibility, making it a preferred choice for data scientists and statisticians when conducting linear regression analyses.</data>
      <data key="d2">825b600cbab3535ce67e9f561ddcb84b,b99ecc2f79f56198a8c2adbdff95d576,f18bacb0a2fbfea44dd6326224184216</data>
    </node>
    <node id="SUMMARY_FUNCTION">
      <data key="d0">FUNCTION</data>
      <data key="d1">summary() is a function in R used to provide a statistical summary of a fitted model</data>
      <data key="d2">b99ecc2f79f56198a8c2adbdff95d576</data>
    </node>
    <node id="USINGR_PACKAGE">
      <data key="d0">PACKAGE</data>
      <data key="d1">UsingR is an R package that provides datasets for statistical analysis</data>
      <data key="d2">b99ecc2f79f56198a8c2adbdff95d576</data>
    </node>
    <node id="DIAMOND_DATASET">
      <data key="d0">DATASET</data>
      <data key="d1">The DIAMOND_DATASET is a comprehensive collection of data featured in the UsingR package, specifically tailored for exploratory analysis and modeling purposes. This dataset encapsulates detailed information about diamonds, encompassing crucial attributes such as their weight and price. It serves as a valuable resource for data scientists and researchers aiming to understand the relations and structure within the diamond market, facilitating in-depth statistical analysis and insights into the valuation of diamonds based on their physical characteristics.</data>
      <data key="d2">b8ec334f8c87bf1d9cb6043fa1a64214,b99ecc2f79f56198a8c2adbdff95d576,f18bacb0a2fbfea44dd6326224184216</data>
    </node>
    <node id="SLR_DIAMOND">
      <data key="d0">MODEL</data>
      <data key="d1">SLR.diamond is the simple linear regression model fitted using the lm() function with price as the response and carat as the predictor</data>
      <data key="d2">b99ecc2f79f56198a8c2adbdff95d576</data>
    </node>
    <node id="USINGR">
      <data key="d0">R_PACKAGE</data>
      <data key="d1">UsingR is an R package that needs to be installed and loaded to use the diamond dataset and perform linear regression analysis</data>
      <data key="d2">2b01334fb633566ba368a764ad579fce</data>
    </node>
    <node id="INSTALL.PACKAGES">
      <data key="d0">R_FUNCTION</data>
      <data key="d1">install.packages is an R function used to install R packages, in this case, the UsingR package</data>
      <data key="d2">2b01334fb633566ba368a764ad579fce</data>
    </node>
    <node id="LIBRARY">
      <data key="d0">R_FUNCTION</data>
      <data key="d1">library is an R function used to load R packages into the current R session, enabling access to the package's functions and datasets</data>
      <data key="d2">2b01334fb633566ba368a764ad579fce</data>
    </node>
    <node id="SLR.DIAMOND">
      <data key="d0">MODEL</data>
      <data key="d1">SLR.diamond is a simple linear regression model object created by fitting the price of diamonds as a function of their carat weight using the lm function</data>
      <data key="d2">2b01334fb633566ba368a764ad579fce</data>
    </node>
    <node id="LM">
      <data key="d0">R_FUNCTION</data>
      <data key="d1">LM, or lm(), is a versatile function in the R programming language, primarily utilized for fitting linear models. In the context provided, LM is specifically employed to fit a simple linear regression model, denoted as SLR.diamond. This function requires a formula and a data argument for its operation; the formula delineates the relationship between the response variable and the predictor variable(s), while the data argument supplies the dataset from which the variables are drawn. LM's functionality is pivotal in statistical analysis, enabling the estimation of model parameters and facilitating the understanding of relationships within the data.</data>
      <data key="d2">084dadebfca8bcb6377c205c45bee295,25fce1af816975003128126b5cfea73b,2b01334fb633566ba368a764ad579fce,35e06960dba699ce0d56fc1e98bdbe96,c48bd062d5f6cc20c5df5758d6285562</data>
    </node>
    <node id="DIAMOND">
      <data key="d0">DATASET</data>
      <data key="d1">The entity in focus is "DIAMOND", a precious stone renowned for its use in jewelry, particularly as the main gem in Solitaire rings. The price of a diamond is significantly influenced by its weight, measured in carats, with the dataset analyzing diamonds weighing from 0.12 carats and above. This dataset, also referred to as "diamond", is central to the statistical analysis being conducted, where linear regression techniques are employed to explore the relationship between the diamonds' carat weight and their price. The analysis aims to provide insights into the valuation of diamonds in the context of Solitaire rings, highlighting the importance of carat weight as a determinant of price.</data>
      <data key="d2">2b01334fb633566ba368a764ad579fce,35e06960dba699ce0d56fc1e98bdbe96,c5b269ff5c94db7ebd2cb9f7be16f171,e1ad57124a08c0e123deda212ea03c32,f18bacb0a2fbfea44dd6326224184216,f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </node>
    <node id="SUMMARY">
      <data key="d0">R_FUNCTION</data>
      <data key="d1">summary is an R function used to provide a statistical summary of a fitted model, in this case, it is used to summarize the SLR.diamond model</data>
      <data key="d2">2b01334fb633566ba368a764ad579fce</data>
    </node>
    <node id="RESIDUAL_STANDARD_ERROR">
      <data key="d0">ERROR</data>
      <data key="d1">The residual standard error is 31.84 on 46 degrees of freedom, indicating the standard deviation of the residuals in the regression model</data>
      <data key="d2">9fc9b618723695c0c593043162a4084b</data>
    </node>
    <node id="MULTIPLE_R_SQUARED">
      <data key="d0">STATISTIC</data>
      <data key="d1">Multiple R-squared is 0.9783, a measure of the proportion of the variance in the dependent variable that can be explained by the independent variables</data>
      <data key="d2">9fc9b618723695c0c593043162a4084b</data>
    </node>
    <node id="ADJUSTED_R_SQUARED">
      <data key="d0">STATISTIC</data>
      <data key="d1">Adjusted R-squared is 0.9778, a modified version of R-squared that adjusts for the number of predictors in the model</data>
      <data key="d2">9fc9b618723695c0c593043162a4084b</data>
    </node>
    <node id="F_STATISTIC">
      <data key="d0">STATISTIC</data>
      <data key="d1">F-statistic is 2070 on 1 and 46 DF, a test statistic used to determine the significance of the model</data>
      <data key="d2">9fc9b618723695c0c593043162a4084b</data>
    </node>
    <node id="P_VALUE">
      <data key="d0">SIGNIFICANCE</data>
      <data key="d1">The p-value is &lt; 2.2e-16, indicating the probability of observing the test statistic under the null hypothesis</data>
      <data key="d2">9fc9b618723695c0c593043162a4084b</data>
    </node>
    <node id="ESTIMATE">
      <data key="d0">COEFFICIENT</data>
      <data key="d1">Estimate column in the middle table lists information about the coefficients, including the intercept and slope of the line of best fit</data>
      <data key="d2">9fc9b618723695c0c593043162a4084b</data>
    </node>
    <node id="INTERCEPT">
      <data key="d0">COEFFICIENT</data>
      <data key="d1">The entity "INTERCEPT" is a crucial parameter in the context of linear regression models, specifically within the domain of estimating the price of Solitaire rings based on diamond weight. The intercept (\u00b5 or \u03b20) represents the expected value of the dependent variable, which in this case is the price of a Solitaire ring, when all independent variables, such as diamond weight, are set to zero. This parameter is pivotal for understanding the baseline value of the response variable in the absence of any explanatory variables' influence.

However, the interpretation of the intercept as -259.63 or -260 Singapore Dollars when the diamond weight is zero is acknowledged to be nonsensical and naive, given that a diamond of zero weight would not logically result in a negative price. This interpretation is limited by the practicality of the dataset's range of price values, highlighting the need for caution when interpreting the intercept in real-world contexts.

The estimated value of the intercept in the linear regression model is also mentioned as -463.31, further emphasizing the theoretical nature of the intercept as the expected value of the response variable when all predictor variables are at their minimum or mean value. This value, while mathematically derived, may not always align with practical or logical interpretations, particularly when dealing with physical quantities like diamond weight that cannot realistically be zero.</data>
      <data key="d2">0cb40986e6c2bb439e1ffcaae2df96ac,25fce1af816975003128126b5cfea73b,2ced3e26eaed2dfcd8e4caf49737cab4,2e5e1bdaa9fcc7b3391d277fd6bb247a,35e06960dba699ce0d56fc1e98bdbe96,426434b67f6a287852ab66b82ca873cf,9fc9b618723695c0c593043162a4084b,a60af43e42c72a41fa90da06beb29d1b,a828fd17fc38e902484872c88a6b242c,c619949b08fc2b7edf3a7635b46dc147,e1ad57124a08c0e123deda212ea03c32,f0e0c5b2deaaf9fc2bc8b63d9ab989b1,f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </node>
    <node id="SLOPE">
      <data key="d0">COEFFICIENT</data>
      <data key="d1">The entity in focus is "SLOPE", a critical parameter in the linear regression model that quantifies the relationship between the explanatory variable, carat, and the response variable, the price of a Solitaire ring. Specifically, the slope value of 3721.02 indicates the average price increase for every additional carat of diamond weight, translating to an increment of S$ 3721 for every extra carat. This interpretation underscores the direct proportionality between diamond weight and ring price, where the slope acts as the measure of this linear relationship's steepness. In statistical terms, the slope signifies the change in the response variable for a one-unit change in the predictor variable, providing a clear indication of how diamond weight influences the price of Solitaire rings.</data>
      <data key="d2">35e06960dba699ce0d56fc1e98bdbe96,9fc9b618723695c0c593043162a4084b,f0e0c5b2deaaf9fc2bc8b63d9ab989b1</data>
    </node>
    <node id="SOLITAIRE_RING_PRICE">
      <data key="d0">DEPENDENT_VARIABLE</data>
      <data key="d1">The price of a Solitaire ring is the dependent variable in the regression model, influenced by the weight of its diamond</data>
      <data key="d2">9fc9b618723695c0c593043162a4084b</data>
    </node>
    <node id="SOLITAIRE_RING">
      <data key="d0">PRODUCT</data>
      <data key="d1">A Solitaire ring, the focus of the dataset analysis, is a distinguished type of jewelry characterized by a solitary diamond serving as its central and primary stone. This single diamond not only defines the essence of the Solitaire ring but also significantly influences its price, with the weight of the diamond being a crucial factor in determining the ring's value. As a piece of jewelry, the Solitaire ring is celebrated for its simplicity and elegance, making it a popular choice for various occasions, particularly for those seeking a classic and refined aesthetic.</data>
      <data key="d2">c5b269ff5c94db7ebd2cb9f7be16f171,e1ad57124a08c0e123deda212ea03c32,f0e0c5b2deaaf9fc2bc8b63d9ab989b1,f18bacb0a2fbfea44dd6326224184216,f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </node>
    <node id="ROUNDING">
      <data key="d0">PROCESS</data>
      <data key="d1">ROUNDING, a ubiquitous concept in statistical analysis and numerical computation, refers to the process of approximating a number to a specified number of decimal places. This practice is particularly significant in the context of regression analysis, where the response variable might be subject to rounding, potentially influencing the outcomes of the analysis. Additionally, the rounding of coefficients in a regression model is a technique employed to adjust the numerical values to a more sensible and interpretable form within the specific context of the study. This rounding process can have implications on the precision and accuracy of the model, necessitating careful consideration during the analysis phase.</data>
      <data key="d2">312309b45c59e1c84695ac3c7e202742,ef24ca5edd06893b737e6a1c8a9825f6,f0e0c5b2deaaf9fc2bc8b63d9ab989b1</data>
    </node>
    <node id="SLOPE_COEFFICIENT">
      <data key="d0">PARAMETER</data>
      <data key="d1">The SLOPE_COEFFICIENT is a pivotal parameter in the linear regression model, serving as an indicator of the relationship between the response variable and the predictor variable. In one context, it is specifically estimated at 82.84, denoting the average increase in height, measured in decimeters, that is associated with a doubling of the diameter of a Western red cedar tree. This value highlights the strong positive correlation between tree diameter and height. In another scenario, the SLOPE_COEFFICIENT is associated with the pricing of Solitaire rings, where it signifies the change in price for every additional 0.1 carat of diamond weight. This demonstrates its versatility in quantifying the impact of predictor variables on response variables across different domains, from natural sciences to economics.</data>
      <data key="d2">c619949b08fc2b7edf3a7635b46dc147,e1ad57124a08c0e123deda212ea03c32</data>
    </node>
    <node id="NEGATIVE_AVERAGE_PRICE">
      <data key="d0">CONCEPT</data>
      <data key="d1">A negative average price is a nonsensical concept that may arise from the interpretation of the intercept in the model</data>
      <data key="d2">e1ad57124a08c0e123deda212ea03c32</data>
    </node>
    <node id="S_372">
      <data key="d0">CURRENCY</data>
      <data key="d1">S$ 372,- is the predicted increase in price for a diamond ring with a diamond that is 0.1 carat heavier than another&gt;</data>
      <data key="d2">f18bacb0a2fbfea44dd6326224184216</data>
    </node>
    <node id="AVG_WEIGHT">
      <data key="d0">MEASUREMENT</data>
      <data key="d1">Avg_weight denotes the average weight of the diamonds in the dataset&gt;</data>
      <data key="d2">f18bacb0a2fbfea44dd6326224184216</data>
    </node>
    <node id="ALPHA_0">
      <data key="d0">PARAMETER</data>
      <data key="d1">ALPHA_0, denoted as &#945;0, is a pivotal parameter in the context of statistical modeling. Specifically, it serves as the intercept in both the straight line model and the reparameterised model, as referenced in equation (1) and equation (1.2). This parameter is crucial as it represents the expected value of the dependent variable when all independent variables are zero, anchoring the model's predictions and providing a baseline for understanding the relationships within the data.</data>
      <data key="d2">d1b6fcd55d937c5fe2d6add69e0bcf05,f18bacb0a2fbfea44dd6326224184216,f2300d613896880cbb7c255a4d858315</data>
    </node>
    <node id="ALPHA_1">
      <data key="d0">PARAMETER</data>
      <data key="d1">ALPHA_1, also denoted as \u03b11, is a significant parameter in the context of statistical modeling. In the straight line model, ALPHA_1 represents the slope, indicating the rate of change in the dependent variable with respect to the independent variable. This parameter is also pivotal in the reparameterised model (1), where it retains its role as the slope coefficient. Additionally, in the model equation (1.2), ALPHA_1 specifically functions as the coefficient for the predictor variable, which is the deviation of weight from the average weight (weight - avg_weight). This highlights its role in quantifying the relationship between the deviation in weight and the response variable, providing insights into how changes in weight, relative to the average, affect the outcome of interest.</data>
      <data key="d2">d1b6fcd55d937c5fe2d6add69e0bcf05,f18bacb0a2fbfea44dd6326224184216,f2300d613896880cbb7c255a4d858315</data>
    </node>
    <node id="MODEL_EQUATION">
      <data key="d0">MATHEMATICAL_EXPRESSION</data>
      <data key="d1">The model equation is a formula used to predict the price of a diamond ring based on the weight of the diamond&gt;</data>
      <data key="d2">f18bacb0a2fbfea44dd6326224184216</data>
    </node>
    <node id="I_FUNCTION">
      <data key="d0">FUNCTION</data>
      <data key="d1">The I() function in R is used to inhibit the conversion of an expression into an operator&gt;</data>
      <data key="d2">f18bacb0a2fbfea44dd6326224184216</data>
    </node>
    <node id="COEF_FUNCTION">
      <data key="d0">FUNCTION</data>
      <data key="d1">The COEF_FUNCTION, specifically referred to as the coef() function in the R programming language, is a versatile tool utilized for extracting model coefficients. This function plays a crucial role in statistical analysis, particularly when dealing with models such as linear regression. By applying the coef() function, users can easily access the coefficients that define the relationship between variables in a model, providing insights into the magnitude and direction of these relationships. This functionality is essential for understanding the structure and dynamics of the model, facilitating further analysis and interpretation of the data.</data>
      <data key="d2">825b600cbab3535ce67e9f561ddcb84b,f18bacb0a2fbfea44dd6326224184216</data>
    </node>
    <node id="SLR_DIAMOND2">
      <data key="d0">MODEL</data>
      <data key="d1">SLR_DIAMOND2, also referred to as slr.diamond2, is a linear regression model that has been meticulously fitted using the lm() function in the R programming language. This model focuses on elucidating the relationship between the price of diamonds and the deviation of their carat weight from the mean carat weight, a variable known as the centered carat. By centering the carat variable, the model aims to provide a clearer understanding of how deviations in carat weight influence the price, independent of the overall mean carat weight of the dataset. This approach facilitates a more nuanced analysis of the linear relationship between carat weight and price, making SLR_DIAMOND2 a valuable tool for statistical inference in the context of diamond pricing.</data>
      <data key="d2">35e06960dba699ce0d56fc1e98bdbe96,f18bacb0a2fbfea44dd6326224184216</data>
    </node>
    <node id="I">
      <data key="d0">FUNCTION</data>
      <data key="d1">In the context of algorithmic analysis and statistical modeling, "I" serves a dual role. Firstly, "I" is utilized as a variable within loop constructs, iterating sequentially from 1 to n, facilitating the execution of repetitive tasks over a defined range. Secondly, "I()" is recognized as a function in the R programming language, specifically designed to prevent the conversion of an expression into an operator. This function is particularly useful when employed within the formula argument of the lm() function, ensuring that the expression is evaluated prior to its use as an explanatory variable in linear regression models. This dual functionality of "I" highlights its versatility in both control structures and statistical inference within the R environment.</data>
      <data key="d2">35e06960dba699ce0d56fc1e98bdbe96,bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </node>
    <node id="COEF">
      <data key="d0">FUNCTION</data>
      <data key="d1">COEF, more commonly referred to as coef() in R, is a versatile function designed to extract model coefficients from objects returned by modeling functions. This function is particularly useful in the context of linear models, where it facilitates the extraction of coefficients from a fitted model. coef() is a fundamental tool in statistical analysis, enabling data scientists and statisticians to interpret the results of modeling functions such as lm(). By providing access to the coefficients, coef() aids in understanding the relationships between variables within the model, making it an essential component in the analysis of complex data structures.</data>
      <data key="d2">084dadebfca8bcb6377c205c45bee295,25fce1af816975003128126b5cfea73b,35e06960dba699ce0d56fc1e98bdbe96,6a6f85d0a6e46196ab3a901fcc82a720,9a28a6420fca4405488ca35762f9dc28,a60af43e42c72a41fa90da06beb29d1b,c48bd062d5f6cc20c5df5758d6285562</data>
    </node>
    <node id="MEAN">
      <data key="d0">FUNCTION</data>
      <data key="d1">mean() is a function in R used to calculate the average of a set of numbers. It is used to calculate the average carat weight of diamonds in the dataset.</data>
      <data key="d2">35e06960dba699ce0d56fc1e98bdbe96</data>
    </node>
    <node id="AVERAGE_WEIGHT">
      <data key="d0">QUANTITY</data>
      <data key="d1">Average weight is the mean carat weight of the diamonds in the Solitaire rings in the dataset, which is 0.204 carats</data>
      <data key="d2">f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </node>
    <node id="SAMPLE_MEAN_PRICE">
      <data key="d0">ESTIMATED_VALUE</data>
      <data key="d1">The sample mean price of the Solitaire rings in the dataset is a statistical measure that represents the average price of these rings. This value is equal to the estimated intercept of S$ 500.08, providing a baseline understanding of the pricing structure for Solitaire rings within the context of the dataset. This figure is crucial for initial insights into the pricing trends and can serve as a reference point for further statistical analysis and comparison with other variables in the dataset.</data>
      <data key="d2">c5b269ff5c94db7ebd2cb9f7be16f171,f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </node>
    <node id="AVERAGE_DIAMOND_WEIGHT">
      <data key="d0">MEASUREMENT</data>
      <data key="d1">The average diamond weight is a statistical measure used as a reference point in the analysis</data>
      <data key="d2">c5b269ff5c94db7ebd2cb9f7be16f171</data>
    </node>
    <node id="REGRESSION_LINE">
      <data key="d0">MATHEMATICAL_MODEL</data>
      <data key="d1">The REGRESSION_LINE is a fundamental concept in statistical analysis, particularly within the context of regression analysis. It is defined as the line that best fits the data points in a scatterplot, serving as a mathematical model to describe and predict the relationship between the predictor (explanatory) variables and the response (dependent) variable. This line is calculated to minimize the sum of the squared distances between the observed data points and the line, providing a visual representation of the general trend in the data.

The REGRESSION_LINE is crucial for understanding the relationship between the weight of diamonds and the price of Solitaire rings, as well as the relationship between sales volume and price in a business context. It is used to represent the relationship between the explanatory variable and the response variable, allowing for predictions of the response variable based on the explanatory variable. However, it is noted that the average distance of the observations to the fitted line tends to increase with the explanatory variable, indicating that the line may not perfectly fit all data points.

In summary, the REGRESSION_LINE is a fitted line that describes the relationship between predictor variables and the response variable, providing a model for understanding and predicting the behavior of the response variable based on changes in the predictor variables. It is a powerful tool in regression analysis, enabling the visualization and interpretation of complex relationships within data.</data>
      <data key="d2">2a5997c641e47fc6c32ebf81101c54e0,312309b45c59e1c84695ac3c7e202742,674b8d5bb1f830d0fb944942514d1a16,7a605c3b689bec7ab2c46df9c123e3f3,82cfcd5865cffe55e965a50745656e60,9f335f1ecb85a1427df926df8bb1e89f,b0ca3e6c22c4cf884d03b1f6f82be5df,b9ec8a6c7960cc6196ec94fd976f05b0,c5b269ff5c94db7ebd2cb9f7be16f171,ef24ca5edd06893b737e6a1c8a9825f6</data>
    </node>
    <node id="FIGURE_1_3">
      <data key="d0">VISUAL_REPRESENTATION</data>
      <data key="d1">Figure 1.3 is a visual representation that shows two scatterplots of the diamond data, illustrating the relationship between diamond weight and ring price</data>
      <data key="d2">c5b269ff5c94db7ebd2cb9f7be16f171</data>
    </node>
    <node id="WEIGHT_DEVIATIONS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">c5b269ff5c94db7ebd2cb9f7be16f171</data>
    </node>
    <node id="REGRESSION_LINES">
      <data key="d0">MODEL</data>
      <data key="d1">Regression lines, a fundamental concept in statistical analysis, are employed to model the relationship between a dependent variable and one or more independent variables. Specifically, in the context of simple linear regression, these lines serve to illustrate the connection between an explanatory variable and a response variable. Notably, the regression lines maintain their identity in relation to the data even when the coordinate system undergoes horizontal shifts, highlighting that the predictions derived from two fitted lines will remain consistent. This characteristic underscores the robustness of regression analysis in various coordinate systems, ensuring that the insights gained are not contingent on the arbitrary placement of the origin.</data>
      <data key="d2">13e6c81b642252522460f4abadaab1cc,9854704301b8df256ca1013b8d53dfac</data>
    </node>
    <node id="COORDINATE_SYSTEM">
      <data key="d0">REFERENCE_FRAME</data>
      <data key="d1">The coordinate system is a reference frame used to plot data points and regression lines. When it shifts horizontally, the regression lines remain identical in relation to the data.</data>
      <data key="d2">13e6c81b642252522460f4abadaab1cc</data>
    </node>
    <node id="PREDICTIONS">
      <data key="d0">FORECASTS</data>
      <data key="d1">Predictions are the estimated values of the response variable based on the fitted regression lines. They are identical when the coordinate system shifts horizontally.</data>
      <data key="d2">13e6c81b642252522460f4abadaab1cc</data>
    </node>
    <node id="PARAMETERISATION">
      <data key="d0">MODEL_SPECIFICATION</data>
      <data key="d1">Parameterisation, a critical aspect in statistical modeling, encompasses the various methods through which a model can be defined or specified. It involves the selection of a reference category and the determination of the specific form or structure of the model. Different parameterisations can represent the same model, utilizing distinct parameters or equations, thereby offering flexibility in model representation. This concept is central to understanding how parameters are expressed and defined within a statistical model, enabling a nuanced exploration of the model's structure and its application to the community of interest.</data>
      <data key="d2">13e6c81b642252522460f4abadaab1cc,86ece4718d27d1a6c6a1f448cc850e2b,9854704301b8df256ca1013b8d53dfac,a1fc936df848a0fbc791e4bcc9b527b6</data>
    </node>
    <node id="RE_PARAMETERISATIONS">
      <data key="d0">MODEL_REFORMULATION</data>
      <data key="d1">Re-parameterisations are reformulations of a model using different parameters or equations. They can represent the same model but from a different perspective or with a different focus.</data>
      <data key="d2">13e6c81b642252522460f4abadaab1cc</data>
    </node>
    <node id="MATHEMATICAL_DESCRIPTION">
      <data key="d0">MODEL_DEFINITION</data>
      <data key="d1">Mathematical description is the formal definition or specification of a model using mathematical equations and symbols. It provides a precise and unambiguous representation of the model.</data>
      <data key="d2">13e6c81b642252522460f4abadaab1cc</data>
    </node>
    <node id="R_IMPLEMENTATION">
      <data key="d0">CODE</data>
      <data key="d1">R implementation is the code written in the R programming language to implement the mathematical model. It translates the mathematical description into a computational form.</data>
      <data key="d2">13e6c81b642252522460f4abadaab1cc</data>
    </node>
    <node id="INTERPRETATION">
      <data key="d0">MODEL_EXPLANATION</data>
      <data key="d1">Interpretation is the explanation or understanding of the model within the context of the data and the problem being studied. It involves translating the mathematical and computational aspects of the model into meaningful insights.</data>
      <data key="d2">13e6c81b642252522460f4abadaab1cc</data>
    </node>
    <node id="REPARAMETERISATIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Re-parameterisations refer to the process of changing the parameters of a model while keeping the model's predictions the same. This concept will be formally defined later in the module.</data>
      <data key="d2">ccced70c40ef9105ac2f7a9bfd151125</data>
    </node>
    <node id="MATERIAL_COVERED">
      <data key="d0">CURRICULUM</data>
      <data key="d1">The material covered so far in the module is familiar to most students, but it exemplifies the skills needed to move confidently between mathematical descriptions, R implementation, and interpretation in the application context.</data>
      <data key="d2">ccced70c40ef9105ac2f7a9bfd151125</data>
    </node>
    <node id="SKILLS">
      <data key="d0">ABILITY</data>
      <data key="d1">Skills required for the module include the ability to move confidently between the mathematical description of a model, its implementation in R, and its interpretation within the application context.</data>
      <data key="d2">ccced70c40ef9105ac2f7a9bfd151125</data>
    </node>
    <node id="EXERCISES">
      <data key="d0">ACTIVITY</data>
      <data key="d1">Exercises are provided to help students assimilate and practice the material discussed so far. The solutions can be found in the ST231 Exercise Booklet on moodle.</data>
      <data key="d2">ccced70c40ef9105ac2f7a9bfd151125</data>
    </node>
    <node id="EXERCISE_1">
      <data key="d0">ACTIVITY</data>
      <data key="d1">Exercise 1 requires students to derive the mathematical relationship between the parameters in the model formulation in (1.1) and the one in (1.2).</data>
      <data key="d2">ccced70c40ef9105ac2f7a9bfd151125</data>
    </node>
    <node id="EXERCISE_2">
      <data key="d0">ACTIVITY</data>
      <data key="d1">Exercise 2 involves using R to show numerically that the fitted model based on (1.2) leads to the same predictions as the fitted model based on (1.1). This demonstrates that the model in (1.2) is a reparameterisation of the model in (1.1).</data>
      <data key="d2">ccced70c40ef9105ac2f7a9bfd151125</data>
    </node>
    <node id="EXERCISE_3">
      <data key="d0">ACTIVITY</data>
      <data key="d1">In Exercise 3, the focus is on addressing the anomaly of negative price predictions for Solitaire rings adorned with very small diamonds. This is achieved by fitting a regression model with a unique constraint: the intercept is fixed to zero, symbolically represented as Price = &#946;1 weight. This approach, known as regression through the origin, is employed to refine the model's accuracy specifically for diamonds of minimal weight. The exercise also includes a comparative analysis, where the performance of this model is juxtaposed against the model detailed in equation (1.1), which does not impose any restrictions on the intercept. This comparison is crucial for understanding the implications of different model specifications on price predictions for diamond rings.</data>
      <data key="d2">ccced70c40ef9105ac2f7a9bfd151125,e47d573a10e64a657e58218df64d8920</data>
    </node>
    <node id="MODEL_1_1">
      <data key="d0">REGRESSION_MODEL</data>
      <data key="d1">Model (1.1) is a regression model that describes the relationship between the price of diamonds and their carat weight, without restricting the intercept to be equal to zero</data>
      <data key="d2">e47d573a10e64a657e58218df64d8920</data>
    </node>
    <node id="MODEL_1_2">
      <data key="d0">REGRESSION_MODEL</data>
      <data key="d1">Model (1.2) is a reparameterisation of model (1.1), leading to the same predictions</data>
      <data key="d2">e47d573a10e64a657e58218df64d8920</data>
    </node>
    <node id="REPARAMETERISATION">
      <data key="d0">MODEL_TRANSFORMATION</data>
      <data key="d1">Reparameterisation, a pivotal concept in statistical modeling, is a mathematical transformation that alters the parameterisation of a model. This technique is employed to enhance the interpretability of parameters and their estimates within a statistical model, making it easier for data scientists to understand and communicate the model's structure and results. Through reparameterisation, a model can be transformed into a different form that leads to identical predictions, as demonstrated in the comparison between models (1.1) and (1.2). This process is crucial for refining models, ensuring they are not only accurate but also comprehensible, thereby facilitating better decision-making based on statistical analysis.</data>
      <data key="d2">9854704301b8df256ca1013b8d53dfac,9fc2b1e8b2b61b557f88eb9e9c708597,e47d573a10e64a657e58218df64d8920</data>
    </node>
    <node id="REGRESSION_THROUGH_ORIGIN">
      <data key="d0">REGRESSION_MODEL</data>
      <data key="d1">Regression through the origin is a type of regression model where the intercept is fixed to be equal to zero, as in the model Price = &#946;1 weight + &#1013;</data>
      <data key="d2">e47d573a10e64a657e58218df64d8920</data>
    </node>
    <node id="R_COMMAND">
      <data key="d0">PROGRAMMING_COMMAND</data>
      <data key="d1">The R command lm(price ~ 0 + carat, data = diamond) is used to fit a regression through the origin model to the diamond data</data>
      <data key="d2">e47d573a10e64a657e58218df64d8920</data>
    </node>
    <node id="OUTCOME_VARIABLE_Y">
      <data key="d0">VARIABLE</data>
      <data key="d1">Outcome variable Y is the variable of interest in a regression model, which is affected by and responds to the explanatory variable X</data>
      <data key="d2">e47d573a10e64a657e58218df64d8920</data>
    </node>
    <node id="EXPLANATORY_VARIABLE_X">
      <data key="d0">VARIABLE</data>
      <data key="d1">Explanatory variable X is the variable that is used to explain the variation in the outcome variable Y in a regression model</data>
      <data key="d2">e47d573a10e64a657e58218df64d8920</data>
    </node>
    <node id="Y">
      <data key="d0">RESPONSE_VARIABLE</data>
      <data key="d1">Y, a pivotal entity in the statistical analysis, is a column vector of observed y variable values, representing the response or dependent variable in various regression models. In the context of the straight line model, Y is the variable whose behavior is predicted based on the explanatory variables. Specifically, in the linear regression model, Y is a vector of observed response values, of dimension n, following a multivariate normal distribution Nn(X&#946;, &#963;^2In). This distribution indicates that when log-transformed, Y adheres to a normal distribution with mean &#956; and variance &#963;^2.

Y is central to the expression of S(&#946;), a function likely related to the sum of squared errors in regression analysis. In the sales example, Y is identified as the sales volume, the dependent variable whose mean is determined by the systematic component and whose variance is equal to &#963;^2. The observed values of Y are denoted as y1 through yn for n paired observations, reflecting the data collected for analysis.

In the quadratic regression model, Y is the response variable, whose expectation is specified by the linear predictor, influenced by the explanatory variables and the error term. Y is also the response variable in the simple linear regression model, where it is expected to change in response to changes in the explanatory variables. This relationship highlights Y's dependency on the explanatory variables and its responsiveness to variations in X.

Y is the response vector in the linear model, assumed to follow a multivariate normal distribution with mean X&#946; and covariance matrix &#963;^2In. This vector comprises n random variables representing the responses in the linear regression model, each element corresponding to the response variable for a single observation. Y's observed values for the n units of observation are crucial for understanding the statistical model's structure and behavior.

In summary, Y is a versatile and critical variable in regression analysis, embodying the response or outcome variable in various models. Its observed values, denoted as y1 through yn, are essential for statistical inference and model validation. Y's distribution and relationship with explanatory variables are fundamental to the analysis of data in econometrics and other quantitative disciplines.</data>
      <data key="d2">070499b11a2fc1530fd2751d0920ad31,084dadebfca8bcb6377c205c45bee295,09caa54ca1372d152e47051be4d44ede,0ac60299320c55d642b3e38440c25f90,0b650eb2f1dcd603b64fec3c4b5cd24b,10ac76f99674a01ca0f4a55586dea07e,11452a08471d93959558de2ece9a69af,2167274129d4cfa74a002c4cc39df8a8,21e429490eeefe7d9c245058fd48ca68,21ec28dfe2b2c18030d541d63e51f45e,248924760a2bfbc82501fd6b11cfa0aa,254a8a17b1be06702934341e3bf41e85,255685e281cc5a9edf073c700f425a6b,2673d078d29f2af78fab9b6eacd15e37,2685edb9e8031c8ea725c43a40af22a8,28cf5ff0c09fa5c0390267bb9aa3ce47,2a5997c641e47fc6c32ebf81101c54e0,2d5cdecc342ddacd2c090f1838430cee,2de7a36b32bf79c8f32612c8aaa9daa8,2f2523c52c6d2869fb19f77b66ce8259,3cbe71f7649e84cd67cb3fa0d3e632cf,3d357cfa3ef0d00f49cf4acaeac1c9d1,416494d940a9f505da9853caca26fe63,45f31b040576e9f3b4def6d0466cc016,50a56c34050fb7f7709300a51399b150,512d9ffebe309a6f944ebce1ae2ff2a3,542f546c5a131196e4701fb33c9b1dee,5609007c6229060ffc85d8056a7fefde,56ff186fc629e1e42f2759fc4b984199,5a0d392715f06d5e873f45ae06aa729a,6648f0d6deed51fb4fb25e6992a71ddf,679722cf8ce5ce5aee4e379528470efe,67e4c1866b0c6e162e6e3317949e8da9,69ffba28a61d98d8d18f91c24b74dd4a,6a47154bf457c25f22c3cf9f649c5db0,6b55b41598d5264f8dc6b72769748722,6c66e9414880964ee899ceb0f16d22e9,6ee02b38ae842fd5eac9a11c4fd6659f,74a5a0e8ae0f846240c782cc1a30f82f,74d190f10bf6e6936242ca3cdfc4a09f,7955aae3fd4ca51b9ef8843e13c1f517,7ad4ccec4c7bb3702aed71c17dc6b96f,7d074208b1259e7d84f9f870d3828bb6,7fc5b8303ab530821bf2140ba6a8a889,82932abd152e0b84a1c26a2daa4c08df,8f1d95acff56e1633dceb775fa713174,9005147593b2f27b9e2a5eede3601bdc,9923e77ac6b3de95cb5026bc5e7fe8c0,9a27580975988e83f6e3a0d9010893b5,9d300fc83afb3261af61b2ab9721cadc,9dddcd96af7b557e578b3f5f36efacd7,9fc2b1e8b2b61b557f88eb9e9c708597,a4a817bb79d6ae8812c808ca41d47f43,a828fd17fc38e902484872c88a6b242c,aa195e72eb5285a4bcae9c856af30a87,b5d0a103e1f34a00aef67fedd0e8c693,b70a75a6412b2e5c44af50734844f4be,c9a01b92d11585f6549f62e8bd78d652,d94760a5f9f6ea115fcc18024035a627,dd7e7d54883ca0f687568a738b95d4d0,e41cc40f061f487b1ea0f256d4a963e4,e4f14e6785c6d7b7469e695aaeb170d0,e7494d6cfc3e38a4d2f3f6b21ef6445d,e7edd8b2874a350779ae20f1ecdf4733,eac62cdd5518e1269fed150639331c2c,f2300d613896880cbb7c255a4d858315,f470791d2d3fedede166f9bb11598c9c,f483798b15ef305e7826fd7142379e03,f5716ce115458c0652124734ca344806,f632f01188d2c6e3091a965580cb4600,f9b615b879f72501f338f8983d4cac3d,fc5b725f3c662c5471af20efdcc2dbff</data>
    </node>
    <node id="X">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">Matrix X, a critical component in the context of linear regression analysis, serves as the design or model matrix, encapsulating the predictor variables that help elucidate the relationships within the dataset. X is typically structured as an n x p matrix, where n denotes the number of observations and p represents the number of parameters, including the intercept and explanatory variables. It is noteworthy that X often includes a column of ones to account for the intercept term in the regression equation.

In the linear regression model, X is the matrix of explanatory variables, playing a pivotal role in the calculation of the least squares estimator and the maximum likelihood estimate. It is assumed to be of full rank, meaning that its rank equals p, ensuring that the matrix has full column rank, a condition necessary for the unique determination of the regression coefficients.

X is also described as a 2xN matrix in some instances, with the first row being a vector of ones and the second row containing the x variable values. This specific structure is particularly relevant in the straight line model, where X is the design matrix for the expression Yj = &#945;0 + &#945;1 (xj - x) + &#951;j, for j = 1, ..., n. Here, X facilitates the estimation of the regression coefficients, &#945;0 and &#945;1, which define the linear relationship between the dependent and independent variables.

In more complex models, X may include not only the predictor variables but also their squared terms, enhancing the model's ability to capture nonlinear relationships. For instance, in the quadratic regression model, X is a vector containing the predictor variables x1, x2, ..., xq, along with their squared terms, providing a more nuanced understanding of the data.

X is further detailed as a matrix with dimensions n x 2, where the first column is a vector of ones and the second column is the vector of observed values X1 to Xn. This structure is particularly useful in the simple linear regression model, where X represents the independent variable, influencing the expected response in Y.

In a specific application, X is the design matrix used in least squares estimation, containing the values of the explanatory variables for all units of observation. For example, in a study involving 139 trees, X is a matrix with 139 rows, each corresponding to a different tree, and two columns: the first column is a vector of 1s (for the intercept term) and the second column contains the log2(diameter)</data>
      <data key="d2">00e186a86624c01ce873dd577df68d17,070499b11a2fc1530fd2751d0920ad31,084dadebfca8bcb6377c205c45bee295,09caa54ca1372d152e47051be4d44ede,10ac76f99674a01ca0f4a55586dea07e,11452a08471d93959558de2ece9a69af,119bc73ddf8eebadfb8eae272fa323a7,1303b66694a101878ca530c0b41cf5ef,1da117a2f92b2db00290d2a0bfc06beb,2167274129d4cfa74a002c4cc39df8a8,21ec28dfe2b2c18030d541d63e51f45e,228bdca7843406def245d755e8df49f6,248924760a2bfbc82501fd6b11cfa0aa,254a8a17b1be06702934341e3bf41e85,255685e281cc5a9edf073c700f425a6b,2673d078d29f2af78fab9b6eacd15e37,28cf5ff0c09fa5c0390267bb9aa3ce47,2a5997c641e47fc6c32ebf81101c54e0,2b6d31b6bff4eae3a4809451c4fb9fa6,2d5cdecc342ddacd2c090f1838430cee,2de7a36b32bf79c8f32612c8aaa9daa8,2f2523c52c6d2869fb19f77b66ce8259,3cbe71f7649e84cd67cb3fa0d3e632cf,3d357cfa3ef0d00f49cf4acaeac1c9d1,416494d940a9f505da9853caca26fe63,45f31b040576e9f3b4def6d0466cc016,46629f2efc6c82e81265a131b4bab2ee,50a56c34050fb7f7709300a51399b150,56ff186fc629e1e42f2759fc4b984199,5a0d392715f06d5e873f45ae06aa729a,5cc49d301d9cd1f8e20b92ab9d8346b0,69ffba28a61d98d8d18f91c24b74dd4a,6b55b41598d5264f8dc6b72769748722,6c1684ed2a4840576c6b0f4d1a3a482f,6c66e9414880964ee899ceb0f16d22e9,6ee02b38ae842fd5eac9a11c4fd6659f,74a5a0e8ae0f846240c782cc1a30f82f,74d190f10bf6e6936242ca3cdfc4a09f,75dc4d8cb195a1f969d9e9496631086b,7955aae3fd4ca51b9ef8843e13c1f517,7ad4ccec4c7bb3702aed71c17dc6b96f,7d074208b1259e7d84f9f870d3828bb6,7fc5b8303ab530821bf2140ba6a8a889,82932abd152e0b84a1c26a2daa4c08df,8f1d95acff56e1633dceb775fa713174,9005147593b2f27b9e2a5eede3601bdc,9923e77ac6b3de95cb5026bc5e7fe8c0,9a27580975988e83f6e3a0d9010893b5,9d300fc83afb3261af61b2ab9721cadc,aa195e72eb5285a4bcae9c856af30a87,aeddef300427d211c74c6008b5b6b328,b70a75a6412b2e5c44af50734844f4be,c9a01b92d11585f6549f62e8bd78d652,d94760a5f9f6ea115fcc18024035a627,dd7e7d54883ca0f687568a738b95d4d0,e41cc40f061f487b1ea0f256d4a963e4,e4f14e6785c6d7b7469e695aaeb170d0,e7edd8b2874a350779ae20f1ecdf4733,e800735d6b2a244875f5e0d292de1527,eac62cdd5518e1269fed150639331c2c,f2300d613896880cbb7c255a4d858315,f5716ce115458c0652124734ca344806,f632f01188d2c6e3091a965580cb4600,f9b615b879f72501f338f8983d4cac3d,fc5b725f3c662c5471af20efdcc2dbff</data>
    </node>
    <node id="LINEAR_REGRESSION_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">Linear regression model describes the relationship between an outcome variable Y and an explanatory variable X</data>
      <data key="d2">070499b11a2fc1530fd2751d0920ad31</data>
    </node>
    <node id="STATISTICAL_INDEPENDENCE">
      <data key="d0">CONCEPT</data>
      <data key="d1">Statistical independence is a fundamental concept in probability and statistics, referring to the lack of connection between events or variables. In the context of the text, it is mentioned as a term that might be misinterpreted.</data>
      <data key="d2">4683e58cf41e5f5d415a63ddb2fe0cac</data>
    </node>
    <node id="OBSERVATIONS">
      <data key="d0">DATA</data>
      <data key="d1">The number of observations is a crucial metric that quantifies the size of the dataset in the context of statistical analysis. Observations, in this scenario, refer to the data points collected during a study, each consisting of paired measurements (x, y) for every unit of observation. Specifically, in the realm of simple linear regression, these observations are represented as (x1, y1), (x2, y2), ..., (xn, yn) pairs, where x and y denote the explanatory and response variables, respectively. These data points are visually depicted in scatterplots, where they illustrate the relationship between the variables. Additionally, the observations are also pivotal in residual analysis, appearing in residual plots. It is noted that the variation of these observations along the horizontal axis of the scatterplot seems constant, indicating a consistent spread of data points. These individual data points are essential components in regression analysis, serving as the foundation for understanding the structure and relationships within the dataset.</data>
      <data key="d2">2a5997c641e47fc6c32ebf81101c54e0,312309b45c59e1c84695ac3c7e202742,4683e58cf41e5f5d415a63ddb2fe0cac,82cfcd5865cffe55e965a50745656e60,b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </node>
    <node id="XJ">
      <data key="d0">VARIABLE</data>
      <data key="d1">XJ, denoted as x_j or Xj, is a variable that consistently represents the jth value of an explanatory or independent variable in the context of linear regression models. This variable is pivotal in the models, serving as the measurement of the explanatory variable X for the jth unit of observation. XJ signifies the value of the independent variable for a specific observation and is the jth predictor value in both simple and multiple linear regression models. It is the jth observed value of the independent variable in a dataset and is multiplied by a coefficient, denoted as C, in the reparameterised model. XJ is the predictor value for the jth data point, highlighting its role in establishing the relationship between the independent and dependent variables in the models under consideration.</data>
      <data key="d2">3fdeeb7593174f5e8a9cff55a7cd92e3,4683e58cf41e5f5d415a63ddb2fe0cac,5609007c6229060ffc85d8056a7fefde,6c1684ed2a4840576c6b0f4d1a3a482f,7fc5b8303ab530821bf2140ba6a8a889,8f7a05b6d231105a6194eebdb2df372e,90b7e0427699cc1bb461e37939935138,d14413709de2897231aaa83be3aa346f,d1b6fcd55d937c5fe2d6add69e0bcf05,f5716ce115458c0652124734ca344806,f9e7b2eac9f82681301da3d1e2f23328</data>
    </node>
    <node id="YJ">
      <data key="d0">VARIABLE</data>
      <data key="d1">YJ, also denoted as Yj, is the observed response value for the jth unit of observation in the context of a linear regression model. It serves as the dependent variable in the models, specifically in the straight line and simple linear regression models. Yj is the jth observed value in the dataset, representing the actual response for the jth observation. This variable is given by the linear combination of the parameters and the explanatory variables plus the error term, encapsulating the measurement of the response variable Y for the jth unit of observation. It represents the value of the dependent variable for a specific observation, and in the model equation, Yj represents the jth observed value of the dependent variable for the jth unit of observation. Yj is crucial in statistical analysis as it helps in understanding the relations and structure of the community of interest within the context of algorithmic analysis.</data>
      <data key="d2">1303b66694a101878ca530c0b41cf5ef,3fdeeb7593174f5e8a9cff55a7cd92e3,4683e58cf41e5f5d415a63ddb2fe0cac,5cc49d301d9cd1f8e20b92ab9d8346b0,6c1684ed2a4840576c6b0f4d1a3a482f,75dc4d8cb195a1f969d9e9496631086b,7ad4ccec4c7bb3702aed71c17dc6b96f,7fc5b8303ab530821bf2140ba6a8a889,87b717ba065d6d7c7431af284137eb12,8f7a05b6d231105a6194eebdb2df372e,b5d0a103e1f34a00aef67fedd0e8c693,d14413709de2897231aaa83be3aa346f,d1b6fcd55d937c5fe2d6add69e0bcf05,e6f79ceb0df54119a4dc71b2162ac50b,f5716ce115458c0652124734ca344806,f9e7b2eac9f82681301da3d1e2f23328</data>
    </node>
    <node id="SOLITAIRE_RING_EXAMPLE">
      <data key="d0">EXAMPLE</data>
      <data key="d1">The Solitaire ring example is used to illustrate the application of simple linear regression. In this example, yj is the price of the jth ring and xj is the weight of its diamond.</data>
      <data key="d2">4683e58cf41e5f5d415a63ddb2fe0cac</data>
    </node>
    <node id="VARIABLES">
      <data key="d0">FEATURES</data>
      <data key="d1">Variables (or variates) are features in the system of interest that are quantifiable. They are the aspects of the system that can be measured and analyzed.</data>
      <data key="d2">4683e58cf41e5f5d415a63ddb2fe0cac</data>
    </node>
    <node id="DATA">
      <data key="d0">MEASUREMENTS</data>
      <data key="d1">DATA, in the context of statistical analysis, encompasses the realized values that are obtained when variates are measured on specific units of observation. These actual numbers or values result from the measurement of variables and constitute the collection of observations or measurements that are central to statistical studies. The data, which needs to be analyzed and understood, specifically refers to the observations used in regression analysis. Notably, these observations are relatively evenly distributed on either side of the fitted regression line, indicating a balanced distribution around the line of best fit, a characteristic that is crucial for the validity of regression models.</data>
      <data key="d2">312309b45c59e1c84695ac3c7e202742,4683e58cf41e5f5d415a63ddb2fe0cac,bb31f1c77bbba73300f735a100086a67,eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </node>
    <node id="RANDOM_VARIABLE">
      <data key="d0">CONCEPT</data>
      <data key="d1">The entity known as "RANDOM_VARIABLE" is a fundamental concept in probability theory. It is characterized as a variable whose potential values are outcomes of a random phenomenon. This concept is crucial in statistical analysis as it allows for the mathematical representation of uncertainty and variability. In the context of algorithmic analysis, the random variable is often denoted with capital letters, distinguishing it from its realization, which is the actual value observed in a given instance. This distinction is essential for understanding the structure and behavior of the community of interest, as it enables the modeling of probabilistic events and the prediction of outcomes based on statistical inference.</data>
      <data key="d2">4683e58cf41e5f5d415a63ddb2fe0cac,7594eee7e77beb023d1cd64aec64920d</data>
    </node>
    <node id="APPLIED_STATISTICS">
      <data key="d0">FIELD</data>
      <data key="d1">Applied statistics, a specialized field within the realm of data science, focuses on applying statistical methods to address and solve real-world problems. This field is characterized by its practical approach to data analysis, leveraging various statistical techniques to extract meaningful insights and solutions from complex data sets. Unlike pure mathematics, applied statistics has a tradition of being less rigid in terms of terminology and notation, reflecting its adaptive nature in dealing with diverse and evolving real-world scenarios. This flexibility allows practitioners to tailor their methods to the specific needs of the problems they encounter, making applied statistics a versatile tool in various domains such as business, healthcare, social sciences, and engineering.</data>
      <data key="d2">4683e58cf41e5f5d415a63ddb2fe0cac,7594eee7e77beb023d1cd64aec64920d</data>
    </node>
    <node id="UNITS_OF_OBSERVATION">
      <data key="d0">OBSERVATION</data>
      <data key="d1">Units of observation are the specific instances or subjects on which data are collected in a statistical study</data>
      <data key="d2">7594eee7e77beb023d1cd64aec64920d</data>
    </node>
    <node id="REALISATION">
      <data key="d0">OBSERVED_VALUE</data>
      <data key="d1">Realisation is the actual value observed for a random variable in a particular instance</data>
      <data key="d2">7594eee7e77beb023d1cd64aec64920d</data>
    </node>
    <node id="THEORY_UNDERNEATH">
      <data key="d0">CONCEPT</data>
      <data key="d1">Theory underneath refers to the foundational principles and concepts that underpin statistical methods and models</data>
      <data key="d2">7594eee7e77beb023d1cd64aec64920d</data>
    </node>
    <node id="PRECISION_OF_LANGUAGE_AND_NOTATION">
      <data key="d0">STANDARD</data>
      <data key="d1">Precision of language and notation is the clarity and consistency in terminology and symbols used in statistical communication and documentation</data>
      <data key="d2">7594eee7e77beb023d1cd64aec64920d</data>
    </node>
    <node id="RESPONSE_VARIABLE_Y">
      <data key="d0">VARIABLE</data>
      <data key="d1">Response variable Y is the variable of primary interest in a statistical model, whose values are influenced by the explanatory variables</data>
      <data key="d2">7594eee7e77beb023d1cd64aec64920d</data>
    </node>
    <node id="EXPLANATORY_VARIABLES_X">
      <data key="d0">VARIABLE</data>
      <data key="d1">Explanatory variables X are the variables that are used to explain or predict the response variable in a statistical model, treated as fixed rather than random</data>
      <data key="d2">7594eee7e77beb023d1cd64aec64920d</data>
    </node>
    <node id="DESIGNED_EXPERIMENT">
      <data key="d0">STUDY_TYPE</data>
      <data key="d1">Designed experiment is a type of study where the values of the explanatory variables are chosen by the experimenter to test specific hypotheses</data>
      <data key="d2">7594eee7e77beb023d1cd64aec64920d</data>
    </node>
    <node id="OBSERVATIONAL_STUDIES">
      <data key="d0">STUDY_TYPE</data>
      <data key="d1">Observational studies, a critical component of statistical research, are characterized by their design where data is collected without any intervention or manipulation by the researcher. These studies are conducted to measure or observe data concurrently with the response variable, primarily aiming to explore associations between variables. The essence of observational studies lies in their ability to provide insights into real-world scenarios, as they do not impose experimental conditions, allowing for the examination of naturally occurring phenomena. This approach is particularly useful in situations where controlled experiments are either unethical, impractical, or impossible to conduct. By avoiding the imposition of experimental conditions, observational studies offer a more holistic view of the relationships and structures within the community of interest, making them a valuable tool in various fields of research.</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22,7594eee7e77beb023d1cd64aec64920d</data>
    </node>
    <node id="NONRANDOM_VARIABLES">
      <data key="d0">VARIABLE</data>
      <data key="d1">Nonrandom variables are variables that are not considered to have a probability distribution, often because their values are fixed or determined by the study design</data>
      <data key="d2">7594eee7e77beb023d1cd64aec64920d</data>
    </node>
    <node id="CONDITIONAL_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">Conditional model is a statistical model that is developed based on the values taken by the explanatory variables, treating them as fixed</data>
      <data key="d2">7594eee7e77beb023d1cd64aec64920d</data>
    </node>
    <node id="RESPONSE_VARIABLE">
      <data key="d0">VARIABLE_TYPE</data>
      <data key="d1">The entity "RESPONSE_VARIABLE" is central to the context of statistical modeling, particularly within regression analysis. It is referred to as the response variable, the dependent variable, or the outcome variable. This variable is the focus of prediction or explanation within a statistical model. Its values are modeled as a function of the explanatory variable(s) and the parameters of the model. The response variable is the variable of interest that the model aims to predict or explain, often in relation to one or more predictor variables. In the context of regression models, the response variable's variation is explained by the explanatory variable(s), making it a crucial component for understanding the relationships and structure within the data.</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22,312309b45c59e1c84695ac3c7e202742,7347b44ffb25a066e43321f4eaf5a806,768c516c8b27fb9800427e848f02fc33,82cfcd5865cffe55e965a50745656e60,b9af17718641389ba07f53be13f31f8c,b9eb75001a4f68f7240b2ca9e0d79eb8,b9ec8a6c7960cc6196ec94fd976f05b0,bb31f1c77bbba73300f735a100086a67,e361ac139c268d5c3f3623f920e68af2,ef24ca5edd06893b737e6a1c8a9825f6</data>
    </node>
    <node id="DATA_MEASUREMENT">
      <data key="d0">SOURCE</data>
      <data key="d1">Data in observational studies might be simply measured or observed at the same time as the response variable</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22</data>
    </node>
    <node id="PROBABILITY_THEORY">
      <data key="d0">THEORY</data>
      <data key="d1">Probability theory uses capital letters to denote random variables and small letters for their realizations, to distinguish between the two</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22</data>
    </node>
    <node id="RANDOM_VARIABLES">
      <data key="d0">VARIABLE_TYPE</data>
      <data key="d1">Random variables are variables whose values are determined by chance, and are denoted by capital letters in probability theory</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22</data>
    </node>
    <node id="VECTORS">
      <data key="d0">DATA_STRUCTURE</data>
      <data key="d1">Vectors, denoted by small letters in linear algebra, are one-dimensional arrays of numbers that can be random or deterministic. They play a crucial role in representing quantities that possess both magnitude and direction. In the context of retail stores, vectors are employed to model various aspects such as sales figures, pricing strategies, and advertising budgets, providing a structured way to analyze and understand the dynamics of these elements within the business environment.</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22,336546bc73cbe1828a0cc1a45faf8f5a</data>
    </node>
    <node id="MATRICES">
      <data key="d0">DATA_STRUCTURE</data>
      <data key="d1">Matrices are two-dimensional arrays of numbers, which can be random or not, and are denoted by capital letters in linear algebra</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22</data>
    </node>
    <node id="SIMULATED_DATASET">
      <data key="d0">DATA_TYPE</data>
      <data key="d1">A simulated dataset is a set of data that has been artificially created for the purpose of testing or demonstrating statistical methods or models</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22</data>
    </node>
    <node id="RETAIL_STORES">
      <data key="d0">ENTITY</data>
      <data key="d1">Retail stores are physical or online establishments where products are sold to consumers</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22</data>
    </node>
    <node id="PRODUCT_SALES">
      <data key="d0">DATA_TYPE</data>
      <data key="d1">Product sales data refers to the number of units of a product sold, often used as a response variable in statistical models</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22</data>
    </node>
    <node id="SIMPLE_REGRESSION">
      <data key="d0">MODEL_TYPE</data>
      <data key="d1">Simple regression, a fundamental statistical method under the expertise of the data scientist, is utilized to analyze the relationship between two variables. Specifically, it involves one response variable and one explanatory variable. This method is characterized by a statistical model that employs the single explanatory variable to predict the response variable. The technique is pivotal in understanding and quantifying the relationship between the two variables, enabling predictions and insights into how changes in the explanatory variable might affect the response variable. Through the application of simple regression, the data scientist can perform detailed analysis, leveraging tools like R for data analysis and employing a deep understanding of statistical inference to provide comprehensive insights and solutions.</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22,336546bc73cbe1828a0cc1a45faf8f5a</data>
    </node>
    <node id="MULTIPLE_REGRESSION">
      <data key="d0">MODEL_TYPE</data>
      <data key="d1">Multiple regression is a comprehensive statistical method utilized to analyze and model the relationship between one response variable, also known as the dependent variable, and two or more explanatory variables, referred to as independent variables. This technique is employed to predict the response variable based on the combined influence of multiple independent variables, providing a robust framework for understanding the complex interplay between the variables of interest. By leveraging the power of statistical inference, multiple regression enables the identification of significant predictors and quantifies their impact on the dependent variable, making it a valuable tool in various fields such as economics, social sciences, and business analytics.</data>
      <data key="d2">22478e53f29f16e3eab9d167fea52b22,336546bc73cbe1828a0cc1a45faf8f5a,60cc94e681863c9fcc6f9be1e500f840,a60af43e42c72a41fa90da06beb29d1b,aa13c33a7e61206e6021e2736002ca9a</data>
    </node>
    <node id="MATRIX">
      <data key="d0">MATRIX</data>
      <data key="d1">The MATRIX is a versatile data structure pivotal in the realm of data representation and analysis. It is typically visualized as a table, organized into rows and columns, which facilitates the systematic arrangement and examination of data. In a specific application, the MATRIX serves as a critical tool for elucidating the intricate relationship between three key variables: brand, price, and sales volume within the dataset. This structure enables analysts to discern patterns, trends, and potential correlations among these elements, thereby enhancing the understanding of market dynamics and consumer behavior.</data>
      <data key="d2">336546bc73cbe1828a0cc1a45faf8f5a,6a6f85d0a6e46196ab3a901fcc82a720</data>
    </node>
    <node id="SALES">
      <data key="d0">RESPONSE_VARIABLE</data>
      <data key="d1">Sales (Salesj) is a critical quantitative variable in the statistical models discussed, representing the sales figures of products or services. Specifically, it denotes the sales at the jth store, capturing the sales volume in thousands of units for 96 different observations. This variable is the dependent variable in the linear regression models for the Retail data, influenced by categorical predictors such as brand. Sales is also the response variable of interest in regression analyses, embodying the sales volumes of stores and the total amount of goods or services sold over a specific period. An economic indicator, Sales reflects the number of units sold, with an increase in sales by 544 units associated with a &#163;1000 increase in the local advertising budget. Measured in thousands, Salesj provides insights into the sales figures of the products and the sales data for different stores, making it a pivotal element in understanding the economic performance and market dynamics.</data>
      <data key="d2">06d5666e6bfdda828b48adba883b4a61,0cb40986e6c2bb439e1ffcaae2df96ac,1820d10ee0f23f34b3ea88ba475bc52d,1b523d1edabe381403fc470a9b8d47fa,1d141ab04db553f78a313e430e54abb5,248924760a2bfbc82501fd6b11cfa0aa,250ee5d766c64e7975bcc427b4bf9074,2e5e1bdaa9fcc7b3391d277fd6bb247a,336546bc73cbe1828a0cc1a45faf8f5a,3dd24a54028976ba54304ec7169bb74b,426434b67f6a287852ab66b82ca873cf,48971100deb5bb374a41c1f2b7b2a86a,7037e0369bfdaad5a730cabb2b44831c,825b600cbab3535ce67e9f561ddcb84b,8326c645426789920a99ed373725fa0e,86ece4718d27d1a6c6a1f448cc850e2b,8a9b984c146f59b2af83d1c2f373d376,906eb7d6b49fa360e7e5b65c56cd4d76,93da9813e10a119798de6982977f1239,a1fc936df848a0fbc791e4bcc9b527b6,ac15b639b0849006471dfe102376c2c0,aeddef300427d211c74c6008b5b6b328,b1690cb1a67892245c0665e5099e322d,b2c33cb151a8e7724ebfb7b2d88bc45f,c103c6d096d52868eda26d991194b5f2,c48bd062d5f6cc20c5df5758d6285562,ee295ebc4c2d6a2a5c738796fcc2ab71,f16299fc00a7a69bdf983dce826b4918</data>
    </node>
    <node id="ADVERT">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">ADVERT, also referred to as the advertising budget, is a significant variable within the dataset, representing the amount spent on local advertising for the product. This economic variable, measured in thousands of pounds, plays a crucial role in the promotion of products or services to potential customers. In the context of statistical analysis, ADVERT is an independent and explanatory variable in the regression models, indicating the average effect on the response variable when its value is increased by one unit. It is considered in both simple and more complex regression models, where its coefficient helps in understanding its influence on sales volume. ADVERT's inclusion in the regression analysis highlights its importance in assessing the impact of advertising expenditure on various outcomes.</data>
      <data key="d2">250ee5d766c64e7975bcc427b4bf9074,2e5e1bdaa9fcc7b3391d277fd6bb247a,336546bc73cbe1828a0cc1a45faf8f5a,8a9b984c146f59b2af83d1c2f373d376,a828fd17fc38e902484872c88a6b242c,b1690cb1a67892245c0665e5099e322d,c48bd062d5f6cc20c5df5758d6285562,e079b7c92d5c0b009ff02040eb652bc6</data>
    </node>
    <node id="RESIDUAL">
      <data key="d0">STATISTICAL_CONCEPT</data>
      <data key="d1">In the context of statistical analysis, RESIDUAL refers to the discrepancy or error between the observed data points and the values predicted by a statistical model. Specifically, for any given observation, the residual is calculated as the difference between the observation and the point on the model's line that shares the same x-coordinate. This measure is crucial for assessing the accuracy of the model's predictions and identifying any unusual observations that may significantly deviate from the model's expectations. The term "ri" specifically denotes the residual associated with the ith observation, serving as a key indicator in the detection of outliers within the dataset.</data>
      <data key="d2">09391efd3b8c510205098b548bc8dc74,2e5e1bdaa9fcc7b3391d277fd6bb247a</data>
    </node>
    <node id="RED_LINES">
      <data key="d0">GRAPHICAL_ELEMENT</data>
      <data key="d1">The red lines in the scatterplot illustrate the residuals. They are vertical lines that represent the difference between an observation and the point on the line that has the same x-coordinate as the observation.</data>
      <data key="d2">2e5e1bdaa9fcc7b3391d277fd6bb247a</data>
    </node>
    <node id="SLR_MODEL">
      <data key="d0">STATISTICAL_MODEL</data>
      <data key="d1">The simple linear regression model (slr.model) is computed using the lm() command in R. It is used to predict the number of units sold based on the price charged for the product.</data>
      <data key="d2">2e5e1bdaa9fcc7b3391d277fd6bb247a</data>
    </node>
    <node id="COEFFICIENT">
      <data key="d0">MODEL_PARAMETER</data>
      <data key="d1">The coefficient is a parameter in the fitted model. It represents the change in the expected value of the response variable for a one unit change in the predictor variable.</data>
      <data key="d2">2e5e1bdaa9fcc7b3391d277fd6bb247a</data>
    </node>
    <node id="BETA_2">
      <data key="d0">PARAMETER</data>
      <data key="d1">BETA_2 (&#946;2) is a significant parameter in both linear and quadratic regression models. In the context of linear regression, BETA_2 is the coefficient for the explanatory variable X2, indicating the average change in the response variable Y for a one unit increase in X2, assuming the value of X1 is held constant. This parameter is also associated with the log-transformed response in a regression model, showing the change in the log-transformed response per unit change in X2. Additionally, BETA_2 is the coefficient for the log of Height and the second explanatory variable in different linear regression models. In the extended linear regression model, BETA_2 is the coefficient for the advert variable. In the quadratic regression model, BETA_2 represents the coefficient of the quadratic term. It is important to note that &#946;2 is a regression coefficient in the log-transformed regression model, further emphasizing its role in handling transformations of the response variable.</data>
      <data key="d2">0fbc9037ca9a440e79e9ac05664b9b3d,21e429490eeefe7d9c245058fd48ca68,250ee5d766c64e7975bcc427b4bf9074,2d5cdecc342ddacd2c090f1838430cee,512d9ffebe309a6f944ebce1ae2ff2a3,7ad4ccec4c7bb3702aed71c17dc6b96f,9611ea31ff53888971694cdefe806f64,995fb26a0261f824952fa7b2fac3382e,a828fd17fc38e902484872c88a6b242c,e5131a1158e58f1b7b44b21ced7b6f60</data>
    </node>
    <node id="FIGURE_1_6">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 1.6 is a scatterplot showing the relationship between sales volume and the local advertising budget, with the line of best fit added to the plot</data>
      <data key="d2">250ee5d766c64e7975bcc427b4bf9074</data>
    </node>
    <node id="R_2">
      <data key="d0">SPACE</data>
      <data key="d1">R^2 is the two-dimensional space in which the simple linear regression model is represented</data>
      <data key="d2">250ee5d766c64e7975bcc427b4bf9074</data>
    </node>
    <node id="R_3">
      <data key="d0">SPACE</data>
      <data key="d1">R^3 is the three-dimensional space in which the extended linear regression model is represented</data>
      <data key="d2">250ee5d766c64e7975bcc427b4bf9074</data>
    </node>
    <node id="R2">
      <data key="d0">MATHEMATICAL_CONCEPT</data>
      <data key="d1">In the context of statistical analysis, R2, also known as R squared, holds a significant place. It is a statistical measure that quantifies the proportion of the variance for a dependent variable that is explained by an independent variable or variables in a regression model. This metric is pivotal in understanding the goodness of fit of the model, essentially indicating how well the model explains the variability of the response data around its mean. Points in R2 represent the observed data when there is one explanatory variable, highlighting its role in the visualization and interpretation of data in a single-dimensional explanatory context. Additionally, r2 is also used to denote a variable representing the squared standardized residual for the ith observation, which is a crucial component in residual analysis, aiding in assessing the model's accuracy and the quality of the fit. Overall, R2 serves as a comprehensive indicator in regression analysis, encapsulating the explanatory power of the model and the residual variability of the data.</data>
      <data key="d2">8a9b984c146f59b2af83d1c2f373d376,98d6982108f2d42fe0437bff8c666e17,c48bd062d5f6cc20c5df5758d6285562</data>
    </node>
    <node id="R3">
      <data key="d0">MATHEMATICAL_CONCEPT</data>
      <data key="d1">In the context of statistical analysis and algorithmic understanding, R3, also known as R cubed, signifies a three-dimensional Euclidean space. This space is characterized by points that are distinctly represented through three coordinates: x, y, and z. When applied to data analysis, points in R3 often represent observed data, particularly in scenarios involving two explanatory variables. This three-dimensional representation allows for a comprehensive visualization and interpretation of data relationships, facilitating deeper insights into the structure and dynamics of the community or phenomena under study. The use of R3 is pivotal in linear regression analysis, where it aids in understanding the least squares estimators and conducting residual analysis, thereby enhancing the accuracy and reliability of statistical models.</data>
      <data key="d2">8a9b984c146f59b2af83d1c2f373d376,c48bd062d5f6cc20c5df5758d6285562</data>
    </node>
    <node id="GRAPHICAL_ILLUSTRATION">
      <data key="d0">VISUALIZATION</data>
      <data key="d1">Graphical illustration refers to a visual representation of data or concepts, often used to explain complex ideas or relationships</data>
      <data key="d2">8a9b984c146f59b2af83d1c2f373d376</data>
    </node>
    <node id="HTML_VERSION">
      <data key="d0">DOCUMENT_FORMAT</data>
      <data key="d1">HTML version is a format of a document that can be displayed in a web browser and supports interactive graphics</data>
      <data key="d2">8a9b984c146f59b2af83d1c2f373d376</data>
    </node>
    <node id="3D_GRAPHICAL_REPRESENTATION">
      <data key="d0">VISUALIZATION</data>
      <data key="d1">3D graphical representation is a type of visualization that uses three dimensions to display data or concepts</data>
      <data key="d2">8a9b984c146f59b2af83d1c2f373d376</data>
    </node>
    <node id="DATAPPOINTS">
      <data key="d0">DATA</data>
      <data key="d1">Datapoints, the fundamental units of data analysis, are individual pieces of information that are crucial for statistical analysis and graphical representation. In the context of a 3D scatterplot, these datapoints are visualized as spheres. The plot employs a distinctive color scheme to enhance interpretability: datapoints located above the plane are depicted in black, while those below the plane are shown in grey. This color differentiation aids in understanding the spatial distribution and orientation of the datapoints in three-dimensional space.</data>
      <data key="d2">8a9b984c146f59b2af83d1c2f373d376,b1690cb1a67892245c0665e5099e322d</data>
    </node>
    <node id="REGRESSION_PLANE">
      <data key="d0">MATHEMATICAL_CONCEPT</data>
      <data key="d1">The REGRESSION_PLANE is a fundamental concept in statistical analysis, particularly within the context of multiple regression analysis. It represents a plane situated within a multidimensional space that optimally aligns with the data points, aiming to minimize the distances between the points and the plane. This plane is instrumental in illustrating the intricate relationships between variables, such as sales, price, and advertising expenditure, in a three-dimensional scatterplot. By visualizing these relationships, analysts can gain deeper insights into how changes in price and advertising might influence sales, providing a comprehensive understanding of the dynamics at play in the market. The regression plane serves as a powerful tool for predictive modeling and understanding the structure of the community of interest, enabling data scientists to make informed decisions based on statistical inference.</data>
      <data key="d2">8a9b984c146f59b2af83d1c2f373d376,b1690cb1a67892245c0665e5099e322d</data>
    </node>
    <node id="PDF_FORMAT">
      <data key="d0">DOCUMENT_FORMAT</data>
      <data key="d1">The PDF_FORMAT, also known as the Portable Document Format, is a document format primarily designed to present and exchange documents reliably, regardless of the application, hardware, or operating system in which they were created. This ensures that the document's layout and appearance remain consistent across different platforms. However, it is important to note that the PDF_FORMAT does not support interactive graphics, only static images, which limits its capabilities in terms of dynamic content presentation.</data>
      <data key="d2">8a9b984c146f59b2af83d1c2f373d376,b1690cb1a67892245c0665e5099e322d</data>
    </node>
    <node id="STATIC_GRAPHIC">
      <data key="d0">VISUALIZATION</data>
      <data key="d1">Static graphic is a type of visualization that does not change or respond to user input, often used in documents or presentations</data>
      <data key="d2">8a9b984c146f59b2af83d1c2f373d376</data>
    </node>
    <node id="PLANE">
      <data key="d0">GEOMETRIC_SHAPE</data>
      <data key="d1">The PLANE, in the context of statistical analysis, serves as a geometric representation to summarize the relationship between a response variable and two explanatory variables. Specifically, it is used to illustrate the connection between sales (the response variable) and price and advertising (the two explanatory variables) in a three-dimensional scatterplot. This plane helps in visualizing how changes in price and advertising affect sales, providing a comprehensive view of the interplay between these variables in a marketing or sales scenario.</data>
      <data key="d2">b1690cb1a67892245c0665e5099e322d,c48bd062d5f6cc20c5df5758d6285562</data>
    </node>
    <node id="Z_AXIS">
      <data key="d0">COORDINATE_AXIS</data>
      <data key="d1">The z-axis is one of the three axes in a 3D coordinate system, used to measure the distance between the observed datapoints and the regression plane</data>
      <data key="d2">b1690cb1a67892245c0665e5099e322d</data>
    </node>
    <node id="FIGURE_1_7">
      <data key="d0">GRAPHICAL_REPRESENTATION</data>
      <data key="d1">Figure 1.7 is a 3D scatterplot of the Retail data including the plane of best fit describing the relationship between sales, price, and advert</data>
      <data key="d2">b1690cb1a67892245c0665e5099e322d</data>
    </node>
    <node id="RETAIL_DATA">
      <data key="d0">DATA_SET</data>
      <data key="d1">The comprehensive summary of the "RETAIL_DATA" dataset reveals its multifaceted utility in the field of statistical analysis and modeling. This dataset is rich in information about sales, prices, and brands, serving as a valuable resource for understanding market dynamics. It is specifically utilized in the practice of reparameterising multiple regression models, enhancing the skills of data scientists in model fitting processes. The "RETAIL_DATA" is not only a tool for analysis but also a fundamental component in the construction and evaluation of regression models, providing insights into the relationships between sales, prices, and brand performance. This dataset's comprehensive nature makes it an essential asset for anyone looking to delve into the intricacies of retail market analysis.</data>
      <data key="d2">2ced3e26eaed2dfcd8e4caf49737cab4,825b600cbab3535ce67e9f561ddcb84b,86ece4718d27d1a6c6a1f448cc850e2b,a1fc936df848a0fbc791e4bcc9b527b6,ac15b639b0849006471dfe102376c2c0,b1690cb1a67892245c0665e5099e322d</data>
    </node>
    <node id="EXPLANATORY_VARIABLE">
      <data key="d0">VARIABLE</data>
      <data key="d1">The Explanatory Variable, often denoted as "x", is a pivotal component in statistical models, particularly in regression analysis. It serves as the predictor or independent variable, with its values for each unit of observation (xj) utilized in the model equation to forecast the response variable. In the context of the given descriptions, the Explanatory Variable is synonymous with brand, which is considered a potential predictor of sales volume. Its role is to elucidate or predict the variation in the response variable, thereby providing insights into the relationship between the brand and sales volume within a regression model. This variable is essential for understanding the structure and relations within the community of interest, as it helps in explaining the dynamics that influence the response variable.</data>
      <data key="d2">7347b44ffb25a066e43321f4eaf5a806,82cfcd5865cffe55e965a50745656e60,b9af17718641389ba07f53be13f31f8c,b9eb75001a4f68f7240b2ca9e0d79eb8,b9ec8a6c7960cc6196ec94fd976f05b0,bb31f1c77bbba73300f735a100086a67,c48bd062d5f6cc20c5df5758d6285562,e079b7c92d5c0b009ff02040eb652bc6,ef24ca5edd06893b737e6a1c8a9825f6</data>
    </node>
    <node id="OBSERVED_DATA">
      <data key="d0">DATA</data>
      <data key="d1">Observed data consists of values for the response variable and one or more explanatory variables</data>
      <data key="d2">c48bd062d5f6cc20c5df5758d6285562</data>
    </node>
    <node id="MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">The entity "MODEL" in this context refers to a statistical model, specifically a parametric formula that effectively links the response variable with one or more explanatory variables. This model is designed to account for the stochastic nature of the response, making it a representation of reality that is particularly useful for description, prediction, and explanation. The model is constructed using the lm() function, a method for fitting linear models, indicating that the relationship between the variables is modeled linearly. This comprehensive model takes into consideration the inherent randomness in the response variable while establishing a structured relationship with the explanatory variables, making it a powerful tool for statistical analysis.</data>
      <data key="d2">22061b1108c7f9963497b7a320be22b8,7cd6069e88e81548a237fa937adfecc6,bb31f1c77bbba73300f735a100086a67,c48bd062d5f6cc20c5df5758d6285562</data>
    </node>
    <node id="BETA0">
      <data key="d0">PARAMETER</data>
      <data key="d1">Beta0 (&#946;0) is a pivotal parameter in the linear regression model, serving as the intercept that delineates the point where the regression line crosses the y-axis. This parameter is estimated through various methods, most commonly as the intercept in the least squares estimation, calculated specifically as ybar - (Sxy/Sxx)xbar, where ybar and xbar represent the mean values of the dependent and independent variables, respectively. Additionally, Beta0 can be understood as y&#773; - &#946;1x&#773; in simple linear regression, where &#946;1 is the slope parameter. It is noteworthy that Beta0's role extends beyond simple linear regression, encompassing its significance in multiple regression models and even log-transformed models, where it continues to represent the intercept term. This parameter is crucial for understanding the baseline value of the dependent variable when all independent variables are zero.</data>
      <data key="d2">09caa54ca1372d152e47051be4d44ede,254a8a17b1be06702934341e3bf41e85,28cf5ff0c09fa5c0390267bb9aa3ce47,34fceaaf7d835828b5ee2327325c37f8,5b24b5382abe9d1898810b3e4b9b455a,60cc94e681863c9fcc6f9be1e500f840,86c401dda130c2d201c3339526062a24,87b717ba065d6d7c7431af284137eb12,90ed6030c5a5a0764b7dcd4115b4d4d3,b5d0a103e1f34a00aef67fedd0e8c693,c48bd062d5f6cc20c5df5758d6285562</data>
    </node>
    <node id="BETA1">
      <data key="d0">PARAMETER</data>
      <data key="d1">BETA1, denoted as &#946;1, is a crucial parameter in various statistical models, predominantly in linear regression. It is the slope parameter estimate, representing the coefficient of the first predictor variable (xi1) or the explanatory variable (xi,1), indicating the relationship between the dependent variable and the independent variable. BETA1 is calculated as Sxy/Sxx, the solution to the partial derivatives set to zero, which quantifies the change in the dependent variable for a unit change in the independent variable, holding all other variables constant. In the context of log-transformed models, BETA1 signifies the slope coefficient between log(BODYi) and log(BRAINi). Additionally, BETA1 is a parameter in polynomial and multiple regression models, contributing to the linear term of the relationship between Y and X, and is one of the slope parameters in the linear regression model. Overall, BETA1 is a fundamental component in statistical analysis, providing insights into the strength and direction of the relationship between variables.</data>
      <data key="d2">09caa54ca1372d152e47051be4d44ede,11452a08471d93959558de2ece9a69af,254a8a17b1be06702934341e3bf41e85,28cf5ff0c09fa5c0390267bb9aa3ce47,34fceaaf7d835828b5ee2327325c37f8,45f31b040576e9f3b4def6d0466cc016,5b24b5382abe9d1898810b3e4b9b455a,60cc94e681863c9fcc6f9be1e500f840,86c401dda130c2d201c3339526062a24,87b717ba065d6d7c7431af284137eb12,90ed6030c5a5a0764b7dcd4115b4d4d3,b5d0a103e1f34a00aef67fedd0e8c693,c48bd062d5f6cc20c5df5758d6285562,c9a01b92d11585f6549f62e8bd78d652</data>
    </node>
    <node id="BETA2">
      <data key="d0">PARAMETER</data>
      <data key="d1">BETA2, denoted as &#946;2, is a significant parameter in various statistical models, primarily featuring in linear regression analyses. In the context of linear regression, BETA2 represents the coefficient of the second predictor variable, xi2, indicating the change in the dependent variable for a unit change in xi2, assuming all other variables are held constant. This parameter is also associated with the explanatory variable xi,2 in a logarithmic transformation, suggesting its role in models where the relationship between the dependent and independent variables is not linear but logarithmic.

Moreover, BETA2 extends its importance to polynomial regression models, where it contributes to the quadratic term, capturing the curvature in the relationship between the dependent variable Y and the independent variable X. This highlights BETA2's versatility in modeling non-linear relationships.

Additionally, in the log-transformed model, &#946;2 is the coefficient for the height of the tree, indicating its specific application in models where the height of a tree is a variable of interest. This parameter's role in the log-transformed model suggests its relevance in adjusting for non-linear effects that might arise from the natural logarithmic transformation of the data.

Overall, BETA2 is a crucial slope parameter in regression models, playing a pivotal role in understanding and quantifying the relationship between the dependent variable and one or more independent variables, particularly in contexts involving logarithmic transformations and polynomial relationships.</data>
      <data key="d2">11452a08471d93959558de2ece9a69af,34fceaaf7d835828b5ee2327325c37f8,45f31b040576e9f3b4def6d0466cc016,60cc94e681863c9fcc6f9be1e500f840,87b717ba065d6d7c7431af284137eb12,90ed6030c5a5a0764b7dcd4115b4d4d3,c48bd062d5f6cc20c5df5758d6285562,c9a01b92d11585f6549f62e8bd78d652</data>
    </node>
    <node id="MLR_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">Mlr.model is the multiple linear regression model fitted using the lm() function</data>
      <data key="d2">c48bd062d5f6cc20c5df5758d6285562</data>
    </node>
    <node id="AVERAGE_EFFECT">
      <data key="d0">CONCEPT</data>
      <data key="d1">The average effect of an explanatory variable is the change in the response variable when the explanatory variable is increased by one unit, assuming all other variables are held constant</data>
      <data key="d2">c48bd062d5f6cc20c5df5758d6285562</data>
    </node>
    <node id="X1">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">X1 is a key quantitative explanatory variable in multiple regression models, serving as a predictor in both linear and log-transformed regression contexts. Specifically, in the sales example, X1 represents the price of the product, highlighting its role as a significant factor that may influence the response variable. The variable's coefficient, &#946;1, quantifies the average effect on the response variable when X1's value is incremented by one unit. As one of the k quantitative explanatory variables in the linear model, X1 can take various values, denoted as x1. Alongside other predictors, X1 contributes to the least squares estimation process, where X1 to Xn represent a series of observed values crucial for estimating model parameters. X1's multifaceted role across different models underscores its importance in statistical analysis, particularly in understanding the relationships between variables and the response.</data>
      <data key="d2">0fbc9037ca9a440e79e9ac05664b9b3d,21e429490eeefe7d9c245058fd48ca68,512d9ffebe309a6f944ebce1ae2ff2a3,56ff186fc629e1e42f2759fc4b984199,6a47154bf457c25f22c3cf9f649c5db0,75dc4d8cb195a1f969d9e9496631086b,995fb26a0261f824952fa7b2fac3382e,a828fd17fc38e902484872c88a6b242c</data>
    </node>
    <node id="X2">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">X2 is a significant predictor variable in the context of multiple regression models. In the linear regression model, X2 is an explanatory variable with a specific coefficient (&#946;2), which quantifies the average impact on the response variable when X2 is incremented by one unit. This variable is also present in a log-transformed regression model, indicating its versatility in different analytical frameworks. In a sales scenario, X2 specifically represents the local advertising budget, highlighting its role in influencing sales outcomes. As one of k quantitative explanatory variables in the linear model, X2 can assume various values (x2), showcasing its dynamic nature. Its inclusion as the second explanatory variable in the model further underscores its importance in potentially affecting the response variable. Throughout these descriptions, X2 is consistently characterized as a fixed element in the given scenarios, ensuring stability in the analysis.</data>
      <data key="d2">0fbc9037ca9a440e79e9ac05664b9b3d,21e429490eeefe7d9c245058fd48ca68,512d9ffebe309a6f944ebce1ae2ff2a3,6a47154bf457c25f22c3cf9f649c5db0,995fb26a0261f824952fa7b2fac3382e,a828fd17fc38e902484872c88a6b242c</data>
    </node>
    <node id="ADVERTISING_BUDGET">
      <data key="d0">BUDGET</data>
      <data key="d1">The local advertising budget is a financial allocation for promoting products or services in a specific area. An increase in this budget is associated with an increase in sales.&gt;</data>
      <data key="d2">426434b67f6a287852ab66b82ca873cf</data>
    </node>
    <node id="PARTIAL_REGRESSION_COEFFICIENTS">
      <data key="d0">PARAMETERS</data>
      <data key="d1">Partial regression coefficients are parameters in a multiple regression model that represent the expected change in the dependent variable for a unit change in an explanatory variable, adjusting for the effects of other explanatory variables.&gt;</data>
      <data key="d2">426434b67f6a287852ab66b82ca873cf</data>
    </node>
    <node id="DATASET">
      <data key="d0">DATA</data>
      <data key="d1">The DATASET is a comprehensive collection of data points, specifically designed for simple linear regression analysis. It features pairs of xi and yi values, where xi represents the explanatory variables and yi corresponds to the observed outcomes. This dataset is particularly noteworthy for its inclusion of paired observations of price and other relevant explanatory variables, providing a rich resource for understanding the relationships between these factors and the units of observation they represent. The dataset's structure and content make it an ideal tool for conducting detailed statistical analyses, particularly in the context of regression modeling, allowing for the exploration of how price and other variables interact to influence the outcomes of interest.</data>
      <data key="d2">2ced3e26eaed2dfcd8e4caf49737cab4,428db872e71a17a2cf7868b03a52def0,5b24b5382abe9d1898810b3e4b9b455a</data>
    </node>
    <node id="EXERCISE_4">
      <data key="d0">EXERCISE</data>
      <data key="d1">Exercise 4 is a practice exercise that involves reparameterising the multiple regression model for the Retail data to give the intercept an interpretation without the need for extrapolation</data>
      <data key="d2">2ced3e26eaed2dfcd8e4caf49737cab4</data>
    </node>
    <node id="LINEAR_MODELS">
      <data key="d0">CLASS_OF_MODELS</data>
      <data key="d1">Linear models, a class of statistical models that includes simple linear regression and multiple linear regression, are employed to analyze the relationship between a dependent variable and one or more independent variables. These models are characterized by their assumption of a linear relationship between the dependent and independent variables, even though they can be linear in the parameters but not necessarily in the explanatory variables. This flexibility allows for the inclusion of polynomials of the explanatory variables, such as in polynomial regression, or other non-linear transformations like logarithms. Linear models can accommodate both quantitative and categorical predictor variables, making them versatile tools for understanding the effect of each independent variable on the dependent variable. The results of linear models can be interpreted to provide insights into how changes in the independent variables are associated with changes in the dependent variable, facilitating a deeper understanding of the relationships within the data.</data>
      <data key="d2">0328e428a30c44572676dd571dd1e9bd,07951ffe6787af44aa60c90c69e62f83,2ced3e26eaed2dfcd8e4caf49737cab4,77e76692753fdf53493182b09018e6bc,87ba4f416a28aabc3b396908f5913b54,d71b402ab9edbb4347e09c7af3257cf5</data>
    </node>
    <node id="Y_GIVEN_X">
      <data key="d0">CONDITIONAL_EXPECTATION</data>
      <data key="d1">E(Y|X=x) is the conditional expectation of Y given X, which is equal to &#946;0 + &#946;1x</data>
      <data key="d2">9a27580975988e83f6e3a0d9010893b5</data>
    </node>
    <node id="Y_VECTOR">
      <data key="d0">VECTOR</data>
      <data key="d1">Y is the column vector of observed values of Y in the linear regression model</data>
      <data key="d2">9a27580975988e83f6e3a0d9010893b5</data>
    </node>
    <node id="X_MATRIX">
      <data key="d0">MATRIX</data>
      <data key="d1">X is the n x 2 matrix in the linear regression model, containing the predictor variable X and a column of ones for the intercept</data>
      <data key="d2">9a27580975988e83f6e3a0d9010893b5</data>
    </node>
    <node id="BETA">
      <data key="d0">PARAMETER_VECTOR</data>
      <data key="d1">BETA, often represented as &#946;, is a critical parameter in the context of linear regression models. It is the vector of parameters that includes the intercept and coefficients of the explanatory variables, playing a pivotal role in understanding the relationships between the predictors and the response variable. Specifically, BETA represents the true coefficients of the predictors, with a particular emphasis on the effect of price on sales volume and sales in general. This parameter is fundamental in various models, such as the simple linear regression model, where it consists of the intercept (&#946;0) and the slope (&#946;1), and in more complex models where it includes coefficients for multiple explanatory variables (&#946;1, ..., &#946;q). BETA is also significant in the parallel lines model, where it denotes the effect of price on sales, and in the reparameterised model, where it captures the impact of price on sales volume for brand A. In the statistical and normal linear models, BETA represents the coefficients of the predictors, highlighting its versatility across different analytical frameworks. As a p-dimensional parameter vector, BETA is assumed to be unknown but fixed, and it is estimated to understand the underlying structure of the data. In the regression model, BETA is associated with the c_price predictor, and its estimate, such as -0.539, indicates the change in sales volume for a unit change in price. Overall, BETA is a comprehensive representation of the coefficients in the linear regression model, encapsulating the effects of various predictors on the response variable.</data>
      <data key="d2">01d5ee79489582b4135fc96f676b24a0,06d5666e6bfdda828b48adba883b4a61,0cb40986e6c2bb439e1ffcaae2df96ac,119bc73ddf8eebadfb8eae272fa323a7,1b523d1edabe381403fc470a9b8d47fa,1d141ab04db553f78a313e430e54abb5,2167274129d4cfa74a002c4cc39df8a8,21ec28dfe2b2c18030d541d63e51f45e,228bdca7843406def245d755e8df49f6,248924760a2bfbc82501fd6b11cfa0aa,255685e281cc5a9edf073c700f425a6b,2673d078d29f2af78fab9b6eacd15e37,28cf5ff0c09fa5c0390267bb9aa3ce47,2b6d31b6bff4eae3a4809451c4fb9fa6,2de7a36b32bf79c8f32612c8aaa9daa8,3cbe71f7649e84cd67cb3fa0d3e632cf,3d357cfa3ef0d00f49cf4acaeac1c9d1,542f546c5a131196e4701fb33c9b1dee,5609007c6229060ffc85d8056a7fefde,5a0d392715f06d5e873f45ae06aa729a,6b55b41598d5264f8dc6b72769748722,6c1684ed2a4840576c6b0f4d1a3a482f,6c66e9414880964ee899ceb0f16d22e9,74d190f10bf6e6936242ca3cdfc4a09f,7955aae3fd4ca51b9ef8843e13c1f517,7a605c3b689bec7ab2c46df9c123e3f3,7ad4ccec4c7bb3702aed71c17dc6b96f,7d074208b1259e7d84f9f870d3828bb6,7fc5b8303ab530821bf2140ba6a8a889,825b600cbab3535ce67e9f561ddcb84b,82932abd152e0b84a1c26a2daa4c08df,8326c645426789920a99ed373725fa0e,8f1d95acff56e1633dceb775fa713174,9005147593b2f27b9e2a5eede3601bdc,906eb7d6b49fa360e7e5b65c56cd4d76,925e17c26fb7d979f52538f4632333e7,93da9813e10a119798de6982977f1239,9854704301b8df256ca1013b8d53dfac,9a27580975988e83f6e3a0d9010893b5,9d300fc83afb3261af61b2ab9721cadc,9dddcd96af7b557e578b3f5f36efacd7,9fc2b1e8b2b61b557f88eb9e9c708597,a1fc936df848a0fbc791e4bcc9b527b6,a4a817bb79d6ae8812c808ca41d47f43,aa195e72eb5285a4bcae9c856af30a87,aac5b4f040b9c773bd1aa696dec469f6,ac15b639b0849006471dfe102376c2c0,ad799500572246a07f983a3b92c0e61f,aeddef300427d211c74c6008b5b6b328,b5d0a103e1f34a00aef67fedd0e8c693,b6870535f3975c49d45e62fbe475f198,b70a75a6412b2e5c44af50734844f4be,bd05fe6a05f9a13d33c4f1b5a771ada5,d14413709de2897231aaa83be3aa346f,d738df7d83784c8a41b3948271c537b6,d94760a5f9f6ea115fcc18024035a627,dd7e7d54883ca0f687568a738b95d4d0,e41cc40f061f487b1ea0f256d4a963e4,e4f14e6785c6d7b7469e695aaeb170d0,e7494d6cfc3e38a4d2f3f6b21ef6445d,e7edd8b2874a350779ae20f1ecdf4733,eac62cdd5518e1269fed150639331c2c,ee295ebc4c2d6a2a5c738796fcc2ab71,f483798b15ef305e7826fd7142379e03,f5716ce115458c0652124734ca344806,f632f01188d2c6e3091a965580cb4600,f9b615b879f72501f338f8983d4cac3d,fc5b725f3c662c5471af20efdcc2dbff</data>
    </node>
    <node id="EPSILON_VECTOR">
      <data key="d0">RANDOM_VARIABLE_VECTOR</data>
      <data key="d1">Epsilon (&#1013;) is the vector of errors in the linear regression model, corresponding to the error term in the equation Y = X&#946; + &#1013;</data>
      <data key="d2">9a27580975988e83f6e3a0d9010893b5</data>
    </node>
    <node id="NN">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">NN, often denoted as Nn, is a multivariate normal distribution of dimension n. This distribution is characterized by a mean vector that can either be an n-vector of zeros or X&#946;, depending on the context. The variance-covariance matrix for Nn is consistently specified as &#963;^2In, where &#963;^2 represents the variance and In is the identity matrix of dimension n. In statistical models, Nn is frequently used to represent the distribution of the error term, often denoted as &#981; (epsilon), assuming it follows a normal distribution with mean 0 and covariance matrix &#963;^2In. Additionally, the variable Y is assumed to follow a multivariate normal distribution with a mean vector X&#946; and the same covariance matrix &#963;^2In, indicating that Y's distribution is influenced by the linear predictor X&#946;.</data>
      <data key="d2">0ac60299320c55d642b3e38440c25f90,3d357cfa3ef0d00f49cf4acaeac1c9d1,45f31b040576e9f3b4def6d0466cc016,542f546c5a131196e4701fb33c9b1dee,67e4c1866b0c6e162e6e3317949e8da9,75dc4d8cb195a1f969d9e9496631086b,8f1d95acff56e1633dceb775fa713174,9005147593b2f27b9e2a5eede3601bdc,9dddcd96af7b557e578b3f5f36efacd7,b70a75a6412b2e5c44af50734844f4be,dd7e7d54883ca0f687568a738b95d4d0,e41cc40f061f487b1ea0f256d4a963e4,e4f14e6785c6d7b7469e695aaeb170d0</data>
    </node>
    <node id="IN">
      <data key="d0">MATRIX</data>
      <data key="d1">The entity "IN" refers to the n x n identity matrix, a fundamental component in various statistical analyses and calculations within the context of linear regression models. This matrix, characterized by ones on the diagonal and zeros elsewhere, plays a crucial role in the covariance matrix of the linear regression model, aiding in the calculation of the error term's covariance matrix, the variance-covariance matrix of the multivariate normal distribution, and the normal linear model. Additionally, "IN" is integral in the computation of residuals, specifically through the expression (In - H)y, where H represents the hat matrix. It is also used in determining the residual vector Eb and assessing the influence of data points on the fitted model. The trace of matrix "IN" is equal to n, where n signifies the number of observations, highlighting its direct relationship with the dataset's size. This matrix is pivotal in understanding the properties of the hat matrix H and is a cornerstone in statistical inference, particularly in evaluating the linear model's performance and reliability.</data>
      <data key="d2">1da117a2f92b2db00290d2a0bfc06beb,2685edb9e8031c8ea725c43a40af22a8,3d357cfa3ef0d00f49cf4acaeac1c9d1,3fb977ccba63e267d2e7dd4de6479ce1,45f31b040576e9f3b4def6d0466cc016,46629f2efc6c82e81265a131b4bab2ee,542f546c5a131196e4701fb33c9b1dee,5a0d392715f06d5e873f45ae06aa729a,679722cf8ce5ce5aee4e379528470efe,67e4c1866b0c6e162e6e3317949e8da9,74d190f10bf6e6936242ca3cdfc4a09f,75dc4d8cb195a1f969d9e9496631086b,7ad4ccec4c7bb3702aed71c17dc6b96f,7d074208b1259e7d84f9f870d3828bb6,8f1d95acff56e1633dceb775fa713174,9005147593b2f27b9e2a5eede3601bdc,9dddcd96af7b557e578b3f5f36efacd7,b70a75a6412b2e5c44af50734844f4be,bd98ac7b4b5df4f63e7ecc8f4a821f57,dd7e7d54883ca0f687568a738b95d4d0,e41cc40f061f487b1ea0f256d4a963e4,e4f14e6785c6d7b7469e695aaeb170d0,e593096f3805c2686423cb91ea276fe6,e7edd8b2874a350779ae20f1ecdf4733,f470791d2d3fedede166f9bb11598c9c,f483798b15ef305e7826fd7142379e03</data>
    </node>
    <node id="XK">
      <data key="d0">PREDICTOR_VARIABLE</data>
      <data key="d1">XK, denoted as Xk, is one of the k quantitative explanatory variables incorporated in both multiple regression and linear model analyses. Xk plays a significant role in these statistical models by taking on a range of values, represented as xk, thereby contributing to the explanation of the variability in the dependent variable. As a quantitative variable, Xk provides numerical data that can be used to establish relationships and make predictions within the context of the models.</data>
      <data key="d2">6a47154bf457c25f22c3cf9f649c5db0,75dc4d8cb195a1f969d9e9496631086b</data>
    </node>
    <node id="XJM">
      <data key="d0">PREDICTOR_VARIABLE_VALUE</data>
      <data key="d1">XJM, denoted as xjm in some contexts, represents the value of the mth explanatory variable, Xm, for the jth unit of observation. This variable plays a crucial role in statistical models, particularly in regression analysis, where it helps in understanding the relationship between the dependent variable and a set of independent variables. XJM's value for each observation unit contributes to the overall structure of the data, enabling the estimation of model parameters and the assessment of their significance in explaining the variability of the dependent variable.</data>
      <data key="d2">75dc4d8cb195a1f969d9e9496631086b,b5d0a103e1f34a00aef67fedd0e8c693</data>
    </node>
    <node id="BETA_M">
      <data key="d0">PARAMETER</data>
      <data key="d1">&#946;m is one of the coefficients for the mth explanatory variable in the multiple regression model</data>
      <data key="d2">75dc4d8cb195a1f969d9e9496631086b</data>
    </node>
    <node id="XN">
      <data key="d0">VECTOR</data>
      <data key="d1">XN, in the context of statistical analysis and algorithmic understanding, serves dual roles that complement each other in the realm of data science. Primarily, XN denotes the vector of explanatory variables, encompassing x1, x2, ..., xk, for a given observation. This vector is crucial for modeling and understanding the relationships between various factors within a dataset, particularly in regression analysis where these variables are used to predict or explain the variability of a response variable.

Additionally, Xn (which can be considered a variant of XN in certain contexts) is also a function that performs a summation operation over the elements of a vector or matrix. This function is essential for various calculations in data analysis, such as computing totals, averages, or other aggregate statistics. The summation over n elements facilitated by Xn is a fundamental operation in statistical inference, enabling the computation of sums of squares, sums of products, and other key metrics in regression analysis and beyond.

In summary, XN is a versatile concept in data science, serving as both a vector of explanatory variables and a summation function, playing a pivotal role in the analysis and interpretation of complex statistical models.</data>
      <data key="d2">10ac76f99674a01ca0f4a55586dea07e,3fdeeb7593174f5e8a9cff55a7cd92e3,b5d0a103e1f34a00aef67fedd0e8c693</data>
    </node>
    <node id="BETAK">
      <data key="d0">PARAMETER</data>
      <data key="d1">BETAK, denoted as Beta_k, is a significant parameter in the context of regression analysis. It specifically features as one of the slope parameters in the linear regression model, highlighting its role in quantifying the relationship between the dependent variable and the kth explanatory variable. In the broader framework of the multiple regression model, BETAK serves as the coefficient for the kth explanatory variable, encapsulating the unique contribution of this variable to the prediction of the dependent variable, while controlling for the effects of other variables in the model. This dual representation of BETAK underscores its importance in understanding the nuanced relationships within the data, making it a critical component in statistical inference and predictive modeling.</data>
      <data key="d2">87b717ba065d6d7c7431af284137eb12,b5d0a103e1f34a00aef67fedd0e8c693</data>
    </node>
    <node id="EPSILONJ">
      <data key="d0">ERROR_TERM</data>
      <data key="d1">EpsilonJ (&#949;j), also denoted as Epsilon_j, is the error term for the jth observation in the linear regression model. It represents the unobserved random error associated with the jth unit of observation or the jth store in the context of the parallel lines model. EpsilonJ is assumed to follow an independent and identically distributed (iid) normal distribution with a mean of 0 and a variance of &#963;^2, denoted as N(0, &#963;^2). This assumption is crucial for the validity of the linear regression model and the reliability of the least squares estimators. EpsilonJ encapsulates the variability in the data that cannot be explained by the model, playing a critical role in residual analysis and statistical inference.</data>
      <data key="d2">87b717ba065d6d7c7431af284137eb12,925e17c26fb7d979f52538f4632333e7,b5d0a103e1f34a00aef67fedd0e8c693,d1b6fcd55d937c5fe2d6add69e0bcf05,e6f79ceb0df54119a4dc71b2162ac50b</data>
    </node>
    <node id="N">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">In the context of statistical analysis and linear regression models, N, also denoted as n, represents the total number of observations or data points in the dataset. Specifically, N is confirmed to be 96 in this case, which is the sample size or the length of the vector X1 to Xn, indicating the number of units of observation. It is crucial in the regression model as it influences the reliability and significance of the results. Additionally, the normal distribution with mean 0 and variance &#963;^2 is assumed for the error terms (epsilon) in the linear regression model, which is a fundamental assumption for the validity of least squares estimators and residual analysis. This distribution characterizes the errors in the model, providing a basis for statistical inference and hypothesis testing. The comprehensive understanding of N as the number of observations and the normal distribution of errors is essential for interpreting the structure and relations within the dataset and the community of interest.</data>
      <data key="d2">00e186a86624c01ce873dd577df68d17,09391efd3b8c510205098b548bc8dc74,0b650eb2f1dcd603b64fec3c4b5cd24b,228bdca7843406def245d755e8df49f6,25fce1af816975003128126b5cfea73b,2d5cdecc342ddacd2c090f1838430cee,56ff186fc629e1e42f2759fc4b984199,6648f0d6deed51fb4fb25e6992a71ddf,6a6f85d0a6e46196ab3a901fcc82a720,6c1684ed2a4840576c6b0f4d1a3a482f,87b717ba065d6d7c7431af284137eb12,8f7a05b6d231105a6194eebdb2df372e,90b7e0427699cc1bb461e37939935138,98d6982108f2d42fe0437bff8c666e17,9923e77ac6b3de95cb5026bc5e7fe8c0,b5d0a103e1f34a00aef67fedd0e8c693,bd05fe6a05f9a13d33c4f1b5a771ada5,bd98ac7b4b5df4f63e7ecc8f4a821f57,d738df7d83784c8a41b3948271c537b6,e4f14e6785c6d7b7469e695aaeb170d0,e7edd8b2874a350779ae20f1ecdf4733,ee22e1f5947947f9bd3f7f8922745e48,fc5b725f3c662c5471af20efdcc2dbff</data>
    </node>
    <node id="YN">
      <data key="d0">DEPENDENT_VARIABLE</data>
      <data key="d1">In the context of the linear regression model, YN represents the dependent variable for the nth observation. YN, along with Y1 through Y(n-1), are the response variables that are assumed to have a constant variance and follow a normal distribution. Each Yn is an independent random variable that represents the responses in the model, highlighting the role of YN as a crucial component in the statistical analysis of the relationship between the dependent and independent variables.</data>
      <data key="d2">0da640a09a395a50b6e16e047fa8d0d6,8f1d95acff56e1633dceb775fa713174,9005147593b2f27b9e2a5eede3601bdc</data>
    </node>
    <node id="BETA_K">
      <data key="d0">PARAMETER</data>
      <data key="d1">BETA_K, denoted as \u03b2k, is a crucial parameter in the linear regression model. It specifically represents the coefficient of the kth explanatory variable, xk, within the model. This coefficient quantifies the relationship between the explanatory variable and the response variable, indicating the change in the response variable for a unit change in the explanatory variable, assuming all other variables are held constant. As one of the parameters in the linear regression model, BETA_K plays a significant role in understanding and predicting the behavior of the community of interest based on the explanatory variable's influence.</data>
      <data key="d2">67e4c1866b0c6e162e6e3317949e8da9,6a47154bf457c25f22c3cf9f649c5db0,7ad4ccec4c7bb3702aed71c17dc6b96f,8f1d95acff56e1633dceb775fa713174</data>
    </node>
    <node id="EPSILON_N">
      <data key="d0">ERROR_TERM</data>
      <data key="d1">EPSILON_n (&#1013;n) is the error term for the nth observation in the linear regression model</data>
      <data key="d2">8f1d95acff56e1633dceb775fa713174</data>
    </node>
    <node id="LINEAR_PREDICTOR">
      <data key="d0">FUNCTION</data>
      <data key="d1">The LINEAR_PREDICTOR is a fundamental concept in statistical modeling, particularly within the context of linear regression analysis. It is mathematically represented as the expression \u03b20 + \u03b21x1 + ... + \u03b2kxk, where \u03b20, \u03b21, ..., \u03b2k are the model parameters that are linear in nature. These parameters are estimated to best fit the data, aiming to predict the mean function of the response variable. The LINEAR_PREDICTOR serves as a crucial component in understanding the relationship between the predictor variables (x1, x2, ..., xk) and the response variable, enabling the model to make predictions based on the linear combination of these variables.</data>
      <data key="d2">67e4c1866b0c6e162e6e3317949e8da9,6a47154bf457c25f22c3cf9f649c5db0</data>
    </node>
    <node id="X_1">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">X_1 (x1) is the first explanatory variable in the linear regression model</data>
      <data key="d2">67e4c1866b0c6e162e6e3317949e8da9</data>
    </node>
    <node id="X_K">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">X_k (xk) is the kth explanatory variable in the linear regression model</data>
      <data key="d2">67e4c1866b0c6e162e6e3317949e8da9</data>
    </node>
    <node id="VARIANCE_FUNCTION">
      <data key="d0">FUNCTION</data>
      <data key="d1">Variance function of the response is given by Var (Y | X1 = x1, ..., Xk = xk) = Var (&#1013;) = &#963;^2, and is constant</data>
      <data key="d2">67e4c1866b0c6e162e6e3317949e8da9</data>
    </node>
    <node id="Y1">
      <data key="d0">RANDOM_VARIABLE</data>
      <data key="d1">Y1 is one of the n independent random variables representing the responses in the linear regression model</data>
      <data key="d2">9005147593b2f27b9e2a5eede3601bdc</data>
    </node>
    <node id="E_Y">
      <data key="d0">EXPECTED_VALUE</data>
      <data key="d1">E_Y, denoted as E(Y), represents the expected value of the response variable Y. This expected value can be understood as the mean function of the response vector Y and is given by X&#946;, where &#946; is the vector of coefficients in a statistical model. In the context of a lognormal distribution, E(Y) can also be calculated using the formula exp(&#956; + &#963;^2/2), where &#956; is the mean of the natural logarithm of Y, and &#963;^2 is the variance of the natural logarithm of Y. This comprehensive definition of E_Y consolidates its role as the mean of the response variable in various statistical frameworks.</data>
      <data key="d2">21e429490eeefe7d9c245058fd48ca68,7fc5b8303ab530821bf2140ba6a8a889,e41cc40f061f487b1ea0f256d4a963e4</data>
    </node>
    <node id="X_BETA">
      <data key="d0">PRODUCT</data>
      <data key="d1">X_BETA, a critical component in statistical modeling, represents the product of the design matrix X and the parameter vector Beta (X&#946;). This product, X&#946;, significantly contributes to the mean function of the response vector Y in a linear regression model. X_BETA is pivotal in the model parameterisation as detailed in equation (2), where it serves as the design matrix that facilitates the estimation of model parameters. The relationship between X, &#946;, and Y is fundamental in understanding and predicting the behavior of the response variable based on the explanatory variables encapsulated within the design matrix X.</data>
      <data key="d2">5609007c6229060ffc85d8056a7fefde,7fc5b8303ab530821bf2140ba6a8a889,e4f14e6785c6d7b7469e695aaeb170d0</data>
    </node>
    <node id="XJ1">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">XJ1, denoted as the first predictor variable for the jth observation, plays a significant role in statistical modeling. It represents the value of the first explanatory variable associated with the jth observation, providing crucial information for understanding the underlying relationships and patterns within the data. This variable is pivotal in the context of algorithmic analysis, particularly in linear regression models, where it helps in estimating the least squares estimators and conducting residual analysis. XJ1's value for each observation contributes to the overall structure and relations within the community of interest, aiding in the accurate interpretation of statistical inference.</data>
      <data key="d2">7ad4ccec4c7bb3702aed71c17dc6b96f,87b717ba065d6d7c7431af284137eb12</data>
    </node>
    <node id="XJ2">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">XJ2, denoted as Xj2, is the value of the second explanatory variable for the jth observation. It plays a crucial role as a predictor variable in statistical models, specifically serving as the second input for understanding and predicting outcomes associated with the jth observation. This variable is essential for capturing the relationship between the entity of interest and other variables within the context of algorithmic analysis.</data>
      <data key="d2">7ad4ccec4c7bb3702aed71c17dc6b96f,87b717ba065d6d7c7431af284137eb12</data>
    </node>
    <node id="XJK">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">XJK represents the value of the kth explanatory or predictor variable for the jth observation. This variable plays a crucial role in statistical modeling, particularly in regression analysis, where it helps in understanding the relationship between the dependent variable and the set of independent variables. XJK is a specific instance within a matrix of predictor variables, where each column corresponds to a different explanatory variable, and each row represents a unique observation. This data point is essential for estimating the coefficients of a regression model using methods such as least squares, which aims to minimize the sum of the squared residuals. By analyzing XJK, one can gain insights into the structure and relationships within the dataset, contributing to a more accurate and comprehensive statistical analysis.</data>
      <data key="d2">7ad4ccec4c7bb3702aed71c17dc6b96f,87b717ba065d6d7c7431af284137eb12</data>
    </node>
    <node id="EPSILON_J">
      <data key="d0">ERROR_TERM</data>
      <data key="d1">Epsilon_j (&#949;j) is the error term specific to the jth observation in the regression model, representing the deviation of the observed height from the model prediction. It is assumed to be independently and identically distributed (iid), following a normal distribution with a mean of 0 and a variance of &#963;^2. This error term is crucial in the context of a simple linear regression model, as it quantifies the unexplained variability in the dependent variable that is not captured by the model's predictors.</data>
      <data key="d2">656dce234514b9db38b5b5616557c1e9,7ad4ccec4c7bb3702aed71c17dc6b96f,825b600cbab3535ce67e9f561ddcb84b,d14413709de2897231aaa83be3aa346f</data>
    </node>
    <node id="P">
      <data key="d0">DIMENSION</data>
      <data key="d1">The entity "P" or "p" refers to the dimension of the parameter vector Beta and signifies the number of parameters in the linear regression model. This includes the intercept, the explanatory variables, and represents the total number of predictors in the model. It is crucial in understanding the complexity and structure of the regression analysis, as it indicates how many factors are being considered in the prediction. The term "P" and "p" are used interchangeably in this context, both denoting the same statistical measure in the linear regression framework.</data>
      <data key="d2">0443ab5e20a4f6b2f243c989ef6c723a,09391efd3b8c510205098b548bc8dc74,0b650eb2f1dcd603b64fec3c4b5cd24b,6648f0d6deed51fb4fb25e6992a71ddf,98d6982108f2d42fe0437bff8c666e17,9923e77ac6b3de95cb5026bc5e7fe8c0,9e2ebbb113c00fa43f0af3c0696baf95,bd05fe6a05f9a13d33c4f1b5a771ada5,bd98ac7b4b5df4f63e7ecc8f4a821f57,e4f14e6785c6d7b7469e695aaeb170d0,fc5b725f3c662c5471af20efdcc2dbff</data>
    </node>
    <node id="K">
      <data key="d0">NUMBER_OF_EXPLANATORY_VARIABLES</data>
      <data key="d1">Number of explanatory variables k in the multiple linear regression model, excluding the intercept</data>
      <data key="d2">e4f14e6785c6d7b7469e695aaeb170d0</data>
    </node>
    <node id="KTH_EXPLANATORY_VARIABLE">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">The kth explanatory variable is a feature used in the linear model to explain the response variable</data>
      <data key="d2">e41cc40f061f487b1ea0f256d4a963e4</data>
    </node>
    <node id="JTH_UNIT_OBSERVATION">
      <data key="d0">UNIT_OF_OBSERVATION</data>
      <data key="d1">The jth unit of observation is a specific instance or subject in the dataset for which the response and explanatory variables are measured</data>
      <data key="d2">e41cc40f061f487b1ea0f256d4a963e4</data>
    </node>
    <node id="VAR_Y">
      <data key="d0">VARIANCE_COVARIANCE_MATRIX</data>
      <data key="d1">Var(Y) is the variance-covariance matrix of the response vector Y, which is given by &#963;^2In</data>
      <data key="d2">e41cc40f061f487b1ea0f256d4a963e4</data>
    </node>
    <node id="RESPONSE_RANDOM_VECTOR">
      <data key="d0">VECTOR</data>
      <data key="d1">The response random vector is a vector of random variables that represents the outcomes of interest in a statistical model</data>
      <data key="d2">6616e10c85e86291147e72776854b8a2</data>
    </node>
    <node id="MODEL_DESCRIPTION">
      <data key="d0">DESCRIPTION</data>
      <data key="d1">Model description refers to the ability to describe a statistical model both in vector/matrix notation and in terms of its model equation for the jth unit of observation</data>
      <data key="d2">6616e10c85e86291147e72776854b8a2</data>
    </node>
    <node id="ESTIMATED_COEFFICIENTS">
      <data key="d0">PARAMETERS</data>
      <data key="d1">Estimated coefficients are the parameters of a model that are estimated from the data, representing the effect of each explanatory variable on the response variable</data>
      <data key="d2">6616e10c85e86291147e72776854b8a2</data>
    </node>
    <node id="TEXTBOOK_RECOMMENDATION">
      <data key="d0">REFERENCE</data>
      <data key="d1">Textbook recommendation refers to a suggested reading material for further understanding of the topics covered in the chapter, specifically "Generalized linear models with examples in R" by Dunn and Smyth</data>
      <data key="d2">6616e10c85e86291147e72776854b8a2</data>
    </node>
    <node id="GRAPHICAL_EXPLORATION">
      <data key="d0">ANALYSIS</data>
      <data key="d1">Graphical exploration, a pivotal technique in data analysis, involves the visual examination of data to uncover patterns, relationships, and trends. This method is particularly useful for understanding the structure within datasets and assessing the adequacy of the systematic component of linear models. By visually analyzing both the raw data and the models fitted to it, graphical exploration helps in identifying whether the linear model accurately describes the relationship within the data. This approach is essential for ensuring that the statistical models used are appropriate and effective in representing the underlying dynamics of the data. Through graphical exploration, data scientists can make informed decisions about model selection and adjustments, enhancing the overall accuracy and reliability of their analyses.</data>
      <data key="d2">6616e10c85e86291147e72776854b8a2,9f335f1ecb85a1427df926df8bb1e89f</data>
    </node>
    <node id="DESIGN_MATRIX">
      <data key="d0">MATRIX</data>
      <data key="d1">The DESIGN_MATRIX, denoted as X, is a fundamental component in statistical modeling, particularly in regression analysis. It is an n x p matrix, where n signifies the number of observations and p represents the number of parameters. Each row in the DESIGN_MATRIX corresponds to a single observation, while each column is dedicated to a specific predictor variable. This matrix encapsulates the values of the predictor variables for all observations in the dataset, serving as a critical input in the calculation of the linear predictor. It is instrumental in least squares estimation, enabling the fitting of models by representing the data in a format suitable for estimating model parameters. The DESIGN_MATRIX is pivotal in the context of a linear model, facilitating the analysis of the Retail data by providing a structured overview of the predictor variables and their transformations. Its role is central to understanding the relationships between the variables and the response, making it an indispensable tool in statistical inference and algorithmic analysis.</data>
      <data key="d2">06199787dd7f75f7338dd24d4f3dc26e,48971100deb5bb374a41c1f2b7b2a86a,6616e10c85e86291147e72776854b8a2,86ece4718d27d1a6c6a1f448cc850e2b,9f335f1ecb85a1427df926df8bb1e89f,b0ca3e6c22c4cf884d03b1f6f82be5df,b9af17718641389ba07f53be13f31f8c,e5131a1158e58f1b7b44b21ced7b6f60,e7494d6cfc3e38a4d2f3f6b21ef6445d</data>
    </node>
    <node id="PARAMETER_VECTOR">
      <data key="d0">VECTOR</data>
      <data key="d1">The parameter vector, denoted as &#946; (PARAMETER_VECTOR), is a pivotal component in statistical models, particularly within the framework of linear regression analysis. It encapsulates the coefficients of the predictor variables, serving as a bridge to quantify the relationship between the explanatory variables and the response variable. In the context of least squares estimation, the parameter vector is estimated to minimize the sum of the squared residuals, ensuring that the model's predictions are as close as possible to the observed data. This vector, represented as [&#956;, &#945;B, &#945;C, &#946;]T, includes the coefficients of the intercept and the independent variables, providing a comprehensive description of the linear model's structure. The least squares estimate of the parameter vector, &#946;b, is a p-dimensional vector that plays a critical role in understanding and interpreting the linear relationship between the predictors and the response variable. Through this estimate, insights into the community of interest can be derived, facilitating a deeper understanding of the underlying statistical relationships.</data>
      <data key="d2">07951ffe6787af44aa60c90c69e62f83,6616e10c85e86291147e72776854b8a2,924be3e598ffeabd1fbd9b57f033b917,9f335f1ecb85a1427df926df8bb1e89f,b9af17718641389ba07f53be13f31f8c,c103c6d096d52868eda26d991194b5f2</data>
    </node>
    <node id="ANSCOMBE_S_QUARTET">
      <data key="d0">EXAMPLE</data>
      <data key="d1">Anscombe's Quartet is a set of four datasets that have nearly identical simple descriptive statistics, yet have very different graphical representations, illustrating the importance of graphical exploration in statistical analysis</data>
      <data key="d2">6616e10c85e86291147e72776854b8a2</data>
    </node>
    <node id="SYSTEMATIC_COMPONENT">
      <data key="d0">FUNCTION</data>
      <data key="d1">The systematic component, a pivotal element in statistical modeling, is characterized by its deterministic nature and adherence to a specific pattern or rule. In the context of the fitted model, it is represented by a function that is linear in the parameters and directly influenced by the explanatory variables. This function is mathematically defined as -5.9957 + 2.7808x - 0.1267x^2, which intriguingly forms an upturned parabola. This parabolic relationship indicates that the systematic component reaches its peak value of approximately 9.26 when the explanatory variable x is around 11, providing a clear illustration of how the systematic component varies with changes in x.</data>
      <data key="d2">9f335f1ecb85a1427df926df8bb1e89f,bb31f1c77bbba73300f735a100086a67,c9a01b92d11585f6549f62e8bd78d652</data>
    </node>
    <node id="ANSCOMBE_QUARTET">
      <data key="d0">DATASET</data>
      <data key="d1">Anscombe's Quartet, also known as ANSCOMBE_QUARTET, is a fascinating collection of four datasets that, despite having nearly identical simple descriptive statistics and corresponding regression lines, exhibit strikingly different patterns when visualized through scatterplots. This quartet serves as a compelling example in statistical analysis, illustrating the importance of graphical representation in understanding the underlying structure and relationships within data. The datasets within Anscombe's Quartet share common summary statistics, such as mean and variance, yet their scatterplots reveal distinct shapes, outliers, and relationships, highlighting the limitations of relying solely on numerical summaries without visual inspection.</data>
      <data key="d2">87ba4f416a28aabc3b396908f5913b54,9f335f1ecb85a1427df926df8bb1e89f</data>
    </node>
    <node id="STATISTICAL_ANALYSIS">
      <data key="d0">METHOD</data>
      <data key="d1">Statistical analysis is the process of collecting, exploring, and presenting data to uncover patterns and trends</data>
      <data key="d2">9f335f1ecb85a1427df926df8bb1e89f</data>
    </node>
    <node id="SUMMARY_STATISTICS">
      <data key="d0">DATA</data>
      <data key="d1">Summary statistics are measures that describe the central tendency, dispersion, and shape of a dataset</data>
      <data key="d2">9f335f1ecb85a1427df926df8bb1e89f</data>
    </node>
    <node id="ANSCOMBE">
      <data key="d0">AUTHOR</data>
      <data key="d1">Anscombe is the author of the paper "Graphs in statistical analysis" which discusses the use of graphs in statistical analysis</data>
      <data key="d2">2a5997c641e47fc6c32ebf81101c54e0</data>
    </node>
    <node id="AMERICAN_STATISTICIAN">
      <data key="d0">JOURNAL</data>
      <data key="d1">The American Statistician is the journal where Anscombe's paper was published</data>
      <data key="d2">2a5997c641e47fc6c32ebf81101c54e0</data>
    </node>
    <node id="DOI">
      <data key="d0">REFERENCE</data>
      <data key="d1">DOI is the digital object identifier for Anscombe's paper</data>
      <data key="d2">2a5997c641e47fc6c32ebf81101c54e0</data>
    </node>
    <node id="DATASET_1">
      <data key="d0">DATASET</data>
      <data key="d1">DATASET_1, also known as Dataset 1, is a meticulously compiled collection of data points characterized by its evenly distributed x-values. The y-values are scattered randomly around the regression line, indicating a typical distribution one might expect in a linear regression analysis. This dataset is one of the four datasets highlighted in the text, and it comes with specific summary statistics that provide deeper insights into its structure and characteristics. These summary statistics are crucial for understanding the central tendencies, dispersion, and other statistical properties of DATASET_1, making it a valuable resource for statistical analysis and inference.</data>
      <data key="d2">2a5997c641e47fc6c32ebf81101c54e0,84ffe1b8496dc660c47248c9f7b5bdea</data>
    </node>
    <node id="DATASET_2">
      <data key="d0">DATASET</data>
      <data key="d1">DATASET_2, one of the four datasets highlighted in the text and notably a part of Anscombe&#8217;s quartet, exhibits a unique pattern in its data structure. The x-values are evenly distributed, indicating a balanced representation across the range of the explanatory variable. However, the y-values do not align well with a simple linear regression model, suggesting a deviation from linearity. This deviation is further elucidated by the presence of a parabolic relationship between the response and the explanatory variable, indicating a curved association that a linear model fails to capture adequately. Despite this, specific summary statistics for DATASET_2 are available, offering additional insights into the dataset's characteristics. The combination of these features makes DATASET_2 a complex case for statistical analysis, requiring models that can accommodate non-linear relationships to accurately describe the data.</data>
      <data key="d2">2a5997c641e47fc6c32ebf81101c54e0,84ffe1b8496dc660c47248c9f7b5bdea,87ba4f416a28aabc3b396908f5913b54</data>
    </node>
    <node id="SAMPLE_MEAN_X">
      <data key="d0">METRIC</data>
      <data key="d1">Sample mean of x is a metric that indicates the average value of the x variable in the dataset</data>
      <data key="d2">2a5997c641e47fc6c32ebf81101c54e0</data>
    </node>
    <node id="SAMPLE_VARIANCE_X">
      <data key="d0">METRIC</data>
      <data key="d1">Sample variance of x is a metric that indicates the spread of the x variable in the dataset</data>
      <data key="d2">2a5997c641e47fc6c32ebf81101c54e0</data>
    </node>
    <node id="SAMPLE_MEAN_Y">
      <data key="d0">METRIC</data>
      <data key="d1">Sample mean of y is a metric that indicates the average value of the y variable in the dataset</data>
      <data key="d2">2a5997c641e47fc6c32ebf81101c54e0</data>
    </node>
    <node id="SAMPLE_VARIANCE_Y">
      <data key="d0">METRIC</data>
      <data key="d1">Sample variance of y is a metric that indicates the spread of the y variable in the dataset</data>
      <data key="d2">2a5997c641e47fc6c32ebf81101c54e0</data>
    </node>
    <node id="CORRELATION_COEFFICIENT">
      <data key="d0">METRIC</data>
      <data key="d1">Correlation coefficient of x and y is a metric that indicates the strength and direction of the linear relationship between x and y</data>
      <data key="d2">2a5997c641e47fc6c32ebf81101c54e0</data>
    </node>
    <node id="FITTED_REGRESSION_LINE">
      <data key="d0">MODEL</data>
      <data key="d1">The fitted regression line, a crucial component in the context of linear regression analysis, is the equation that best describes the relationship between the independent variable, x, and the dependent variable, y, within a given dataset. This line is determined through the application of the least squares method, a statistical technique aimed at minimizing the sum of the squared residuals. By doing so, the fitted regression line optimally represents the linear association between the variables, providing a clear and quantifiable measure of how changes in x are associated with changes in y. This method ensures that the line of best fit is the one that minimizes the overall discrepancy between the observed data points and the values predicted by the model, making it a fundamental tool in statistical inference and predictive analytics.</data>
      <data key="d2">23fc620f1238c6a1b5c5e3a08e149c53,2a5997c641e47fc6c32ebf81101c54e0</data>
    </node>
    <node id="DATASET_3">
      <data key="d0">DATASET</data>
      <data key="d1">DATASET_3, a component of Anscombe's quartet, is a unique dataset that presents an intriguing case study for data scientists and statisticians. This dataset primarily adheres to a simple linear regression model, showcasing a clear linear relationship between the variables. However, it is notably distinguished by the presence of a significant outlier. This outlier exerts a substantial influence on the fitted regression line, thereby challenging the robustness of simple linear models and highlighting the importance of residual analysis in identifying such influential points. As part of Anscombe's quartet, DATASET_3 serves as a critical reminder of the limitations of purely statistical measures and the necessity of graphical data analysis to fully understand the structure and relations within a dataset.</data>
      <data key="d2">84ffe1b8496dc660c47248c9f7b5bdea,b8ec334f8c87bf1d9cb6043fa1a64214</data>
    </node>
    <node id="DATASET_4">
      <data key="d0">DATASET</data>
      <data key="d1">DATASET_4, a component of Anscombe's quartet, is characterized by a unique data structure where all but one of the x-values are identical. This peculiarity results in the slope of the line being significantly influenced by a single, atypical data point. This singular outlier plays a crucial role in determining the trend line's inclination, highlighting the dataset's sensitivity to extreme values.</data>
      <data key="d2">84ffe1b8496dc660c47248c9f7b5bdea,b8ec334f8c87bf1d9cb6043fa1a64214</data>
    </node>
    <node id="OUTLIER">
      <data key="d0">DATA_POINT</data>
      <data key="d1">An outlier is a data point that differs significantly from other observations, potentially affecting the results of statistical analyses</data>
      <data key="d2">84ffe1b8496dc660c47248c9f7b5bdea</data>
    </node>
    <node id="SIMPLE_LINEAR_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">The SIMPLE_LINEAR_MODEL is a statistical model that falls under the category of linear regression models. It fundamentally assumes a linear relationship between the dependent (response) variable and the independent (explanatory) variable(s). This model simplifies the analysis by considering a straightforward, direct association between the variables, making it a useful tool for understanding and predicting the behavior of one variable based on the knowledge of another. The SIMPLE_LINEAR_MODEL is characterized by its simplicity and ease of interpretation, which makes it a popular choice in various fields for preliminary analysis and quick insights into data relationships.</data>
      <data key="d2">84ffe1b8496dc660c47248c9f7b5bdea,b8ec334f8c87bf1d9cb6043fa1a64214</data>
    </node>
    <node id="ANSCOME_QUARTET">
      <data key="d0">DATASET</data>
      <data key="d1">Anscombe's quartet consists of four datasets that have nearly identical simple descriptive statistics, yet have very different distributions and appear very different when graphed</data>
      <data key="d2">b8ec334f8c87bf1d9cb6043fa1a64214</data>
    </node>
    <node id="DATASAUROS_DOZEN">
      <data key="d0">DATASET</data>
      <data key="d1">The Datasaurus Dozen is a set of 12 datasets that have the same summary statistics but very different distributions, created by Alberto Cairo to highlight the importance of visualizing data</data>
      <data key="d2">b8ec334f8c87bf1d9cb6043fa1a64214</data>
    </node>
    <node id="HISTOGRAMS">
      <data key="d0">TOOL</data>
      <data key="d1">Histograms, a pivotal tool in the realm of exploratory data analysis, serve as graphical representations that vividly illustrate the distribution of numerical data. These visual aids effectively display the frequency of occurrence for various data values, providing insights into the structure and patterns within the dataset. By plotting the data into bins or intervals, histograms enable a clear visualization of the data's spread and central tendency, making them indispensable for understanding the underlying characteristics of the variables under study.</data>
      <data key="d2">60cc94e681863c9fcc6f9be1e500f840,87ba4f416a28aabc3b396908f5913b54</data>
    </node>
    <node id="SCATTERPLOTS">
      <data key="d0">TOOL</data>
      <data key="d1">Scatterplots are graphs that display the relationship between two variables, often used to identify patterns or trends in data.</data>
      <data key="d2">87ba4f416a28aabc3b396908f5913b54</data>
    </node>
    <node id="PARABOLA">
      <data key="d0">SHAPE</data>
      <data key="d1">PARABOLA is a significant geometric figure in the realm of mathematics, characterized by its unique property where any point on the curve is at an equal distance from a fixed point, known as the focus, and a fixed straight line, referred to as the directrix. This curve is not only a fundamental concept in geometry but also plays a crucial role in statistical modeling. Specifically, PARABOLA is the curve of best fit for the quadratic regression model, which is a statistical technique used to describe and analyze the relationship between a dependent variable (Y) and one or more independent variables (X). The quadratic regression model utilizes the parabolic shape to accurately represent the nonlinear relationship between the variables, making PARABOLA an essential element in understanding and predicting data trends in various scientific and engineering applications.</data>
      <data key="d2">11452a08471d93959558de2ece9a69af,87ba4f416a28aabc3b396908f5913b54</data>
    </node>
    <node id="QUADRATIC_REGRESSION">
      <data key="d0">MODEL</data>
      <data key="d1">Quadratic regression, a sophisticated variant of polynomial regression, is employed to model non-linear relationships between predictor and response variables by incorporating a squared term of the independent variable. This approach enables the model to capture parabolic relationships, making it particularly useful when the data exhibits a curved pattern. As a more intricate model compared to linear regression, quadratic regression can accurately describe the complex dynamics between variables, enhancing predictive capabilities in scenarios where linear models fall short.</data>
      <data key="d2">15c7b5750483a382ce59751008e86751,87ba4f416a28aabc3b396908f5913b54,e5131a1158e58f1b7b44b21ced7b6f60</data>
    </node>
    <node id="PARAMETERS">
      <data key="d0">MODEL_COMPONENT</data>
      <data key="d1">The PARAMETERS in the context of statistical modeling refer to the coefficients or variables that are integral components of a model, such as \u03b20, \u03b21, and \u03b22 in polynomial regression. These PARAMETERS are the values that need to be estimated or optimized during the modeling process to best fit the data and make accurate predictions. They play a crucial role in determining the relationship between the independent and dependent variables, and their accurate estimation is essential for the model's effectiveness and reliability.</data>
      <data key="d2">87ba4f416a28aabc3b396908f5913b54,bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="SUM_OF_SQUARED_DISTANCES">
      <data key="d0">METRIC</data>
      <data key="d1">The sum of squared distances, a critical metric in least squares estimation, quantifies the discrepancy between observed values and those predicted by the model. Specifically, it refers to the sum of squared vertical distances between the observations and the parabola defined by the equation \u03b20 + \u03b21x + \u03b22x^2. This criterion is pivotal in determining the parameters of the model, ensuring that the parabola best fits the observed data points by minimizing the total sum of squared distances. This process is fundamental in statistical analysis, particularly in the context of linear regression, where the goal is to find the line or curve that best represents the relationship between the independent variable x and the dependent variable y.</data>
      <data key="d2">87ba4f416a28aabc3b396908f5913b54,e5131a1158e58f1b7b44b21ced7b6f60</data>
    </node>
    <node id="ERRORS">
      <data key="d0">ERRORS</data>
      <data key="d1">In the context of statistical analysis, particularly within linear regression models, ERRORS (denoted as &#1013;1, ..., &#1013;n) are the discrepancies or differences between the observed values and the values predicted by the model. These ERRORS are assumed to be independent and identically distributed (iid), following a normal distribution with a mean of 0 and a variance of &#963;^2. This assumption is crucial for the validity of least squares estimators and residual analysis in linear models, as it ensures that the errors have constant variance and are not correlated with each other, which is a fundamental requirement for the proper functioning of statistical inference techniques. The normal distribution assumption for ERRORS facilitates the use of various statistical tests and confidence intervals, providing a robust framework for understanding the relations and structure of the community of interest within the context of algorithmic analysis.</data>
      <data key="d2">25fce1af816975003128126b5cfea73b,3bfc9b92571973e54c8095302acc1aaa,7cd6069e88e81548a237fa937adfecc6,d738df7d83784c8a41b3948271c537b6,e361ac139c268d5c3f3623f920e68af2,e5131a1158e58f1b7b44b21ced7b6f60</data>
    </node>
    <node id="QUADRATIC_TERM">
      <data key="d0">PREDICTOR</data>
      <data key="d1">Quadratic term is an additional predictor term added to the systematic component of the model. It is used to model the curvature in the relationship between the predictor and the response variable</data>
      <data key="d2">e5131a1158e58f1b7b44b21ced7b6f60</data>
    </node>
    <node id="MODEL_EQUATIONS">
      <data key="d0">EQUATIONS</data>
      <data key="d1">The model equations in (2.1) expressed in matrix notation are the equations that describe the relationship between the response variable and the predictor variables in the model</data>
      <data key="d2">e5131a1158e58f1b7b44b21ced7b6f60</data>
    </node>
    <node id="PLR_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">plr.model is the quadratic regression model fitted using the lm() function</data>
      <data key="d2">084dadebfca8bcb6377c205c45bee295</data>
    </node>
    <node id="FIGURE_2_5">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 2.5 is a scatterplot of Anscombe&#8217;s Dataset 2 with the parabola of best fit</data>
      <data key="d2">084dadebfca8bcb6377c205c45bee295</data>
    </node>
    <node id="QUADRATIC_CURVE">
      <data key="d0">CURVE</data>
      <data key="d1">Quadratic curve is the curve of best fit for the quadratic regression model, representing the relationship between X and Y</data>
      <data key="d2">11452a08471d93959558de2ece9a69af</data>
    </node>
    <node id="EXPECTED_CHANGE">
      <data key="d0">EXPECTED_VALUE</data>
      <data key="d1">Expected change in the response due to a unit change in the explanatory variable is given by the formula (Y |X = x + 1) &#8722; E(Y |X = x) = &#946;1 + &#946;2 + 2&#946;2x, which is not constant but depends on the current value x of the predictor variable</data>
      <data key="d2">c9a01b92d11585f6549f62e8bd78d652</data>
    </node>
    <node id="PLOT">
      <data key="d0">VISUALIZATION</data>
      <data key="d1">A plot, as understood in the context of statistical analysis, is a visual tool that graphically represents data, primarily aiming to illustrate the relationship between two variables. This graphical depiction is particularly useful when dealing with a single explanatory variable, as it allows for a clear visualization of the relationship between the expected response and the explanatory variable. An example of such a plot can be found in Figure 2.5, which effectively demonstrates how plots can aid in understanding the structure and nature of the relationship between variables in a dataset.</data>
      <data key="d2">b9ec8a6c7960cc6196ec94fd976f05b0,c9a01b92d11585f6549f62e8bd78d652</data>
    </node>
    <node id="QUADRATIC_RELATIONSHIP">
      <data key="d0">RELATIONSHIP</data>
      <data key="d1">The expected response has a quadratic relationship with the explanatory variable, as described by the systematic component of the fitted model</data>
      <data key="d2">c9a01b92d11585f6549f62e8bd78d652</data>
    </node>
    <node id="POLYNOMIAL_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">The model describing the relationship between the response variable and the predictor variable is a polynomial of order 2, which is a non-linear function of the predictor variable</data>
      <data key="d2">c9a01b92d11585f6549f62e8bd78d652</data>
    </node>
    <node id="ERROR_TERM">
      <data key="d0">VARIABLE</data>
      <data key="d1">The error term, denoted as &#966; (phi), plays a crucial role in the context of linear regression models, particularly in relation to the entity "ERROR_TERM". This term, &#966;j, specifically, signifies the discrepancy between the actual observed sales volume and the sales volume that is predicted by the model. It encapsulates the variability in the response variable that cannot be explained by the predictors included in the model. It is fundamental to assume that this error term is random and independent for each observation, adhering to the statistical assumptions required for the validity of linear regression analysis. This characteristic ensures that the model's predictions are unbiased and that the least squares estimators are efficient. In essence, the error term &#966;j provides a measure of the model's accuracy in predicting sales volume, highlighting the residual analysis as a critical component in assessing the model's fit and performance.</data>
      <data key="d2">7a605c3b689bec7ab2c46df9c123e3f3,b9af17718641389ba07f53be13f31f8c</data>
    </node>
    <node id="YI">
      <data key="d0">DEPENDENT_VARIABLE</data>
      <data key="d1">YI, denoted as Yi or yi, is the response variable for the ith observation in various statistical models. In the context of the dataset provided for Exercise 9, Yi takes on the specific values 1, 2, 4, 7, 12. It serves as the dependent variable in the quadratic regression model and the non-linear transformation model for the ith unit of observation. Yi is transformed using the log-transform in some models, and in others, it is represented as the exponential of the linear combination of predictors and error term. Yi is a function of the explanatory variables and random error in the statistical models, highlighting its role as the observed value for the ith observation in both linear and non-linear regression models.</data>
      <data key="d2">0fbc9037ca9a440e79e9ac05664b9b3d,1da117a2f92b2db00290d2a0bfc06beb,22093a562f5f05dc9891b45ab9bcbea8,2685edb9e8031c8ea725c43a40af22a8,34fceaaf7d835828b5ee2327325c37f8,5a0d392715f06d5e873f45ae06aa729a,6c1684ed2a4840576c6b0f4d1a3a482f,90b7e0427699cc1bb461e37939935138,90ed6030c5a5a0764b7dcd4115b4d4d3</data>
    </node>
    <node id="XI">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">In the context of statistical modeling, XI, denoted as "xi", represents the value of the explanatory variable for the ith unit of observation. This variable plays a crucial role in both simple linear regression and quadratic regression models. In simple linear regression, xi is the ith observation of the explanatory variable, contributing to the understanding of the linear relationship between the explanatory variable and the response variable. In the quadratic regression model, xi retains its significance as the value of the explanatory variable for the ith unit, but here it helps in modeling the relationship that includes a squared term of the explanatory variable, allowing for the exploration of non-linear relationships. Thus, XI serves as a fundamental component in these regression analyses, facilitating the examination of various patterns and trends within the data.</data>
      <data key="d2">6c1684ed2a4840576c6b0f4d1a3a482f,6ee02b38ae842fd5eac9a11c4fd6659f</data>
    </node>
    <node id="EXERCISE_7">
      <data key="d0">EXERCISE</data>
      <data key="d1">Exercise 7 is a quadratic regression model exercise where the model equation and assumptions about the errors are given</data>
      <data key="d2">6c1684ed2a4840576c6b0f4d1a3a482f</data>
    </node>
    <node id="DBH">
      <data key="d0">DIAMETER</data>
      <data key="d1">DBH, which stands for Diameter at Breast Height, is a measurement of the tree's diameter taken at a standard height of 137 cm from the ground. This measurement is specifically taken in millimeters to provide a precise indication of the tree's size. DBH is a crucial parameter in forestry and ecology, as it helps in estimating the age, volume, and biomass of trees, as well as in understanding forest structure and dynamics. The consistency in the measurement method ensures comparability across different studies and locations.</data>
      <data key="d2">2d5cdecc342ddacd2c090f1838430cee,656dce234514b9db38b5b5616557c1e9</data>
    </node>
    <node id="HEIGHT">
      <data key="d0">HEIGHT</data>
      <data key="d1">Height, a crucial variable in the trees dataset, signifies the measurement of the tree from the ground to its top. It is the response variable in the linear regression model, representing the height of Western red cedar trees. Initially measured in feet, the data was later standardized to decimeters (dm) for consistency. The observed heights, when plotted against the log2-diameters, exhibit a fairly linear relationship, indicating a strong correlation between the tree's diameter and its height. This variable plays a pivotal role in understanding the growth patterns and structural characteristics of the Western red cedar trees within the dataset.</data>
      <data key="d2">00e186a86624c01ce873dd577df68d17,2d5cdecc342ddacd2c090f1838430cee,656dce234514b9db38b5b5616557c1e9,9611ea31ff53888971694cdefe806f64,c619949b08fc2b7edf3a7635b46dc147,efeeb664622c1ee594e6a08a8322ffe3</data>
    </node>
    <node id="UFCWC">
      <data key="d0">DATASET</data>
      <data key="d1">ufcwc is a dataset from the alr4 package containing information on a sample of 139 Western cedar trees</data>
      <data key="d2">2d5cdecc342ddacd2c090f1838430cee</data>
    </node>
    <node id="ALR4">
      <data key="d0">PACKAGE</data>
      <data key="d1">alr4 is a package containing the ufcwc dataset</data>
      <data key="d2">2d5cdecc342ddacd2c090f1838430cee</data>
    </node>
    <node id="WEISBERG">
      <data key="d0">AUTHOR</data>
      <data key="d1">Weisberg is the author of the book "Applied Linear Regression, 4th edition"</data>
      <data key="d2">2d5cdecc342ddacd2c090f1838430cee</data>
    </node>
    <node id="UNIVERSITY IDAHO EXPERIMENTAL FOREST">
      <data key="d0">LOCATION</data>
      <data key="d1">University Idaho Experimental Forest is the location where the data for the ufcwc dataset were collected</data>
      <data key="d2">2d5cdecc342ddacd2c090f1838430cee</data>
    </node>
    <node id="UPPER FLAT CREEK STAND">
      <data key="d0">LOCATION</data>
      <data key="d1">Upper Flat Creek stand is the specific location within the University Idaho Experimental Forest where the data for the ufcwc dataset were collected</data>
      <data key="d2">2d5cdecc342ddacd2c090f1838430cee</data>
    </node>
    <node id="CREEK_STAND">
      <data key="d0">LOCATION</data>
      <data key="d1">Creek stand is the location where the Western red cedar trees are found</data>
      <data key="d2">656dce234514b9db38b5b5616557c1e9</data>
    </node>
    <node id="FIGURE_2_6">
      <data key="d0">PLOT</data>
      <data key="d1">Figure 2.6 is a scatterplot showing the relationship between height and diameter of the Western red cedar trees, with the line of best fit</data>
      <data key="d2">656dce234514b9db38b5b5616557c1e9</data>
    </node>
    <node id="WEISBERG_S_2014">
      <data key="d0">REFERENCE</data>
      <data key="d1">Weisberg, S. (2014). Applied Linear Regression, 4th edition. New York: Wiley is a reference for the linear regression model</data>
      <data key="d2">656dce234514b9db38b5b5616557c1e9</data>
    </node>
    <node id="DIAMETER">
      <data key="d0">VARIABLE</data>
      <data key="d1">Diameter, a crucial variable in the context of tree measurements, signifies the tree's width, specifically taken at a height of 4 ft 6 inches from the ground level. Initially mislabeled as girth, this measurement is pivotal in predicting the height of the tree through a regression model. To enhance the model's accuracy, the diameter values are transformed using the logarithm of base 2 (log2). The relationship between diameter and tree height exhibits a curved yet monotonic pattern, indicating that as the diameter increases, so does the tree's height. However, it's noteworthy that data points for trees with exceptionally small or large diameters tend to deviate below the regression line, suggesting potential complexities in the relationship at these extremes.</data>
      <data key="d2">00e186a86624c01ce873dd577df68d17,9611ea31ff53888971694cdefe806f64</data>
    </node>
    <node id="LOG2">
      <data key="d0">FUNCTION</data>
      <data key="d1">LOG2, also known as Log2, is a mathematical function that calculates the base 2 logarithm of a number. This function is notably utilized in the transformation of predictor variables in linear regression models. Specifically, in the context provided, Log2 is applied to the diameter variable, transforming each diameter value into its corresponding log2-diameter value. This transformation is integral to the construction of the design matrix for the regression model, serving to ease the interpretation of the coefficients by adjusting the scale of the predictor variable. The use of Log2 in this manner highlights its utility in statistical modeling, particularly in scenarios where the original scale of the data may complicate the analysis or interpretation of results.</data>
      <data key="d2">00e186a86624c01ce873dd577df68d17,c619949b08fc2b7edf3a7635b46dc147</data>
    </node>
    <node id="BASE_2">
      <data key="d0">LOGARITHM_BASE</data>
      <data key="d1">Base 2 is the logarithm base used in the transformation of the diameters for the linear regression model</data>
      <data key="d2">25fce1af816975003128126b5cfea73b</data>
    </node>
    <node id="HEIGHTS">
      <data key="d0">OBSERVED_VARIABLE</data>
      <data key="d1">Heights are the observed response variable in the linear regression model</data>
      <data key="d2">25fce1af816975003128126b5cfea73b</data>
    </node>
    <node id="LOG2_DIAMETERS">
      <data key="d0">TRANSFORMED_VARIABLE</data>
      <data key="d1">Log2-diameters are the explanatory variable transformed using the base 2 logarithm for the linear regression model</data>
      <data key="d2">25fce1af816975003128126b5cfea73b</data>
    </node>
    <node id="TREE_MODEL">
      <data key="d0">FITTED_MODEL</data>
      <data key="d1">The TREE_MODEL is a linear regression model specifically designed to predict the height of Western red cedar trees. In this model, the log2-transformed diameter at breast height (Dbh) serves as the predictor variable, while the height of the tree is the response variable. The TREE_MODEL has been fitted using the Height as the dependent variable and log2(Dbh) as the independent variable, enabling accurate predictions of tree height based on the transformed Dbh measurements.</data>
      <data key="d2">25fce1af816975003128126b5cfea73b,c619949b08fc2b7edf3a7635b46dc147</data>
    </node>
    <node id="LOG2_DBH">
      <data key="d0">MODEL_PARAMETER</data>
      <data key="d1">Log2(Dbh) is the estimated coefficient for the log2-transformed diameter at breast height (Dbh) in the linear regression model</data>
      <data key="d2">25fce1af816975003128126b5cfea73b</data>
    </node>
    <node id="NATURAL_LOGARITHM">
      <data key="d0">LOGARITHM_BASE</data>
      <data key="d1">Natural logarithm is the logarithm with base e, which could have been used but was not chosen for this model</data>
      <data key="d2">25fce1af816975003128126b5cfea73b</data>
    </node>
    <node id="LOG2_X_PLUS_1">
      <data key="d0">MATHEMATICAL_EXPRESSION</data>
      <data key="d1">Log2(x) + 1 is the mathematical expression that shows the relationship between the base 2 logarithm and the natural logarithm, used to explain the choice of base 2</data>
      <data key="d2">25fce1af816975003128126b5cfea73b</data>
    </node>
    <node id="PREDICTOR">
      <data key="d0">VARIABLE</data>
      <data key="d1">The entity "PREDICTOR" in the context of the given descriptions, refers specifically to the Dbh (Diameter at Breast Height) in the linear regression model. This predictor variable is transformed using the log2 function, which not only enhances the model's fit but also facilitates a clearer interpretation of the results. In regression analysis, predictor variables, like Dbh in this case, play a crucial role as they are used to forecast the response variable. These variables can be either quantitative, as is the case with Dbh, or categorical, depending on the nature of the data and the model being employed. The transformation of Dbh using the log2 function is a common statistical technique aimed at stabilizing variance and normalizing the distribution of the predictor, thereby improving the overall performance and reliability of the regression model.</data>
      <data key="d2">b0ca3e6c22c4cf884d03b1f6f82be5df,c619949b08fc2b7edf3a7635b46dc147</data>
    </node>
    <node id="NATURAL_LOG">
      <data key="d0">FUNCTION</data>
      <data key="d1">Natural log is a mathematical function that calculates the logarithm of a number with base e. It is mentioned as an alternative to the log2 function but is not used in the current model.</data>
      <data key="d2">c619949b08fc2b7edf3a7635b46dc147</data>
    </node>
    <node id="LOG10">
      <data key="d0">FUNCTION</data>
      <data key="d1">Log10 is a mathematical function that calculates the logarithm of a number with base 10. It is mentioned as another common choice in statistical practice for transforming predictor variables, where for interpretation, we consider a ten-fold increase in the predictor variable.</data>
      <data key="d2">c619949b08fc2b7edf3a7635b46dc147</data>
    </node>
    <node id="VARIABLE">
      <data key="d0">VARIABLE</data>
      <data key="d1">The term 'variable' refers to a quantity that can change or vary, such as the response variable Y and the explanatory variables X1 and X2 in a statistical model</data>
      <data key="d2">ae1e66f5b64284090abc285c1d4389f5</data>
    </node>
    <node id="LOG_TRANSFORMED_VARIABLE">
      <data key="d0">VARIABLE</data>
      <data key="d1">Log-transformed variable is an explanatory variable that has been transformed using a logarithmic function, often to improve the fit of a statistical model</data>
      <data key="d2">ae1e66f5b64284090abc285c1d4389f5</data>
    </node>
    <node id="FIT">
      <data key="d0">FIT</data>
      <data key="d1">Fit refers to how well a statistical model describes the relationship between variables. A better fit indicates that the model more accurately represents the data</data>
      <data key="d2">ae1e66f5b64284090abc285c1d4389f5</data>
    </node>
    <node id="ARITHMETIC_OPERATORS">
      <data key="d0">OPERATOR</data>
      <data key="d1">Arithmetic operators are symbols that represent mathematical operations such as addition, subtraction, multiplication, and division</data>
      <data key="d2">ae1e66f5b64284090abc285c1d4389f5</data>
    </node>
    <node id="INHIBITOR_FUNCTION_I">
      <data key="d0">FUNCTION</data>
      <data key="d1">The inhibitor function I() is used in R to treat certain operators as arithmetic operators, allowing for proper mathematical operations in statistical models</data>
      <data key="d2">ae1e66f5b64284090abc285c1d4389f5</data>
    </node>
    <node id="LOGARITHM_BASE_10">
      <data key="d0">FUNCTION</data>
      <data key="d1">Logarithm with base 10 is a mathematical function that is often used in statistical practice for interpretation, especially when considering a ten-fold increase in the predictor variable</data>
      <data key="d2">ae1e66f5b64284090abc285c1d4389f5</data>
    </node>
    <node id="WESTERN_RED_CEDAR_DATA">
      <data key="d0">DATA</data>
      <data key="d1">The Western red cedar data, a comprehensive dataset featured in Section 2.3, encompasses detailed measurements of Western red cedar trees, including their height and diameter. This dataset was utilized for statistical analysis, specifically to illustrate the application of log transformation on the explanatory variable. The log transformation was implemented as a strategic measure to rectify model violations, ensuring that the statistical models applied to the data adhered to the necessary assumptions for accurate analysis.</data>
      <data key="d2">0fbc9037ca9a440e79e9ac05664b9b3d,ae1e66f5b64284090abc285c1d4389f5,c03eb12d07d48f9e94260f08dae10cdf</data>
    </node>
    <node id="LOGARITHMIC_CURVE_OF_BEST_FIT">
      <data key="d0">FIT</data>
      <data key="d1">Logarithmic curve of best fit is a curve that best represents the relationship between a log-transformed variable and another variable, often used when the relationship is not linear</data>
      <data key="d2">ae1e66f5b64284090abc285c1d4389f5</data>
    </node>
    <node id="EXERCISE_8">
      <data key="d0">EXERCISE</data>
      <data key="d1">Exercise 8 is a problem or task designed to test understanding or skills in statistical modelling, involving observations of a response variable Y and two explanatory variables X1 and X2</data>
      <data key="d2">ae1e66f5b64284090abc285c1d4389f5</data>
    </node>
    <node id="PARAMETER_VECTOR_BETA">
      <data key="d0">VECTOR</data>
      <data key="d1">Parameter vector &#946; is a vector that contains the parameters of a statistical model, such as &#946;0, &#946;1, and &#946;2, which are coefficients that determine the relationship between variables</data>
      <data key="d2">ae1e66f5b64284090abc285c1d4389f5</data>
    </node>
    <node id="EPSILONI">
      <data key="d0">ERROR_TERM</data>
      <data key="d1">EPSILONi, denoted as \u03f5i, is the error term associated with the ith species in the linear regression model. It serves as the random error component in statistical models, encapsulating the unexplained variation that the model does not account for. This error term, \u03f5i, is assumed to follow a normal distribution, a critical assumption in the context of linear regression analysis. This normality assumption facilitates the use of least squares estimators and enables a robust residual analysis, providing insights into the model's fit and the underlying data's structure. EPSILONi's role is pivotal in understanding the discrepancies between the observed data and the model's predictions, thereby aiding in the identification of potential outliers and the assessment of model adequacy.</data>
      <data key="d2">34fceaaf7d835828b5ee2327325c37f8,86c401dda130c2d201c3339526062a24,90ed6030c5a5a0764b7dcd4115b4d4d3</data>
    </node>
    <node id="XI,1">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">xi,1 is an explanatory variable in the statistical models, used in various forms to explain the response variable</data>
      <data key="d2">90ed6030c5a5a0764b7dcd4115b4d4d3</data>
    </node>
    <node id="XI,2">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">xi,2 is an explanatory variable in the statistical models, used in the logarithmic form to explain the response variable</data>
      <data key="d2">90ed6030c5a5a0764b7dcd4115b4d4d3</data>
    </node>
    <node id="FEATURES">
      <data key="d0">FEATURES</data>
      <data key="d1">Features are the characteristics or variables within the data that are relevant for analysis</data>
      <data key="d2">bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="STOCHASTIC_NATURE">
      <data key="d0">PROPERTY</data>
      <data key="d1">The stochastic nature of the response refers to the inherent randomness or variability in the data</data>
      <data key="d2">bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="FITTING_MODEL">
      <data key="d0">PROCEDURE</data>
      <data key="d1">Fitting the model involves finding the best set of values for the parameters that minimize the difference between the model predictions and the actual data</data>
      <data key="d2">bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="MODEL_ADEQUACY">
      <data key="d0">CRITERION</data>
      <data key="d1">MODEL_ADEQUACY, a critical aspect in statistical analysis, encapsulates the evaluation of a model's consistency with the data, its correct specification, and its ability to effectively address the primary inquiries of the analysis. This concept ensures that the statistical model is not only suitable for its intended purpose, encompassing description, estimation, and prediction, but also that it accurately reflects the underlying structure and relationships within the data. MODEL_ADEQUACY is fundamental in determining the reliability and validity of the model's outputs, thereby guiding the decision-making process based on the model's predictions or estimations.</data>
      <data key="d2">768c516c8b27fb9800427e848f02fc33,bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="PLAUSIBLE_MODELS">
      <data key="d0">MODELS</data>
      <data key="d1">Plausible models are alternative models that are also consistent with the data and can be compared to choose the most appropriate one</data>
      <data key="d2">bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="CHOSEN_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">The chosen model is the model that is selected as the most appropriate for answering the questions of interest</data>
      <data key="d2">bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="QUESTIONS_OF_INTEREST">
      <data key="d0">QUESTIONS</data>
      <data key="d1">Questions of interest are the specific research questions that the analysis aims to answer</data>
      <data key="d2">bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="GEORGE_BOX">
      <data key="d0">PERSON</data>
      <data key="d1">George E. Box is an eminent statistician who made significant contributions to the field of statistics and is known for his quote about models being wrong but useful</data>
      <data key="d2">bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="WASSERSTEIN_PAPER">
      <data key="d0">PAPER</data>
      <data key="d1">The Wasserstein paper is a publication that provides more information about George Box and his contributions to statistics</data>
      <data key="d2">bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="SIGNIFICANCE_JOURNAL">
      <data key="d0">JOURNAL</data>
      <data key="d1">Significance is a journal of the Royal Statistical Society that publishes articles on statistical topics</data>
      <data key="d2">bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="ROYAL_STATISTICAL_SOCIETY">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The Royal Statistical Society is a professional organization for statisticians that publishes journals and promotes the development of statistical science</data>
      <data key="d2">bb31f1c77bbba73300f735a100086a67</data>
    </node>
    <node id="GEORGE E. BOX">
      <data key="d0">PERSON</data>
      <data key="d1">George E. Box is a statistician known for the quote "Essentially all models are wrong, but some are useful."</data>
      <data key="d2">22061b1108c7f9963497b7a320be22b8</data>
    </node>
    <node id="WASSERSTEIN6">
      <data key="d0">PAPER</data>
      <data key="d1">Wasserstein6 is a paper that provides more information about George E. Box, published in Significance, a journal of the Royal Statistical Society.</data>
      <data key="d2">22061b1108c7f9963497b7a320be22b8</data>
    </node>
    <node id="SIGNIFICANCE">
      <data key="d0">JOURNAL</data>
      <data key="d1">Significance is a journal of the Royal Statistical Society where the paper Wasserstein6 was published.</data>
      <data key="d2">22061b1108c7f9963497b7a320be22b8</data>
    </node>
    <node id="ROYAL STATISTICAL SOCIETY">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">The Royal Statistical Society is an organization that publishes the journal Significance.</data>
      <data key="d2">22061b1108c7f9963497b7a320be22b8</data>
    </node>
    <node id="GAPMINDER">
      <data key="d0">ORGANIZATION</data>
      <data key="d1">Gapminder is an organization that provides data and visualizations on various economic indices for comparisons between countries.</data>
      <data key="d2">22061b1108c7f9963497b7a320be22b8</data>
    </node>
    <node id="ECONOMIC INDICES">
      <data key="d0">CONCEPT</data>
      <data key="d1">Economic indices are measures used to compare economic conditions between countries.</data>
      <data key="d2">22061b1108c7f9963497b7a320be22b8</data>
    </node>
    <node id="PARSIMONY">
      <data key="d0">CRITERION</data>
      <data key="d1">Parsimony, a fundamental criterion in the realm of model evaluation, is a principle deeply ingrained in both science and statistics. It advocates for the selection of the simplest explanation or model that sufficiently fits the data, emphasizing the importance of simplicity and efficiency. By adhering to the principle of parsimony, one avoids unnecessary complexity, ensuring that models are not overly complicated without a substantial gain in explanatory power. This approach is crucial in achieving a balance between model complexity and explanatory adequacy, guiding researchers and data scientists in their quest for the most effective and efficient models.</data>
      <data key="d2">188219b9e5b6b6368360840921877de9,22061b1108c7f9963497b7a320be22b8,768c516c8b27fb9800427e848f02fc33</data>
    </node>
    <node id="PLAUSIBILITY">
      <data key="d0">CRITERION</data>
      <data key="d1">Plausibility serves as a pivotal criterion in the evaluation of statistical models. It emphasizes the selection of models that not only align coherently with existing scientific knowledge but also exhibit practical sensibility within the context of the analyzed data. This dual consideration ensures that the models under scrutiny are both theoretically sound and applicable in real-world scenarios, thereby enhancing their reliability and utility in the field of statistical analysis.</data>
      <data key="d2">22061b1108c7f9963497b7a320be22b8,768c516c8b27fb9800427e848f02fc33</data>
    </node>
    <node id="DESCRIPTION">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">22061b1108c7f9963497b7a320be22b8</data>
    </node>
    <node id="PREDICTION">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">22061b1108c7f9963497b7a320be22b8</data>
    </node>
    <node id="EXPLANATION">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">22061b1108c7f9963497b7a320be22b8</data>
    </node>
    <node id="PREDICTOR_VARIABLES">
      <data key="d0">VARIABLES</data>
      <data key="d1">Predictor variables are the independent variables in a statistical model that are used to predict the response variable</data>
      <data key="d2">768c516c8b27fb9800427e848f02fc33</data>
    </node>
    <node id="PARAMETER_ESTIMATION">
      <data key="d0">STATISTICAL_METHOD</data>
      <data key="d1">Parameter estimation is the process of using data to estimate the values of unknown parameters in a statistical model</data>
      <data key="d2">768c516c8b27fb9800427e848f02fc33</data>
    </node>
    <node id="SIMPLER_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">A simpler model is a model with fewer parameters or assumptions, often preferred for its ease of interpretation and prediction</data>
      <data key="d2">768c516c8b27fb9800427e848f02fc33</data>
    </node>
    <node id="TRUE_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">The true model is the model that accurately represents the underlying data-generating process</data>
      <data key="d2">768c516c8b27fb9800427e848f02fc33</data>
    </node>
    <node id="ACCELERATION_DUE_TO_GRAVITY">
      <data key="d0">PHYSICAL_QUANTITY</data>
      <data key="d1">Acceleration due to gravity is the acceleration experienced by an object due to the gravitational force of the Earth</data>
      <data key="d2">768c516c8b27fb9800427e848f02fc33</data>
    </node>
    <node id="WASSERSTEIN_R">
      <data key="d0">AUTHOR</data>
      <data key="d1">R. Wasserstein is the author of the article "George Box: A model statistician" published in Significance in 2010</data>
      <data key="d2">768c516c8b27fb9800427e848f02fc33</data>
    </node>
    <node id="BALL">
      <data key="d0">OBJECT</data>
      <data key="d1">The ball is an object whose path is being modelled under the influence of gravity</data>
      <data key="d2">188219b9e5b6b6368360840921877de9</data>
    </node>
    <node id="GRAVITY">
      <data key="d0">FORCE</data>
      <data key="d1">Gravity is the force that causes the ball to fall, with an acceleration that is assumed to be constant in a room</data>
      <data key="d2">188219b9e5b6b6368360840921877de9</data>
    </node>
    <node id="ROOM">
      <data key="d0">ENVIRONMENT</data>
      <data key="d1">Room is the environment where the ball is falling, and where the acceleration due to gravity is assumed to be constant</data>
      <data key="d2">188219b9e5b6b6368360840921877de9</data>
    </node>
    <node id="SPACE">
      <data key="d0">ENVIRONMENT</data>
      <data key="d1">Space is an environment where the assumption of constant gravity does not hold, and the relationship between gravity and the path of objects is only valid locally</data>
      <data key="d2">188219b9e5b6b6368360840921877de9</data>
    </node>
    <node id="OCKHAMS_RAZOR">
      <data key="d0">PHILOSOPHICAL_PRINCIPLE</data>
      <data key="d1">Ockham's razor is a philosophical principle that states that the simplest explanation is usually the best</data>
      <data key="d2">188219b9e5b6b6368360840921877de9</data>
    </node>
    <node id="SIMPLE_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">A simple model is a statistical model with the fewest parameters that still adequately fits the data and captures its essential features</data>
      <data key="d2">188219b9e5b6b6368360840921877de9</data>
    </node>
    <node id="COMPLEX_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">The COMPLEX_MODEL is a statistical model characterized by its inclusion of numerous parameters and interactions, designed to more accurately fit and describe the data. This model is particularly useful when a simple linear regression fails to adequately capture the relationship between variables, as it can accommodate more complex relationships, such as those found in a quadratic regression model. However, the increased complexity can sometimes lead to a decrease in the precision of estimation and prediction. The COMPLEX_MODEL is a sophisticated tool in statistical analysis, enabling a deeper understanding of the data by considering a wider range of variables and their interactions.</data>
      <data key="d2">15c7b5750483a382ce59751008e86751,188219b9e5b6b6368360840921877de9,7347b44ffb25a066e43321f4eaf5a806,b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </node>
    <node id="TRANSFORMED_EXPLANATORY_VARIABLES">
      <data key="d0">VARIABLES</data>
      <data key="d1">The TRANSFORMED_EXPLANATORY_VARIABLES are a set of explanatory variables that have undergone modifications through non-linear transformations. These transformations, such as taking logarithms, are applied to the original explanatory variables in a linear model context. The purpose of these transformations is twofold: to improve the fit of the data to the model and to ensure compliance with the underlying assumptions of the linear model. By doing so, the TRANSFORMED_EXPLANATORY_VARIABLES can better capture the underlying relationships within the data, making the model more accurate and reliable.</data>
      <data key="d2">0328e428a30c44572676dd571dd1e9bd,188219b9e5b6b6368360840921877de9</data>
    </node>
    <node id="MODEL_DEVELOPMENT_STRATEGY">
      <data key="d0">CONCEPT</data>
      <data key="d1">A model development strategy is a set of guidelines for building and refining statistical models. It includes principles of plausibility and parsimony, which suggest that models should be both realistic and as simple as possible.</data>
      <data key="d2">0328e428a30c44572676dd571dd1e9bd</data>
    </node>
    <node id="RESIDUAL_PLOTS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Residual plots are graphical tools utilized in regression analysis to assess the fit and validity of a statistical model, specifically focusing on a linear model. These plots are created by graphing the residuals, which are the differences between observed and predicted values, against the fitted values or explanatory variables. They serve as a critical method for identifying patterns or trends in the residuals that might indicate violations of model assumptions. Such violations can include non-linearity, heteroscedasticity (non-constant variance), or non-normality of the residuals. Residual plots help in ensuring that the assumptions of constant variance and linearity, among others, are met, thereby providing a comprehensive check on the reliability and accuracy of the regression model. By visually inspecting these plots, data scientists and statisticians can determine if the model adequately represents the data or if adjustments are necessary to improve the model's fit.</data>
      <data key="d2">0328e428a30c44572676dd571dd1e9bd,0da640a09a395a50b6e16e047fa8d0d6,23fc620f1238c6a1b5c5e3a08e149c53,7cd6069e88e81548a237fa937adfecc6,a60af43e42c72a41fa90da06beb29d1b,b9ec8a6c7960cc6196ec94fd976f05b0,e7494d6cfc3e38a4d2f3f6b21ef6445d</data>
    </node>
    <node id="Q_Q_PLOTS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Q-Q plots, also known as Quantile-Quantile plots, are essential graphical tools in statistical analysis, particularly useful for comparing the distribution of a dataset to a theoretical distribution, often the normal distribution. These plots are pivotal in assessing the normality assumption by plotting the quantiles of the observed data against the quantiles of a theoretical distribution. When the points on the plot align closely to a straight line, it indicates that the data closely follow the theoretical distribution, suggesting normality.

In the context of residual analysis, Q-Q plots serve as a critical diagnostic tool. They are employed to compare the distribution of residuals to a theoretical distribution, such as the normal distribution, aiding in the evaluation of whether the residuals are normally distributed. This is crucial for validating assumptions in regression models and other statistical analyses, ensuring that the model's predictions are reliable and the errors are randomly distributed.</data>
      <data key="d2">0328e428a30c44572676dd571dd1e9bd,e7494d6cfc3e38a4d2f3f6b21ef6445d</data>
    </node>
    <node id="MODEL_ASSUMPTIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">MODEL_ASSUMPTIONS are the essential conditions that must hold true for a statistical model, particularly a normal linear or regression model, to be considered valid, reliable, and meaningful. These conditions include linearity, which ensures that the relationship between the independent and dependent variables is linear; independence, which requires that the errors are not correlated with each other; homoscedasticity, which stipulates that the variance of the errors is constant across all levels of the independent variables; and normality of errors or residuals, which assumes that the distribution of errors follows a normal distribution. Additionally, it is assumed that the response variable is measured on a continuous scale. Fulfilling these assumptions is crucial for the model's reliability and the validity of the inferences drawn from it, ensuring that the model accurately represents the underlying data and can be used to make valid predictions and conclusions.</data>
      <data key="d2">22093a562f5f05dc9891b45ab9bcbea8,674b8d5bb1f830d0fb944942514d1a16,aa13c33a7e61206e6021e2736002ca9a,c03eb12d07d48f9e94260f08dae10cdf,e7494d6cfc3e38a4d2f3f6b21ef6445d</data>
    </node>
    <node id="VIOLATIONS_OF_ASSUMPTIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Violations of assumptions refer to situations where the underlying conditions required for a statistical model to be valid are not met. This can lead to biased or inefficient parameter estimates, incorrect standard errors, and invalid hypothesis tests. Common violations include non-linearity, heteroscedasticity, non-normality, and autocorrelation.</data>
      <data key="d2">e7494d6cfc3e38a4d2f3f6b21ef6445d</data>
    </node>
    <node id="BETA_HAT">
      <data key="d0">ESTIMATOR VECTOR</data>
      <data key="d1">BETA_HAT, denoted as &#946;&#770; or &#946;b, is the least squares estimator for the parameters in a linear regression model. It is the estimated coefficient for the parameter Beta, calculated as the solution to the normal equations (XTX)^-1XTY, where X is the design matrix and Y is the observed data. This estimator minimizes the sum of squared residuals, making it the vector that reduces the residual sum of squares function S(&#946;). BETA_HAT is also the maximum likelihood estimate (MLE) of the parameter vector Beta, calculated from the observed data, and it is the vector of dimension p that minimizes the sum of squared differences in the least squares estimation. In the context of the price variable in the regression model, BETA_HAT represents the estimated regression coefficient, specifically -0.539 for the price variable in the linear model. It is noteworthy that BETA_HAT is the stationary point of S(&#946;) that satisfies XTX&#946;b - XT y = 0, and it is calculated using R for the estimates of the parameters Beta. BETA_HAT is a crucial component in statistical inference, providing insights into the relations and structure of the community of interest within the context of algorithmic analysis.</data>
      <data key="d2">01d5ee79489582b4135fc96f676b24a0,0443ab5e20a4f6b2f243c989ef6c723a,0ac60299320c55d642b3e38440c25f90,0b650eb2f1dcd603b64fec3c4b5cd24b,21ec28dfe2b2c18030d541d63e51f45e,248924760a2bfbc82501fd6b11cfa0aa,255685e281cc5a9edf073c700f425a6b,2673d078d29f2af78fab9b6eacd15e37,28cf5ff0c09fa5c0390267bb9aa3ce47,2b6d31b6bff4eae3a4809451c4fb9fa6,2de7a36b32bf79c8f32612c8aaa9daa8,2f2523c52c6d2869fb19f77b66ce8259,3d357cfa3ef0d00f49cf4acaeac1c9d1,45f31b040576e9f3b4def6d0466cc016,50a56c34050fb7f7709300a51399b150,542f546c5a131196e4701fb33c9b1dee,56ff186fc629e1e42f2759fc4b984199,5cc49d301d9cd1f8e20b92ab9d8346b0,6648f0d6deed51fb4fb25e6992a71ddf,679722cf8ce5ce5aee4e379528470efe,69ffba28a61d98d8d18f91c24b74dd4a,6a6f85d0a6e46196ab3a901fcc82a720,6b55b41598d5264f8dc6b72769748722,6ee02b38ae842fd5eac9a11c4fd6659f,7037e0369bfdaad5a730cabb2b44831c,7955aae3fd4ca51b9ef8843e13c1f517,82932abd152e0b84a1c26a2daa4c08df,9923e77ac6b3de95cb5026bc5e7fe8c0,9d300fc83afb3261af61b2ab9721cadc,9dddcd96af7b557e578b3f5f36efacd7,9fc2b1e8b2b61b557f88eb9e9c708597,a4a817bb79d6ae8812c808ca41d47f43,aa195e72eb5285a4bcae9c856af30a87,aac5b4f040b9c773bd1aa696dec469f6,ad799500572246a07f983a3b92c0e61f,b2c33cb151a8e7724ebfb7b2d88bc45f,b70a75a6412b2e5c44af50734844f4be,d14413709de2897231aaa83be3aa346f,d738df7d83784c8a41b3948271c537b6,d94760a5f9f6ea115fcc18024035a627,e7edd8b2874a350779ae20f1ecdf4733,f632f01188d2c6e3091a965580cb4600,f9b615b879f72501f338f8983d4cac3d,fc5b725f3c662c5471af20efdcc2dbff</data>
    </node>
    <node id="BETA_BAR">
      <data key="d0">ESTIMATED PARAMETER VECTOR</data>
      <data key="d1">Beta bar (&#946;b) is the notation used for the estimated parameter vector Beta</data>
      <data key="d2">01d5ee79489582b4135fc96f676b24a0</data>
    </node>
    <node id="COEF_MODEL">
      <data key="d0">FUNCTION</data>
      <data key="d1">coef(model) is an R function that returns the estimate of the parameter vector for a linear model object</data>
      <data key="d2">01d5ee79489582b4135fc96f676b24a0</data>
    </node>
    <node id="YB">
      <data key="d0">FITTED VALUES VECTOR</data>
      <data key="d1">YB, also denoted as Yb, represents the vector of fitted values in the linear regression model. It is calculated as X&#946;b, where X is the matrix of predictor variables, &#946; is the vector of coefficients, and b is the estimator of &#946;. YB signifies the back-transformed prediction for the response variable Y when the predictor X1 increases by one unit while X2 remains constant. This prediction is made in the original scale of the response variable, providing an estimate of the expected response for given values of the predictor variables. YB, therefore, encapsulates the estimated expected responses based on the regression model, serving as a crucial component in understanding the relationship between the predictors and the response variable.</data>
      <data key="d2">0443ab5e20a4f6b2f243c989ef6c723a,09391efd3b8c510205098b548bc8dc74,21e429490eeefe7d9c245058fd48ca68,2b6d31b6bff4eae3a4809451c4fb9fa6,5cc49d301d9cd1f8e20b92ab9d8346b0,6ee02b38ae842fd5eac9a11c4fd6659f,82932abd152e0b84a1c26a2daa4c08df,995fb26a0261f824952fa7b2fac3382e</data>
    </node>
    <node id="ZJ">
      <data key="d0">EXPLANATORY VARIABLE</data>
      <data key="d1">ZJ represents the jth observation of the explanatory variable within the context of a simple linear regression model. This variable plays a crucial role in establishing the relationship between the dependent and independent variables, where ZJ is used to predict the corresponding outcome based on its association with the dependent variable. In the simple linear regression framework, ZJ is one of several data points that contribute to estimating the parameters of the model, specifically the slope and intercept, through methods such as least squares estimation. The analysis of ZJ, alongside other observations, facilitates a deeper understanding of the underlying structure and patterns within the dataset, enabling accurate predictions and interpretations of the regression model.</data>
      <data key="d2">2b6d31b6bff4eae3a4809451c4fb9fa6,5cc49d301d9cd1f8e20b92ab9d8346b0</data>
    </node>
    <node id="YBJ">
      <data key="d0">FITTED VALUE</data>
      <data key="d1">YBJ, denoted as ybj in the context of statistical analysis, represents the fitted value for the jth unit of observation. This value is the predicted response for the jth observation, derived from the linear regression model. YBJ is specifically the estimated expected response for given values of the predictor variables, highlighting its role in understanding the relationship between the predictors and the response variable within the model. This fitted value is crucial for assessing the accuracy of the model's predictions and for conducting residual analysis, which is essential for validating the model's assumptions and identifying any patterns or outliers in the data.</data>
      <data key="d2">2b6d31b6bff4eae3a4809451c4fb9fa6,e6f79ceb0df54119a4dc71b2162ac50b</data>
    </node>
    <node id="EPSILON_BJ">
      <data key="d0">RESIDUAL</data>
      <data key="d1">Epsilon bj (&#949;bj) is the jth residual in the linear regression model associated with the entity EPSILON_BJ. This residual is calculated as the difference between the jth observed response value (yj) and its fitted value (ybj), which is the predicted value based on the linear regression model. It serves as an estimate of the error (&#949;j) in the model. The residuals (&#949;bj) are crucial for assessing the model's accuracy and the goodness of fit. They are computed as yj - xTj &#946;b, where yj represents the observed values, xTj denotes the transpose of the jth row of the design matrix, and &#946;b is the vector of estimated coefficients from the linear regression model. This calculation helps in understanding the discrepancies between the actual data points and the values predicted by the model, providing insights into the model's performance and potential areas for improvement.</data>
      <data key="d2">255685e281cc5a9edf073c700f425a6b,5cc49d301d9cd1f8e20b92ab9d8346b0,e6f79ceb0df54119a4dc71b2162ac50b</data>
    </node>
    <node id="XTJ">
      <data key="d0">PREDICTOR</data>
      <data key="d1">XTj is the transpose of the jth row of the predictor matrix X, used in the calculation of the fitted value Ybj.</data>
      <data key="d2">e6f79ceb0df54119a4dc71b2162ac50b</data>
    </node>
    <node id="BETA_B">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">BETA_B, also denoted as Beta b (\u03b2b) or Beta hat (b\u03b2), is a critical least squares estimator utilized in the statistical analysis of data. It serves as the coefficient for the price variable in the context of sales for Brand A, indicating the average reduction in sales associated with every additional pound charged. This parameter is pivotal in the calculation of fitted values, Ybj, providing insights into the relationship between price and sales volume for Brand A. BETA_B's role in the model highlights its significance in understanding and predicting sales trends based on pricing strategies.</data>
      <data key="d2">adbc52b340a69a8633c919c4fd2cd3f6,e6f79ceb0df54119a4dc71b2162ac50b</data>
    </node>
    <node id="RESIDUALS_COMMAND">
      <data key="d0">FUNCTION</data>
      <data key="d1">Residuals(model) is a command in R used to obtain the residuals from a fitted model.</data>
      <data key="d2">e6f79ceb0df54119a4dc71b2162ac50b</data>
    </node>
    <node id="DEVIANCE">
      <data key="d0">SUM_OF_SQUARES</data>
      <data key="d1">Deviance or Residual Sum of Squares (ResidSS) is a measure of the goodness of fit of a model, calculated as the sum of the squared residuals. It is used to assess how well the model fits the data.</data>
      <data key="d2">e6f79ceb0df54119a4dc71b2162ac50b</data>
    </node>
    <node id="PARAMETER_ESTIMATES">
      <data key="d0">ESTIMATION</data>
      <data key="d1">The PARAMETER_ESTIMATES refer to the values of the model parameters that are estimated from the data. These estimates are calculated to minimize the sum of squared residuals, thereby aiming to enhance the model's fit to the data. They represent the best guesses for the parameters of the statistical model based on the available data, optimizing the model's performance by reducing the discrepancies between the observed data and the model predictions.</data>
      <data key="d2">06199787dd7f75f7338dd24d4f3dc26e,22093a562f5f05dc9891b45ab9bcbea8,86ece4718d27d1a6c6a1f448cc850e2b</data>
    </node>
    <node id="RESIDUAL_SUM_OF_SQUARES">
      <data key="d0">STATISTICAL_MEASURE</data>
      <data key="d1">Residual sum of squares (ResidSS) is the sum of the squared residuals, a measure of the total deviation of the observed data points from the fitted model</data>
      <data key="d2">22093a562f5f05dc9891b45ab9bcbea8</data>
    </node>
    <node id="MODEL_DEVIANCE">
      <data key="d0">STATISTICAL_MEASURE</data>
      <data key="d1">Model deviance is another term for the residual sum of squares, indicating the discrepancy between the observed data and the model's predictions</data>
      <data key="d2">22093a562f5f05dc9891b45ab9bcbea8</data>
    </node>
    <node id="SIMPLE_REGRESSION_LINE">
      <data key="d0">MATHEMATICAL_MODEL</data>
      <data key="d1">The simple regression line y&#710; = 0.34 + 1.52z is a linear model that describes the relationship between the response variable y and the explanatory variable z</data>
      <data key="d2">22093a562f5f05dc9891b45ab9bcbea8</data>
    </node>
    <node id="EXERCISE_9">
      <data key="d0">EXERCISE</data>
      <data key="d1">Exercise 9 involves calculating the fitted values, residuals, and deviance for a given simple regression line and dataset</data>
      <data key="d2">22093a562f5f05dc9891b45ab9bcbea8</data>
    </node>
    <node id="ZI">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">Zi is the explanatory variable in the dataset provided for Exercise 9, with values 1, 2, 2, 3, 8</data>
      <data key="d2">22093a562f5f05dc9891b45ab9bcbea8</data>
    </node>
    <node id="NORMAL_LINEAR_MODEL">
      <data key="d0">MATHEMATICAL_MODEL</data>
      <data key="d1">The NORMAL_LINEAR_MODEL is a comprehensive statistical framework that posits the response variable Y as a linear combination of the explanatory variables X and an error term epsilon, mathematically represented as Y = X&#946; + epsilon. This model is grounded in the assumption that the errors are normally distributed, which is a critical component for ensuring the validity of statistical inferences. Additionally, the normal linear model assumes that the data adheres to a normal distribution and is linearly related to the parameters, reinforcing the linear relationship between the response variable and the explanatory variables. The normal distribution of errors, coupled with the linear relationship, forms the backbone of the normal linear model, making it a powerful tool for analyzing and predicting outcomes in various fields such as economics, social sciences, and engineering.</data>
      <data key="d2">22093a562f5f05dc9891b45ab9bcbea8,c03eb12d07d48f9e94260f08dae10cdf,d738df7d83784c8a41b3948271c537b6,f483798b15ef305e7826fd7142379e03</data>
    </node>
    <node id="INDEPENDENCE">
      <data key="d0">ASSUMPTION</data>
      <data key="d1">Independence, a fundamental assumption in regression analysis, pertains to the property of errors being independent of each other. This assumption, symbolically represented as \u03f51, . . . , \u03f5n, is crucial for ensuring the validity of the regression model. It signifies that the errors in the model are not influenced by one another, a condition that is essential for accurate statistical inference and model reliability. Without independence, the model's estimators may become biased, leading to incorrect conclusions about the relationships between variables. Thus, Independence is a critical aspect to verify in the context of algorithmic analysis, particularly when dealing with linear regression models.</data>
      <data key="d2">0da640a09a395a50b6e16e047fa8d0d6,3cbe71f7649e84cd67cb3fa0d3e632cf,e361ac139c268d5c3f3623f920e68af2</data>
    </node>
    <node id="HOMOSCEDASTICITY">
      <data key="d0">ASSUMPTION</data>
      <data key="d1">Homoscedasticity is a fundamental assumption in statistical modeling, particularly in linear regression analysis, where it pertains to the property of the errors or residuals. This assumption posits that the variance of the errors (denoted as \u03f51, . . . , \u03f5n) is constant across all levels of the independent or explanatory variables. In other words, homoscedasticity ensures that the spread of the residuals remains uniform regardless of the value of the predictor variables. This property is crucial for the validity of statistical inferences in regression analysis, as it underpins the reliability of hypothesis tests and confidence intervals. Homoscedasticity is also assumed for the response variables, reinforcing the notion that the variability in the data does not change with the level of the independent variables. This consistency in variance is a key component in ensuring that the least squares estimators are unbiased and have minimum variance, thus providing accurate predictions and reliable model interpretations.</data>
      <data key="d2">0da640a09a395a50b6e16e047fa8d0d6,3cbe71f7649e84cd67cb3fa0d3e632cf,60cc94e681863c9fcc6f9be1e500f840,7cd6069e88e81548a237fa937adfecc6,a60af43e42c72a41fa90da06beb29d1b,e361ac139c268d5c3f3623f920e68af2</data>
    </node>
    <node id="NORMALITY">
      <data key="d0">ASSUMPTION</data>
      <data key="d1">NORMALITY is a fundamental assumption in regression analysis, particularly in linear regression models. It posits that the errors or residuals, which are the differences between the observed and predicted values, are normally distributed. This normal distribution of errors is crucial for the validity of statistical inferences and hypothesis tests in regression analysis. Additionally, it is often assumed that the response variables also adhere to a normal distribution, further emphasizing the importance of NORMALITY in ensuring the reliability of regression models. This property ensures that the least squares estimators, which are used to estimate the parameters of the model, are unbiased and have minimum variance, making them the best linear unbiased estimators (BLUE) under the Gauss-Markov theorem. Residual analysis, a key component of regression diagnostics, is used to check the NORMALITY assumption by examining the distribution of residuals. Any deviation from NORMALITY can lead to inaccurate predictions and unreliable hypothesis testing, necessitating the use of alternative models or transformations to achieve NORMALITY.</data>
      <data key="d2">0da640a09a395a50b6e16e047fa8d0d6,3cbe71f7649e84cd67cb3fa0d3e632cf,7cd6069e88e81548a237fa937adfecc6,e361ac139c268d5c3f3623f920e68af2,ef24ca5edd06893b737e6a1c8a9825f6</data>
    </node>
    <node id="HETEROSCEDASTICITY">
      <data key="d0">VIOLATION</data>
      <data key="d1">Heteroscedasticity is a critical concept in statistical analysis, particularly within the context of linear regression models. It represents a violation of the fundamental assumption of homoscedasticity, which posits that the variance of the error terms should remain constant across all levels of the independent variables. When heteroscedasticity is present, the variance of the errors is not constant, leading to biased standard errors and potentially unreliable hypothesis tests. This condition can undermine the validity of regression analysis, as it affects the precision of the estimated coefficients and the reliability of statistical inferences. To address heteroscedasticity, one possible solution is to apply a transformation to the response variable, which can help stabilize the variance of the error terms and restore the assumptions necessary for accurate statistical modeling.</data>
      <data key="d2">07951ffe6787af44aa60c90c69e62f83,0da640a09a395a50b6e16e047fa8d0d6,7347b44ffb25a066e43321f4eaf5a806,b9eb75001a4f68f7240b2ca9e0d79eb8,c03eb12d07d48f9e94260f08dae10cdf,d71b402ab9edbb4347e09c7af3257cf5</data>
    </node>
    <node id="IDENTICALLY_DISTRIBUTED">
      <data key="d0">ASSUMPTION</data>
      <data key="d1">Identically distributed is the assumption that the errors are identically distributed, which is not true for the response variables due to their dependence on the explanatory variables</data>
      <data key="d2">0da640a09a395a50b6e16e047fa8d0d6</data>
    </node>
    <node id="DATA_COLLECTION">
      <data key="d0">PROCESS</data>
      <data key="d1">The data collection process should ensure that the errors are independent, and potential violations of this assumption can be identified by considering the process</data>
      <data key="d2">0da640a09a395a50b6e16e047fa8d0d6</data>
    </node>
    <node id="SEQUENTIAL_DATA">
      <data key="d0">VIOLATION</data>
      <data key="d1">SEQUENTIAL_DATA, which pertains to sequential data collection, is characterized by data points that are gathered in a specific sequence. This method of data gathering can introduce errors that are correlated over time, a phenomenon that directly challenges the fundamental assumption of independence in statistical models. It is crucial to properly account for these potential time-correlated errors when designing and interpreting models to ensure accurate and reliable results.</data>
      <data key="d2">0da640a09a395a50b6e16e047fa8d0d6,7cd6069e88e81548a237fa937adfecc6</data>
    </node>
    <node id="SPATIAL_DATA">
      <data key="d0">VIOLATION</data>
      <data key="d1">SPATIAL_DATA, also known as spatial data, is characterized by data points that incorporate a geographical or spatial component. This type of data is unique in that it can result in nearby observations being correlated, a feature that often challenges the fundamental assumption of independence in statistical analysis. This correlation among nearby observations is an inherent property of SPATIAL_DATA, reflecting the geographical proximity of the data points. Understanding and accounting for this correlation is crucial when analyzing SPATIAL_DATA to ensure accurate and reliable statistical inferences.</data>
      <data key="d2">0da640a09a395a50b6e16e047fa8d0d6,7cd6069e88e81548a237fa937adfecc6</data>
    </node>
    <node id="NORMAL_Q_Q_PLOTS">
      <data key="d0">ANALYSIS_TOOL</data>
      <data key="d1">NORMAL_Q_Q_PLOTS: Normal Q-Q plots are sophisticated graphical tools utilized in statistical analysis to evaluate the assumption of normality within the residuals of a model. These plots achieve this by comparing the quantiles of the observed residuals against the quantiles of a standard normal distribution. This comparison helps in identifying any deviations from normality, which is a critical assumption in many statistical models, particularly in regression analysis. By visually inspecting the Normal Q-Q plot, analysts can discern patterns that indicate whether the data conform to a normal distribution, thereby validating or questioning the normality assumption underlying the statistical model.</data>
      <data key="d2">7cd6069e88e81548a237fa937adfecc6,e361ac139c268d5c3f3623f920e68af2</data>
    </node>
    <node id="R_SOFTWARE">
      <data key="d0">SOFTWARE</data>
      <data key="d1">R is a programming language and software environment for statistical computing and graphics</data>
      <data key="d2">7cd6069e88e81548a237fa937adfecc6</data>
    </node>
    <node id="NULL_PLOT">
      <data key="d0">ANALYSIS_TOOL</data>
      <data key="d1">A null plot, in the context of statistical analysis, refers to a specific type of residual plot that exhibits no discernible patterns. This characteristic is crucial as it signifies that the residuals are randomly distributed around a horizontal line, without any systematic structure. The absence of patterns in a null plot is a strong indicator that the model assumptions underlying the statistical analysis are met, ensuring the validity and reliability of the model. This feature is particularly useful in verifying that the residuals are not correlated with the fitted values, a critical assumption in linear regression models. Thus, a null plot serves as a visual diagnostic tool for assessing the appropriateness of the model fit to the data.</data>
      <data key="d2">7cd6069e88e81548a237fa937adfecc6,aa13c33a7e61206e6021e2736002ca9a</data>
    </node>
    <node id="DISCRETE">
      <data key="d0">PROPERTY</data>
      <data key="d1">Discrete refers to data that can only take certain values, typically integers or categories</data>
      <data key="d2">aa13c33a7e61206e6021e2736002ca9a</data>
    </node>
    <node id="SMOOTHER">
      <data key="d0">ANALYSIS_TOOL</data>
      <data key="d1">The entity "SMOOTHER" refers to a non-parametric statistical tool employed in residual analysis. It is specifically used to estimate the mean of the residuals as a function of the fitted values, providing valuable insights into the structure of the residual plot. In the context provided, the smoother is described to have the shape of an upturned parabola, which aids in identifying patterns within the residuals, thereby assisting in the evaluation of the model's fit and the detection of any systematic deviations from randomness. This function is crucial for understanding the relations and structure of the community of interest, enhancing the accuracy and reliability of statistical inferences drawn from the data.</data>
      <data key="d2">82cfcd5865cffe55e965a50745656e60,aa13c33a7e61206e6021e2736002ca9a</data>
    </node>
    <node id="HORIZONTAL_LINE">
      <data key="d0">PLOT_ELEMENT</data>
      <data key="d1">A horizontal line is a line that runs parallel to the x-axis in a plot, often used as a reference line in residual plots to indicate no systematic pattern in the residuals</data>
      <data key="d2">aa13c33a7e61206e6021e2736002ca9a</data>
    </node>
    <node id="VARIABILITY">
      <data key="d0">PROPERTY</data>
      <data key="d1">Variability refers to the spread or dispersion of data points around a central value, often measured by the standard deviation or variance</data>
      <data key="d2">aa13c33a7e61206e6021e2736002ca9a</data>
    </node>
    <node id="SMOOTHED_RESIDUAL_PLOT">
      <data key="d0">PLOT</data>
      <data key="d1">A smoothed residual plot is a graphical representation of the residuals from a regression model, with a smoother added to estimate the mean of the residuals as a function of the fitted values</data>
      <data key="d2">674b8d5bb1f830d0fb944942514d1a16</data>
    </node>
    <node id="SMOOTHING_CURVE">
      <data key="d0">CURVE</data>
      <data key="d1">A smoothing curve, in the context of statistical analysis, is a versatile tool employed to elucidate underlying trends or patterns within a set of data points. This curve is meticulously fitted to the data, serving a dual purpose: it smooths out erratic fluctuations, thereby facilitating a clearer identification of trends, and it assists in residual analysis. In regression analysis, the smoothing curve plays a pivotal role by providing a non-parametric estimate of the mean of the residuals as a function of the fitted values. This function is crucial for visualizing the overall pattern of the residuals and assessing the assumption of constant variance, a critical component in ensuring the reliability of regression models. Through its application in residual plots, the smoothing curve enables a comprehensive evaluation of model fit and the identification of any systematic patterns that might indicate model inadequacies.</data>
      <data key="d2">23fc620f1238c6a1b5c5e3a08e149c53,312309b45c59e1c84695ac3c7e202742,674b8d5bb1f830d0fb944942514d1a16,b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </node>
    <node id="HORIZONTAL_LINE_AT_ZERO">
      <data key="d0">LINE</data>
      <data key="d1">A horizontal line at zero is the expected pattern of the smoothing curve if the model assumptions hold</data>
      <data key="d2">674b8d5bb1f830d0fb944942514d1a16</data>
    </node>
    <node id="WELL_FITTING_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">A well-fitting model is a regression model that adequately describes the relationship between the explanatory variable and the response variable</data>
      <data key="d2">674b8d5bb1f830d0fb944942514d1a16</data>
    </node>
    <node id="UNACCEPTABLE_RESIDUAL_PLOTS">
      <data key="d0">PLOT</data>
      <data key="d1">Unacceptable residual plots are residual plots that indicate that the model assumptions do not hold</data>
      <data key="d2">674b8d5bb1f830d0fb944942514d1a16</data>
    </node>
    <node id="WESTERN_CEDAR_TREE_DATA">
      <data key="d0">DATA</data>
      <data key="d1">The Western cedar tree data is a comprehensive dataset that was prominently featured in the context of the previous chapter. This dataset serves as a critical case study, particularly in the realm of residual plots, where it exemplifies an unacceptable residual plot scenario. Notably, the data has been utilized to highlight issues and complexities within statistical analysis, specifically focusing on the residuals and their interpretation. Additionally, the Western cedar tree data is noteworthy for its application of a transformation to the explanatory variable, demonstrating the impact of such modifications on the analysis and potentially improving the model's fit and interpretability. This dataset, therefore, not only provides insights into the challenges of residual analysis but also underscores the importance of variable transformation in achieving more reliable statistical outcomes.</data>
      <data key="d2">15c7b5750483a382ce59751008e86751,674b8d5bb1f830d0fb944942514d1a16,b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </node>
    <node id="CURVED_RELATIONSHIP">
      <data key="d0">CONCEPT</data>
      <data key="d1">A curved relationship is a non-linear relationship between the explanatory variable and the response variable</data>
      <data key="d2">674b8d5bb1f830d0fb944942514d1a16</data>
    </node>
    <node id="LINEARITY_ASSUMPTION">
      <data key="d0">ASSUMPTION</data>
      <data key="d1">The linearity assumption, a fundamental principle in regression analysis, posits that the relationship between the response variable and the explanatory variables is linear. This condition is crucial for the validity of statistical models, ensuring that the model accurately represents the data. The assumption underpins the basic structure of regression analysis, enabling the use of linear equations to predict the response variable based on the explanatory variables. It is a critical aspect of model specification, as deviations from linearity can lead to biased estimates and incorrect inferences. Thus, the linearity assumption is a cornerstone of statistical modeling, particularly in the context of regression analysis, where it ensures the model's ability to capture the underlying linear relationship between variables.</data>
      <data key="d2">0fbc9037ca9a440e79e9ac05664b9b3d,7347b44ffb25a066e43321f4eaf5a806,b9eb75001a4f68f7240b2ca9e0d79eb8,b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </node>
    <node id="TRANSFORMATION">
      <data key="d0">FUNCTION</data>
      <data key="d1">Transformation, in the context of statistical analysis, refers to a mathematical operation that is strategically applied to a variable, either the response or explanatory, with the purpose of altering its scale or distribution. This technique is commonly employed to meet the assumptions of a statistical model, particularly when dealing with issues such as heteroscedasticity (non-constant variance) and non-linearity. By stabilizing the variance and addressing non-linearity, transformation helps in reducing the influence of unusual observations, thereby enhancing the model's performance and reliability. This method is crucial in regression analysis, where it ensures that the data meets the necessary conditions for accurate and meaningful inference.</data>
      <data key="d2">0fbc9037ca9a440e79e9ac05664b9b3d,7347b44ffb25a066e43321f4eaf5a806,b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </node>
    <node id="SQUARE_ROOT_TRANSFORM">
      <data key="d0">FUNCTION</data>
      <data key="d1">The square root transform, often denoted as "SQUARE_ROOT_TRANSFORM", is a prevalent data transformation technique applied to non-negative explanatory variables. This method is particularly useful in addressing non-linearity issues within the data. By taking the square root of a variable, the square root transform helps to stabilize variance and normalize the distribution, making it a valuable tool in statistical analysis and modeling. This transformation is commonly used when dealing with variables that exhibit non-negative values, enhancing the linearity of relationships between variables and improving the overall model fit.</data>
      <data key="d2">15c7b5750483a382ce59751008e86751,b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </node>
    <node id="LOG_TRANSFORM">
      <data key="d0">FUNCTION</data>
      <data key="d1">The LOG_TRANSFORM, also known as the log transformation, is a widely utilized mathematical operation in statistical analysis. It involves taking the logarithm of a variable, typically when the variable is positive, to address several issues in data. This transformation is commonly applied to both response and explanatory variables to stabilize variance, making the distribution more symmetrical and reducing the influence of unusual observations or outliers. By linearizing relationships and addressing non-linearity, the log transform helps in meeting the assumption of linearity and can also mitigate the problem of heteroscedasticity, ensuring that the variance remains constant across different levels of an independent variable. This method is particularly effective in reducing the positive skew of a distribution, thereby enhancing the accuracy and reliability of statistical models.</data>
      <data key="d2">0fbc9037ca9a440e79e9ac05664b9b3d,15c7b5750483a382ce59751008e86751,995fb26a0261f824952fa7b2fac3382e,b9ec8a6c7960cc6196ec94fd976f05b0,c03eb12d07d48f9e94260f08dae10cdf,f9d6c3504b8f8b5c25550076e45f8270</data>
    </node>
    <node id="MONOTONIC_NON_LINEAR">
      <data key="d0">CONCEPT</data>
      <data key="d1">Describes a relationship between variables that is consistently increasing or decreasing but not following a straight line</data>
      <data key="d2">15c7b5750483a382ce59751008e86751</data>
    </node>
    <node id="TRANSFORM">
      <data key="d0">METHOD</data>
      <data key="d1">Transformation is a remedial action applied to the explanatory variable to address non-linearity, such as square root or log-transform</data>
      <data key="d2">15c7b5750483a382ce59751008e86751</data>
    </node>
    <node id="NON_MONOTONIC_NON_LINEAR">
      <data key="d0">CONCEPT</data>
      <data key="d1">Describes a relationship between variables that is not consistently increasing or decreasing and not following a straight line</data>
      <data key="d2">15c7b5750483a382ce59751008e86751</data>
    </node>
    <node id="RESIDUAL_PLOT">
      <data key="d0">PLOT</data>
      <data key="d1">A Residual Plot is a crucial diagnostic tool in statistical analysis, particularly in the context of regression models. It is a graphical representation that displays the residuals, which are the differences between the observed and predicted values, against the fitted values or the independent variable on the horizontal axis. This plot helps in assessing the fit of a model, identifying patterns in the residuals that might indicate non-linearity or heteroscedasticity, and checking the assumptions of a regression model. By visually inspecting the Residual Plot, one can determine if the model's predictions are systematically off or if there are any unusual patterns that could suggest issues with the model's assumptions. Overall, the Residual Plot serves as a comprehensive visual summary of the model's performance and the structure of the data, providing insights into the relations and structure of the community of interest within the context of algorithmic analysis.</data>
      <data key="d2">15c7b5750483a382ce59751008e86751,312309b45c59e1c84695ac3c7e202742,521acf88540d5897188c9ec65b17e6a6,7347b44ffb25a066e43321f4eaf5a806,82cfcd5865cffe55e965a50745656e60,b9eb75001a4f68f7240b2ca9e0d79eb8,ef24ca5edd06893b737e6a1c8a9825f6</data>
    </node>
    <node id="VARIATION">
      <data key="d0">CONCEPT</data>
      <data key="d1">Variation, a critical concept in statistical analysis, characterizes the dispersion or spread of data points around a central value. It is observed that as one moves along the horizontal axis, the spread of the observations, which is a measure of variation, tends to increase. This phenomenon highlights the dynamic nature of data distribution and its tendency to become more dispersed with changes in the variable measured along the horizontal axis. Understanding variation is essential for interpreting the structure and relations within a dataset, as it provides insights into the consistency of the data and the presence of outliers or anomalies.</data>
      <data key="d2">15c7b5750483a382ce59751008e86751,7347b44ffb25a066e43321f4eaf5a806</data>
    </node>
    <node id="HORIZONTAL_AXIS">
      <data key="d0">AXIS</data>
      <data key="d1">Horizontal axis is the x-axis in a scatterplot or residual plot, typically representing the explanatory variable</data>
      <data key="d2">15c7b5750483a382ce59751008e86751</data>
    </node>
    <node id="FIGURE_3_3">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 3.3 is a visual representation showing a scatterplot and residual plot of an example where the relationship between the response variable and the explanatory variable is non-monotonic and non-linear</data>
      <data key="d2">15c7b5750483a382ce59751008e86751</data>
    </node>
    <node id="NON_MONOTONIC_NON_LINEAR_RELATIONSHIP">
      <data key="d0">RELATIONSHIP_TYPE</data>
      <data key="d1">The relationship between the response variable and the explanatory variable is non-monotonic and non-linear, meaning it does not follow a consistent increasing or decreasing pattern.</data>
      <data key="d2">82cfcd5865cffe55e965a50745656e60</data>
    </node>
    <node id="QUADRATIC_REGRESSION_MODEL">
      <data key="d0">MODEL_TYPE</data>
      <data key="d1">A quadratic regression model is a statistical model that uses a quadratic function to fit the data. It is likely to work well when the relationship between variables is non-linear.</data>
      <data key="d2">82cfcd5865cffe55e965a50745656e60</data>
    </node>
    <node id="VARIANCE_INCREASING_WITH_FITTED_VALUES">
      <data key="d0">VARIANCE_PATTERN</data>
      <data key="d1">The variance of the residuals is increasing with the fitted values, which can be observed in the residual plot resembling a 'right-opening megaphone'.</data>
      <data key="d2">82cfcd5865cffe55e965a50745656e60</data>
    </node>
    <node id="TRANSFORM_RESPONSE_VARIABLE">
      <data key="d0">DATA_TRANSFORMATION</data>
      <data key="d1">Transforming the response variable is an approach to correct for increasing variance. Common transformations include taking the square-root or logarithm of the response variable.</data>
      <data key="d2">82cfcd5865cffe55e965a50745656e60</data>
    </node>
    <node id="VARIANCE">
      <data key="d0">STATISTICAL_MEASURE</data>
      <data key="d1">Variance, a pivotal concept in statistical analysis, serves as a measure of the spread or dispersion of a set of values. It quantifies the degree to which these values deviate from the mean, providing insights into the variability within the dataset. Notably, in certain scenarios, the variance is observed to increase in tandem with the mean, highlighting a direct relationship between the central tendency and the dispersion of data points. This statistical measure is crucial for understanding the structure and relations within a community of interest, as it helps in identifying patterns and anomalies that might not be apparent from the mean alone. Utilizing tools like R for data analysis, a data scientist can leverage variance to make informed decisions and draw meaningful conclusions from complex datasets.</data>
      <data key="d2">66f7fae9d896ff2b3fd40186cc833503,b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </node>
    <node id="SQUARE_ROOT_TRANSFORMATION">
      <data key="d0">TRANSFORMATION_METHOD</data>
      <data key="d1">Square root transformation is a method used to stabilize the variance of the response variable</data>
      <data key="d2">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </node>
    <node id="LOGARITHM_TRANSFORMATION">
      <data key="d0">TRANSFORMATION_METHOD</data>
      <data key="d1">Logarithm transformation is a method used to stabilize the variance of the response variable, particularly when the response variable is positive</data>
      <data key="d2">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </node>
    <node id="MEGAPHONE_SHAPE_RESIDUAL_PLOT">
      <data key="d0">PLOT_TYPE</data>
      <data key="d1">A megaphone shape residual plot is a common pattern in residual plots, indicating increasing variance with the explanatory variable</data>
      <data key="d2">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </node>
    <node id="NON-LINEARITY">
      <data key="d0">STATISTICAL_TERM</data>
      <data key="d1">In the context of statistical analysis, NON-LINEARITY is a significant condition that characterizes the relationship between the response variable and the explanatory variables. This relationship is notably non-linear, meaning that the change in the response variable does not increase or decrease at a constant rate with respect to the explanatory variables. NON-LINEARITY indicates that the pattern of association between the variables is more complex than a straight line, often requiring more sophisticated models to accurately capture the underlying dynamics. This condition is crucial to identify and understand, as it can significantly impact the accuracy and reliability of statistical models and predictions.</data>
      <data key="d2">7347b44ffb25a066e43321f4eaf5a806,b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </node>
    <node id="TRANSFORMATION_OF_EXPLANATORY_VARIABLE">
      <data key="d0">TRANSFORMATION_METHOD</data>
      <data key="d1">Transformation of the explanatory variable is a method used to address non-linearity in the relationship between the response and explanatory variables</data>
      <data key="d2">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </node>
    <node id="CURVE">
      <data key="d0">SHAPE</data>
      <data key="d1">Curve refers to a non-linear relationship between the response and explanatory variables, as opposed to a straight line</data>
      <data key="d2">7347b44ffb25a066e43321f4eaf5a806</data>
    </node>
    <node id="REMEDIAL_ACTION">
      <data key="d0">ACTION</data>
      <data key="d1">Remedial action, in the context of statistical analysis, encompasses a series of strategic steps undertaken to address and rectify issues that arise within a model. These issues can manifest as non-linearity, heteroscedasticity, or violations of model assumptions. The entity, Remedial_Action, highlights the importance of proactive measures in maintaining the integrity and accuracy of statistical models. Such actions can involve the transformation of variables to better fit model requirements, the adoption of alternative models that may be more suitable for the data at hand, or the collection of additional data to strengthen the model's predictive power. Through these interventions, the aim is to enhance the model's performance, ensuring that it accurately reflects the underlying relationships within the data.</data>
      <data key="d2">7347b44ffb25a066e43321f4eaf5a806,c03eb12d07d48f9e94260f08dae10cdf</data>
    </node>
    <node id="RANDOM_VARIATION">
      <data key="d0">STATISTICAL_VARIATION</data>
      <data key="d1">Random variation is the natural fluctuation in data that is not systematic and cannot be explained by the model</data>
      <data key="d2">7347b44ffb25a066e43321f4eaf5a806</data>
    </node>
    <node id="COURSEWORK">
      <data key="d0">ACADEMIC_WORK</data>
      <data key="d1">Coursework refers to the assignments or projects given to students as part of their academic program</data>
      <data key="d2">7347b44ffb25a066e43321f4eaf5a806</data>
    </node>
    <node id="ACCEPTABILITY">
      <data key="d0">CRITERION</data>
      <data key="d1">Acceptability is a standard used to judge whether a statistical model or its residual plot meets the necessary criteria for validity</data>
      <data key="d2">7347b44ffb25a066e43321f4eaf5a806</data>
    </node>
    <node id="ACCEPTABLE_RESIDUAL_PLOTS">
      <data key="d0">PLOTS</data>
      <data key="d1">Acceptable residual plots are those that do not show any systematic patterns, suggesting that the model assumptions are met. They typically exhibit a random scatter of points around the horizontal line at zero, indicating no bias in the residuals and constant variance.</data>
      <data key="d2">23fc620f1238c6a1b5c5e3a08e149c53</data>
    </node>
    <node id="REMEDIAL_ACTIONS">
      <data key="d0">ACTIONS</data>
      <data key="d1">Remedial actions, in the context of statistical analysis, are strategic steps taken by data scientists to address and rectify issues that arise when residual plots reveal violations of model assumptions. These violations can stem from a variety of sources, including non-linearity, heteroscedasticity, or non-normality of residuals. The primary goal of remedial actions is to improve the model's fit and reliability by ensuring that its underlying assumptions are met as closely as possible.

To achieve this, a range of techniques might be employed. These can include data transformations, such as logarithmic or power transformations, which are used to stabilize variance or linearize relationships between variables. Additionally, remedial actions might involve the addition or removal of variables from the model to better capture the underlying structure of the data. In some cases, changing the model itself, such as switching from a linear regression model to a generalized linear model or a non-parametric model, might be necessary to address the specific violations observed.

Overall, remedial actions are an essential component of the iterative process of model building and refinement, ensuring that the final model is robust, accurate, and suitable for making reliable predictions or inferences.</data>
      <data key="d2">23fc620f1238c6a1b5c5e3a08e149c53,e361ac139c268d5c3f3623f920e68af2</data>
    </node>
    <node id="SIMULATED_DATA">
      <data key="d0">DATA</data>
      <data key="d1">The SIMULATED_DATA refers to artificial data sets that have been meticulously created to evaluate and test the efficacy of statistical methods or models, particularly in the context of regression analysis. This data is utilized to ensure that the models or methods function as anticipated under controlled and known conditions. By employing SIMULATED_DATA, data scientists and statisticians can validate the reliability and accuracy of their models before applying them to real-world data, thereby enhancing the robustness of their statistical analyses.</data>
      <data key="d2">23fc620f1238c6a1b5c5e3a08e149c53,312309b45c59e1c84695ac3c7e202742</data>
    </node>
    <node id="LINEAR_REGRESSION_MODELS">
      <data key="d0">MODELS</data>
      <data key="d1">Linear regression models are statistical models used to analyze the relationship between one or more independent variables and a dependent variable. They assume a linear relationship between the variables and are used to predict the dependent variable based on the independent variables.</data>
      <data key="d2">23fc620f1238c6a1b5c5e3a08e149c53</data>
    </node>
    <node id="LINEAR_MODEL_ASSUMPTIONS">
      <data key="d0">ASSUMPTIONS</data>
      <data key="d1">Linear model assumptions are the conditions that need to be met for linear regression models to provide valid and reliable results. These include assumptions of linearity, independence, homoscedasticity (constant variance), and normality of residuals.</data>
      <data key="d2">23fc620f1238c6a1b5c5e3a08e149c53</data>
    </node>
    <node id="CONSTANT_VARIANCE_ASSUMPTION">
      <data key="d0">ASSUMPTION</data>
      <data key="d1">The constant variance assumption, also known as homoscedasticity, is one of the assumptions of linear regression models. It states that the variance of the residuals should be constant across all levels of the independent variables.</data>
      <data key="d2">23fc620f1238c6a1b5c5e3a08e149c53</data>
    </node>
    <node id="DISTRIBUTION">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">The DISTRIBUTION, in the context of statistical analysis, encompasses the characteristics of data distribution, which can manifest as either skewed or conforming to a normal distribution. This statistical DISTRIBUTION is pivotal in identifying influential observations within a dataset, particularly when considering parameters such as p and n - p degrees of freedom. These degrees of freedom are crucial in determining the shape and reliability of the DISTRIBUTION, thereby facilitating a deeper understanding of the data's structure and potential outliers.</data>
      <data key="d2">312309b45c59e1c84695ac3c7e202742,323899f01972255cd3278bccee20d5d8</data>
    </node>
    <node id="VARIANCE_ASSUMPTION">
      <data key="d0">VARIANCE_ASSUMPTION</data>
      <data key="d1">The constant variance assumption is one of the assumptions of the regression model, which states that the variance of the residuals is constant across the range of the predictor variable</data>
      <data key="d2">312309b45c59e1c84695ac3c7e202742</data>
    </node>
    <node id="PREDICTOR_VARIABLE">
      <data key="d0">PREDICTOR_VARIABLE</data>
      <data key="d1">The predictor variable is the variable used to predict the response variable</data>
      <data key="d2">312309b45c59e1c84695ac3c7e202742</data>
    </node>
    <node id="STATISTICAL_INFERENCE">
      <data key="d0">ANALYSIS_METHOD</data>
      <data key="d1">Statistical inference, a fundamental component of data science and statistical analysis, is a method employed to draw conclusions about a population based on sample data. This process involves using data analysis to infer properties of an underlying distribution of probability, enabling the making of informed conclusions about the population from which the data was drawn. Through statistical inference, one can estimate population parameters, test hypotheses, and construct confidence intervals, all of which are crucial for understanding the relations and structure of the community of interest within the context of algorithmic analysis. Tools like R for data analysis are often utilized to leverage the deep understanding of statistical inference, providing insights and solutions in various fields such as linear regression, least squares estimators, and residual analysis.</data>
      <data key="d2">521acf88540d5897188c9ec65b17e6a6,ef24ca5edd06893b737e6a1c8a9825f6</data>
    </node>
    <node id="ROBUSTNESS">
      <data key="d0">PROPERTY</data>
      <data key="d1">Robustness is a property of statistical inference that it is not greatly affected by departures from the normality assumption, especially for large sample sizes</data>
      <data key="d2">ef24ca5edd06893b737e6a1c8a9825f6</data>
    </node>
    <node id="SAMPLE_SIZE">
      <data key="d0">QUANTITY</data>
      <data key="d1">Sample size (n) is the number of observations in the sample used for regression analysis</data>
      <data key="d2">ef24ca5edd06893b737e6a1c8a9825f6</data>
    </node>
    <node id="REPEATED_VALUES">
      <data key="d0">DATA</data>
      <data key="d1">Repeated values of the explanatory variable refer to instances where the same value of the explanatory variable occurs more than once in the dataset.</data>
      <data key="d2">521acf88540d5897188c9ec65b17e6a6</data>
    </node>
    <node id="ASSESSING_NORMALITY">
      <data key="d0">STATISTICAL_METHOD</data>
      <data key="d1">Assessing normality involves checking whether the data or the errors in a statistical model are normally distributed. This is important for the validity of certain statistical tests and inferences.</data>
      <data key="d2">521acf88540d5897188c9ec65b17e6a6</data>
    </node>
    <node id="NORMALITY_ASSUMPTION">
      <data key="d0">ASSUMPTION</data>
      <data key="d1">The normality assumption is the hypothesis that the errors in a statistical model are normally distributed. This is a common assumption in many statistical methods, particularly in regression analysis.</data>
      <data key="d2">521acf88540d5897188c9ec65b17e6a6</data>
    </node>
    <node id="NORMAL_Q_Q_PLOT">
      <data key="d0">PLOT</data>
      <data key="d1">The NORMAL_Q_Q_PLOT is a pivotal graphical technique utilized in statistical analysis to assess the normality of a data set or the errors within it. This method involves plotting the observed values from the data set against the expected values under the assumption of normal distribution. The primary purpose of the Normal Q-Q plot is to determine if the data set comes from a population that follows a normal distribution, thereby facilitating a deeper understanding of the data's distribution characteristics and potential deviations from normality. This technique is particularly useful in validating assumptions made in various statistical models, ensuring that the data adheres to the necessary conditions for accurate analysis and interpretation.</data>
      <data key="d2">521acf88540d5897188c9ec65b17e6a6,eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </node>
    <node id="LARGE_SAMPLE_SIZES">
      <data key="d0">DATA</data>
      <data key="d1">Large sample sizes refer to datasets with a large number of observations. In statistical analysis, larger sample sizes can make certain assumptions, such as normality, less critical.</data>
      <data key="d2">521acf88540d5897188c9ec65b17e6a6</data>
    </node>
    <node id="DEPARTURES_FROM_NORMALITY">
      <data key="d0">DATA</data>
      <data key="d1">Departures from normality refer to situations where the distribution of the data or errors deviates from a normal distribution. This can affect the validity of statistical inferences and tests.</data>
      <data key="d2">521acf88540d5897188c9ec65b17e6a6</data>
    </node>
    <node id="PERCENTAGE_SCALE">
      <data key="d0">SCALE</data>
      <data key="d1">Percentage scale is a type of scale used for representing data, often in the context of statistical analysis and plotting.</data>
      <data key="d2">eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </node>
    <node id="NORMAL_SCORE_SCALE">
      <data key="d0">SCALE</data>
      <data key="d1">Normal score scale is a scale used for representing data in terms of their normal distribution, often used in the vertical axis of plots.</data>
      <data key="d2">eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </node>
    <node id="NORMAL_DISTRIBUTION">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">The NORMAL_DISTRIBUTION, also known as the Gaussian distribution, is a fundamental probability distribution characterized by its symmetry around the mean. This distribution highlights that data points closer to the mean are more frequently observed than those further away. In the context of the NORMAL_DISTRIBUTION, 68% of the data fall within one standard deviation of the mean, 95% within two standard deviations, and 99.7% within three standard deviations, illustrating its bell-shaped curve. This distribution is widely utilized to model real-world phenomena and simulate data points, making it a crucial tool in statistical analysis and inference.</data>
      <data key="d2">3bfc9b92571973e54c8095302acc1aaa,d7f3a28534ffe830fe6f4cef8c41a9b4,eafa2cc6cc64d8bca1c080bdd2ad7654,ee22e1f5947947f9bd3f7f8922745e48</data>
    </node>
    <node id="STANDARDISED_RESIDUALS">
      <data key="d0">RESIDUALS</data>
      <data key="d1">Standardised residuals, a critical component in statistical analysis, particularly in assessing the fit of a model, are residuals that have been scaled by dividing by an estimate of their standard deviation. This scaling process adjusts the residuals to have a mean of 0 and a standard deviation of 1, effectively standardizing them. By doing so, standardised residuals provide a measure of the size of the residual relative to the variability of the residuals, allowing for a more accurate assessment of model fit. These residuals are particularly useful in regression analysis, where they are scaled to have unit variance, facilitating a more nuanced understanding of the data's structure and the model's performance.</data>
      <data key="d2">428db872e71a17a2cf7868b03a52def0,629ce6550294d332948e19171a4acd2d,e361ac139c268d5c3f3623f920e68af2,eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </node>
    <node id="FITTED_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">The Fitted Model, a central concept in statistical analysis, is a model that has been estimated or fit to a set of data. This model serves as a representation of the relationship between the variables within the data. It is important to note that the Fitted Model can be potentially influenced by influential data points, which are observations that have a disproportionate effect on the model's parameters. This influence can alter the estimated relationships between variables, highlighting the need for careful consideration of data quality and the potential impact of outliers during the model estimation process.</data>
      <data key="d2">83bb91cf725e5116ca2f5748fddccfae,eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </node>
    <node id="FIGURE_3_9">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 3.9 is a graphical representation showing two Normal Q-Q plots. The left plot shows data from a standard normal distribution, while the right plot shows data from a t-distribution on 2 degrees of freedom.</data>
      <data key="d2">eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </node>
    <node id="T_DISTRIBUTION">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">The T_DISTRIBUTION, also referred to as Student's t-distribution, is a critical probability distribution in statistical analysis. It is prominently used in estimating the mean of a normally distributed population, particularly in scenarios where the sample size is small, and the population standard deviation or variance is unknown. This distribution is characterized by its similarity to the normal distribution but with heavier tails, indicating a higher probability of producing values that are significantly distant from the mean. This feature makes the t-distribution particularly useful in situations where data may exhibit more variability than expected under a normal distribution, providing a more robust method for estimating population parameters under uncertainty.</data>
      <data key="d2">3bfc9b92571973e54c8095302acc1aaa,eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </node>
    <node id="T2_DISTRIBUTION">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">T2 distribution is a probability distribution with 2 degrees of freedom, used to simulate data points</data>
      <data key="d2">ee22e1f5947947f9bd3f7f8922745e48</data>
    </node>
    <node id="Q_Q_PLOT">
      <data key="d0">PLOT</data>
      <data key="d1">The Q-Q (Quantile-Quantile) plot is a sophisticated graphical method utilized in statistical analysis to test whether a dataset adheres to a specified distribution. This plot compares two probability distributions by plotting their quantiles against each other. In essence, it juxtaposes the quantiles of the first data set with those of the second data set. When the two sets originate from a common distribution, the points on the plot should align closely with the line y = x, indicating a perfect match. Any deviation from this straight line suggests discrepancies from the hypothesized distribution, providing valuable insights into the structure and conformity of the data to the proposed statistical model. This method is particularly useful in identifying the relations and structure of the community of interest within the context of algorithmic analysis, making it a crucial tool for data scientists and statisticians alike.</data>
      <data key="d2">3bfc9b92571973e54c8095302acc1aaa,ee22e1f5947947f9bd3f7f8922745e48</data>
    </node>
    <node id="REFERENCE_LINE">
      <data key="d0">LINE</data>
      <data key="d1">Reference line is a line on the Q-Q plot that represents the expected quantiles of the theoretical distribution</data>
      <data key="d2">ee22e1f5947947f9bd3f7f8922745e48</data>
    </node>
    <node id="SAMPLE_QUANTILES">
      <data key="d0">QUANTILES</data>
      <data key="d1">Sample quantiles are the observed quantiles of the data, used in the Q-Q plot to compare with the theoretical quantiles</data>
      <data key="d2">ee22e1f5947947f9bd3f7f8922745e48</data>
    </node>
    <node id="LEFT_TAIL">
      <data key="d0">REGION</data>
      <data key="d1">Left tail is the lower end of the distribution, where the sample quantiles are smaller than expected under a normal distribution</data>
      <data key="d2">ee22e1f5947947f9bd3f7f8922745e48</data>
    </node>
    <node id="RIGHT_TAIL">
      <data key="d0">REGION</data>
      <data key="d1">Right tail is the upper end of the distribution, where the sample quantiles are larger than expected under a normal distribution</data>
      <data key="d2">ee22e1f5947947f9bd3f7f8922745e48</data>
    </node>
    <node id="STANDARD_NORMAL_DENSITY">
      <data key="d0">DENSITY</data>
      <data key="d1">Standard normal density is the density function of the standard normal distribution</data>
      <data key="d2">ee22e1f5947947f9bd3f7f8922745e48</data>
    </node>
    <node id="T2_DENSITY">
      <data key="d0">DENSITY</data>
      <data key="d1">T2 density is the density function of the t2 distribution</data>
      <data key="d2">ee22e1f5947947f9bd3f7f8922745e48</data>
    </node>
    <node id="MORE_COMPLEX_MODELS">
      <data key="d0">ACTION</data>
      <data key="d1">Using more complex models is another remedial action that can be taken to address violations of the modelling assumptions</data>
      <data key="d2">e361ac139c268d5c3f3623f920e68af2</data>
    </node>
    <node id="DIAGNOSIS">
      <data key="d0">ACTIVITY</data>
      <data key="d1">Diagnosis refers to the process of checking whether the model assumptions are met. This involves analyzing residuals, checking for patterns, and using diagnostic plots.</data>
      <data key="d2">c03eb12d07d48f9e94260f08dae10cdf</data>
    </node>
    <node id="TEXTBOOKS">
      <data key="d0">RESOURCE</data>
      <data key="d1">Textbooks are educational resources that provide in-depth information on various topics. The recommended textbooks for this module include "A Modern Approach to Regression with R" by Sheather and "Generalized linear models with examples in R" by Dunn and Smyth.</data>
      <data key="d2">c03eb12d07d48f9e94260f08dae10cdf</data>
    </node>
    <node id="EPSILON_I">
      <data key="d0">ERROR_TERM</data>
      <data key="d1">Epsilon_i (&#949;i), denoted as EPSILON_I in the dataset, is the error term associated with the ith observation in the regression model. It plays a crucial role in statistical analysis by representing the deviation of the observed log-transformed response from the expected value. This error term encapsulates the unexplained variability in the data that is not accounted for by the model's predictors, providing insights into the model's fit and the presence of any systematic patterns or randomness in the residuals.</data>
      <data key="d2">0fbc9037ca9a440e79e9ac05664b9b3d,90b7e0427699cc1bb461e37939935138</data>
    </node>
    <node id="EXP">
      <data key="d0">FUNCTION</data>
      <data key="d1">Exp is the exponential function used to transform the log-transformed response back to its original scale.</data>
      <data key="d2">0fbc9037ca9a440e79e9ac05664b9b3d</data>
    </node>
    <node id="MULTIPLICATIVE_ERRORS">
      <data key="d0">ERRORS</data>
      <data key="d1">Multiplicative errors are the errors in the original scale of the response variable, which act multiplicatively rather than additively.</data>
      <data key="d2">0fbc9037ca9a440e79e9ac05664b9b3d</data>
    </node>
    <node id="XI1">
      <data key="d0">PREDICTOR</data>
      <data key="d1">xi1 is the first predictor variable in the model</data>
      <data key="d2">34fceaaf7d835828b5ee2327325c37f8</data>
    </node>
    <node id="XI2">
      <data key="d0">PREDICTOR</data>
      <data key="d1">xi2 is the second predictor variable in the model</data>
      <data key="d2">34fceaaf7d835828b5ee2327325c37f8</data>
    </node>
    <node id="E">
      <data key="d0">EXPECTATION</data>
      <data key="d1">In the context of statistical analysis, E is the expectation operator, a fundamental tool used to calculate the expected value of a random variable. This operator plays a crucial role in understanding the average behavior of random variables over numerous trials. However, it's important to note that e, a different entity, represents the base of the natural logarithm. This mathematical constant is often utilized in the exponential function, particularly in transforming the error term, denoted as \u03f5i, in various statistical models. While E and e are both significant in data science and statistical analysis, they serve distinct purposes, with E focusing on expected values and e being central to exponential functions and error term transformations.</data>
      <data key="d2">1da117a2f92b2db00290d2a0bfc06beb,34fceaaf7d835828b5ee2327325c37f8,60cc94e681863c9fcc6f9be1e500f840</data>
    </node>
    <node id="LOG">
      <data key="d0">FUNCTION</data>
      <data key="d1">The LOG, referring to the natural logarithm function, plays a pivotal role in the statistical model under consideration. It is employed in two distinct yet interconnected capacities. Firstly, the LOG function is utilized to transform the average body weight, a critical variable in the model, ensuring that the data meets the assumptions required for accurate statistical analysis. Secondly, the LOG function is also integral in the calculation of the likelihood function, a key component in estimating model parameters. This dual application of the LOG function highlights its versatility and importance in enhancing the model's performance and interpretability.</data>
      <data key="d2">34fceaaf7d835828b5ee2327325c37f8,e7edd8b2874a350779ae20f1ecdf4733,efeeb664622c1ee594e6a08a8322ffe3</data>
    </node>
    <node id="ZB0">
      <data key="d0">PREDICTION</data>
      <data key="d1">zb0 is the prediction for the logged response from the model</data>
      <data key="d2">34fceaaf7d835828b5ee2327325c37f8</data>
    </node>
    <node id="M">
      <data key="d0">MEAN</data>
      <data key="d1">&#181; (mu) is the mean of the normal distribution of log(Y)</data>
      <data key="d2">34fceaaf7d835828b5ee2327325c37f8</data>
    </node>
    <node id="LOG_Y">
      <data key="d0">TRANSFORMED_RANDOM_VARIABLE</data>
      <data key="d1">log(Y) is the natural logarithm of Y, which follows a normal distribution with mean &#181; and variance &#963;^2</data>
      <data key="d2">21e429490eeefe7d9c245058fd48ca68</data>
    </node>
    <node id="LOGNORMAL_DISTRIBUTION">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">Lognormal(&#181;, &#963;^2) is the lognormal distribution that Y follows, characterized by parameters &#181; and &#963;^2</data>
      <data key="d2">21e429490eeefe7d9c245058fd48ca68</data>
    </node>
    <node id="GEOMETRIC_MEAN_Y">
      <data key="d0">STATISTIC</data>
      <data key="d1">The geometric mean of Y is equal to exp[E(log(Y))], which is also the median of Y when Y is lognormally distributed</data>
      <data key="d2">21e429490eeefe7d9c245058fd48ca68</data>
    </node>
    <node id="YA">
      <data key="d0">PREDICTED_VALUE</data>
      <data key="d1">YA represents the back-transformed prediction for the response variable Y in its original scale, prior to any change in the explanatory variable X1. This indicates that YA is a forecasted value obtained before the increase in X1, providing a baseline measure of Y's expected response under the initial conditions. This baseline is crucial for understanding how the response might vary as a result of changes in X1, allowing for a comparative analysis of the effects of X1 on Y.</data>
      <data key="d2">21e429490eeefe7d9c245058fd48ca68,995fb26a0261f824952fa7b2fac3382e</data>
    </node>
    <node id="POSITIVE_MEASUREMENTS">
      <data key="d0">VARIABLE_TYPE</data>
      <data key="d1">Positive physical measurements like weights or lengths are variables that are often log-transformed</data>
      <data key="d2">995fb26a0261f824952fa7b2fac3382e</data>
    </node>
    <node id="PRECISION_OF_MEASUREMENT">
      <data key="d0">MEASUREMENT</data>
      <data key="d1">The precision of measurement is higher for smaller observations, indicating that the accuracy of measurement is better for lower values</data>
      <data key="d2">66f7fae9d896ff2b3fd40186cc833503</data>
    </node>
    <node id="RIGHT_SKEWED_DISTRIBUTION">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">A right skewed distribution is a type of distribution where the tail is longer on the right side, indicating that most of the data is concentrated on the left side</data>
      <data key="d2">66f7fae9d896ff2b3fd40186cc833503</data>
    </node>
    <node id="POSITIVE_RESPONSE_VARIABLE">
      <data key="d0">VARIABLE</data>
      <data key="d1">A positive response variable is a variable in a statistical model that only takes positive values</data>
      <data key="d2">66f7fae9d896ff2b3fd40186cc833503</data>
    </node>
    <node id="BODY_WEIGHT">
      <data key="d0">WEIGHT</data>
      <data key="d1">The variable "BODY_WEIGHT" in the mammals dataset signifies the weight of the animals, measured in kilograms. It is noteworthy that the distribution of the log-transformed body weight exhibits less skewness compared to the original distribution, indicating a more normal distribution when the body weight is logarithmically transformed. This transformation is often applied to normalize the data, which is crucial for certain statistical analyses, particularly when assumptions of normality are required. The log transformation can help in stabilizing variance and reducing the influence of extreme values, making the data more suitable for parametric statistical methods such as linear regression.</data>
      <data key="d2">66f7fae9d896ff2b3fd40186cc833503,e2422d8b80004aab4ea74d5209587861</data>
    </node>
    <node id="BRAIN_WEIGHT">
      <data key="d0">WEIGHT</data>
      <data key="d1">Brain weight, measured in grams, is a crucial variable in the mammals dataset, serving as the dependent variable in the statistical model. It represents the weight of the animals' brains and is predicted based on the average body weight of the mammals. The relationship between body weight and brain weight exhibits a non-linear pattern, suggesting that a transformation of the data might be necessary to enhance the linearity and, consequently, the accuracy of the model predictions. This variable plays a significant role in understanding the structure and relationships within the mammal community, particularly in the context of algorithmic analysis and statistical inference.</data>
      <data key="d2">66f7fae9d896ff2b3fd40186cc833503,e2422d8b80004aab4ea74d5209587861,efeeb664622c1ee594e6a08a8322ffe3</data>
    </node>
    <node id="AFRICAN_ELEPHANT">
      <data key="d0">SPECIES</data>
      <data key="d1">The African elephant, a remarkable mammal species, is characterized by its unusually high average body weight and brain weight. As a species of elephant, it stands out due to its substantial size and cognitive capabilities, reflecting the unique evolutionary adaptations that have allowed it to thrive in its natural habitat. Despite the slight redundancy in the descriptions, it is clear that the African elephant is a significant creature in the animal kingdom, with notable physical and neurological attributes.</data>
      <data key="d2">66f7fae9d896ff2b3fd40186cc833503,f9d6c3504b8f8b5c25550076e45f8270</data>
    </node>
    <node id="ASIAN_ELEPHANT">
      <data key="d0">SPECIES</data>
      <data key="d1">The Asian elephant, a remarkable mammal species, is characterized by its notably high average body weight and brain weight. This distinctive feature sets it apart in the animal kingdom, showcasing the unique evolutionary adaptations and biological attributes of this majestic creature. Despite some slight variation in the descriptions, it is clear that the Asian elephant is distinguished by its substantial size and brain mass, which are both above average compared to other species. This information provides a comprehensive and enriched understanding of the Asian elephant's physical attributes, highlighting its significant presence in the ecosystem.</data>
      <data key="d2">66f7fae9d896ff2b3fd40186cc833503,f9d6c3504b8f8b5c25550076e45f8270</data>
    </node>
    <node id="HUMAN">
      <data key="d0">SPECIES</data>
      <data key="d1">The human, scientifically categorized under the species of mammals, is noted as an influential observation within the mammals dataset. This species is characterized by an average body weight, yet it remarkably stands out due to its unusually high average brain weight in relation to its body weight. This distinctive feature of the human species, the significant brain-to-body weight ratio, contributes to its notable presence in the mammals dataset, making it a subject of particular interest in comparative studies of mammals.</data>
      <data key="d2">428db872e71a17a2cf7868b03a52def0,66f7fae9d896ff2b3fd40186cc833503,f9d6c3504b8f8b5c25550076e45f8270</data>
    </node>
    <node id="BRAIN">
      <data key="d0">VARIABLE</data>
      <data key="d1">Brain weight is one of the variables in the mammals dataset, representing the weight of the brain of different mammal species</data>
      <data key="d2">f9d6c3504b8f8b5c25550076e45f8270</data>
    </node>
    <node id="BODY">
      <data key="d0">VARIABLE</data>
      <data key="d1">Body weight is another variable in the mammals dataset, representing the weight of the body of different mammal species</data>
      <data key="d2">f9d6c3504b8f8b5c25550076e45f8270</data>
    </node>
    <node id="FIGURE_4_1">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 4.1 is a scatterplot of the mammals dataset, showing the relationship between brain weight and body weight</data>
      <data key="d2">f9d6c3504b8f8b5c25550076e45f8270</data>
    </node>
    <node id="FIGURE_4_2">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 4.2 is a comparison of the histogram of body weight and the histogram of log(body weight) for the mammals data, showing that the distribution of the log-transformed variable is much less skewed</data>
      <data key="d2">f9d6c3504b8f8b5c25550076e45f8270</data>
    </node>
    <node id="LOG_BODY_WEIGHT">
      <data key="d0">TRANSFORMED_VARIABLE</data>
      <data key="d1">The entity "LOG_BODY_WEIGHT" refers to the logarithmically transformed version of the average body weight of mammals. This transformation is applied to reduce the skewness in the original body weight distribution, making it more suitable for statistical analysis. In the context of regression analysis, "LOG_BODY_WEIGHT" serves as an independent, predictor variable, indicating its role in explaining variations in the dependent variable of the regression model. This transformation enhances the linearity and stability of the relationship between the independent and dependent variables, thereby improving the accuracy and reliability of the regression analysis.</data>
      <data key="d2">9e2ebbb113c00fa43f0af3c0696baf95,bd05fe6a05f9a13d33c4f1b5a771ada5,c47968226557bc2eb5aec5bb7994fd0e,e2422d8b80004aab4ea74d5209587861</data>
    </node>
    <node id="LOG_BRAIN_WEIGHT">
      <data key="d0">TRANSFORMED_VARIABLE</data>
      <data key="d1">The entity "LOG_BRAIN_WEIGHT" represents the logarithmically transformed average brain weight of mammals, which serves as a crucial variable in statistical analysis. This transformation is applied to enhance linearity, as evidenced by the scatterplot that plots log(brain weight) against log(body weight), showcasing a more linear relationship. In the context of regression analysis, LOG_BRAIN_WEIGHT acts as the response variable, indicating its role as the dependent variable in the regression model. This transformation is a common statistical technique used to normalize the distribution of the data and stabilize the variance, making it more suitable for regression analysis.</data>
      <data key="d2">9e2ebbb113c00fa43f0af3c0696baf95,bd05fe6a05f9a13d33c4f1b5a771ada5,c47968226557bc2eb5aec5bb7994fd0e,e2422d8b80004aab4ea74d5209587861</data>
    </node>
    <node id="BRAINI">
      <data key="d0">VARIABLE</data>
      <data key="d1">BRAINi represents the brain weight of the ith species in the dataset</data>
      <data key="d2">86c401dda130c2d201c3339526062a24</data>
    </node>
    <node id="BODYI">
      <data key="d0">VARIABLE</data>
      <data key="d1">BODYi represents the body weight of the ith species in the dataset</data>
      <data key="d2">86c401dda130c2d201c3339526062a24</data>
    </node>
    <node id="LOG_BRAINI">
      <data key="d0">TRANSFORMED_VARIABLE</data>
      <data key="d1">LOG_BRAINi is the logarithm of the brain weight of the ith species</data>
      <data key="d2">86c401dda130c2d201c3339526062a24</data>
    </node>
    <node id="LOG_BODYI">
      <data key="d0">TRANSFORMED_VARIABLE</data>
      <data key="d1">LOG_BODYi is the logarithm of the body weight of the ith species</data>
      <data key="d2">86c401dda130c2d201c3339526062a24</data>
    </node>
    <node id="BETA_HAT1">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">BETA_HAT1, denoted as &#946;&#770;1, is the estimated value of the slope coefficient Beta1, which is a crucial parameter in linear regression models. This estimate, calculated as 0.75, was obtained using the maximum likelihood estimation method, a statistical technique that selects the parameter values that maximize the likelihood of the observed data. BETA_HAT1 serves as an approximation of the true value of Beta1, providing insights into the relationship between the independent and dependent variables within the context of the model.</data>
      <data key="d2">09caa54ca1372d152e47051be4d44ede,86c401dda130c2d201c3339526062a24</data>
    </node>
    <node id="YBA">
      <data key="d0">PREDICTED_VALUE</data>
      <data key="d1">YbA is the predicted average brain weight for species A with average body weight x</data>
      <data key="d2">86c401dda130c2d201c3339526062a24</data>
    </node>
    <node id="YBB">
      <data key="d0">PREDICTED_VALUE</data>
      <data key="d1">YbB is the predicted average brain weight for species B with average body weight 1.1x</data>
      <data key="d2">86c401dda130c2d201c3339526062a24</data>
    </node>
    <node id="AVERAGE_BODY_WEIGHT">
      <data key="d0">MEASUREMENT</data>
      <data key="d1">Average body weight is the independent variable in the model, used to predict the brain weight</data>
      <data key="d2">efeeb664622c1ee594e6a08a8322ffe3</data>
    </node>
    <node id="GIRTH">
      <data key="d0">MEASUREMENT</data>
      <data key="d1">Girth is a variable in the trees dataset, measured as the diameter of the tree in inches</data>
      <data key="d2">efeeb664622c1ee594e6a08a8322ffe3</data>
    </node>
    <node id="VOLUME">
      <data key="d0">MEASUREMENT</data>
      <data key="d1">The entity "VOLUME" in the context of the trees dataset, refers to the quantity of timber that a tree has produced. This volume is measured in cubic feet, serving as a critical variable for understanding the timber yield from each tree within the dataset. The VOLUME data point is a direct measure of the tree's timber output, providing insights into the tree's growth and the potential value of its wood.</data>
      <data key="d2">9611ea31ff53888971694cdefe806f64,efeeb664622c1ee594e6a08a8322ffe3</data>
    </node>
    <node id="TREE">
      <data key="d0">PLANT</data>
      <data key="d1">The tree is the subject of the study, measured at 4 ft 6 inches from the ground, with its diameter (wrongly labelled as girth) and height recorded</data>
      <data key="d2">9611ea31ff53888971694cdefe806f64</data>
    </node>
    <node id="VOLUMEI">
      <data key="d0">VARIABLE</data>
      <data key="d1">Volumei is the dependent variable in the model, representing the volume of a tree, calculated as exp(&#946;0) * Diameter&#946;1 * Height&#946;2 * exp(e &#1013;i).</data>
      <data key="d2">60cc94e681863c9fcc6f9be1e500f840</data>
    </node>
    <node id="DIAMETERI">
      <data key="d0">VARIABLE</data>
      <data key="d1">Di is the diameter of the tree, an independent variable in the model, raised to the power of &#946;1.</data>
      <data key="d2">60cc94e681863c9fcc6f9be1e500f840</data>
    </node>
    <node id="HEIGHTI">
      <data key="d0">VARIABLE</data>
      <data key="d1">Heighti is the height of the tree, an independent variable in the model, raised to the power of &#946;2.</data>
      <data key="d2">60cc94e681863c9fcc6f9be1e500f840</data>
    </node>
    <node id="&#917;I">
      <data key="d0">ERROR_TERM</data>
      <data key="d1">&#1013;i is the error term in the model, representing the deviation of the observed volume from the expected volume.</data>
      <data key="d2">60cc94e681863c9fcc6f9be1e500f840</data>
    </node>
    <node id="SCATTERPLOT_MATRIX">
      <data key="d0">PLOT</data>
      <data key="d1">Scatterplot matrix is a graphical representation of the pairwise scatterplots of the variables, including histograms of the variables, used for initial exploratory analysis.</data>
      <data key="d2">60cc94e681863c9fcc6f9be1e500f840</data>
    </node>
    <node id="FIGURE_4_5">
      <data key="d0">PLOT</data>
      <data key="d1">Figure 4.5 is a plot of residuals versus fitted values for the linear model fitted to the log-transformed tree data</data>
      <data key="d2">a60af43e42c72a41fa90da06beb29d1b</data>
    </node>
    <node id="LOG_TRANSFORMED_TREE_DATA">
      <data key="d0">DATA</data>
      <data key="d1">Log-transformed tree data is the dataset used in the regression analysis, where the dependent and independent variables have been log-transformed</data>
      <data key="d2">a60af43e42c72a41fa90da06beb29d1b</data>
    </node>
    <node id="FIGURE_4_6">
      <data key="d0">PLOT</data>
      <data key="d1">Figure 4.6 is a plot of residuals versus log-height and versus log-diameter for the model fitted to the log-transformed tree data</data>
      <data key="d2">a60af43e42c72a41fa90da06beb29d1b</data>
    </node>
    <node id="LOGTREES">
      <data key="d0">DATA_FRAME</data>
      <data key="d1">LOGTREES, also referred to as logTrees, is a data frame that comprises log-transformed tree data. This dataset has been specifically processed through logarithmic transformation to normalize the distribution of tree measurements, making it more suitable for statistical analysis. The log transformation is a common technique used in data analysis to reduce skewness and stabilize variance, particularly when dealing with data that exhibits exponential growth patterns, such as tree diameters or heights. LOGTREES provides a structured format for analyzing tree data, facilitating the application of various statistical methods including regression analysis, where the assumption of normality is crucial for accurate inference.</data>
      <data key="d2">9a28a6420fca4405488ca35762f9dc28,a60af43e42c72a41fa90da06beb29d1b</data>
    </node>
    <node id="TREES_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">Trees.model is the linear model fitted to the log-transformed tree data using the lm() function in R</data>
      <data key="d2">a60af43e42c72a41fa90da06beb29d1b</data>
    </node>
    <node id="LOG_DIAMETER">
      <data key="d0">PARAMETER</data>
      <data key="d1">The entity "LOG_DIAMETER" refers to the log-transformed diameter of trees, which is utilized as an independent variable in the regression model. This transformation is applied to the raw diameter measurements to stabilize variance and potentially linearize relationships between the tree diameter and other variables in the model. The log-diameter, therefore, serves as a crucial component in the statistical analysis, enabling more accurate and reliable predictions and interpretations within the context of the regression analysis.</data>
      <data key="d2">a60af43e42c72a41fa90da06beb29d1b,d71b402ab9edbb4347e09c7af3257cf5</data>
    </node>
    <node id="LOG_HEIGHT">
      <data key="d0">PARAMETER</data>
      <data key="d1">Log-height, denoted as "LOG_HEIGHT", is a significant independent variable incorporated into the regression model. It specifically refers to the logarithmic transformation of the height of trees. This log-transformed height is utilized to normalize the data and potentially improve the linearity of the relationship between the height of the tree and other variables in the model. By applying a logarithmic transformation, the model can better capture the underlying patterns and relationships within the data, making it a crucial component in the statistical analysis of tree height's impact on the community of interest.</data>
      <data key="d2">a60af43e42c72a41fa90da06beb29d1b,d71b402ab9edbb4347e09c7af3257cf5</data>
    </node>
    <node id="TREES.MODEL">
      <data key="d0">LINEAR_MODEL</data>
      <data key="d1">trees.model is a linear model that regresses logVolume on logDiameter and logHeight using the logTrees data frame</data>
      <data key="d2">9a28a6420fca4405488ca35762f9dc28</data>
    </node>
    <node id="(INTERCEPT)">
      <data key="d0">INTERCEPT</data>
      <data key="d1">(Intercept) is the intercept coefficient in the linear model</data>
      <data key="d2">9a28a6420fca4405488ca35762f9dc28</data>
    </node>
    <node id="LOGDIAMETER">
      <data key="d0">PREDICTOR</data>
      <data key="d1">logDiameter is a predictor variable in the linear model, representing the log-transformed diameter of the tree</data>
      <data key="d2">9a28a6420fca4405488ca35762f9dc28</data>
    </node>
    <node id="LOGHEIGHT">
      <data key="d0">PREDICTOR</data>
      <data key="d1">logHeight is a predictor variable in the linear model, representing the log-transformed height of the tree</data>
      <data key="d2">9a28a6420fca4405488ca35762f9dc28</data>
    </node>
    <node id="LOGVOLUME">
      <data key="d0">RESPONSE</data>
      <data key="d1">logVolume is the response variable in the linear model, representing the log-transformed volume of the tree</data>
      <data key="d2">9a28a6420fca4405488ca35762f9dc28</data>
    </node>
    <node id="LOG_VOLUME">
      <data key="d0">DEPENDENT_VARIABLE</data>
      <data key="d1">Log-volume is the dependent variable in the regression model, representing the logarithm of the volume of trees</data>
      <data key="d2">d71b402ab9edbb4347e09c7af3257cf5</data>
    </node>
    <node id="COEFFICIENT_LOG_DIAMETER">
      <data key="d0">REGRESSION_COEFFICIENT</data>
      <data key="d1">The coefficient for log-diameter in the regression model represents the change in log-volume associated with a one-unit change in log-diameter</data>
      <data key="d2">d71b402ab9edbb4347e09c7af3257cf5</data>
    </node>
    <node id="INTERCEPT_CYLINDER">
      <data key="d0">REGRESSION_INTERCEPT</data>
      <data key="d1">The intercept for the regression when the shape of a tree is approximated by a cylinder represents the expected log-volume when log-diameter and log-height are zero</data>
      <data key="d2">d71b402ab9edbb4347e09c7af3257cf5</data>
    </node>
    <node id="INTERCEPT_CONE">
      <data key="d0">REGRESSION_INTERCEPT</data>
      <data key="d1">The intercept for the regression when the shape of a tree is approximated by a cone represents the expected log-volume when log-diameter and log-height are zero</data>
      <data key="d2">d71b402ab9edbb4347e09c7af3257cf5</data>
    </node>
    <node id="ESTIMATED_INTERCEPT">
      <data key="d0">REGRESSION_INTERCEPT</data>
      <data key="d1">The estimated intercept in the regression model of log-volume on log-diameter and log-height represents the expected log-volume when log-diameter and log-height are zero</data>
      <data key="d2">d71b402ab9edbb4347e09c7af3257cf5</data>
    </node>
    <node id="DUNN_AND_SMYTH_BOOK">
      <data key="d0">REFERENCE_MATERIAL</data>
      <data key="d1">Generalized linear models with examples in R (2018) by Dunn and Smyth is a reference book that discusses the regression problem presented</data>
      <data key="d2">d71b402ab9edbb4347e09c7af3257cf5</data>
    </node>
    <node id="LOG_TRANSFORMED_VARIABLES">
      <data key="d0">TRANSFORMED_DATA</data>
      <data key="d1">Log-transformed variables, a critical component in statistical modeling, are variables that have undergone transformation through the application of the logarithmic function. This process is employed to address specific model assumptions and to mitigate the disproportionate impact of certain data points on the analysis. By reducing the influence of these data points, log transformation ensures a more balanced representation within the model, enhancing the accuracy and reliability of statistical inferences. This technique is particularly useful in scenarios where the original data exhibits non-linear relationships or where the variance is not constant across the range of values, as it helps in stabilizing variance and linearizing relationships.</data>
      <data key="d2">83bb91cf725e5116ca2f5748fddccfae,d71b402ab9edbb4347e09c7af3257cf5</data>
    </node>
    <node id="TRANSFORMATIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Transformations, in the context of statistical analysis, are mathematical operations that are applied to the data or variables within a model. Their primary purpose is to change the scale or distribution of the data, thereby potentially reducing the undue influence of certain data points. These operations are crucial for addressing violations of model assumptions, ensuring that the data adheres to the necessary conditions for accurate analysis. Commonly employed transformations include logarithmic, power, and other specialized types, each selected based on the specific requirements of the data and the model being used. Through these transformations, the data scientist can enhance the model's performance and reliability, making the results more robust and interpretable.</data>
      <data key="d2">07951ffe6787af44aa60c90c69e62f83,83bb91cf725e5116ca2f5748fddccfae</data>
    </node>
    <node id="MODEL_REPARAMETERISATIONS">
      <data key="d0">CONCEPT</data>
      <data key="d1">Model reparameterisations are changes made to the structure of a statistical model to improve its fit to the data or to make it more interpretable. This can involve transformations of variables, changes in the functional form of the model, or the inclusion of additional parameters.</data>
      <data key="d2">07951ffe6787af44aa60c90c69e62f83</data>
    </node>
    <node id="SUM_OF_SQUARED_DIFFERENCES">
      <data key="d0">CONCEPT</data>
      <data key="d1">The sum of squared differences, S(&#946;), is a measure of the total squared error between the observed data and the predictions made by a model. It is defined as the sum over all observations of the squared differences between the observed values and the values predicted by the model. In the context of least squares estimation, the goal is to find the parameter vector &#946; that minimizes this sum, leading to the least squares estimate &#946;b.&gt;</data>
      <data key="d2">924be3e598ffeabd1fbd9b57f033b917</data>
    </node>
    <node id="S">
      <data key="d0">FUNCTION</data>
      <data key="d1">S, in the context of linear regression analysis, is a multifaceted function that plays a crucial role in assessing the model's goodness of fit and estimating parameters. Specifically, S(\u03b2) represents the sum of squared errors or residuals, quantifying the total squared difference between the observed values y and the values predicted by the model using the parameter vector \u03b2. This function is pivotal in the least squares estimation process, where its minimization leads to the identification of the least squares estimator \u03b2b, optimizing the fit of the model to the data.

Moreover, S(\u03b2) is synonymous with the residual sum of squares (RSS), a measure that is minimized to find the least squares estimates of the model parameters, particularly Beta_0 and Beta_1 in the simple linear regression model. The sum of squared differences function, defined as the sum over j from 1 to n of (yj - xj^T \u03b2)^2, is another representation of S, highlighting its role in the least squares estimation process.

Additionally, S is recognized as the estimated standard deviation of the error term in the regression model, providing insights into the variability of the errors. Furthermore, s serves as the unbiased estimate of the error variance \u03c3^2, offering a measure of the spread of the errors around the regression line.

In summary, S and S(\u03b2) are integral components in linear regression analysis, serving as measures of model fit, tools for parameter estimation, and indicators of error variability.</data>
      <data key="d2">10ac76f99674a01ca0f4a55586dea07e,2167274129d4cfa74a002c4cc39df8a8,255685e281cc5a9edf073c700f425a6b,416494d940a9f505da9853caca26fe63,50a56c34050fb7f7709300a51399b150,6b55b41598d5264f8dc6b72769748722,7955aae3fd4ca51b9ef8843e13c1f517,90b7e0427699cc1bb461e37939935138,9d300fc83afb3261af61b2ab9721cadc,9dddcd96af7b557e578b3f5f36efacd7,a4a817bb79d6ae8812c808ca41d47f43,aa195e72eb5285a4bcae9c856af30a87,c47968226557bc2eb5aec5bb7994fd0e,eac62cdd5518e1269fed150639331c2c,f632f01188d2c6e3091a965580cb4600</data>
    </node>
    <node id="S_BETA">
      <data key="d0">FUNCTION</data>
      <data key="d1">S_BETA, denoted as S(&#946;), is a pivotal function in statistical analysis, specifically within the context of least squares estimation. It is derived from the expression yT y + &#946;TXTX&#946; &#8722; 2&#946;TXT y, which encapsulates the essence of the sum of squared residuals. This function plays a critical role in determining the least squares estimate by quantifying the discrepancy between the observed data and the values predicted by a linear model. By minimizing S(&#946;), one can identify the optimal values of the parameters &#946; that best fit the data, thereby facilitating a deeper understanding of the relationships within the dataset.</data>
      <data key="d2">21ec28dfe2b2c18030d541d63e51f45e,8f7a05b6d231105a6194eebdb2df372e</data>
    </node>
    <node id="BETA_HAT_0">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">BETA_HAT_0, denoted as &#946;&#770;0, is the least squares estimate of the intercept parameter Beta_0 (&#946;0). This statistical measure is crucial in linear regression analysis, where it represents the expected mean value of the dependent variable when all independent variables are set to zero. BETA_HAT_0 is derived through a method that minimizes the sum of the squared differences between the observed and predicted values, providing a robust estimate of the intercept in the regression model.</data>
      <data key="d2">8f7a05b6d231105a6194eebdb2df372e,d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </node>
    <node id="BETA_HAT_1">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">BETA_HAT_1, denoted as &#946;&#770;1, is the least squares estimate of the slope parameter Beta_1 (&#946;1). This statistical measure is derived from the linear regression analysis, where it represents the estimated change in the dependent variable for a one-unit change in the independent variable, holding all other variables constant. BETA_HAT_1 is a crucial component in understanding the linear relationship between variables in a regression model.</data>
      <data key="d2">8f7a05b6d231105a6194eebdb2df372e,d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </node>
    <node id="Y_BAR">
      <data key="d0">MEAN</data>
      <data key="d1">Y_BAR, denoted as y\u00af, represents the mean of the Y variable, which is also referred to as the response variable y. Y_BAR is the average of the observed values Y, specifically the dependent variable Y in the context of statistical models. It is calculated as the mean of the yj values, where yj are the individual response values. The computation of Y_BAR involves summing all the yj values and dividing by the total number of observations n, expressed as 1/n * &#931;yj. This entity is central in understanding the structure and relations within the community of interest, particularly in the context of algorithmic analysis and statistical inference.</data>
      <data key="d2">1303b66694a101878ca530c0b41cf5ef,28cf5ff0c09fa5c0390267bb9aa3ce47,2f2523c52c6d2869fb19f77b66ce8259,3fdeeb7593174f5e8a9cff55a7cd92e3,5b24b5382abe9d1898810b3e4b9b455a,8f7a05b6d231105a6194eebdb2df372e,f2300d613896880cbb7c255a4d858315,f5716ce115458c0652124734ca344806,f9e7b2eac9f82681301da3d1e2f23328</data>
    </node>
    <node id="X_BAR">
      <data key="d0">MEAN</data>
      <data key="d1">X_BAR, also denoted as x&#772;, represents the mean of the predictor variable x, which is the explanatory or independent variable in the context of a simple linear regression model. It is the average of the observed values X1 to Xn, or the xj values, in the dataset. X_BAR is calculated as the sample mean of the x values, obtained by summing all the predictor values Xj and dividing by the total number of observations n. This statistic is crucial for understanding the central tendency of the x values and plays a significant role in statistical analysis, particularly in regression models where it helps in estimating the relationship between the predictor and response variables.</data>
      <data key="d2">1303b66694a101878ca530c0b41cf5ef,28cf5ff0c09fa5c0390267bb9aa3ce47,2f2523c52c6d2869fb19f77b66ce8259,3fdeeb7593174f5e8a9cff55a7cd92e3,5609007c6229060ffc85d8056a7fefde,56ff186fc629e1e42f2759fc4b984199,5b24b5382abe9d1898810b3e4b9b455a,6c66e9414880964ee899ceb0f16d22e9,6ee02b38ae842fd5eac9a11c4fd6659f,82932abd152e0b84a1c26a2daa4c08df,8f7a05b6d231105a6194eebdb2df372e,90b7e0427699cc1bb461e37939935138,bd05fe6a05f9a13d33c4f1b5a771ada5,f2300d613896880cbb7c255a4d858315,f5716ce115458c0652124734ca344806,f9e7b2eac9f82681301da3d1e2f23328</data>
    </node>
    <node id="BETA_0_HAT">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">BETA_0_HAT, also denoted as b&#946;0, is the least squares estimate of the intercept parameter BETA_0 in a linear regression model. This estimator is derived from the method of least squares, which minimizes the sum of the squared residuals to find the best fit line for the data. BETA_0_HAT plays a crucial role in determining the baseline value of the dependent variable when all independent variables are zero. It is a fundamental component in the statistical analysis of linear relationships between variables.</data>
      <data key="d2">3fdeeb7593174f5e8a9cff55a7cd92e3,69ffba28a61d98d8d18f91c24b74dd4a</data>
    </node>
    <node id="BETA_1_HAT">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">BETA_1_HAT, also denoted as b&#946;1, is the least squares estimate or estimator of the slope parameter BETA_1. This statistical measure is derived from the method of least squares, which is a standard approach in regression analysis for estimating the unknown parameters in a linear regression model. BETA_1_HAT plays a crucial role in understanding the relationship between the independent and dependent variables, as it quantifies the change in the dependent variable associated with a one-unit change in the independent variable, assuming all other variables are held constant.</data>
      <data key="d2">3fdeeb7593174f5e8a9cff55a7cd92e3,69ffba28a61d98d8d18f91c24b74dd4a</data>
    </node>
    <node id="SXX">
      <data key="d0">VARIABLE</data>
      <data key="d1">SXX is a statistical variable that plays a pivotal role in the context of linear regression analysis. It represents the sum of squares of deviations of predictor values from their mean, which is a measure of the variability or dispersion of the independent variable X. SXX is calculated as the sum of the squared deviations of the observed values X1 to Xn from their mean, X_bar. This calculation is fundamental in determining the variance of the X variable, which is essential for understanding the structure and relations within the dataset.

More specifically, SXX is the variance of the x variable, obtained by summing the squared deviations of x from its mean and then dividing by the sample size n. This variance is also referred to as the variance of the xj values, calculated as the summation of (xj - X_bar)^2. SXX is a critical component in the calculation of Beta1, which is the slope coefficient in a simple linear regression model. It quantifies the degree to which the independent variable X deviates from its mean, providing insights into the strength and direction of the relationship between the predictor variable x and the dependent variable.

In summary, SXX is the variance of the predictor variable x, serving as a key statistic in the analysis of linear relationships and the estimation of regression coefficients. Its calculation involves the sum of the squared deviations of x from its mean, divided by n, and it is central to understanding the variability of the independent variable in the context of statistical modeling.</data>
      <data key="d2">1303b66694a101878ca530c0b41cf5ef,254a8a17b1be06702934341e3bf41e85,28cf5ff0c09fa5c0390267bb9aa3ce47,2f2523c52c6d2869fb19f77b66ce8259,3fdeeb7593174f5e8a9cff55a7cd92e3,56ff186fc629e1e42f2759fc4b984199,5b24b5382abe9d1898810b3e4b9b455a,f2300d613896880cbb7c255a4d858315,f5716ce115458c0652124734ca344806,f9e7b2eac9f82681301da3d1e2f23328</data>
    </node>
    <node id="SXY">
      <data key="d0">VARIABLE</data>
      <data key="d1">SXY, often denoted as Sxy, is a statistical variable that represents the covariance between two variables, typically an independent variable X and a dependent variable Y, or more generally, between two variables x and y. This covariance is calculated as the sum of the products of the deviations of x and y from their respective means. Specifically, Sxy is computed by summing the products of the deviations of xj and yj from their means, X_bar and Y_bar, respectively, over all observations. This calculation is crucial in the context of linear regression analysis, where Sxy is used in the computation of Beta1, the slope of the regression line. It quantifies the direction and strength of the linear relationship between the predictor variable x and the response variable y, providing insights into how changes in x are associated with changes in y.</data>
      <data key="d2">1303b66694a101878ca530c0b41cf5ef,254a8a17b1be06702934341e3bf41e85,28cf5ff0c09fa5c0390267bb9aa3ce47,3fdeeb7593174f5e8a9cff55a7cd92e3,5b24b5382abe9d1898810b3e4b9b455a,f2300d613896880cbb7c255a4d858315,f5716ce115458c0652124734ca344806,f9e7b2eac9f82681301da3d1e2f23328</data>
    </node>
    <node id="BETA1_HAT">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">BETA1_HAT, also denoted as &#946;1 hat or Beta1_hat (b&#946;1), is the least squares estimate for the slope parameter &#946;1 in a simple linear regression model. This parameter estimate is crucial for understanding the relationship between the independent variable and the dependent variable, as it quantifies the change in the dependent variable for a unit change in the independent variable, assuming all other variables are held constant. The least squares estimator method is used to minimize the sum of the squared residuals, providing the best fit line for the data points. BETA1_HAT is a fundamental component in statistical analysis, particularly in the context of algorithmic analysis, where it helps in identifying the relations and structure of the community of interest.</data>
      <data key="d2">87b717ba065d6d7c7431af284137eb12,f9e7b2eac9f82681301da3d1e2f23328</data>
    </node>
    <node id="BETA0_HAT">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">BETA0_HAT, also denoted as &#946;0_hat (b&#946;0), is the least squares estimate for the intercept parameter &#946;0 in a simple linear regression model. This parameter estimate is crucial for understanding the baseline prediction of the model when all predictor variables are set to zero. BETA0_HAT is obtained through the method of least squares, which minimizes the sum of the squared differences between the observed and predicted values, providing a robust estimate of the intercept in the context of the given data.</data>
      <data key="d2">87b717ba065d6d7c7431af284137eb12,f9e7b2eac9f82681301da3d1e2f23328</data>
    </node>
    <node id="EXERCISE_12">
      <data key="d0">EXERCISE</data>
      <data key="d1">Exercise 12 asks to show that the fitted regression line goes through the point (x&#175;, y&#175;)</data>
      <data key="d2">5b24b5382abe9d1898810b3e4b9b455a</data>
    </node>
    <node id="EXERCISE_13">
      <data key="d0">EXERCISE</data>
      <data key="d1">Exercise 13 involves a specific dataset and asks to complete a table, compute parameter estimates, fitted values, residuals, and the deviance of the model</data>
      <data key="d2">5b24b5382abe9d1898810b3e4b9b455a</data>
    </node>
    <node id="XT">
      <data key="d0">TRANSPOSE_MATRIX</data>
      <data key="d1">XT, the transpose of the design matrix X, is a pivotal component in statistical analysis, particularly within the context of linear regression models. This p x n matrix of constants plays a crucial role in the calculation of the least squares estimator, a fundamental technique for estimating the parameters of a linear regression model. XT is instrumental not only in the least squares estimation but also in the formulation of the normal equations, which are essential for solving the least squares problem. Additionally, XT is utilized in the calculation of the maximum likelihood estimate of Beta (&#946;b), further highlighting its significance in statistical inference. As the transpose of matrix X, XT facilitates the computation of various estimators, making it an indispensable element in the toolbox of a data scientist or statistician.</data>
      <data key="d2">1303b66694a101878ca530c0b41cf5ef,254a8a17b1be06702934341e3bf41e85,255685e281cc5a9edf073c700f425a6b,2f2523c52c6d2869fb19f77b66ce8259,46629f2efc6c82e81265a131b4bab2ee,542f546c5a131196e4701fb33c9b1dee,6b55b41598d5264f8dc6b72769748722,7955aae3fd4ca51b9ef8843e13c1f517,82932abd152e0b84a1c26a2daa4c08df,9dddcd96af7b557e578b3f5f36efacd7,9fc2b1e8b2b61b557f88eb9e9c708597,a4a817bb79d6ae8812c808ca41d47f43,aa195e72eb5285a4bcae9c856af30a87,f5716ce115458c0652124734ca344806,f9b615b879f72501f338f8983d4cac3d</data>
    </node>
    <node id="XTX">
      <data key="d0">PRODUCT_MATRIX</data>
      <data key="d1">XTX is a pivotal matrix in statistical analysis, particularly within the context of regression models and least squares estimation. It is defined as the product of matrix X and its transpose XT, or equivalently, XT and X. This matrix multiplication results in a symmetric matrix, often a 2x2 matrix in simpler cases, with elements that include the number of observations (n) and the sum of squares of the independent variable (Sxx). XTX plays a crucial role in the calculation of regression coefficients and the least squares estimator, as well as in the determination of the variance of the estimator and the trace of the hat matrix H. It is derived from the multiplication of the transpose of the design matrix X with the design matrix X itself, serving as a fundamental component in the normal equations used to solve for the optimal coefficients in linear regression models.</data>
      <data key="d2">1303b66694a101878ca530c0b41cf5ef,1da117a2f92b2db00290d2a0bfc06beb,254a8a17b1be06702934341e3bf41e85,28cf5ff0c09fa5c0390267bb9aa3ce47,2f2523c52c6d2869fb19f77b66ce8259,46629f2efc6c82e81265a131b4bab2ee,56ff186fc629e1e42f2759fc4b984199,9d300fc83afb3261af61b2ab9721cadc,a4a817bb79d6ae8812c808ca41d47f43,bd98ac7b4b5df4f63e7ecc8f4a821f57,d14413709de2897231aaa83be3aa346f,eac62cdd5518e1269fed150639331c2c,f9b615b879f72501f338f8983d4cac3d</data>
    </node>
    <node id="XTX_INVERSE">
      <data key="d0">INVERSE_MATRIX</data>
      <data key="d1">(XTX)^-1 is the inverse of the matrix XTX</data>
      <data key="d2">f9b615b879f72501f338f8983d4cac3d</data>
    </node>
    <node id="RANK_X">
      <data key="d0">RANK</data>
      <data key="d1">Rank (X) is the rank of the design matrix X, which is equal to p if X is of full rank</data>
      <data key="d2">f9b615b879f72501f338f8983d4cac3d</data>
    </node>
    <node id="A">
      <data key="d0">VECTOR</data>
      <data key="d1">Entity "A" is a versatile and pivotal component in various statistical operations and transformations. Primarily, A is recognized as a matrix, often square and non-singular, with dimensions varying from p x n to p x p, and occasionally q x n. Its roles are multifaceted, serving as a tool for reparameterizing models, transforming vectors such as Beta into Gamma or Alpha, and facilitating the transition of the design matrix X from its original form to a reparameterized state. A is also involved in the function f(&#946;) = &#946;T A&#946;, indicating its utility in quadratic forms. Additionally, A is used in the context of the trace operation, where the trace of A equals the sum of its diagonal elements, highlighting its importance in matrix algebra. Furthermore, A is an invertible matrix, essential for reparameterization processes. It is also noted that A is conformable with matrix B for multiplication, suggesting its compatibility in matrix operations. Lastly, A is employed in the transformation of vector Z to obtain U, showcasing its role in vector manipulations. Despite the presence of a vector "a" of dimension m, its connection to matrix A is not explicitly detailed in the descriptions provided.</data>
      <data key="d2">0ac60299320c55d642b3e38440c25f90,2167274129d4cfa74a002c4cc39df8a8,21ec28dfe2b2c18030d541d63e51f45e,542f546c5a131196e4701fb33c9b1dee,5609007c6229060ffc85d8056a7fefde,6c66e9414880964ee899ceb0f16d22e9,82932abd152e0b84a1c26a2daa4c08df,aac5b4f040b9c773bd1aa696dec469f6,bd98ac7b4b5df4f63e7ecc8f4a821f57,d94760a5f9f6ea115fcc18024035a627,f5716ce115458c0652124734ca344806</data>
      <data key="d3">MATRIX</data>
    </node>
    <node id="B">
      <data key="d0">VECTOR</data>
      <data key="d1">Matrix B, often denoted as "B", is a pivotal element in various mathematical operations, particularly in the realm of linear algebra. B is a matrix that is conformable with matrix A for multiplication, enabling the computation of the product AB. This conformity is crucial as it satisfies the property that the trace of the product AB is equal to the trace of the product BA, a fundamental characteristic in matrix theory. Additionally, B is a p x n matrix, indicating its dimensions, which are essential for its role in transforming vector Z. Specifically, Matrix B is utilized in the transformation of vector Z to derive another vector, V, showcasing its utility in vector operations. The vector b, mentioned alongside, is of dimension m and is employed in various vector operations, although its direct relation to Matrix B is not explicitly stated, it can be inferred that it might be involved in operations where Matrix B is a key component. Overall, Matrix B is a versatile and significant matrix in the context of matrix multiplication, trace properties, and vector transformations.</data>
      <data key="d2">0ac60299320c55d642b3e38440c25f90,2167274129d4cfa74a002c4cc39df8a8,aac5b4f040b9c773bd1aa696dec469f6,bd98ac7b4b5df4f63e7ecc8f4a821f57</data>
      <data key="d3">MATRIX</data>
    </node>
    <node id="J">
      <data key="d0">VECTOR</data>
      <data key="d1">J is a vector used in the calculation of the squared error</data>
      <data key="d2">eac62cdd5518e1269fed150639331c2c</data>
    </node>
    <node id="XTY">
      <data key="d0">VECTOR</data>
      <data key="d1">XTY is the matrix product resulting from multiplying the transpose of matrix X with vector Y. This operation is pivotal in the context of least squares estimation, where XTY plays a crucial role in calculating the least squares estimate or estimator. The process involves leveraging the properties of matrix algebra to find the best fit line or model that minimizes the sum of the squared residuals, making XTY an essential component in statistical modeling and data analysis.</data>
      <data key="d2">1303b66694a101878ca530c0b41cf5ef,254a8a17b1be06702934341e3bf41e85,28cf5ff0c09fa5c0390267bb9aa3ce47,eac62cdd5518e1269fed150639331c2c</data>
    </node>
    <node id="F_BETA">
      <data key="d0">FUNCTION</data>
      <data key="d1">f(&#946;) is a function defined as &#946;T A&#946;</data>
      <data key="d2">21ec28dfe2b2c18030d541d63e51f45e</data>
    </node>
    <node id="NORMAL_EQUATIONS">
      <data key="d0">EQUATION</data>
      <data key="d1">The normal equations, a pivotal concept in statistical analysis, particularly within the context of linear regression, are a set of equations derived from the log-likelihood function. These equations are crucial for determining the least squares estimate, denoted as Beta hat (\u03b2b), which is the optimal parameter estimate that minimizes the sum of the squared residuals. The normal equations, as referenced in equation (5.3), are satisfied by \u03b2b, providing a systematic approach to solving for the best fit line in a regression analysis. This method ensures that the estimated parameters are those that maximize the likelihood of observing the given data, making it a fundamental tool in the estimation process for linear models.</data>
      <data key="d2">21ec28dfe2b2c18030d541d63e51f45e,ad799500572246a07f983a3b92c0e61f,f632f01188d2c6e3091a965580cb4600</data>
    </node>
    <node id="S_BETA_BETA_HAT">
      <data key="d0">FUNCTION_VALUE</data>
      <data key="d1">S(&#946;b) is the value of the function S(&#946;) at the stationary point &#946;b</data>
      <data key="d2">21ec28dfe2b2c18030d541d63e51f45e</data>
    </node>
    <node id="RANK">
      <data key="d0">PROPERTY</data>
      <data key="d1">Rank is a property of a matrix that indicates the maximum number of linearly independent rows or columns</data>
      <data key="d2">9d300fc83afb3261af61b2ab9721cadc</data>
    </node>
    <node id="PARTIAL_DERIVATIVE">
      <data key="d0">FUNCTION</data>
      <data key="d1">Partial derivative of S(&#946;) with respect to Beta_0 is used to find the least squares estimate of Beta_0</data>
      <data key="d2">10ac76f99674a01ca0f4a55586dea07e</data>
    </node>
    <node id="PARTIAL_DERIVATIVES">
      <data key="d0">FUNCTION</data>
      <data key="d1">Partial derivatives of S(&#946;) with respect to Beta_0 and Beta_1 are used to find the least squares estimates of Beta_0 and Beta_1</data>
      <data key="d2">416494d940a9f505da9853caca26fe63</data>
    </node>
    <node id="ALPHA0">
      <data key="d0">PARAMETER</data>
      <data key="d1">Alpha0 is a parameter in the straight line model, representing the intercept</data>
      <data key="d2">1303b66694a101878ca530c0b41cf5ef</data>
    </node>
    <node id="ALPHA1">
      <data key="d0">PARAMETER</data>
      <data key="d1">Alpha1 is a parameter in the straight line model, representing the slope</data>
      <data key="d2">1303b66694a101878ca530c0b41cf5ef</data>
    </node>
    <node id="ALPHA_HAT">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">ALPHA_HAT, denoted as \u03b1b, serves as the least squares estimator for parameters in different model contexts. Primarily, it estimates the parameter alpha in the reparameterized model, showcasing its utility in adjusting model parameters for better fit and interpretation. Additionally, ALPHA_HAT plays a pivotal role in the straight line model, where it provides the least squares estimate for both Alpha0 and Alpha1, indicating its versatility in handling linear relationships. This dual functionality highlights ALPHA_HAT's significance in statistical modeling, particularly in scenarios requiring the estimation of parameters through the method of least squares.</data>
      <data key="d2">1303b66694a101878ca530c0b41cf5ef,f5716ce115458c0652124734ca344806</data>
    </node>
    <node id="GAMMA">
      <data key="d0">PARAMETER</data>
      <data key="d1">Gamma, a significant parameter in the reparameterised statistical model, is intricately linked to Beta. This connection is established through the application of an invertible matrix A, which facilitates the transformation from Beta to Gamma. Gamma is explicitly defined as the product of matrix A and Beta (Gamma = A&#946;), showcasing its role as a new parameter vector. This reparameterization process, employing matrix A, is pivotal in reshaping the model's structure, thereby offering a fresh perspective on the underlying relationships and dynamics within the data. Gamma's introduction through this mathematical transformation underscores its importance in enhancing model flexibility and interpretability.</data>
      <data key="d2">82932abd152e0b84a1c26a2daa4c08df,d94760a5f9f6ea115fcc18024035a627,f5716ce115458c0652124734ca344806</data>
    </node>
    <node id="Z">
      <data key="d0">DESIGN_MATRIX</data>
      <data key="d1">In the context of statistical modeling and analysis, Vector Z holds a multifaceted role. Primarily, Z is a vector composed of independent standard normal random variables, pivotal in the formulation of U and V. Additionally, Z is utilized as a matrix in the computation of fitted values within the reparameterized model, highlighting its versatility in different model components. It is also noteworthy that Z serves as a vector of independent and identically distributed (iid) samples, crucial for estimating the variance. This role is further emphasized by the reference to z1, ..., zn, a vector of observed values from an iid sample, which is employed for variance estimation.

Moreover, Z is characterized by a multivariate normal distribution, with a mean denoted by \u00b5 and a covariance matrix represented by \u03a3, indicating its probabilistic nature and the complexity of its distribution. In the realm of model transformation, Z takes on the identity of the design matrix in the transformed model, achieved through the application of the transformation XA^-1 to the original design matrix X. This transformation underscores Z's role in facilitating model adjustments and enhancements.

In summary, Z is a dynamic entity, functioning as a vector of standard normal random variables, a matrix for fitted value calculation, a vector for variance estimation from iid samples, a representation of a multivariate normal distribution, and a design matrix in the transformed model. Its diverse roles in statistical analysis and modeling highlight its significance in various analytical contexts.</data>
      <data key="d2">09caa54ca1372d152e47051be4d44ede,0ac60299320c55d642b3e38440c25f90,542f546c5a131196e4701fb33c9b1dee,6648f0d6deed51fb4fb25e6992a71ddf,82932abd152e0b84a1c26a2daa4c08df,aac5b4f040b9c773bd1aa696dec469f6,d94760a5f9f6ea115fcc18024035a627,f5716ce115458c0652124734ca344806</data>
    </node>
    <node id="GAMMA_HAT">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">Gamma hat (&#947;&#770;), denoted as GAMMA_HAT, is the least squares estimator of the parameter Gamma. It is calculated using the formula A(XTX)^-1XTY, where X is the matrix of explanatory variables, Y is the vector of the dependent variable, and A is a constant or matrix that may be applied to scale or adjust the result. This estimator is pivotal in statistical modeling, particularly in linear regression analysis, as it provides an estimate of the relationship between the explanatory variables and the dependent variable.</data>
      <data key="d2">82932abd152e0b84a1c26a2daa4c08df,d94760a5f9f6ea115fcc18024035a627</data>
    </node>
    <node id="AT">
      <data key="d0">MATRIX</data>
      <data key="d1">AT is the transpose of matrix A</data>
      <data key="d2">82932abd152e0b84a1c26a2daa4c08df</data>
    </node>
    <node id="ALPHA">
      <data key="d0">PARAMETER</data>
      <data key="d1">ALPHA is a significant parameter in the reparameterised model, playing a dual role within the model's structure. Firstly, ALPHA is comprised of components alpha_0 and alpha_1, which are integral to the model's formulation. Secondly, ALPHA is also described as a parameter vector in the reparameterised model, indicating its comprehensive nature in encapsulating multiple dimensions of the model's parameters. Additionally, ALPHA is related to another parameter, Beta, through a specific transformation, xj - x\u00af, which highlights its role in the interplay between different parameters within the model. This relationship underscores the interconnectedness of ALPHA with other model components, emphasizing its importance in the overall structure and function of the reparameterised model.</data>
      <data key="d2">5609007c6229060ffc85d8056a7fefde,6c66e9414880964ee899ceb0f16d22e9,82932abd152e0b84a1c26a2daa4c08df</data>
    </node>
    <node id="A_INVERSE">
      <data key="d0">INVERSE_MATRIX</data>
      <data key="d1">A_inverse is the inverse of matrix A, used to transform Alpha back into Beta</data>
      <data key="d2">5609007c6229060ffc85d8056a7fefde</data>
    </node>
    <node id="X_ALPHA">
      <data key="d0">DESIGN_MATRIX</data>
      <data key="d1">X_ALPHA, also referred to as X_alpha, is the design matrix that plays a pivotal role in the model parameterisation process. It is derived through a transformation of X_beta, using the matrix A_inverse, as detailed in equation (1). This transformation is crucial for reparameterising the model, making X_ALPHA an essential component in the adjusted model structure. Alternatively, X_ALPHA can also be obtained by directly transforming the matrix X using the matrix A, highlighting its versatility and importance in the context of model parameter adjustments.</data>
      <data key="d2">5609007c6229060ffc85d8056a7fefde,6c66e9414880964ee899ceb0f16d22e9</data>
    </node>
    <node id="C">
      <data key="d0">CONSTANT</data>
      <data key="d1">In the context of the reparameterised model, C, a non-zero constant, plays a crucial role by serving as a multiplier for all observations of the explanatory variable. This transformation, facilitated by C, adjusts the scale of the explanatory variable, thereby impacting the model's coefficients and potentially improving the model's fit or interpretability. As a constant, C ensures that each observation of the explanatory variable is uniformly adjusted, maintaining the integrity of the data's relative relationships within the model.</data>
      <data key="d2">6c66e9414880964ee899ceb0f16d22e9,d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </node>
    <node id="ALPHA_HAT_0">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">Alpha_hat_0 is the least squares estimate of Alpha_0</data>
      <data key="d2">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </node>
    <node id="ALPHA_HAT_1">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">Alpha_hat_1 is the least squares estimate of Alpha_1</data>
      <data key="d2">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </node>
    <node id="MAYA">
      <data key="d0">PERSON</data>
      <data key="d1">Maya is a person who has fitted a simple linear regression model to a set of data</data>
      <data key="d2">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </node>
    <node id="AMBIENT_TEMPERATURE">
      <data key="d0">EXPLANATORY_VARIABLE</data>
      <data key="d1">Ambient temperature is the explanatory variable in Maya's model, measured in Celsius</data>
      <data key="d2">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </node>
    <node id="FAHRENHEIT">
      <data key="d0">UNIT</data>
      <data key="d1">Fahrenheit is a unit of measurement for temperature</data>
      <data key="d2">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </node>
    <node id="Y_HAT_J">
      <data key="d0">FITTED_VALUE</data>
      <data key="d1">Y_hat_j is the fitted value for the jth observation, calculated as xT_j &#946;_hat</data>
      <data key="d2">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </node>
    <node id="RESIDSS">
      <data key="d0">STATISTIC</data>
      <data key="d1">ResidSS, also known as the residual sum of squares, is a pivotal statistical measure that quantifies the total squared difference between the observed values and the values predicted by a model. This metric serves as a critical indicator of the model's goodness of fit, where a lower ResidSS value suggests a better fit. Mathematically, ResidSS is calculated as the sum of the squared residuals, represented by the formula &#8721;(yi - ybi)^2, where yi denotes the observed value and ybi represents the predicted value for the ith observation. Additionally, ResidSS is sometimes referred to as the deviance (D), further emphasizing its role in assessing model accuracy and performance.</data>
      <data key="d2">255685e281cc5a9edf073c700f425a6b,d1b6fcd55d937c5fe2d6add69e0bcf05,fc5b725f3c662c5471af20efdcc2dbff</data>
    </node>
    <node id="LSE">
      <data key="d0">ESTIMATION_METHOD</data>
      <data key="d1">LSE, or Least Squares Estimate, is a statistical method primarily used in the context of linear regression models to estimate the parameter vector \u03b2. This estimation technique does not require the assumption that the errors are independently and identically distributed (iid) as normal random variables with mean 0 and variance \u03c3^2. LSE is a versatile approach that can be applied to a wide range of data sets, providing a robust way to determine the best fit line for the data without stringent assumptions about the error distribution. The method minimizes the sum of the squares of the residuals, making it a powerful tool for estimating the parameters of a linear model.</data>
      <data key="d2">9fc2b1e8b2b61b557f88eb9e9c708597,d738df7d83784c8a41b3948271c537b6</data>
    </node>
    <node id="MLE">
      <data key="d0">ESTIMATION_METHOD</data>
      <data key="d1">Maximum Likelihood Estimate (MLE) is a robust statistical method employed to estimate the parameters of a model. Specifically, in the context of a normal linear model, MLE is used to derive the estimate for the parameter vector &#946;. This estimation is achieved by maximizing the likelihood function, assuming that the errors are independently and identically distributed (iid) as a normal distribution with a mean of 0 and a variance of &#963;^2. MLE provides a principled approach to parameter estimation, making it a fundamental technique in statistical analysis and inference.</data>
      <data key="d2">9dddcd96af7b557e578b3f5f36efacd7,9fc2b1e8b2b61b557f88eb9e9c708597,d738df7d83784c8a41b3948271c537b6</data>
    </node>
    <node id="SHEATHER_BOOK">
      <data key="d0">REFERENCE</data>
      <data key="d1">A Modern Approach to Regression with R (2009) by Sheather is a book that provides further reading on regression analysis</data>
      <data key="d2">9fc2b1e8b2b61b557f88eb9e9c708597</data>
    </node>
    <node id="MULTIVARIATE_NORMAL_DISTRIBUTION">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">The MULTIVARIATE_NORMAL_DISTRIBUTION, denoted as Nn(&#956;, &#931;), is a comprehensive probability distribution that extends the concept of the univariate normal distribution to higher dimensions. This distribution is defined over vectors in R^n, where n represents the number of dimensions. It is characterized by two key parameters: a mean vector &#956;, which specifies the expected value of the distribution in each dimension, and a covariance matrix &#931;, which describes the statistical relationship, or dependence, between the dimensions. The covariance matrix not only captures the variance of each individual dimension but also the covariances between all pairs of dimensions, providing a complete description of the multivariate normal distribution's structure.</data>
      <data key="d2">aac5b4f040b9c773bd1aa696dec469f6,f483798b15ef305e7826fd7142379e03</data>
    </node>
    <node id="LOG_LIKELIHOOD">
      <data key="d0">FUNCTION</data>
      <data key="d1">The entity "LOG_LIKELIHOOD" refers to the log-likelihood function utilized in the context of a normal linear regression model. This function serves as a pivotal measure in statistical inference, enabling the estimation of model parameters. Specifically, the log-likelihood function, denoted as \u2113, is derived from the likelihood function L and is maximized by the vector \u03b2. This maximization occurs when \u03b2 minimizes the residual sum of squares function S(\u03b2), for a given value of \u03c3^2, thereby facilitating the identification of the model parameters that best fit the data under the assumption of normality.</data>
      <data key="d2">87b717ba065d6d7c7431af284137eb12,9dddcd96af7b557e578b3f5f36efacd7,ad799500572246a07f983a3b92c0e61f</data>
    </node>
    <node id="BETAK_HAT">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">Beta_k hat is the least squares estimate of one of the slope parameters</data>
      <data key="d2">87b717ba065d6d7c7431af284137eb12</data>
    </node>
    <node id="PI">
      <data key="d0">CONSTANT</data>
      <data key="d1">Pi (&#960;) is a mathematical constant used in the calculation of the likelihood function</data>
      <data key="d2">e7edd8b2874a350779ae20f1ecdf4733</data>
    </node>
    <node id="D">
      <data key="d0">DEVIANCE</data>
      <data key="d1">D is the deviance or residual sum of squares of the fitted model</data>
      <data key="d2">e7edd8b2874a350779ae20f1ecdf4733</data>
    </node>
    <node id="L">
      <data key="d0">LIKELIHOOD</data>
      <data key="d1">The entity "L" represents the likelihood function associated with the normal linear model. This function, denoted as L(&#946;, &#963;^2|y), is specifically defined in the context of the parameters &#946; (Beta) and &#963;^2 (Sigma squared), given the observed data y. It encapsulates the probability of the observed data under different values of the model parameters, facilitating the estimation of these parameters through methods such as maximum likelihood estimation. The function's role is pivotal in statistical inference, particularly in assessing the fit of the normal linear model to the data at hand.</data>
      <data key="d2">9dddcd96af7b557e578b3f5f36efacd7,e7edd8b2874a350779ae20f1ecdf4733,f632f01188d2c6e3091a965580cb4600</data>
    </node>
    <node id="RESIDUAL_SUM_SQUARES">
      <data key="d0">FUNCTION</data>
      <data key="d1">The residual sum of squares function S(&#946;) is a measure of the difference between the observed data and the values predicted by the model. It is minimised by the least squares estimate &#946;b.</data>
      <data key="d2">ad799500572246a07f983a3b92c0e61f</data>
    </node>
    <node id="SIGMA_HAT_SQUARED">
      <data key="d0">MLE</data>
      <data key="d1">SIGMA_HAT_SQUARED, denoted as \u03c3b^2MLE, represents the maximum likelihood estimate of the variance Sigma squared (\u03c3^2). This statistical measure is crucial in understanding the dispersion of data points around the mean. Additionally, \u03c3b^2 also refers to the unbiased estimator of the variance, which is calculated using the formula (1/(n-1)) * \u03a3(zi - z_bar)^2, where 'n' is the number of observations, 'zi' are the individual data points, and 'z_bar' is the mean of the dataset. This unbiased estimator adjusts for the bias in the sample variance, providing a more accurate estimate of the population variance. The use of SIGMA_HAT_SQUARED in both its maximum likelihood and unbiased forms is essential in statistical analysis, particularly in the context of linear regression and residual analysis, where it helps in assessing the goodness of fit and the reliability of the model.</data>
      <data key="d2">6648f0d6deed51fb4fb25e6992a71ddf,f632f01188d2c6e3091a965580cb4600</data>
    </node>
    <node id="BIAS">
      <data key="d0">BIAS</data>
      <data key="d1">Bias refers to the difference between the expected value of an estimator and the true value of the parameter being estimated</data>
      <data key="d2">f632f01188d2c6e3091a965580cb4600</data>
    </node>
    <node id="ERROR_VARIANCE">
      <data key="d0">VARIANCE</data>
      <data key="d1">The entity of interest is ERROR_VARIANCE, which is a fundamental concept in statistical modeling, particularly in regression analysis. It is defined as the variance of the error term in a statistical or regression model, denoted by the symbol \u03c3^2. This error variance represents the amount of variation in the response variable that cannot be accounted for by the model, essentially capturing the randomness or unpredictability in the data. In the context of regression analysis, ERROR_VARIANCE is estimated by s^2, providing a measure of the spread of the residuals around the fitted regression line. This estimation is crucial for understanding the goodness-of-fit of the model and for making accurate predictions.</data>
      <data key="d2">09391efd3b8c510205098b548bc8dc74,7e05f1b457a496c8b3630e7044fc5981,d7f3a28534ffe830fe6f4cef8c41a9b4,f632f01188d2c6e3091a965580cb4600</data>
    </node>
    <node id="BETA_HAT_Y">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">Beta hat (&#946;b(Y)) is the estimator of Beta, a function of the random vector Y</data>
      <data key="d2">2de7a36b32bf79c8f32612c8aaa9daa8</data>
    </node>
    <node id="NP">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">NP, in the context of statistical analysis, refers to the multivariate normal distribution. This distribution is characterized by a mean vector and a covariance matrix. In the first description, the mean of NP is given as AX&#946;, where A is a matrix, X is a set of independent variables, and &#946; is the vector of regression coefficients. The covariance matrix is A&#963;^2I_nA^T, where &#963;^2 is the variance, I_n is the identity matrix of size n, and A^T is the transpose of matrix A. This formulation suggests that NP is used in the context of a linear regression model, where the response variable is assumed to follow a multivariate normal distribution with the specified mean and covariance structure.

In the second description, the mean of NP is simply &#946;, which can be interpreted as the vector of regression coefficients in a linear regression model. The covariance matrix is given as &#963;^2(X^TX)^-1, where X^TX is the matrix product of X and its transpose, and (X^TX)^-1 is the inverse of this product. This formulation is consistent with the properties of the least squares estimator in linear regression, where the variance-covariance matrix of the estimator is proportional to the inverse of the information matrix (X^TX).

To reconcile the two descriptions, it can be inferred that NP represents the multivariate normal distribution of the response variable in a linear regression model. The mean of NP can be either AX&#946;, which is the expected value of the response variable given the independent variables, or simply &#946;, which represents the regression coefficients. The covariance matrix of NP can be either A&#963;^2I_nA^T, which accounts for the variability in the response variable due to the independent variables and the error term, or &#963;^2(X^TX)^-1, which represents the variance-covariance matrix of the least squares estimator of the regression coefficients.

In summary, NP is the multivariate normal distribution used in the context of linear regression analysis. It has a mean vector that can be either AX&#946; or &#946;, and a covariance matrix that can be either A&#963;^2I_nA^T or &#963;^2(X^TX)^-1, depending on whether the focus is on the distribution of the response variable or the distribution of the regression coefficients.</data>
      <data key="d2">3d357cfa3ef0d00f49cf4acaeac1c9d1,45f31b040576e9f3b4def6d0466cc016,542f546c5a131196e4701fb33c9b1dee,d14413709de2897231aaa83be3aa346f</data>
    </node>
    <node id="AZ">
      <data key="d0">TRANSFORMED VECTOR</data>
      <data key="d1">AZ is the transformed vector obtained by multiplying Z by A, following a multivariate normal distribution with mean A&#181; and covariance matrix A&#931;AT</data>
      <data key="d2">542f546c5a131196e4701fb33c9b1dee</data>
    </node>
    <node id="MU">
      <data key="d0">MEAN</data>
      <data key="d1">In the context of statistical modeling for sales analysis, MU (&#956;) is a critical parameter that plays a multifaceted role. Primarily, &#956; represents the baseline sales volume or level for different brands, serving as the intercept in regression models. Specifically, for Brand A, &#956; denotes the average sales volume and the intercept of the regression line, indicating the expected sales level when other variables are zero. This parameter is pivotal in the parallel lines model, where it signifies the baseline sales, and in the reparameterised model, where it retains its role as the baseline sales volume.

Moreover, &#956; is the intercept parameter in the model equations for all brands, suggesting its importance in establishing a common starting point for sales predictions across various brands. It is also the mean vector (&#956;A, &#956;B, &#956;C) of a multivariate normal distribution that the variable Z follows, where each component of the vector represents the mean sales for each brand (A, B, and C). This highlights the role of &#956; in multivariate analysis, where it encapsulates the average sales performance for each brand within a distribution, allowing for the assessment of sales variability and correlation among brands.</data>
      <data key="d2">06199787dd7f75f7338dd24d4f3dc26e,06d5666e6bfdda828b48adba883b4a61,096afa471635bc59c3bfa9af4d04d625,1d141ab04db553f78a313e430e54abb5,542f546c5a131196e4701fb33c9b1dee,825b600cbab3535ce67e9f561ddcb84b,906eb7d6b49fa360e7e5b65c56cd4d76,a1fc936df848a0fbc791e4bcc9b527b6,ac15b639b0849006471dfe102376c2c0,b6870535f3975c49d45e62fbe475f198</data>
    </node>
    <node id="SIGMA">
      <data key="d0">COVARIANCE MATRIX</data>
      <data key="d1">SIGMA, denoted by the Greek letter \u03a3, is the covariance matrix of the multivariate normal distribution that the variable Z follows. This matrix encapsulates the variances and covariances among the variables in the distribution, providing a comprehensive measure of the spread and interrelation of the data points in a multivariate setting. SIGMA plays a crucial role in understanding the structure and dependencies within the multivariate normal distribution, which is a fundamental concept in statistical analysis and data science.</data>
      <data key="d2">542f546c5a131196e4701fb33c9b1dee,aac5b4f040b9c773bd1aa696dec469f6</data>
    </node>
    <node id="VARIANCE_BETA_HAT">
      <data key="d0">VARIANCE</data>
      <data key="d1">VARIANCE_BETA_HAT is the variance of the least squares estimator BETA_HAT, calculated as &#963;^2(XTX)^-1</data>
      <data key="d2">69ffba28a61d98d8d18f91c24b74dd4a</data>
    </node>
    <node id="COVARIANCE_BETA_HAT">
      <data key="d0">COVARIANCE</data>
      <data key="d1">COVARIANCE_BETA_HAT is the covariance between the least squares estimates BETA_0_HAT and BETA_1_HAT</data>
      <data key="d2">69ffba28a61d98d8d18f91c24b74dd4a</data>
    </node>
    <node id="SIGMA_SQUARED_MLE">
      <data key="d0">ESTIMATE</data>
      <data key="d1">SIGMA_SQUARED_MLE (&#963;b^2_MLE) represents the maximum likelihood estimate for the error variance in a statistical model. This parameter is crucial for understanding the variability of the errors around the predicted values. It is calculated using the formula (1/n)(Y - X BETA_HAT(Y))^T(Y - X BETA_HAT(Y)), where Y is the vector of observed responses, X is the design matrix, and BETA_HAT(Y) is the vector of estimated coefficients obtained from the model. This estimator provides a measure of the goodness of fit for the model, with lower values indicating a better fit. The alternative representation of the formula as (1/n)(y - X &#946;b(y))^T(y - X &#946;b(y)) is consistent with the primary definition, where y and &#946;b(y) are scalar representations of Y and BETA_HAT(Y), respectively. SIGMA_SQUARED_MLE is a fundamental component in the assessment of model reliability and the precision of predictions.</data>
      <data key="d2">09caa54ca1372d152e47051be4d44ede,69ffba28a61d98d8d18f91c24b74dd4a</data>
    </node>
    <node id="BETA_HAT0">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">Beta hat 0 (b&#946;0) is the estimator of the parameter Beta0, calculated using the maximum likelihood estimation method</data>
      <data key="d2">09caa54ca1372d152e47051be4d44ede</data>
    </node>
    <node id="Z_BAR">
      <data key="d0">MEAN</data>
      <data key="d1">Z_bar is the sample mean of the observed values z1, ..., zn</data>
      <data key="d2">6648f0d6deed51fb4fb25e6992a71ddf</data>
    </node>
    <node id="SIGMA_HAT_SQUARED_BIAS">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">&#963;b^2 is a biased estimator of the variance, calculated as (1/n) * &#931;(zi - z_bar)^2</data>
      <data key="d2">6648f0d6deed51fb4fb25e6992a71ddf</data>
    </node>
    <node id="S_HAT_SQUARED">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">s^2(Y) is the unbiased estimator for the error variance, calculated as (1/(n-p)) * (Y - X&#946;b)^T(Y - X&#946;b)</data>
      <data key="d2">6648f0d6deed51fb4fb25e6992a71ddf</data>
    </node>
    <node id="THEOREM_6_4">
      <data key="d0">THEOREM</data>
      <data key="d1">Theorem 6.4, also referred to as THEOREM_6_4, highlights a significant statistical property within the context of normal linear models. According to the theorem, s^2(Y), the sample variance of the response variable Y, serves as an unbiased estimator for \u03c3^2, the population variance. This finding is crucial as it ensures that, on average, the estimator s^2(Y) will yield an estimate equal to the true value of \u03c3^2, thereby facilitating accurate statistical inference and model evaluation in the realm of normal linear models.</data>
      <data key="d2">6648f0d6deed51fb4fb25e6992a71ddf,9923e77ac6b3de95cb5026bc5e7fe8c0</data>
    </node>
    <node id="S2">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">S2, also denoted as s^2, is the unbiased estimator for the error variance (\u03c3^2) in a statistical model, specifically within the context of linear regression. This unbiased estimate of sigma squared is calculated by dividing the residual sum of squares by (n - p), where n represents the number of observations and p signifies the number of parameters in the model. This calculation provides a precise estimate of the variance of the error term in the regression model, offering valuable insights into the model's accuracy and reliability.</data>
      <data key="d2">9923e77ac6b3de95cb5026bc5e7fe8c0,9e2ebbb113c00fa43f0af3c0696baf95,aac5b4f040b9c773bd1aa696dec469f6,fc5b725f3c662c5471af20efdcc2dbff</data>
    </node>
    <node id="EXERCISE_18">
      <data key="d0">EXERCISE</data>
      <data key="d1">Exercise 18 is a problem that involves determining the design matrix X and the estimator &#946;b for a given linear model</data>
      <data key="d2">fc5b725f3c662c5471af20efdcc2dbff</data>
    </node>
    <node id="E_Y3_GIVEN_X3">
      <data key="d0">EXPECTED_VALUE</data>
      <data key="d1">Expected value of Y3 given X3=x3, used in the calculation involving Beta1 and Beta2</data>
      <data key="d2">45f31b040576e9f3b4def6d0466cc016</data>
    </node>
    <node id="SIGMA_HAT_SQUARED_MLE">
      <data key="d0">MLE_ESTIMATE</data>
      <data key="d1">The entity "SIGMA_HAT_SQUARED_MLE" refers to the Maximum Likelihood Estimate (MLE) for the variance parameter, denoted as \u03c3^2. This estimate is computed using the formula (1/n)(y - X\u03b2b(y))^T(y - X\u03b2b(y)), where y represents the observed data, X is the design matrix, and \u03b2b(y) is the vector of estimated parameters. Despite its status as the MLE, "SIGMA_HAT_SQUARED_MLE" is noted to be a biased estimator, meaning that its expected value does not equal the true value of \u03c3^2. This bias arises from the specific structure of the estimator and the nature of the data. The term "Sigma hat squared (\u03c3b^2)" is used interchangeably with "SIGMA_HAT_SQUARED_MLE" to denote this biased MLE for the variance.</data>
      <data key="d2">2673d078d29f2af78fab9b6eacd15e37,45f31b040576e9f3b4def6d0466cc016</data>
    </node>
    <node id="S_SQUARED">
      <data key="d0">UNBIASED_ESTIMATE</data>
      <data key="d1">S_SQUARED, denoted as s^2, is the unbiased estimator of the error variance &#963;^2 in a linear regression model. It is calculated as (1/(n-p))(y - X&#946;b(y))^T(y - X&#946;b(y)), where n is the number of observations, p is the number of parameters in the model, y is the vector of observed responses, X is the design matrix, and &#946;b(y) is the vector of estimated coefficients. This formula provides an unbiased estimate for &#963;^2, based on the model fitted to the full dataset, ensuring that the estimate is not systematically too high or too low. S_SQUARED plays a crucial role in assessing the goodness of fit and the reliability of the regression model.</data>
      <data key="d2">0443ab5e20a4f6b2f243c989ef6c723a,09391efd3b8c510205098b548bc8dc74,0ac60299320c55d642b3e38440c25f90,2673d078d29f2af78fab9b6eacd15e37,45f31b040576e9f3b4def6d0466cc016</data>
    </node>
    <node id="H">
      <data key="d0">HAT_MATRIX</data>
      <data key="d1">H, also known as the hat matrix, is a pivotal component in the theory and application of linear regression models. It is a square, symmetric, and idempotent matrix, defined as H = X(X'X)^-1X', where X is the design matrix. H plays a special role in linear models, particularly in relation to the leverage of data points. It is used to transform the observed response values into fitted values, effectively projecting the response vector onto the space of the predictors. As a linear map, H takes the observed values y to the fitted values yb, hence its colloquial name, the hat matrix, as it "puts the hat on y". Its properties, including being of rank p, are crucial in the calculation of predicted values Yc and residuals Eb, thereby facilitating residual analysis. The hat matrix's idempotent nature, H^2 = H, is a key feature in understanding the structure and relations within the community of interest in the context of algorithmic analysis.</data>
      <data key="d2">0b650eb2f1dcd603b64fec3c4b5cd24b,1da117a2f92b2db00290d2a0bfc06beb,2685edb9e8031c8ea725c43a40af22a8,46629f2efc6c82e81265a131b4bab2ee,5a0d392715f06d5e873f45ae06aa729a,679722cf8ce5ce5aee4e379528470efe,6ee02b38ae842fd5eac9a11c4fd6659f,74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6,bd98ac7b4b5df4f63e7ecc8f4a821f57,e593096f3805c2686423cb91ea276fe6,f470791d2d3fedede166f9bb11598c9c</data>
    </node>
    <node id="Y_HAT">
      <data key="d0">FITTED_VALUES</data>
      <data key="d1">Y hat (yb) are the fitted values obtained by applying the hat matrix H to the observed values Y.</data>
      <data key="d2">f470791d2d3fedede166f9bb11598c9c</data>
    </node>
    <node id="EPSILON_HAT">
      <data key="d0">RESIDUALS</data>
      <data key="d1">Residuals (b&#1013;) are the estimates of the errors of a statistical model, calculated as the difference between the observed values Y and the fitted values Y hat (yb).</data>
      <data key="d2">f470791d2d3fedede166f9bb11598c9c</data>
    </node>
    <node id="LEMMA_7_1">
      <data key="d0">LEMMA</data>
      <data key="d1">Lemma 7.1 describes the properties of the hat matrix H, including its symmetry, idempotence, and the properties of (In - H).</data>
      <data key="d2">f470791d2d3fedede166f9bb11598c9c</data>
    </node>
    <node id="IP">
      <data key="d0">MATRIX</data>
      <data key="d1">The entity "IP" refers to the p x p identity matrix, denoted as Ip. This matrix is characterized by having ones on the diagonal and zeros elsewhere. The trace of Ip, which is the sum of the elements on the main diagonal, is equal to p, the number of predictors. This indicates that Ip plays a significant role in contexts where the number of predictors is a relevant parameter, such as in statistical modeling and algorithmic analysis.</data>
      <data key="d2">46629f2efc6c82e81265a131b4bab2ee,bd98ac7b4b5df4f63e7ecc8f4a821f57</data>
    </node>
    <node id="HII">
      <data key="d0">LEVERAGE</data>
      <data key="d1">HII, denoted as hii, is the leverage value for the ith observation in the context of a regression model. It serves as a measure of how far the independent variable value of the ith observation is from the mean of that variable, highlighting the weight that the observed value yi has when computing the fitted value ybi. HII is specifically the ith diagonal element of the hat matrix H, which plays a crucial role in determining the influence of each data point on the regression line. This leverage value indicates the potential impact of the ith data point on the regression coefficients, with higher values suggesting greater influence. The term "leverage" in this context refers to the ability of a data point to pull the regression line towards itself, and HII quantifies this property for each observation.</data>
      <data key="d2">0443ab5e20a4f6b2f243c989ef6c723a,0b650eb2f1dcd603b64fec3c4b5cd24b,1da117a2f92b2db00290d2a0bfc06beb,2685edb9e8031c8ea725c43a40af22a8,5a0d392715f06d5e873f45ae06aa729a,679722cf8ce5ce5aee4e379528470efe,6ee02b38ae842fd5eac9a11c4fd6659f,90b7e0427699cc1bb461e37939935138,bd98ac7b4b5df4f63e7ecc8f4a821f57,c47968226557bc2eb5aec5bb7994fd0e</data>
    </node>
    <node id="H2">
      <data key="d0">MATRIX</data>
      <data key="d1">H2 is the square of the hat matrix, which is idempotent and symmetric, and its diagonal elements are related to the leverage values in regression analysis</data>
      <data key="d2">679722cf8ce5ce5aee4e379528470efe</data>
    </node>
    <node id="HIK">
      <data key="d0">MATRIX_ELEMENT</data>
      <data key="d1">hik is the element in the ith row and kth column of the hat matrix H, representing the influence of the kth observation on the fitted value of the ith observation</data>
      <data key="d2">679722cf8ce5ce5aee4e379528470efe</data>
    </node>
    <node id="YC">
      <data key="d0">FITTED_VALUES</data>
      <data key="d1">YC, denoted as the random vector of fitted values in the context of a linear model, is calculated through the operation HY. Here, H signifies the hat matrix, a pivotal component in regression analysis that plays a crucial role in the computation of fitted values. The vector Y represents the response random variables, which, when multiplied by the hat matrix H, yields YC&#8212;the vector of predicted values. This process encapsulates the essence of linear regression, where the hat matrix transforms the observed response variables into their corresponding fitted values, providing insights into the structure and relationships within the data.</data>
      <data key="d2">1da117a2f92b2db00290d2a0bfc06beb,679722cf8ce5ce5aee4e379528470efe,74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6</data>
    </node>
    <node id="HX">
      <data key="d0">MATRIX</data>
      <data key="d1">HX is the result of the matrix operation X(X'X)^-1X', which simplifies to X</data>
      <data key="d2">1da117a2f92b2db00290d2a0bfc06beb</data>
    </node>
    <node id="VAR">
      <data key="d0">VARIANCE</data>
      <data key="d1">VAR, also known as the variance function or operator, is a statistical tool utilized in the calculation of the variance for both vectors and random variables. It quantifies the dispersion or spread of a set of values or a probability distribution by averaging the squared deviations from their mean. This function is fundamental in statistical analysis, providing insights into the variability and structure of the data under investigation.</data>
      <data key="d2">1da117a2f92b2db00290d2a0bfc06beb,74d190f10bf6e6936242ca3cdfc4a09f</data>
    </node>
    <node id="YB_I">
      <data key="d0">VECTOR</data>
      <data key="d1">YB_I, denoted as Yb(i) or yb(i) interchangeably, represents the fitted value of the response variable calculated from a dataset with the ith observation removed. This entity is crucial in assessing the influence of individual observations on the regression model. YB_I is also referred to as the ith predicted value in the vector Yc, highlighting its role in the vector of fitted values. The use of YB_I in residual analysis and model diagnostics is pivotal for understanding the stability and reliability of the model predictions when individual data points are excluded. This entity provides insights into the leverage and influence of each observation on the overall model fit, enabling a comprehensive evaluation of the model's robustness.</data>
      <data key="d2">0443ab5e20a4f6b2f243c989ef6c723a,09391efd3b8c510205098b548bc8dc74,1da117a2f92b2db00290d2a0bfc06beb,9e2ebbb113c00fa43f0af3c0696baf95</data>
    </node>
    <node id="EB">
      <data key="d0">VECTOR</data>
      <data key="d1">EB, also denoted as Eb, is the vector of residuals in the context of a linear model. It is calculated by subtracting the fitted or predicted values from the observed values, which can be mathematically represented as (In - H)Y. Here, In is the identity matrix and H is the hat matrix used in the calculation of predicted values. EB plays a crucial role in assessing the goodness of fit of the linear model, as it quantifies the discrepancies between the observed data points and the values predicted by the model. This residual vector, EB, encapsulates the portion of the variability in the data that is not explained by the model, providing insights into the model's accuracy and potential areas for improvement.</data>
      <data key="d2">1da117a2f92b2db00290d2a0bfc06beb,2685edb9e8031c8ea725c43a40af22a8,5a0d392715f06d5e873f45ae06aa729a,74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6</data>
    </node>
    <node id="Y_HAT_I">
      <data key="d0">FITTED_VALUE</data>
      <data key="d1">Y_HAT_I, denoted as y_hat_i, represents the fitted value for the ith observation within the context of a linear regression model. This value is obtained through the application of the model's coefficients to the independent variables of the ith observation, providing an estimate of the dependent variable's value. As a crucial component of regression analysis, y_hat_i facilitates the assessment of model accuracy and the identification of patterns in the data. Specifically, it serves as a prediction of the dependent variable based on the linear relationship established by the regression model.</data>
      <data key="d2">2685edb9e8031c8ea725c43a40af22a8,5a0d392715f06d5e873f45ae06aa729a</data>
    </node>
    <node id="EI">
      <data key="d0">RESIDUAL</data>
      <data key="d1">Ei is the ith residual in a regression model, with an expectation of zero</data>
      <data key="d2">2685edb9e8031c8ea725c43a40af22a8</data>
    </node>
    <node id="VAR_EI">
      <data key="d0">VARIANCE</data>
      <data key="d1">Var(Ei) is the variance of the ith residual, given by &#963;^2(1 - hii)</data>
      <data key="d2">2685edb9e8031c8ea725c43a40af22a8</data>
    </node>
    <node id="&#931;_SQUARED">
      <data key="d0">VARIANCE</data>
      <data key="d1">&#963;^2 is the variance of the error term in the regression model</data>
      <data key="d2">2685edb9e8031c8ea725c43a40af22a8</data>
    </node>
    <node id="COV_EB_Y">
      <data key="d0">COVARIANCE_MATRIX</data>
      <data key="d1">Cov(Eb, Y) is the covariance matrix of the vector of residuals Eb and the vector of response variables Y</data>
      <data key="d2">2685edb9e8031c8ea725c43a40af22a8</data>
    </node>
    <node id="COV">
      <data key="d0">COVARIANCE</data>
      <data key="d1">COV, also referred to as Cov, is a statistical function or operator that plays a pivotal role in calculating the covariance between vectors. This function quantifies the degree to which two vectors move in relation to each other, providing insights into their linear dependence. By analyzing the covariance, one can understand how much the vectors change together, which is crucial in various statistical analyses and machine learning applications. COV is a fundamental tool in the statistical analysis of data, enabling researchers and data scientists to explore the relationships and patterns within datasets.</data>
      <data key="d2">74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6</data>
    </node>
    <node id="0N_X_N">
      <data key="d0">MATRIX</data>
      <data key="d1">0n&#215;n is the n x n matrix whose entries are all equal to zero</data>
      <data key="d2">74d190f10bf6e6936242ca3cdfc4a09f</data>
    </node>
    <node id="0N&#215;N">
      <data key="d0">ZERO_MATRIX</data>
      <data key="d1">0n&#215;n is the n x n zero matrix</data>
      <data key="d2">7d074208b1259e7d84f9f870d3828bb6</data>
    </node>
    <node id="REGRESSION_OUTLIERS">
      <data key="d0">DATA_POINTS</data>
      <data key="d1">Regression outliers are data points with a large positive or negative residual, indicating they do not fit well with the model</data>
      <data key="d2">e593096f3805c2686423cb91ea276fe6</data>
    </node>
    <node id="COOKS_DISTANCE">
      <data key="d0">METRIC</data>
      <data key="d1">Cook's distance (Di) is a comprehensive statistical measure used in regression analysis to identify influential data points. It quantifies the impact of each observation on the fitted values of all regression model predictions. Specifically, Cook's distance is calculated as (yb(i) - yb)^T(yb(i) - yb) / (ps^2), where yb(i) represents the predicted value with the ith observation removed, and ps^2 is the estimated variance of the error term. This measure takes into account both the leverage and standardized residuals of the data points, effectively combining information on how much the fitted values change when a particular observation is excluded from the dataset. By doing so, Cook's distance helps in detecting influential observations that may significantly affect the regression model's results.</data>
      <data key="d2">09391efd3b8c510205098b548bc8dc74,323899f01972255cd3278bccee20d5d8,428db872e71a17a2cf7868b03a52def0,629ce6550294d332948e19171a4acd2d,7e05f1b457a496c8b3630e7044fc5981,9e2ebbb113c00fa43f0af3c0696baf95,e593096f3805c2686423cb91ea276fe6</data>
    </node>
    <node id="X_I">
      <data key="d0">VARIABLE</data>
      <data key="d1">X_i is the ith observation of the explanatory variable in the dataset</data>
      <data key="d2">bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </node>
    <node id="LEVERAGE">
      <data key="d0">STATISTIC</data>
      <data key="d1">Leverage, denoted as hii, is a pivotal statistical metric utilized in regression analysis to quantify the influence of the ith unit of observation. This measure evaluates how far an independent variable value of a specific data point is from the mean of that variable, effectively indicating its distance from the center of the predictor space. By assessing the leverage, one can determine the degree to which an observation affects the fitted model, as it highlights data points that are significantly distant from the mean of the explanatory variable. This statistical concept is crucial for identifying potential outliers and understanding their impact on the regression analysis.</data>
      <data key="d2">09391efd3b8c510205098b548bc8dc74,428db872e71a17a2cf7868b03a52def0,629ce6550294d332948e19171a4acd2d,7e05f1b457a496c8b3630e7044fc5981,bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </node>
    <node id="INFLUENCE_INDEX_PLOT">
      <data key="d0">FUNCTION</data>
      <data key="d1">InfluenceIndexPlot is a function from the car package used to plot the leverages (hat-values) for a given model</data>
      <data key="d2">bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </node>
    <node id="CAR_PACKAGE">
      <data key="d0">SOFTWARE</data>
      <data key="d1">The CAR_PACKAGE, also known as "car," is a versatile R package designed for comprehensive statistical analysis. It offers a wide array of functions that are particularly useful for regression diagnostics, enabling users to assess the quality of regression models effectively. Additionally, the package includes functionalities for creating leverage plots, which are graphical representations that help in identifying influential observations in regression analysis. The car package is an essential tool for data scientists and statisticians working with R, providing them with the necessary resources to conduct thorough regression analysis and interpret the results accurately.</data>
      <data key="d2">bd05fe6a05f9a13d33c4f1b5a771ada5,f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </node>
    <node id="MASS_PACKAGE">
      <data key="d0">SOFTWARE</data>
      <data key="d1">The MASS_PACKAGE, also known simply as MASS, is a comprehensive R package designed to offer a wide array of statistical and graphical methodologies. This package is particularly useful for data scientists and statisticians as it supports various modeling techniques. It includes linear and generalized linear models, which are essential for understanding relationships between variables. Additionally, the package facilitates nonlinear regression models, allowing for the analysis of complex data relationships that do not conform to linear patterns. Moreover, MASS_PACKAGE provides tools for classification and clustering, which are crucial for pattern recognition and data segmentation. By leveraging these features, users can gain deeper insights into their data, perform sophisticated analyses, and make informed decisions based on statistical evidence.</data>
      <data key="d2">bd05fe6a05f9a13d33c4f1b5a771ada5,f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </node>
    <node id="FIGURE_8_1">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 8.1 is an index plot of the leverages for the mammals dataset regression model</data>
      <data key="d2">bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </node>
    <node id="REGRESSION">
      <data key="d0">STATISTICAL_METHOD</data>
      <data key="d1">Regression, a fundamental statistical method, is extensively utilized for modeling the relationship between a dependent variable and one or more independent variables. In a specific application, it is employed to analyze the regression of the logarithm of average brain weight on the logarithm of average body weight. This particular use case highlights Regression's capability to explore and quantify the association between two variables, where the dependent variable is the logarithm of average brain weight and the independent variable is the logarithm of average body weight. This method is crucial for understanding how changes in body size (as represented by body weight) might relate to changes in brain size (as represented by brain weight), providing insights into the evolutionary and biological relationships between body and brain size in organisms.</data>
      <data key="d2">428db872e71a17a2cf7868b03a52def0,f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </node>
    <node id="LOGARITHM">
      <data key="d0">MATHEMATICAL_FUNCTION</data>
      <data key="d1">Logarithm is a mathematical function that is used to transform data, often to make it more suitable for analysis. In this context, it is used to transform the average brain weight and average body weight in the regression model.</data>
      <data key="d2">f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </node>
    <node id="INDEX_PLOT">
      <data key="d0">GRAPHICAL_REPRESENTATION</data>
      <data key="d1">An index plot, in the context of statistical analysis, is a graphical representation that serves a dual purpose. Primarily, it displays the values of a variable against their index or row number, providing a visual understanding of the data's structure and potential patterns. In the specific scenario of regression analysis, the index plot is utilized to illustrate the leverages for the model, offering insights into the influence of individual observations on the regression coefficients. Additionally, the index plot can also depict the absolute standardized residuals against the observation index, which is crucial for residual analysis. This feature aids in identifying any systematic patterns in the residuals that might indicate violations of model assumptions, such as homoscedasticity or independence of errors. Through these functionalities, the index plot becomes an indispensable tool for diagnosing and understanding the performance of regression models.</data>
      <data key="d2">428db872e71a17a2cf7868b03a52def0,f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </node>
    <node id="RULE_OF_THUMB_THRESHOLD">
      <data key="d0">THRESHOLD</data>
      <data key="d1">The rule of thumb threshold is a commonly used threshold for identifying influential observations in regression analysis. It is typically set at 2 times the average leverage value.</data>
      <data key="d2">f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </node>
    <node id="EXERCISE_21">
      <data key="d0">EXERCISE</data>
      <data key="d1">Exercise 21 is a problem that asks the reader to show a specific formula for calculating leverages in simple linear regression. It provides a hint to parametrize the model in a certain way to simplify the matrix calculations.</data>
      <data key="d2">f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </node>
    <node id="RI">
      <data key="d0">STANDARDISED_RESIDUAL</data>
      <data key="d1">RI, also denoted as ri, is the standardised residual for the ith observation within a statistical model. It is calculated by dividing the raw residual, symbolised as &#966;bi, by an estimate of its standard deviation. This process is crucial for adjusting the residual to account for the variability in the data. The formula for RI involves dividing &#966;bi by s(1 &#8722; hii)^1/2, where s represents the standard deviation of the residuals, and hii is the leverage statistic for the ith observation, which measures the observation's influence on the fitted values of the model. This standardisation process ensures that the residuals are on a comparable scale, facilitating a more accurate analysis of the model's fit and the data's structure.</data>
      <data key="d2">90b7e0427699cc1bb461e37939935138,98d6982108f2d42fe0437bff8c666e17,c47968226557bc2eb5aec5bb7994fd0e</data>
    </node>
    <node id="EBI">
      <data key="d0">RAW_RESIDUAL</data>
      <data key="d1">Eb_i is the raw residual for the ith observation, the difference between the observed and predicted values</data>
      <data key="d2">90b7e0427699cc1bb461e37939935138</data>
    </node>
    <node id="EPSILON_BI">
      <data key="d0">ERROR_TERM</data>
      <data key="d1">Epsilon bi (&#949;bi) represents the residual for the ith observation in the regression model, which is also identified as the ith error term. This dual characterization highlights its role in measuring the discrepancy between the observed value and the value predicted by the model. In the context of statistical analysis, &#949;bi is a critical component for assessing the accuracy and reliability of the regression model, as it quantifies the unexplained variation in the dependent variable for each data point. Understanding the distribution and behavior of these residuals is essential for diagnosing potential issues with the model, such as non-linearity, heteroscedasticity, or autocorrelation, which can affect the validity of statistical inferences and predictions.</data>
      <data key="d2">0443ab5e20a4f6b2f243c989ef6c723a,c47968226557bc2eb5aec5bb7994fd0e</data>
    </node>
    <node id="DATASET_SIZE">
      <data key="d0">SIZE</data>
      <data key="d1">The dataset size, denoted as "DATASET_SIZE", is a crucial parameter in statistical analysis as it indicates the number of observations present in the dataset under scrutiny. This metric is significant because it directly impacts the threshold for identifying outliers within the data. Specifically, in larger datasets, the threshold for detecting outliers based on standardized residuals tends to be more stringent compared to smaller datasets. This relationship underscores the importance of considering the dataset size when conducting residual analysis, as it influences the sensitivity of the statistical model to anomalous data points.</data>
      <data key="d2">428db872e71a17a2cf7868b03a52def0,c47968226557bc2eb5aec5bb7994fd0e</data>
    </node>
    <node id="THRESHOLD">
      <data key="d0">CRITERION</data>
      <data key="d1">The threshold for identifying outliers is set to 2 for small to moderately sized datasets and 3 for large datasets</data>
      <data key="d2">c47968226557bc2eb5aec5bb7994fd0e</data>
    </node>
    <node id="HUMAN_OBSERVATION">
      <data key="d0">OBSERVATION</data>
      <data key="d1">The "HUMAN_OBSERVATION" in the mammals dataset is a data point that stands out significantly in statistical terms. This observation has a standardised residual larger than 2, which suggests that it deviates substantially from the mean in a way that is unlikely under normal circumstances. Moreover, it is noted to have the largest Cook's distance among all observations, a measure that indicates the observation's strong influence on the regression results. This high influence is further highlighted by its flagging as potentially influential in the "Residuals vs Leverage" plot, a graphical tool used to identify influential cases in regression analysis. Collectively, these statistical indicators suggest that the "HUMAN_OBSERVATION" plays a critical role in the mammals dataset, potentially affecting the overall model fit and parameter estimates.</data>
      <data key="d2">323899f01972255cd3278bccee20d5d8,9e2ebbb113c00fa43f0af3c0696baf95,c47968226557bc2eb5aec5bb7994fd0e</data>
    </node>
    <node id="WATER_OPOSSUM_OBSERVATION">
      <data key="d0">OBSERVATION</data>
      <data key="d1">The WATER_OPOSSUM_OBSERVATION in the mammals dataset is a significant data point that has been identified as potentially influential. This observation stands out due to its large standardised residual, which exceeds the threshold of 2, indicating a substantial deviation from the model's predictions. This peculiarity is further highlighted in the "Residuals vs Leverage" plot, where the WATER_OPOSSUM_OBSERVATION is flagged, suggesting that it may have a considerable impact on the regression model's coefficients. This observation's distinct characteristics warrant careful examination to understand its influence on the overall statistical model and the community of interest within the context of algorithmic analysis.</data>
      <data key="d2">9e2ebbb113c00fa43f0af3c0696baf95,c47968226557bc2eb5aec5bb7994fd0e</data>
    </node>
    <node id="RHESUS_MONKEY_OBSERVATION">
      <data key="d0">OBSERVATION</data>
      <data key="d1">Rhesus monkey observation in the mammals dataset has a standardised residual larger than 2</data>
      <data key="d2">c47968226557bc2eb5aec5bb7994fd0e</data>
    </node>
    <node id="RHESUS_MONKEY">
      <data key="d0">ANIMAL</data>
      <data key="d1">Rhesus monkey is a species of animal that may be included in the mammals dataset for analysis</data>
      <data key="d2">428db872e71a17a2cf7868b03a52def0</data>
    </node>
    <node id="MODEL_FIT">
      <data key="d0">STATISTICAL_MODEL</data>
      <data key="d1">Model fit is the measure of how well a statistical model fits the data</data>
      <data key="d2">428db872e71a17a2cf7868b03a52def0</data>
    </node>
    <node id="LOG_AVERAGE_BRAIN_WEIGHT">
      <data key="d0">VARIABLE</data>
      <data key="d1">The entity "LOG_AVERAGE_BRAIN_WEIGHT" is a critical variable utilized in the context of regression analysis. It specifically denotes the logarithmic transformation of the average brain weight of mammals. Serving as the dependent variable in the regression model, this metric is pivotal for understanding and quantifying the relationships between brain size and other factors within the mammalian community of interest. The logarithmic transformation is applied to normalize the distribution of brain weights and to linearize potential relationships, thereby facilitating a more accurate statistical inference.</data>
      <data key="d2">323899f01972255cd3278bccee20d5d8,428db872e71a17a2cf7868b03a52def0</data>
    </node>
    <node id="LOG_AVERAGE_BODY_WEIGHT">
      <data key="d0">VARIABLE</data>
      <data key="d1">The entity "LOG_AVERAGE_BODY_WEIGHT" is a crucial variable incorporated in the regression analysis. It signifies the logarithmic transformation of the average body weight of mammals, serving as an independent variable in the statistical model. This transformation is often applied to normalize the distribution of body weights and to linearize relationships between body weight and other variables, thereby facilitating a more accurate and interpretable regression analysis. The use of the logarithm of average body weight as an independent variable allows for a more nuanced understanding of how changes in body size might influence other biological or ecological outcomes in the mammalian community under study.</data>
      <data key="d2">323899f01972255cd3278bccee20d5d8,428db872e71a17a2cf7868b03a52def0</data>
    </node>
    <node id="INFLUENTIAL_OBSERVATION">
      <data key="d0">DATA_POINT</data>
      <data key="d1">An influential observation is a data point whose removal causes a large change to the fitted model</data>
      <data key="d2">428db872e71a17a2cf7868b03a52def0</data>
    </node>
    <node id="RESIDUALS_VS_LEVERAGE_PLOT">
      <data key="d0">PLOT_TYPE</data>
      <data key="d1">The Residuals vs Leverage plot is a critical diagnostic tool in regression analysis, designed to pinpoint influential observations that may significantly impact the model's outcomes. This graphical representation displays the residuals, which are the differences between the observed and predicted values, plotted against the leverage values of each data point. Leverage values measure the potential influence of an observation on the regression line, with higher values indicating greater influence. By examining this plot, data scientists can identify outliers and influential points that may require special attention or adjustment to ensure the robustness and reliability of the regression model.</data>
      <data key="d2">428db872e71a17a2cf7868b03a52def0,9e2ebbb113c00fa43f0af3c0696baf95</data>
    </node>
    <node id="WATER_OPOSSUM">
      <data key="d0">ANIMAL</data>
      <data key="d1">Water opossum is a species of animal that is flagged as an influential observation in the mammals dataset</data>
      <data key="d2">428db872e71a17a2cf7868b03a52def0</data>
    </node>
    <node id="MUSK_SHREW">
      <data key="d0">ANIMAL</data>
      <data key="d1">Musk shrew is a species of animal that is flagged as an influential observation in the mammals dataset</data>
      <data key="d2">428db872e71a17a2cf7868b03a52def0</data>
    </node>
    <node id="MUSK_SHREW_OBSERVATION">
      <data key="d0">DATA_POINT</data>
      <data key="d1">The observation for musk shrew is a data point in the mammals dataset that is flagged as potentially influential in the "Residuals vs Leverage" plot.&gt;</data>
      <data key="d2">9e2ebbb113c00fa43f0af3c0696baf95</data>
    </node>
    <node id="BETA_HAT_I">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">BETA_HAT_I, denoted as \u03b2b(i), is the vector of least squares estimators for the parameters calculated from the dataset with the ith observation removed. This statistical measure provides insights into the influence of the ith observation on the estimated parameters of the model. By removing the ith observation, BETA_HAT_I allows for the assessment of the stability and reliability of the parameter estimates, contributing to a deeper understanding of the model's sensitivity to individual data points. This process is crucial in residual analysis and model diagnostics, aiding in the identification of potential outliers or influential points that may skew the overall model fit.</data>
      <data key="d2">0443ab5e20a4f6b2f243c989ef6c723a,9e2ebbb113c00fa43f0af3c0696baf95</data>
    </node>
    <node id="DI">
      <data key="d0">DIAGNOSTIC</data>
      <data key="d1">DI, also known as Cook's distance for the ith data point or observation, is a critical diagnostic measure in regression analysis. It quantifies the influence of individual data points on the fitted model, serving as an indicator for identifying influential data points that may significantly affect the regression results. DI is calculated as (yb(i) - yb)^T(yb(i) - yb)/ps^2 or (\u03b2b(i) - \u03b2b)TXTX(\u03b2b(i) - \u03b2b)/ps^2, where yb(i) and \u03b2b(i) represent the predicted values and regression coefficients obtained by omitting the ith data point, yb and \u03b2b are the overall predicted values and regression coefficients, ps^2 is the estimated variance of the error term, and TX is the transpose of the design matrix. This measure helps in assessing the stability of the regression model by highlighting data points that, if removed, would lead to substantial changes in the model's parameters.</data>
      <data key="d2">0443ab5e20a4f6b2f243c989ef6c723a,323899f01972255cd3278bccee20d5d8,98d6982108f2d42fe0437bff8c666e17</data>
    </node>
    <node id="TXTX">
      <data key="d0">MATRIX</data>
      <data key="d1">TXTX is the matrix product of the transpose of X (XT) and X, used in the calculation of the Cook's distance</data>
      <data key="d2">0443ab5e20a4f6b2f243c989ef6c723a</data>
    </node>
    <node id="PS_SQUARED">
      <data key="d0">VARIANCE</data>
      <data key="d1">Ps^2 is the estimated variance used in the calculation of the Cook's distance</data>
      <data key="d2">0443ab5e20a4f6b2f243c989ef6c723a</data>
    </node>
    <node id="R_SQUARED_I">
      <data key="d0">RESIDUAL</data>
      <data key="d1">Ri^2 is the squared standardised residual for the ith observation</data>
      <data key="d2">0443ab5e20a4f6b2f243c989ef6c723a</data>
    </node>
    <node id="BI">
      <data key="d0">SUBSCRIPT</data>
      <data key="d1">bi is a subscript used in the calculation of the squared error term</data>
      <data key="d2">98d6982108f2d42fe0437bff8c666e17</data>
    </node>
    <node id="PS2">
      <data key="d0">VARIABLE</data>
      <data key="d1">ps2 is a variable used in the calculation of the squared error term</data>
      <data key="d2">98d6982108f2d42fe0437bff8c666e17</data>
    </node>
    <node id="HI">
      <data key="d0">SUBSCRIPT</data>
      <data key="d1">hi is a subscript used in the calculation of the leverage and influence terms</data>
      <data key="d2">98d6982108f2d42fe0437bff8c666e17</data>
    </node>
    <node id="F_DISTRIBUTION">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">The F-distribution, denoted as Fp,n-p(0.5), is a statistical distribution that plays a pivotal role in assessing the influence of data points within a dataset. Specifically, it is utilized to ascertain whether a particular data point is influential based on its Cook's distance. The F-distribution with p and n-p degrees of freedom serves as a threshold for Cook's distance, providing a critical benchmark for identifying influential observations. This distribution is essential in regression analysis, where it helps in diagnosing the presence of outliers or influential cases that may disproportionately affect the regression model's parameters and predictions.</data>
      <data key="d2">09391efd3b8c510205098b548bc8dc74,98d6982108f2d42fe0437bff8c666e17</data>
    </node>
    <node id="FIGURE_8_4">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 8.4, also referred to as FIGURE_8_4, presents an index plot of the Cook's distances. This plot is specifically associated with a regression analysis where the dependent variable is the logarithm of average brain weight, and the independent variable is the logarithm of average body weight for a dataset comprising mammals. The Cook's distances, as depicted in the figure, are utilized to identify influential observations that have a significant impact on the regression model's coefficients. This graphical representation aids in understanding the leverage and influence of each data point within the mammals dataset, thereby facilitating a deeper insight into the relationship between body size and brain size among mammals.</data>
      <data key="d2">323899f01972255cd3278bccee20d5d8,98d6982108f2d42fe0437bff8c666e17</data>
    </node>
    <node id="FP_N_P">
      <data key="d0">CRITICAL_VALUE</data>
      <data key="d1">Fp,n-p(0.5) is the critical value from the F distribution used to flag influential observations</data>
      <data key="d2">323899f01972255cd3278bccee20d5d8</data>
    </node>
    <node id="STATISTICS">
      <data key="d0">DISCIPLINE</data>
      <data key="d1">Statistics is the field of study that deals with the collection, analysis, interpretation, presentation, and organization of data</data>
      <data key="d2">83bb91cf725e5116ca2f5748fddccfae</data>
    </node>
    <node id="DATA_ENTRY_ERROR">
      <data key="d0">ERROR</data>
      <data key="d1">A data entry error is a mistake made during the input of data, which can result in an influential data point</data>
      <data key="d2">83bb91cf725e5116ca2f5748fddccfae</data>
    </node>
    <node id="ANOTHER_POPULATION">
      <data key="d0">POPULATION</data>
      <data key="d1">Another population refers to a group of observations that are not part of the primary population of interest, and may be treated as outliers</data>
      <data key="d2">83bb91cf725e5116ca2f5748fddccfae</data>
    </node>
    <node id="COMPLETE_DATASET">
      <data key="d0">DATASET</data>
      <data key="d1">The complete dataset is the full set of observations used in the analysis, including all data points</data>
      <data key="d2">83bb91cf725e5116ca2f5748fddccfae</data>
    </node>
    <node id="ELEPHANT_SPECIES">
      <data key="d0">SPECIES</data>
      <data key="d1">Elephant species are a type of mammal that can be highly influential in the mammals dataset when using original variables</data>
      <data key="d2">83bb91cf725e5116ca2f5748fddccfae</data>
    </node>
    <node id="ROBUST_REGRESSION">
      <data key="d0">METHOD</data>
      <data key="d1">Robust regression, an alternative approach to traditional regression analysis, is a method specifically designed to handle outliers and leverage points more effectively. This statistical technique provides a systematic approach to down-weighting influential data points when fitting a model, ensuring that the analysis is not unduly affected by these points. Unlike the methods discussed in the module, robust regression offers a more resilient way to deal with influential observations, making it a valuable tool in the data scientist's arsenal for achieving more reliable and accurate model fitting.</data>
      <data key="d2">09391efd3b8c510205098b548bc8dc74,629ce6550294d332948e19171a4acd2d,7e05f1b457a496c8b3630e7044fc5981,83bb91cf725e5116ca2f5748fddccfae</data>
    </node>
    <node id="YBI">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">Ybi is the estimated value of the response variable for a given observation, calculated using the fitted model</data>
      <data key="d2">83bb91cf725e5116ca2f5748fddccfae</data>
    </node>
    <node id="STANDARDISED_RESIDUAL">
      <data key="d0">METRIC</data>
      <data key="d1">The entity in focus is the "STANDARDISED_RESIDUAL". A standardised residual is a statistical measure that represents the difference between the observed value and the predicted value in a statistical model, scaled by the standard deviation of the residuals. This scaling process standardizes the residuals, making them comparable across different data points. Specifically, the ith standardised or internally studentised residual (ri) is calculated as \u03f5bi divided by s(1 - hii)^1/2, where \u03f5bi is the residual for the ith data point, s^2 is an unbiased estimate of the error variance \u03c3^2, and hii is the leverage value for the ith data point. This formula takes into account the leverage of each data point, which is a measure of how far away the point is from the center of the data. The standardised residual is a crucial tool in residual analysis, aiding in the identification of outliers and the assessment of model fit in regression analysis.</data>
      <data key="d2">09391efd3b8c510205098b548bc8dc74,7e05f1b457a496c8b3630e7044fc5981</data>
    </node>
    <node id="INFLUENTIAL_DATA_POINT">
      <data key="d0">DATA_POINT</data>
      <data key="d1">An influential data point, in the context of statistical analysis, is a specific observation within a dataset that exerts a substantial influence on the regression model. This impact is manifested in its ability to notably alter the characteristics of the regression line, including the slope and intercept, thereby significantly affecting the model's predictions and interpretations. Such data points are critical in algorithmic analysis, particularly when assessing the robustness and reliability of linear regression models. Identifying and understanding influential data points is essential for ensuring the accuracy and validity of statistical inferences drawn from the data.</data>
      <data key="d2">09391efd3b8c510205098b548bc8dc74,629ce6550294d332948e19171a4acd2d</data>
    </node>
    <node id="REGRESSION_OUTLIER">
      <data key="d0">DATA_POINT</data>
      <data key="d1">In the context of statistical analysis, particularly within the realm of regression analysis, a regression outlier, denoted as "REGRESSION_OUTLIER," is characterized as an observation that exhibits a significantly large residual. This means that the observation is notably distant from the regression line in the vertical direction, indicating a substantial discrepancy between the observed value and the value predicted by the regression model. Such outliers can potentially influence the slope of the regression line and the overall fit of the model, thereby affecting the accuracy of predictions and the reliability of statistical inferences drawn from the data. Identifying and understanding regression outliers is crucial for ensuring the robustness and validity of regression analysis.</data>
      <data key="d2">09391efd3b8c510205098b548bc8dc74,629ce6550294d332948e19171a4acd2d</data>
    </node>
    <node id="HIGH_LEVERAGE_DATA_POINT">
      <data key="d0">DATA_POINT</data>
      <data key="d1">A high leverage data point is an observation that has extreme values in the predictor variables, potentially having a large influence on the regression line</data>
      <data key="d2">629ce6550294d332948e19171a4acd2d</data>
    </node>
    <node id="PLOTS">
      <data key="d0">VISUALIZATION</data>
      <data key="d1">Various plots, such as scatter plots, residual plots, and influence plots, can be used to illustrate leverage, standardized residuals, and Cook's distance</data>
      <data key="d2">629ce6550294d332948e19171a4acd2d</data>
    </node>
    <node id="RULES_OF_THUMB">
      <data key="d0">GUIDELINES</data>
      <data key="d1">Rules of thumb are guidelines for interpreting the values of leverage, standardized residuals, and Cook's distance to identify influential data points</data>
      <data key="d2">629ce6550294d332948e19171a4acd2d</data>
    </node>
    <node id="HANDLING_INFLUENTIAL_DATA_POINTS">
      <data key="d0">DATA_HANDLING</data>
      <data key="d1">Options on how to handle influential data points include removing them, transforming the data, or using robust regression methods</data>
      <data key="d2">629ce6550294d332948e19171a4acd2d</data>
    </node>
    <node id="GENERALIZED_LINEAR_MODELS_BOOK">
      <data key="d0">REFERENCE_MATERIAL</data>
      <data key="d1">Generalized linear models with examples in R (2018) by Dunn and Smyth is a reference book that covers related material on robust regression and influential data points</data>
      <data key="d2">629ce6550294d332948e19171a4acd2d</data>
    </node>
    <node id="DATA_ANALYSIS_AND_GRAPHICS_BOOK">
      <data key="d0">REFERENCE_MATERIAL</data>
      <data key="d1">Data analysis and graphics using R: an example-based approach (2010) by Maindonald and Braun is a reference book that covers related material on robust regression and influential data points</data>
      <data key="d2">629ce6550294d332948e19171a4acd2d</data>
    </node>
    <node id="CATEGORICAL_PREDICTOR_VARIABLES">
      <data key="d0">PREDICTOR_VARIABLE</data>
      <data key="d1">Categorical predictor variables, also known as factors, are qualitative explanatory variables that can take on a limited number of discrete values or levels</data>
      <data key="d2">629ce6550294d332948e19171a4acd2d</data>
    </node>
    <node id="GENDER">
      <data key="d0">CATEGORICAL_VARIABLE</data>
      <data key="d1">Gender serves as a significant categorical predictor variable in statistical models, offering qualitative explanatory power by distinguishing among categories such as male, female, and other. This variable is crucial for understanding and predicting outcomes within the context of algorithmic analysis, enabling nuanced insights into the relations and structure of the community under study. By incorporating gender as a predictor, data scientists can enhance the accuracy and relevance of their models, ensuring that the unique characteristics of different gender identities are adequately represented and considered in their analyses. Utilizing tools like R for data analysis, experts can leverage their understanding of statistical inference to interpret the impact of gender on various outcomes, contributing to a more comprehensive and inclusive understanding of the data.</data>
      <data key="d2">39aef0392258a09378ce45d8b03a268a,629ce6550294d332948e19171a4acd2d</data>
    </node>
    <node id="DEGREE_COURSE">
      <data key="d0">CATEGORICAL_VARIABLE</data>
      <data key="d1">The Degree Course is a categorical predictor variable, also recognized as a qualitative explanatory variable, which plays a significant role in statistical modeling. It can assume various values, representing different fields of study, such as Data Science, MathStat (Mathematics and Statistics), and MORSE (Mathematics, Operational Research, Statistics, and Economics). These categories are not numerical but qualitative, indicating different types of degree courses that can be included as predictors in various statistical models. The inclusion of such a variable allows for the analysis of the effect of different degree courses on the response variable, providing insights into how the field of study might influence outcomes in a given context.</data>
      <data key="d2">39aef0392258a09378ce45d8b03a268a,629ce6550294d332948e19171a4acd2d</data>
    </node>
    <node id="LEVELS">
      <data key="d0">CATEGORICAL_VARIABLE_ATTRIBUTE</data>
      <data key="d1">Levels are the values that a categorical predictor variable can take, such as Data Science, MathStat, and MORSE for the degree course variable</data>
      <data key="d2">629ce6550294d332948e19171a4acd2d</data>
    </node>
    <node id="QUANTITATIVE_PREDICTOR">
      <data key="d0">VARIABLE</data>
      <data key="d1">Quantitative predictor variables are numerical variables used in statistical models to predict outcomes</data>
      <data key="d2">39aef0392258a09378ce45d8b03a268a</data>
    </node>
    <node id="QUALITATIVE_PREDICTOR">
      <data key="d0">VARIABLE</data>
      <data key="d1">Qualitative predictor variables, also known as categorical predictor variables or factors, are non-numerical variables used in statistical models to predict outcomes</data>
      <data key="d2">39aef0392258a09378ce45d8b03a268a</data>
    </node>
    <node id="RETAIL_DATASET">
      <data key="d0">DATASET</data>
      <data key="d1">The RETAIL_DATASET is a comprehensive collection of data that encompasses detailed information about sales volumes, brands, and other significant variables associated with retail stores. This dataset serves as a valuable resource for understanding the performance and characteristics of various retail establishments, providing insights into sales trends, brand popularity, and other key aspects of the retail industry.</data>
      <data key="d2">39aef0392258a09378ce45d8b03a268a,ab898d123f48e380384aa01e035a83ca</data>
    </node>
    <node id="AVERAGE_RESPONSE">
      <data key="d0">STATISTICAL_MEASURE</data>
      <data key="d1">The average response is a statistical measure that can be modeled to be dependent on the value that a categorical predictor takes</data>
      <data key="d2">39aef0392258a09378ce45d8b03a268a</data>
    </node>
    <node id="COMPARATIVE_BOXPLOTS">
      <data key="d0">VISUALIZATION</data>
      <data key="d1">Comparative boxplots are a valuable visualization tool in statistical analysis, specifically utilized to illustrate the relationship between a categorical variable and a quantitative variable. This graphical representation allows for a clear comparison of how different categories of the categorical variable affect the distribution of the quantitative variable. For instance, in a business context, comparative boxplots can be used to compare sales volumes across different brands, providing insights into the variability and central tendency of sales for each brand. This type of plot is particularly useful in identifying outliers, understanding the spread of the data, and comparing the median and quartiles of the quantitative variable across various categories of the categorical variable.</data>
      <data key="d2">39aef0392258a09378ce45d8b03a268a,ab898d123f48e380384aa01e035a83ca</data>
    </node>
    <node id="BOXPLOT_COMMAND">
      <data key="d0">FUNCTION</data>
      <data key="d1">The boxplot command is a function used to produce comparative boxplots of a quantitative variable grouped by a categorical predictor variable</data>
      <data key="d2">39aef0392258a09378ce45d8b03a268a</data>
    </node>
    <node id="SALES_VOLUMES">
      <data key="d0">QUANTITATIVE_VARIABLE</data>
      <data key="d1">Sales volumes are the quantitative data points representing the sales of stores from different brands</data>
      <data key="d2">ab898d123f48e380384aa01e035a83ca</data>
    </node>
    <node id="BRAND_C">
      <data key="d0">CATEGORICAL_VARIABLE</data>
      <data key="d1">Brand C is a significant entity in the retail data analysis, representing a specific brand category of stores being examined in the model. It is distinct from Brand A and Brand B, serving as the reference category in the model equations. Brand C's sales model is characterized by an estimated sales formula: sales = 162.879 - 0.231 * pricei, applicable when the store is of Brand C. This brand is noted for having a less steep gradient for the fitted regression line compared to its counterparts, indicating a potentially different sensitivity to price changes.

Brand C boasts an average sales volume of 126,680 units, significantly higher than the other two brands, suggesting that it enjoys a higher sales volume, possibly influenced by factors beyond price. Observations for Brand C in the dataset tend to lie above the fitted regression line, reinforcing the notion that its sales volume is impacted by additional variables. The brand is represented by the indicator variable xjC in the model, which is 1 if store j is of Brand C and 0 otherwise, facilitating the comparison of sales volumes with Brand A.

In the context of the regression model, Brand C has its own coefficient for the intercept and interaction with price, highlighting its unique contribution to the sales prediction. It is one of the levels in the categorical predictor used in the linear model for the Retail data, and its sales volumes are compared to Brand A, providing insights into the relative performance of the brands. As one of the three store brands considered in the study, Brand C's analysis is crucial for understanding the retail market dynamics and the factors influencing sales volumes across different brands.</data>
      <data key="d2">06199787dd7f75f7338dd24d4f3dc26e,096afa471635bc59c3bfa9af4d04d625,0cb40986e6c2bb439e1ffcaae2df96ac,1820d10ee0f23f34b3ea88ba475bc52d,1b523d1edabe381403fc470a9b8d47fa,1d141ab04db553f78a313e430e54abb5,228bdca7843406def245d755e8df49f6,3dd24a54028976ba54304ec7169bb74b,48971100deb5bb374a41c1f2b7b2a86a,7037e0369bfdaad5a730cabb2b44831c,7a605c3b689bec7ab2c46df9c123e3f3,825b600cbab3535ce67e9f561ddcb84b,8326c645426789920a99ed373725fa0e,86ece4718d27d1a6c6a1f448cc850e2b,906eb7d6b49fa360e7e5b65c56cd4d76,9854704301b8df256ca1013b8d53dfac,a1fc936df848a0fbc791e4bcc9b527b6,ab898d123f48e380384aa01e035a83ca,ac15b639b0849006471dfe102376c2c0,adbc52b340a69a8633c919c4fd2cd3f6,baa0dc3d4ec0e51c0a321e5579caf8aa,c103c6d096d52868eda26d991194b5f2,e079b7c92d5c0b009ff02040eb652bc6</data>
    </node>
    <node id="BRAND_A">
      <data key="d0">CATEGORICAL_VARIABLE</data>
      <data key="d1">Brand A is a prominent retail brand being analyzed in a comprehensive sales and marketing study. It is one of three store brands considered in the model, serving as the baseline or reference category in the categorical predictor used in the linear regression analysis for the Retail dataset. The brand's stores follow a specific sales model where sales are estimated as 210.379 - 0.657 * pricei, showcasing a steeper gradient for the fitted regression line than Brand C. Brand A's average sales volume is 109,679 units, with observations for this brand tending to lie below the fitted regression line, indicating that factors other than price significantly influence its sales volume. The CEO of Brand A oversees the stores of this brand, which are part of the retail data being analyzed. Brand A is represented by the indicator variable xjA in the model, calculated as 1 - xjB - xjC, highlighting its role as one of the categories of the brand variable. The brand's stores are characterized by a specific brand, making Brand A one of the groups defined by the brand variable in the regression model. Overall, Brand A is a key entity in the retail analysis, used as a reference for comparison against other brands in terms of sales volumes and marketing strategies.</data>
      <data key="d2">06199787dd7f75f7338dd24d4f3dc26e,0cb40986e6c2bb439e1ffcaae2df96ac,1820d10ee0f23f34b3ea88ba475bc52d,1b523d1edabe381403fc470a9b8d47fa,1d141ab04db553f78a313e430e54abb5,228bdca7843406def245d755e8df49f6,3dd24a54028976ba54304ec7169bb74b,48971100deb5bb374a41c1f2b7b2a86a,7037e0369bfdaad5a730cabb2b44831c,7a605c3b689bec7ab2c46df9c123e3f3,8326c645426789920a99ed373725fa0e,906eb7d6b49fa360e7e5b65c56cd4d76,9854704301b8df256ca1013b8d53dfac,a1fc936df848a0fbc791e4bcc9b527b6,ab898d123f48e380384aa01e035a83ca,ac15b639b0849006471dfe102376c2c0,adbc52b340a69a8633c919c4fd2cd3f6,baa0dc3d4ec0e51c0a321e5579caf8aa,c103c6d096d52868eda26d991194b5f2,e079b7c92d5c0b009ff02040eb652bc6</data>
    </node>
    <node id="BRAND_B">
      <data key="d0">CATEGORICAL_VARIABLE</data>
      <data key="d1">Brand B is one of the three distinct store brands being analyzed in the retail sales data, alongside Brand A and Brand C. It is characterized by a specific sales model where sales are estimated as 225.252 - 0.803 * pricei if the store is of Brand B, indicating a steeper gradient for the fitted regression line than Brand C. Brand B stores have an average sales volume of 105,728 units, with observations tending to lie below the fitted regression line in the initial model, suggesting that sales volume is influenced by factors other than price. In the context of the regression analysis, Brand B is represented by the indicator variable xjB, which is 1 if store j is of Brand B and 0 otherwise. This brand is also one of the levels in the categorical predictor used in the linear model for the Retail data, and its sales volumes are compared to Brand A. The analysis reveals that Brand B has its own coefficient for the intercept and interaction with price, highlighting its unique role in the sales dynamics of the retail dataset.</data>
      <data key="d2">06199787dd7f75f7338dd24d4f3dc26e,0cb40986e6c2bb439e1ffcaae2df96ac,1820d10ee0f23f34b3ea88ba475bc52d,1b523d1edabe381403fc470a9b8d47fa,1d141ab04db553f78a313e430e54abb5,228bdca7843406def245d755e8df49f6,3dd24a54028976ba54304ec7169bb74b,48971100deb5bb374a41c1f2b7b2a86a,7037e0369bfdaad5a730cabb2b44831c,7a605c3b689bec7ab2c46df9c123e3f3,825b600cbab3535ce67e9f561ddcb84b,8326c645426789920a99ed373725fa0e,906eb7d6b49fa360e7e5b65c56cd4d76,9854704301b8df256ca1013b8d53dfac,a1fc936df848a0fbc791e4bcc9b527b6,ab898d123f48e380384aa01e035a83ca,ac15b639b0849006471dfe102376c2c0,adbc52b340a69a8633c919c4fd2cd3f6,baa0dc3d4ec0e51c0a321e5579caf8aa,c103c6d096d52868eda26d991194b5f2,e079b7c92d5c0b009ff02040eb652bc6</data>
    </node>
    <node id="REGRESSION_MODELS">
      <data key="d0">STATISTICAL_MODEL</data>
      <data key="d1">Regression models, under the entity "REGRESSION_MODELS," are sophisticated statistical tools employed to forecast the mean of a dependent variable, typically a quantitative measure like sales volumes, by leveraging one or more independent variables. These models are versatile, capable of incorporating both continuous and categorical predictor variables, such as brand, to provide insights into the relationships and predictive patterns within the data. By analyzing these relationships, regression models enable a deeper understanding of how changes in the independent variables can influence the dependent variable, making them invaluable in various fields including business, economics, and social sciences.</data>
      <data key="d2">ab898d123f48e380384aa01e035a83ca,e079b7c92d5c0b009ff02040eb652bc6</data>
    </node>
    <node id="MEDIAN">
      <data key="d0">STATISTICAL_MEASURE</data>
      <data key="d1">The median is a statistical measure shown as a horizontal line in data visualization.</data>
      <data key="d2">e079b7c92d5c0b009ff02040eb652bc6</data>
    </node>
    <node id="SALES_VOLUME">
      <data key="d0">DEPENDENT_VARIABLE</data>
      <data key="d1">Sales volume, denoted as "SALES_VOLUME", is a critical variable in the context of the linear regression model being analyzed. It serves as the dependent variable, representing the total quantity of units sold for a specific brand, notably Brand C. The sales volume is influenced by two primary factors: price and brand, which are used as predictors in the model. This variable encapsulates the response to changes in these predictors, providing insights into how alterations in price and brand affect the quantity of products sold. The model aims to predict sales volume based on these variables, offering a quantitative assessment of sales performance for Brand C.</data>
      <data key="d2">06199787dd7f75f7338dd24d4f3dc26e,096afa471635bc59c3bfa9af4d04d625,7a605c3b689bec7ab2c46df9c123e3f3,b0ca3e6c22c4cf884d03b1f6f82be5df,b6870535f3975c49d45e62fbe475f198,baa0dc3d4ec0e51c0a321e5579caf8aa,e079b7c92d5c0b009ff02040eb652bc6</data>
    </node>
    <node id="STATISTICAL_MODEL">
      <data key="d0">MODEL_TYPE</data>
      <data key="d1">A statistical model is a mathematical representation of the relationships between variables, used here to predict sales volume.</data>
      <data key="d2">e079b7c92d5c0b009ff02040eb652bc6</data>
    </node>
    <node id="FIGURE_9.2">
      <data key="d0">DATA_VISUALIZATION</data>
      <data key="d1">Figure 9.2 is a scatterplot of sales against price, with the fitted regression line and color-coded data points according to brand.</data>
      <data key="d2">e079b7c92d5c0b009ff02040eb652bc6</data>
    </node>
    <node id="DATAPOINTS">
      <data key="d0">DATA_POINTS</data>
      <data key="d1">Datapoints are individual pieces of data plotted on the scatterplot, with different plotting symbols and color-coding according to brand.</data>
      <data key="d2">e079b7c92d5c0b009ff02040eb652bc6</data>
    </node>
    <node id="MU_A">
      <data key="d0">PARAMETER</data>
      <data key="d1">MU_A (&#956;A) is a pivotal parameter in the linear regression model associated with Brand A. It serves as the intercept parameter, indicating the expected sales volume for Brand A's stores when the price is zero. This parameter encapsulates the baseline sales performance of Brand A, providing insights into the brand's market strength in the absence of pricing effects. MU_A's role in the model highlights its significance in understanding the structural relationship between sales volume and pricing strategies for Brand A.</data>
      <data key="d2">1b523d1edabe381403fc470a9b8d47fa,48971100deb5bb374a41c1f2b7b2a86a,7a605c3b689bec7ab2c46df9c123e3f3,aeddef300427d211c74c6008b5b6b328</data>
    </node>
    <node id="MU_B">
      <data key="d0">PARAMETER</data>
      <data key="d1">MU_B (&#956;B) is a significant parameter in the linear regression model, specifically serving as the intercept coefficient for Brand B. This parameter encapsulates the expected sales volume for Brand B when the price is zero, highlighting its role in predicting the baseline sales performance of Brand B stores. MU_B's value is crucial for understanding the inherent sales potential of Brand B in the market, independent of pricing strategies. Contrary to an initial mention associating MU_B with Brand A, the prevailing context and descriptions consistently link MU_B to Brand B, thus establishing its role as a key indicator for Brand B's sales dynamics within the linear regression model.</data>
      <data key="d2">1b523d1edabe381403fc470a9b8d47fa,48971100deb5bb374a41c1f2b7b2a86a,7a605c3b689bec7ab2c46df9c123e3f3,adbc52b340a69a8633c919c4fd2cd3f6,aeddef300427d211c74c6008b5b6b328</data>
    </node>
    <node id="MU_C">
      <data key="d0">PARAMETER</data>
      <data key="d1">MU_C (&#956;C) is a pivotal parameter in the linear regression model associated with Brand C. It serves as the intercept parameter, indicating the expected sales volume for Brand C when the price is zero. This parameter is specifically tailored for stores of Brand C, encapsulating the baseline sales performance under the assumption that price does not influence sales. MU_C (&#956;C) thus provides a foundational measure of Brand C's sales potential in the absence of pricing effects, making it a critical component in understanding the structural relationship between price and sales volume for Brand C.</data>
      <data key="d2">1b523d1edabe381403fc470a9b8d47fa,48971100deb5bb374a41c1f2b7b2a86a,7a605c3b689bec7ab2c46df9c123e3f3,aeddef300427d211c74c6008b5b6b328</data>
    </node>
    <node id="BRAND_MODEL1">
      <data key="d0">MODEL</data>
      <data key="d1">BRAND_MODEL1, also referred to as Brand.model1, is a linear statistical model implemented using the R programming language. This model employs the lm function to analyze sales data, with sales serving as the dependent variable. The independent variables in this model include brand and price, indicating that the analysis aims to understand the impact of these factors on sales. The model follows a parallel lines structure, suggesting that the relationship between the independent variables and sales is assumed to be linear and consistent across different levels of the categorical variable, brand. This comprehensive model allows for the examination of how brand and price independently and jointly influence sales, providing insights into market dynamics and consumer behavior.</data>
      <data key="d2">8326c645426789920a99ed373725fa0e,ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </node>
    <node id="MUA">
      <data key="d0">PARAMETER</data>
      <data key="d1">MuA, a critical parameter in the statistical model, serves a dual role in the context of Brand A. Primarily, it acts as the intercept, establishing a baseline value for Brand A in the model. Additionally, MuA quantifies the effect of Brand A on sales, highlighting its significance in sales analysis. This parameter is specifically utilized in the parallel lines model, where it helps in understanding the baseline sales performance of Brand A, making it a pivotal component for insights into sales dynamics and brand impact.</data>
      <data key="d2">228bdca7843406def245d755e8df49f6,925e17c26fb7d979f52538f4632333e7,ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </node>
    <node id="MUB">
      <data key="d0">PARAMETER</data>
      <data key="d1">MuB, often represented as \u00b5b, is a significant parameter in the statistical model, specifically in the context of sales analysis. It serves as the estimated intercept for Brand B, highlighting its foundational role in the regression model. MuB encapsulates the base effect of Brand B on sales, providing insights into the sales performance of Brand B in the absence of other influencing factors. This parameter is pivotal in the parallel lines model, where it denotes the intercept specific to Brand B, offering a clear starting point for understanding the brand's sales dynamics within the model.</data>
      <data key="d2">228bdca7843406def245d755e8df49f6,825b600cbab3535ce67e9f561ddcb84b,925e17c26fb7d979f52538f4632333e7,ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </node>
    <node id="MUC">
      <data key="d0">PARAMETER</data>
      <data key="d1">In the context of the statistical model being analyzed, MUC, also denoted as muC, serves a dual role. Primarily, it acts as the intercept parameter for Brand C, indicating the baseline sales level when all other variables are held constant. This intercept is specific to Brand C within the parallel lines model, suggesting that the parameter is pivotal in understanding the starting point of sales for this brand. Additionally, muC also represents the effect of Brand C on sales, encapsulating the unique contribution of Brand C to the overall sales figures, beyond the mere baseline level. This dual interpretation of MUC underscores its significance in the model, as it not only sets the initial sales expectation for Brand C but also quantifies the brand's specific impact on sales performance.</data>
      <data key="d2">228bdca7843406def245d755e8df49f6,925e17c26fb7d979f52538f4632333e7,ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </node>
    <node id="FIGURE_9_3">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 9.3, referred to as "FIGURE_9_3," is a comprehensive scatterplot that visually represents the relationship between sales and price, specifically highlighting the impact of brand on this relationship. The plot features points that are distinctly coded by brand, providing a clear differentiation among brands. It showcases the three brand-specific lines of best fit derived from the parallel lines model, which are fitted regression lines tailored to each brand. This graphical representation allows for a detailed analysis of how sales are influenced by price across different brands, offering insights into brand-specific trends and the overall market dynamics.</data>
      <data key="d2">b2c33cb151a8e7724ebfb7b2d88bc45f,ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </node>
    <node id="FIGURE_9_2">
      <data key="d0">SCATTERPLOT</data>
      <data key="d1">Figure 9.2 is a scatterplot that revealed a systematic pattern of sales and price, with most Brand A and B data points below the regression line and Brand C above</data>
      <data key="d2">b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </node>
    <node id="PARALLEL_LINES_MODEL">
      <data key="d0">REGRESSION_MODEL</data>
      <data key="d1">The PARALLEL_LINES_MODEL is a sophisticated regression model designed to analyze data across multiple groups or categories, particularly focusing on different brands in this context. This model assumes that the regression lines for each brand are parallel, meaning the slopes of these lines are identical across all categories. This parallelism indicates that the effect of an increase in price on the expected sales volume is consistent for each brand. Additionally, the model incorporates indicator variables for Brand B and Brand C, alongside price as predictors, to account for the varying intercepts that might exist between brands. The PARALLEL_LINES_MODEL enables the examination of how the difference in expected sales volume for stores of different brands remains constant for every fixed value of price, providing a structured approach to compare and understand brand-specific sales dynamics within a uniform price sensitivity framework.</data>
      <data key="d2">0cb40986e6c2bb439e1ffcaae2df96ac,1820d10ee0f23f34b3ea88ba475bc52d,77e76692753fdf53493182b09018e6bc,86ece4718d27d1a6c6a1f448cc850e2b,ac15b639b0849006471dfe102376c2c0,b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </node>
    <node id="MU_HAT_A">
      <data key="d0">INTERCEPT</data>
      <data key="d1">MU_HAT_A, denoted as &#956;&#770;A, represents the estimated intercept specific to Brand A within the context of both the regression model and the parallel lines model. This statistical measure is crucial for understanding the baseline response of Brand A, independent of any predictor variables in the models. The dual mention in different model contexts highlights its significance in algorithmic analysis, particularly in assessing the structural relationship between the community of interest and Brand A. MU_HAT_A serves as a foundational parameter in these models, aiding in the interpretation of how Brand A performs under the assumption of no influence from other variables.</data>
      <data key="d2">7037e0369bfdaad5a730cabb2b44831c,b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </node>
    <node id="MU_HAT_B">
      <data key="d0">INTERCEPT</data>
      <data key="d1">MU_HAT_B, denoted as \u00b5\u02c6B, represents the estimated intercept for Brand B in the context of both the regression model and the parallel lines model. This statistical measure is crucial for understanding the baseline response of Brand B, independent of any predictor variables in the models. The consistency of MU_HAT_B across different modeling approaches highlights its significance in comparative analysis, allowing for insights into Brand B's performance relative to other brands within the same study.</data>
      <data key="d2">7037e0369bfdaad5a730cabb2b44831c,b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </node>
    <node id="MU_HAT_C">
      <data key="d0">INTERCEPT</data>
      <data key="d1">MU_HAT_C, denoted as &#956;&#770;C, represents the estimated intercept specific to Brand C within the context of both the regression model and the parallel lines model. This statistical measure is crucial for understanding the baseline response of Brand C in the absence of other influencing factors, as it provides a foundational point from which the effects of other variables can be assessed in both model frameworks. The dual reference to MU_HAT_C in these models highlights its significance in comparative analyses, particularly when evaluating how Brand C's baseline performance aligns with or deviates from other brands within similar studies.</data>
      <data key="d2">7037e0369bfdaad5a730cabb2b44831c,b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </node>
    <node id="STORE_BRAND">
      <data key="d0">VARIABLE</data>
      <data key="d1">Store brand is a categorical variable in the model, indicating the brand of the store</data>
      <data key="d2">248924760a2bfbc82501fd6b11cfa0aa</data>
    </node>
    <node id="XJA">
      <data key="d0">VARIABLE</data>
      <data key="d1">XjA is an indicator variable for store brand A, taking the value 1 if store j is of brand A, and 0 otherwise</data>
      <data key="d2">248924760a2bfbc82501fd6b11cfa0aa</data>
    </node>
    <node id="XJB">
      <data key="d0">VARIABLE</data>
      <data key="d1">XJB is an indicator variable specifically designed to denote the presence or absence of Brand B in a given context. This variable assumes a value of 1 when store j, or the jth observation, is associated with Brand B, signifying that the store or observation belongs to Brand B. Conversely, XJB takes the value 0 when store j, or the jth observation, is not of Brand B, indicating the absence of Brand B in that particular store or observation. This binary nature of XJB makes it a crucial tool for identifying and categorizing stores or observations based on their affiliation with Brand B.</data>
      <data key="d2">248924760a2bfbc82501fd6b11cfa0aa,ac15b639b0849006471dfe102376c2c0,c103c6d096d52868eda26d991194b5f2</data>
    </node>
    <node id="XJC">
      <data key="d0">VARIABLE</data>
      <data key="d1">XJC is an indicator variable specifically designed to denote the presence or absence of Brand C in a given context. This variable assumes a binary value, with 1 signifying that store j, or the jth observation, is of Brand C, and 0 indicating otherwise. This binary representation facilitates the identification and analysis of Brand C's presence within a dataset, enabling detailed statistical analysis and comparisons with other brands. XJC serves as a crucial component in algorithmic analysis, particularly in regression models, where it helps in understanding the impact of Brand C on various outcomes.</data>
      <data key="d2">248924760a2bfbc82501fd6b11cfa0aa,ac15b639b0849006471dfe102376c2c0,c103c6d096d52868eda26d991194b5f2</data>
    </node>
    <node id="SALESJ">
      <data key="d0">SALES</data>
      <data key="d1">Salesj represents the sales at store j, which is modeled as a function of brand and price</data>
      <data key="d2">228bdca7843406def245d755e8df49f6</data>
    </node>
    <node id="PRICEJ">
      <data key="d0">PRICE</data>
      <data key="d1">PRICEJ refers to the price at store j, serving as a crucial variable in the model for analyzing Salesj. It represents the price of the jth observation within the retail data, highlighting its significance in understanding market trends and consumer behavior. PRICEJ is also identified as the vector of prices for the jth store, providing a comprehensive view of pricing strategies across different retail outlets. This entity plays a pivotal role in statistical analysis, particularly in linear regression models, where its relationship with sales figures is closely examined to derive insights into market dynamics.</data>
      <data key="d2">228bdca7843406def245d755e8df49f6,925e17c26fb7d979f52538f4632333e7,ac15b639b0849006471dfe102376c2c0</data>
    </node>
    <node id="BRANDA">
      <data key="d0">DUMMY_VARIABLE</data>
      <data key="d1">BRANDA, a dummy variable within the dataset, signifies the presence (1) or absence (0) of brand A. This variable is integral to the regression model analysis, where brand A is one of several brands being examined. In the context of the regression model, BRANDA has a notable coefficient of 109.922, indicating the strength of its association with the dependent variable. Additionally, brand A is identified as one of the categories under the broader 'Brand' variable, highlighting its significance in the categorical analysis of brands.</data>
      <data key="d2">06d5666e6bfdda828b48adba883b4a61,93da9813e10a119798de6982977f1239,e800735d6b2a244875f5e0d292de1527</data>
    </node>
    <node id="BRANDB">
      <data key="d0">DUMMY_VARIABLE</data>
      <data key="d1">BRANDB is a dummy variable utilized in the dataset to denote the presence or absence of brand B, where a value of 1 signifies its presence and 0 indicates its absence. In the context of the regression model analysis, BRANDB is identified as one of the significant brands, featuring a notable coefficient of 103.617. This highlights its substantial impact on the model's outcomes. BRANDB is also recognized as one of the categories within the broader Brand variable, emphasizing its role in the segmentation and analysis of different brands within the study.</data>
      <data key="d2">06d5666e6bfdda828b48adba883b4a61,93da9813e10a119798de6982977f1239,e800735d6b2a244875f5e0d292de1527</data>
    </node>
    <node id="BRANDC">
      <data key="d0">DUMMY_VARIABLE</data>
      <data key="d1">BrandC, a category within the Brand variable, is a dummy variable utilized in the dataset to denote the presence (1) or absence (0) of brand C. In the context of the regression model analysis, BrandC is one of the brands under scrutiny, and it carries a significant coefficient of 128.549, indicating its substantial impact on the model's outcomes. This highlights BrandC's importance in understanding the relations and structure of the community of interest within the statistical analysis.</data>
      <data key="d2">06d5666e6bfdda828b48adba883b4a61,93da9813e10a119798de6982977f1239,e800735d6b2a244875f5e0d292de1527</data>
    </node>
    <node id="BRAND.MODEL1">
      <data key="d0">REGRESSION_MODEL</data>
      <data key="d1">Brand.model1 is the regression model being analyzed, which includes the brand and price variables</data>
      <data key="d2">e800735d6b2a244875f5e0d292de1527</data>
    </node>
    <node id="C_PRICE">
      <data key="d0">DERIVED_VARIABLE</data>
      <data key="d1">C_PRICE, denoted as c_price or c_pricej in the context of the regression model, is a derived variable that encapsulates the deviation of the price from the sample mean price. This deviation is calculated as the difference between the original price (pricej) and the sample mean price, which is specifically identified as 152.76, serving as a baseline price in the analysis. C_PRICE thus quantifies how much each price observation deviates from this established mean, providing a measure of price difference that is central to the statistical model being employed.</data>
      <data key="d2">06d5666e6bfdda828b48adba883b4a61,1b523d1edabe381403fc470a9b8d47fa,93da9813e10a119798de6982977f1239</data>
    </node>
    <node id="BRAND_MODEL2">
      <data key="d0">MODEL</data>
      <data key="d1">Brand.model2 is a statistical model used to analyze sales data for different brands, including brandA, brandB, and brandC, with c_price as a variable</data>
      <data key="d2">93da9813e10a119798de6982977f1239</data>
    </node>
    <node id="DIAMONDS_EXAMPLE">
      <data key="d0">CASE_STUDY</data>
      <data key="d1">The diamonds example is a case study used to illustrate the concept of reparameterisation and its impact on the interpretability of parameters</data>
      <data key="d2">9854704301b8df256ca1013b8d53dfac</data>
    </node>
    <node id="BRAND_VARIABLE">
      <data key="d0">FACTOR</data>
      <data key="d1">The brand variable is a factor variable that defines groups within the data, in this case, the stores of various brands</data>
      <data key="d2">9854704301b8df256ca1013b8d53dfac</data>
    </node>
    <node id="AVERAGE_PRICE">
      <data key="d0">VALUE</data>
      <data key="d1">The average price is the mean price within the observed sample, used as a reference point for interpreting the estimated coefficients</data>
      <data key="d2">9854704301b8df256ca1013b8d53dfac</data>
    </node>
    <node id="CEO">
      <data key="d0">ROLE</data>
      <data key="d1">The CEO is the chief executive officer, responsible for overseeing the stores of a specific brand</data>
      <data key="d2">9854704301b8df256ca1013b8d53dfac</data>
    </node>
    <node id="RETAIL_EXAMPLE">
      <data key="d0">CASE_STUDY</data>
      <data key="d1">The retail example is a case study used to illustrate the application of regression analysis in a retail context</data>
      <data key="d2">9854704301b8df256ca1013b8d53dfac</data>
    </node>
    <node id="FACTOR_VARIABLE">
      <data key="d0">VARIABLE</data>
      <data key="d1">A factor variable defines groups within the data, such as the stores of various brands</data>
      <data key="d2">906eb7d6b49fa360e7e5b65c56cd4d76</data>
    </node>
    <node id="ALPHA_B">
      <data key="d0">PARAMETER</data>
      <data key="d1">Alpha B (&#945;B), a pivotal parameter in the statistical models under consideration, encapsulates the nuanced relationship between Brand B and Brand A in terms of sales and intercept differences. Specifically, in the context of the parallel lines model, Alpha B signifies the impact of Brand B on sales, highlighting its role in influencing sales volume. This parameter is also central to the reparameterised model, where it quantifies the disparity in sales volume between Brand B and Brand A stores, offering insights into the comparative performance of the two brands. Additionally, Alpha B serves as the additional intercept parameter for Brand B, delineating the difference in intercept for Brand B relative to Brand A. In the linear regression model, Alpha_B (&#945;B) is the parameter estimate for Brand B, indicating the expected difference in sales volume compared to Brand A for a fixed product price, thereby providing a comprehensive measure of Brand B's sales potential in relation to Brand A.</data>
      <data key="d2">06199787dd7f75f7338dd24d4f3dc26e,0cb40986e6c2bb439e1ffcaae2df96ac,1d141ab04db553f78a313e430e54abb5,906eb7d6b49fa360e7e5b65c56cd4d76,a1fc936df848a0fbc791e4bcc9b527b6,ac15b639b0849006471dfe102376c2c0,b6870535f3975c49d45e62fbe475f198,baa0dc3d4ec0e51c0a321e5579caf8aa</data>
    </node>
    <node id="ALPHA_C">
      <data key="d0">PARAMETER</data>
      <data key="d1">Alpha C (&#945;C) is a pivotal parameter in the statistical models under consideration, specifically in the context of sales analysis for different brands. In the regression model, Alpha C signifies the effect of Brand C on sales, acting as an additional intercept parameter that highlights the difference in sales volume between Brand C and Brand A stores. This parameter is crucial in the parallel lines model, where it quantifies the impact of Brand C on sales, and in the reparameterised model, where it specifically denotes the difference in sales volume between Brand C and Brand A, for a given product price. Alpha C (&#945;C) is also interpreted as the difference in intercept for Brand C compared to Brand A, providing insights into how Brand C's sales volume deviates from that of Brand A under the same conditions. In essence, Alpha C (&#945;C) is the parameter estimate for Brand C in the linear regression model, indicating the expected difference in sales volume relative to Brand A, thereby offering a comprehensive view of Brand C's performance in the market.</data>
      <data key="d2">06199787dd7f75f7338dd24d4f3dc26e,0cb40986e6c2bb439e1ffcaae2df96ac,1d141ab04db553f78a313e430e54abb5,825b600cbab3535ce67e9f561ddcb84b,906eb7d6b49fa360e7e5b65c56cd4d76,a1fc936df848a0fbc791e4bcc9b527b6,ac15b639b0849006471dfe102376c2c0,b6870535f3975c49d45e62fbe475f198,baa0dc3d4ec0e51c0a321e5579caf8aa</data>
    </node>
    <node id="X_B">
      <data key="d0">VARIABLE</data>
      <data key="d1">X_B (xjB) is an indicator variable, equal to 1 if store j is of Brand B, and 0 otherwise</data>
      <data key="d2">1d141ab04db553f78a313e430e54abb5</data>
    </node>
    <node id="X_C">
      <data key="d0">VARIABLE</data>
      <data key="d1">X_C (xjC) is an indicator variable, equal to 1 if store j is of Brand C, and 0 otherwise</data>
      <data key="d2">1d141ab04db553f78a313e430e54abb5</data>
    </node>
    <node id="DESIGN_MATRIX_X">
      <data key="d0">MATRIX</data>
      <data key="d1">The design matrix X has 96 rows and 4 columns, representing the values of the intercept, indicator variables for Brand B and C, and price for each store</data>
      <data key="d2">c103c6d096d52868eda26d991194b5f2</data>
    </node>
    <node id="BRAND_MODEL3">
      <data key="d0">MODEL</data>
      <data key="d1">Brand_model3 is a linear model fitted using the lm() function in R, with sales as the dependent variable and brand and price as independent variables</data>
      <data key="d2">6a6f85d0a6e46196ab3a901fcc82a720</data>
    </node>
    <node id="MU_HAT">
      <data key="d0">PARAMETER</data>
      <data key="d1">Mu_hat (192.246) is the estimated intercept parameter in the linear model</data>
      <data key="d2">6a6f85d0a6e46196ab3a901fcc82a720</data>
    </node>
    <node id="ALPHA_HAT_B">
      <data key="d0">PARAMETER</data>
      <data key="d1">Alpha_hat_B (-6.305) is the estimated parameter for brand B in the linear model</data>
      <data key="d2">6a6f85d0a6e46196ab3a901fcc82a720</data>
    </node>
    <node id="ALPHA_HAT_C">
      <data key="d0">PARAMETER</data>
      <data key="d1">Alpha_hat_C (18.627) is the estimated parameter for brand C in the linear model</data>
      <data key="d2">6a6f85d0a6e46196ab3a901fcc82a720</data>
    </node>
    <node id="PRODUCT_PRICE">
      <data key="d0">INDEPENDENT_VARIABLE</data>
      <data key="d1">Product price is an independent variable in the linear regression model, assumed to be fixed in the context of comparing sales volumes</data>
      <data key="d2">baa0dc3d4ec0e51c0a321e5579caf8aa</data>
    </node>
    <node id="TREATMENT_CODING">
      <data key="d0">PARAMETERISATION</data>
      <data key="d1">Treatment coding is a type of parameterisation used in the regression model, where the levels of the categorical predictor are compared to a baseline or reference category</data>
      <data key="d2">baa0dc3d4ec0e51c0a321e5579caf8aa</data>
    </node>
    <node id="SAMPLE_MEANS">
      <data key="d0">STATISTIC</data>
      <data key="d1">The SAMPLE_MEANS, in the context of the provided data, refer specifically to the average sales volume for stores of each brand, namely Brand A, Brand B, and Brand C. These statistics are calculated from the observed data, providing insights into the typical performance of stores under each brand. The sample means serve as crucial indicators of central tendency, helping to understand the general sales volume levels that can be expected from stores belonging to these brands.</data>
      <data key="d2">06199787dd7f75f7338dd24d4f3dc26e,48971100deb5bb374a41c1f2b7b2a86a</data>
    </node>
    <node id="MODEL_FITTED_IN_H">
      <data key="d0">MODEL</data>
      <data key="d1">The model fitted in h. is used to predict the sales volume for Brand C stores</data>
      <data key="d2">096afa471635bc59c3bfa9af4d04d625</data>
    </node>
    <node id="MODEL_FITTED_IN_C">
      <data key="d0">MODEL</data>
      <data key="d1">The model fitted in c. also predicts the sales volume for Brand C stores, which is compared to the prediction from the model fitted in h.</data>
      <data key="d2">096afa471635bc59c3bfa9af4d04d625</data>
    </node>
    <node id="EXERCISE_22A">
      <data key="d0">EXERCISE</data>
      <data key="d1">Exercise 22a refers to a previous exercise that introduced the model being considered in Exercise 23</data>
      <data key="d2">096afa471635bc59c3bfa9af4d04d625</data>
    </node>
    <node id="SA">
      <data key="d0">SET</data>
      <data key="d1">SA is the set containing the indices of the observations that are of Brand A</data>
      <data key="d2">096afa471635bc59c3bfa9af4d04d625,925e17c26fb7d979f52538f4632333e7</data>
    </node>
    <node id="SB">
      <data key="d0">SET</data>
      <data key="d1">SB is the set containing the indices of the observations that are of Brand B</data>
      <data key="d2">096afa471635bc59c3bfa9af4d04d625,925e17c26fb7d979f52538f4632333e7</data>
    </node>
    <node id="SC">
      <data key="d0">SET</data>
      <data key="d1">SC is the set containing the indices of the observations that are of Brand C</data>
      <data key="d2">096afa471635bc59c3bfa9af4d04d625,925e17c26fb7d979f52538f4632333e7</data>
    </node>
    <node id="NA">
      <data key="d0">NUMBER</data>
      <data key="d1">nA is the number of observations that are of Brand A</data>
      <data key="d2">096afa471635bc59c3bfa9af4d04d625,925e17c26fb7d979f52538f4632333e7</data>
    </node>
    <node id="NB">
      <data key="d0">NUMBER</data>
      <data key="d1">nB is the number of observations that are of Brand B</data>
      <data key="d2">096afa471635bc59c3bfa9af4d04d625,925e17c26fb7d979f52538f4632333e7</data>
    </node>
    <node id="NC">
      <data key="d0">NUMBER</data>
      <data key="d1">nC is the number of observations that are of Brand C</data>
      <data key="d2">096afa471635bc59c3bfa9af4d04d625,925e17c26fb7d979f52538f4632333e7</data>
    </node>
    <node id="SALESI">
      <data key="d0">VECTOR</data>
      <data key="d1">salesi is the vector of sales for the ith observation</data>
      <data key="d2">925e17c26fb7d979f52538f4632333e7</data>
    </node>
    <node id="ALPHAB">
      <data key="d0">PARAMETER</data>
      <data key="d1">alphaB is the difference in intercept for Brand B compared to Brand A in the parallel lines model</data>
      <data key="d2">925e17c26fb7d979f52538f4632333e7</data>
    </node>
    <node id="ALPHAC">
      <data key="d0">PARAMETER</data>
      <data key="d1">alphaC is the difference in intercept for Brand C compared to Brand A in the parallel lines model</data>
      <data key="d2">925e17c26fb7d979f52538f4632333e7</data>
    </node>
    <node id="ROUNDING_ERRORS">
      <data key="d0">ERRORS</data>
      <data key="d1">Rounding errors are small discrepancies that can occur due to the rounding of numbers during calculations</data>
      <data key="d2">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </node>
    <node id="REFERENCE_CATEGORY">
      <data key="d0">CATEGORY</data>
      <data key="d1">The reference category, in the context of statistical modeling, is a specific level of a categorical variable that is selected to serve as the baseline for comparison. This category is not included as an indicator variable in the model, thereby acting as a standard against which the other categories are contrasted. Its role is pivotal in understanding the relative differences between the various levels of the categorical variable, enabling a clearer interpretation of the model's coefficients.</data>
      <data key="d2">77e76692753fdf53493182b09018e6bc,86ece4718d27d1a6c6a1f448cc850e2b</data>
    </node>
    <node id="RELEVEL">
      <data key="d0">COMMAND</data>
      <data key="d1">Relevel is an R command used to change the reference category of a factor variable</data>
      <data key="d2">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </node>
    <node id="BRAND_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">Brand model is the model fitted using the lm() function in R, with sales as the response variable and price and brand as predictor variables</data>
      <data key="d2">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </node>
    <node id="INTERACTION">
      <data key="d0">MODEL_FEATURE</data>
      <data key="d1">In the context of statistical modeling, INTERACTION is a pivotal concept that signifies a scenario where the effect of one predictor variable on the response variable is contingent upon the level or value of another predictor variable. This phenomenon is crucial in regression analysis, particularly in linear models, where the relationship between the response and a quantitative predictor can vary across different levels of a categorical predictor. Interaction is formally introduced into models through the inclusion of product terms in the model equations, allowing for the assessment of how the impact of one explanatory variable changes at various values of another explanatory variable. This type of effect is essential for understanding complex relationships within data, as it enables the model to capture more nuanced and realistic associations between variables.</data>
      <data key="d2">0cb40986e6c2bb439e1ffcaae2df96ac,77e76692753fdf53493182b09018e6bc,86ece4718d27d1a6c6a1f448cc850e2b,b0ca3e6c22c4cf884d03b1f6f82be5df,d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </node>
    <node id="FITTED_LINE">
      <data key="d0">REGRESSION_LINE</data>
      <data key="d1">The fitted line is the regression line obtained from the regression model, used to predict sales based on price and brand</data>
      <data key="d2">ac15b639b0849006471dfe102376c2c0</data>
    </node>
    <node id="FIGURE_9_4">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 9.4, referred to as "FIGURE_9_4," is a comprehensive graphical representation that illustrates the relationship between sales and price for three distinct store brands. This figure presents three separate scatterplots, each corresponding to a different brand, showcasing the distribution of sales data points against the price. Superimposed on each scatterplot are the fitted regression lines derived from the parallel lines model. These regression lines provide a visual summary of the linear relationship between sales and price for each brand, allowing for a detailed analysis of how price variations impact sales across the different brands. The parallel lines model assumes that the relationship between sales and price is consistent across all brands, offering insights into the comparative behavior of consumers towards each brand in the context of price sensitivity.</data>
      <data key="d2">0cb40986e6c2bb439e1ffcaae2df96ac,ac15b639b0849006471dfe102376c2c0</data>
    </node>
    <node id="GAMMA_A">
      <data key="d0">PARAMETER</data>
      <data key="d1">Gamma A (&#947;A) is a parameter in the regression model, representing the interaction effect between Brand A and price on sales</data>
      <data key="d2">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </node>
    <node id="GAMMA_B">
      <data key="d0">PARAMETER</data>
      <data key="d1">Gamma B (&#947;B) is a significant parameter in the regression model, serving a dual role in the analysis of sales data. Firstly, it quantifies the interaction effect between Brand B and price on sales, illustrating how the relationship between price and sales changes specifically for Brand B. Secondly, Gamma B also denotes the difference in slope for brand B in comparison to brand A, indicating the distinct sensitivity of Brand B's sales to price changes relative to Brand A. This dual interpretation of Gamma B highlights its importance in understanding the nuanced dynamics of brand performance within the market, particularly in relation to pricing strategies.</data>
      <data key="d2">0cb40986e6c2bb439e1ffcaae2df96ac,b6870535f3975c49d45e62fbe475f198</data>
    </node>
    <node id="INDICATOR_VARIABLES">
      <data key="d0">VARIABLE</data>
      <data key="d1">The INDICATOR_VARIABLES, specifically xjB and xjC, are binary variables, also known as dummy variables, utilized in the regression model to encode categorical predictor variables related to the brand of the store. These indicator variables facilitate the representation of categorical data in a linear model. When a categorical predictor has d levels, the model employs d-1 indicator variables, with the omitted level serving as the reference category. This approach ensures that the model can effectively analyze the impact of different brands on the outcome variable, while accounting for the categorical nature of the brand variable.</data>
      <data key="d2">0cb40986e6c2bb439e1ffcaae2df96ac,77e76692753fdf53493182b09018e6bc</data>
    </node>
    <node id="STORE">
      <data key="d0">ENTITY</data>
      <data key="d1">A store is a place where sales occur, and its brand affects the sales volume</data>
      <data key="d2">b6870535f3975c49d45e62fbe475f198</data>
    </node>
    <node id="GAMMA_C">
      <data key="d0">PARAMETER</data>
      <data key="d1">Gamma C (&#947;C) is a pivotal parameter in the statistical analysis of brand performance, specifically highlighting the differential slope between Brand C and Brand A in a regression context. This parameter quantifies the extent to which the relationship between the variables of interest for Brand C deviates from that of Brand A, offering insights into comparative market trends and consumer behavior. By examining &#947;C, data scientists can discern the unique characteristics and market positioning of Brand C relative to Brand A, facilitating a deeper understanding of the competitive landscape and informing strategic decision-making.</data>
      <data key="d2">825b600cbab3535ce67e9f561ddcb84b,b6870535f3975c49d45e62fbe475f198</data>
    </node>
    <node id="STORE_J">
      <data key="d0">STORE</data>
      <data key="d1">Store j is a specific store in the dataset, which can be of brand B or C</data>
      <data key="d2">825b600cbab3535ce67e9f561ddcb84b</data>
    </node>
    <node id="PRICE_J">
      <data key="d0">PREDICTOR</data>
      <data key="d1">Price j is the price of the product in store j</data>
      <data key="d2">825b600cbab3535ce67e9f561ddcb84b</data>
    </node>
    <node id="BRAND_MODEL4">
      <data key="d0">MODEL</data>
      <data key="d1">brand.model4 is the linear model object created by fitting the regression model in R</data>
      <data key="d2">825b600cbab3535ce67e9f561ddcb84b</data>
    </node>
    <node id="ROUND_FUNCTION">
      <data key="d0">FUNCTION</data>
      <data key="d1">round() is the function in R used to round numbers to a specified number of decimal places</data>
      <data key="d2">825b600cbab3535ce67e9f561ddcb84b</data>
    </node>
    <node id="BBETA">
      <data key="d0">ESTIMATED_PARAMETER</data>
      <data key="d1">Beta hat (b&#946;) is the estimated coefficient for the price predictor in the regression model</data>
      <data key="d2">825b600cbab3535ce67e9f561ddcb84b</data>
    </node>
    <node id="ALPHAB_B">
      <data key="d0">ESTIMATED_PARAMETER</data>
      <data key="d1">Alpha hat for brand B (&#945;bB) is the estimated parameter for brand B in the regression model</data>
      <data key="d2">825b600cbab3535ce67e9f561ddcb84b</data>
    </node>
    <node id="ALPHAB_C">
      <data key="d0">ESTIMATED_PARAMETER</data>
      <data key="d1">Alpha hat for brand C (&#945;bC) is the estimated parameter for brand C in the regression model</data>
      <data key="d2">825b600cbab3535ce67e9f561ddcb84b</data>
    </node>
    <node id="ALPHA_B_B">
      <data key="d0">COEFFICIENT</data>
      <data key="d1">Alpha hat for Brand B (&#945;bB) is the coefficient for Brand B, representing the difference in intercept compared to Brand A</data>
      <data key="d2">adbc52b340a69a8633c919c4fd2cd3f6</data>
    </node>
    <node id="ALPHA_B_C">
      <data key="d0">COEFFICIENT</data>
      <data key="d1">Alpha hat for Brand C (&#945;bC) is the coefficient for Brand C, representing the difference in intercept compared to Brand A</data>
      <data key="d2">adbc52b340a69a8633c919c4fd2cd3f6</data>
    </node>
    <node id="GAMMA_B_B">
      <data key="d0">COEFFICIENT</data>
      <data key="d1">Gamma hat for Brand B (&#947;bB) is the coefficient for the interaction between Brand B and price, representing the difference in price effect compared to Brand A</data>
      <data key="d2">adbc52b340a69a8633c919c4fd2cd3f6</data>
    </node>
    <node id="GAMMA_B_C">
      <data key="d0">COEFFICIENT</data>
      <data key="d1">Gamma hat for Brand C (&#947;bC) is the coefficient for the interaction between Brand C and price, representing the difference in price effect compared to Brand A</data>
      <data key="d2">adbc52b340a69a8633c919c4fd2cd3f6</data>
    </node>
    <node id="FIGURE_9_5">
      <data key="d0">FIGURE</data>
      <data key="d1">Figure 9.5 is a scatterplot of sales against price with coding according to brand, showing the three lines of best fit, one for each brand</data>
      <data key="d2">3dd24a54028976ba54304ec7169bb74b</data>
    </node>
    <node id="INTERACTION_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">The interaction model is a regression model where the slopes of the regression lines for different brands are allowed to differ, meaning the effect of an increase in price on the expected sales volume differs between brands, and the difference in expected sales volume for stores of different brands depends on the price being charged for the product</data>
      <data key="d2">1820d10ee0f23f34b3ea88ba475bc52d</data>
    </node>
    <node id="FACTOR">
      <data key="d0">CATEGORICAL_VARIABLE</data>
      <data key="d1">Factor is a categorical predictor variable in regression models, often represented by indicator variables or dummy variables.</data>
      <data key="d2">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </node>
    <node id="INDICATOR_VARIABLE">
      <data key="d0">VARIABLE_TYPE</data>
      <data key="d1">Indicator variables, also known as dummy variables, are used to encode categorical predictor variables in regression models.</data>
      <data key="d2">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </node>
    <node id="DUMMY_VARIABLE">
      <data key="d0">VARIABLE_TYPE</data>
      <data key="d1">Dummy variables are a type of indicator variable used to represent categorical predictors in regression models.</data>
      <data key="d2">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </node>
    <node id="EXERCISE_26">
      <data key="d0">EXERCISE</data>
      <data key="d1">Exercise 26 asks for the design matrix for the model defined in equation (9.6).</data>
      <data key="d2">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </node>
    <node id="CHAPTER_9">
      <data key="d0">DOCUMENT</data>
      <data key="d1">Chapter 9 of a book or module introduces categorical predictor variables, also known as factors, in linear models.</data>
      <data key="d2">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </node>
    <node id="CATEGORICAL_PREDICTORS">
      <data key="d0">VARIABLE_TYPE</data>
      <data key="d1">CATEGORICAL_PREDICTORS, also known as factors, are variables in a linear model that have discrete categories or levels. These predictors can take on one of a limited, and usually fixed, number of possible values, such as gender, race, or country of origin. To accommodate them within the linear model framework, indicator variables, also called dummy variables, are used. CATEGORICAL_PREDICTORS are incorporated into a linear model to analyze their effect on the response variable, enabling the assessment of how different categories influence the outcome.</data>
      <data key="d2">77e76692753fdf53493182b09018e6bc,d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </node>
    <node id="QUANTITATIVE_VARIABLES">
      <data key="d0">VARIABLE_TYPE</data>
      <data key="d1">Quantitative variables are numerical variables that can take on any value within a range. They are used as predictors in linear models.</data>
      <data key="d2">77e76692753fdf53493182b09018e6bc</data>
    </node>
    <node id="PRODUCT_TERMS">
      <data key="d0">MODEL_COMPONENT</data>
      <data key="d1">Product terms are introduced into linear model equations to allow for interaction between explanatory variables. They represent the effect of one explanatory variable on the response variable depending on the value of another explanatory variable.</data>
      <data key="d2">77e76692753fdf53493182b09018e6bc</data>
    </node>
    <node id="PARALLEL_LINES_REGRESSION_MODEL">
      <data key="d0">MODEL</data>
      <data key="d1">A parallel lines regression model is a type of linear model that assumes the regression lines for different levels of a categorical predictor are parallel. This model is used when the effect of the quantitative predictor is the same across all levels of the categorical predictor.</data>
      <data key="d2">d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </node>
    <node id="T_STATISTIC">
      <data key="d0">STATISTICAL_MEASURE</data>
      <data key="d1">The T-statistic is a statistical measure used in hypothesis testing to determine whether the estimated value of a parameter is significantly different from a hypothesized value. It is calculated by dividing the estimated parameter by its standard error.</data>
      <data key="d2">d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </node>
    <node id="MAXIMUM_LIKELIHOOD_ESTIMATOR">
      <data key="d0">ESTIMATOR</data>
      <data key="d1">The maximum likelihood estimator (MLE) is a method of estimating the parameters of a statistical model, given observations. It is based on the likelihood function, which is the probability of the observed data given the parameters of the model.</data>
      <data key="d2">d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </node>
    <node id="U">
      <data key="d0">TRANSFORMED_VECTOR</data>
      <data key="d1">U is a random variable that results from the transformation of a vector Z through matrix multiplication with matrix A. Here, Z is a vector composed of independent standard normal random variables, and A is a matrix that, when multiplied by Z, produces the transformed vector U, also known as AZ. This transformation is a common technique in statistical analysis, particularly in the context of linear regression and other statistical models, where it can be used to adjust the scale or distribution of the data. The resulting vector U inherits the properties of the standard normal distribution due to the nature of Z, making it a useful tool for various statistical inferences and analyses.</data>
      <data key="d2">0ac60299320c55d642b3e38440c25f90,aac5b4f040b9c773bd1aa696dec469f6</data>
    </node>
    <node id="V">
      <data key="d0">TRANSFORMED_VECTOR</data>
      <data key="d1">V, a significant entity in the context of statistical analysis, is characterized as a random variable that is precisely defined through a transformation involving a matrix, B, and a vector, Z. Specifically, V is the product of matrix B and a vector Z, where Z comprises independent standard normal random variables. This definition underscores the role of V as a transformed vector, BZ, obtained by multiplying the matrix B with the vector Z. The nature of V as a random variable is further emphasized by its construction from standard normal random variables, highlighting its relevance in probabilistic and statistical modeling.</data>
      <data key="d2">0ac60299320c55d642b3e38440c25f90,aac5b4f040b9c773bd1aa696dec469f6</data>
    </node>
    <node id="CHI_SQUARED_DISTRIBUTION">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">The chi-squared distribution is a continuous probability distribution that arises in the context of the sum of squares of independent standard normal random variables</data>
      <data key="d2">aac5b4f040b9c773bd1aa696dec469f6</data>
    </node>
    <node id="CHI_SQUARED">
      <data key="d0">DISTRIBUTION</data>
      <data key="d1">The chi-squared distribution with n degrees of freedom is a probability distribution used in hypothesis testing and goodness of fit tests</data>
      <data key="d2">0ac60299320c55d642b3e38440c25f90</data>
    </node>
    <node id="COV_U_V">
      <data key="d0">COVARIANCE</data>
      <data key="d1">The covariance between U and V is zero, indicating that U and V are uncorrelated</data>
      <data key="d2">0ac60299320c55d642b3e38440c25f90</data>
    </node>
    <edge source="ST231" target="SIMPLE LINEAR REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">ST231 lecture notes cover simple linear regression, which is a fundamental concept in the field of statistics</data>
      <data key="d6">54206a4a813f5be515f41653e9422eeb</data>
    </edge>
    <edge source="ST231" target="TERMINOLOGY AND NOTATION">
      <data key="d4">1.0</data>
      <data key="d5">ST231 lecture notes include terminology and notation used in the field of statistics</data>
      <data key="d6">54206a4a813f5be515f41653e9422eeb</data>
    </edge>
    <edge source="ST231" target="MULTIPLE REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">ST231 lecture notes cover multiple regression, which is an extension of simple linear regression</data>
      <data key="d6">54206a4a813f5be515f41653e9422eeb</data>
    </edge>
    <edge source="ST231" target="LINEAR MODELS">
      <data key="d4">1.0</data>
      <data key="d5">ST231 lecture notes discuss the class of linear models, which includes simple and multiple regression</data>
      <data key="d6">54206a4a813f5be515f41653e9422eeb</data>
    </edge>
    <edge source="ST231" target="ANSCOMBE'S QUARTET">
      <data key="d4">1.0</data>
      <data key="d5">ST231 lecture notes include Anscombe's Quartet, which is a set of datasets used to illustrate the importance of graphing data</data>
      <data key="d6">54206a4a813f5be515f41653e9422eeb</data>
    </edge>
    <edge source="ST231" target="POLYNOMIAL REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">ST231 lecture notes cover polynomial regression, which is a form of regression analysis that models the relationship between variables as a polynomial</data>
      <data key="d6">54206a4a813f5be515f41653e9422eeb</data>
    </edge>
    <edge source="LINEARITY" target="ANSCOMBES_QUARTET">
      <data key="d4">1.0</data>
      <data key="d5">Anscombe's Quartet is an example that demonstrates the importance of linearity in statistical models and the need for graphical analysis to understand the data</data>
      <data key="d6">35bac6a2c3eb466ca9fc7b31bf2cc42c</data>
    </edge>
    <edge source="LINEARITY" target="POLYNOMIAL_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Polynomial regression is an extension of linear regression that allows for the modelling of nonlinear relationships, which are still considered under the broader concept of linearity in statistical models</data>
      <data key="d6">35bac6a2c3eb466ca9fc7b31bf2cc42c</data>
    </edge>
    <edge source="LINEARITY" target="LOG_TRANSFORMED_PREDICTOR">
      <data key="d4">1.0</data>
      <data key="d5">A linear model with a log-transformed predictor is a specific case of linear models where the predictor is transformed to meet the linearity assumption</data>
      <data key="d6">35bac6a2c3eb466ca9fc7b31bf2cc42c</data>
    </edge>
    <edge source="LINEARITY" target="STRATEGY_FOR_STATISTICAL_MODELLING">
      <data key="d4">1.0</data>
      <data key="d5">A strategy for statistical modelling includes considerations for ensuring linearity in the relationship between variables</data>
      <data key="d6">35bac6a2c3eb466ca9fc7b31bf2cc42c</data>
    </edge>
    <edge source="LINEARITY" target="GOOD_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">A good model should meet the assumption of linearity, which is a fundamental aspect of statistical models</data>
      <data key="d6">35bac6a2c3eb466ca9fc7b31bf2cc42c</data>
    </edge>
    <edge source="LINEARITY" target="SUMMARY_OF_CHAPTER_2">
      <data key="d4">1.0</data>
      <data key="d5">The summary of Chapter 2 includes a recap of the concept of linearity and its importance in statistical models</data>
      <data key="d6">35bac6a2c3eb466ca9fc7b31bf2cc42c</data>
    </edge>
    <edge source="LINEARITY" target="RESIDUAL_ANALYSIS">
      <data key="d4">1.0</data>
      <data key="d5">Residual analysis is used to check the assumption of linearity in statistical models by examining the residuals</data>
      <data key="d6">35bac6a2c3eb466ca9fc7b31bf2cc42c</data>
    </edge>
    <edge source="LINEARITY" target="GRAPHICAL_EXPLORATION">
      <data key="d4">1.0</data>
      <data key="d5">Graphical exploration is used to assess the adequacy of the linear model's systematic component</data>
      <data key="d6">6616e10c85e86291147e72776854b8a2</data>
    </edge>
    <edge source="LINEARITY" target="LEAST_SQUARES_ESTIMATION">
      <data key="d4">1.0</data>
      <data key="d5">Least squares estimation can be used to fit models even when the systematic component is not strictly linear</data>
      <data key="d6">6616e10c85e86291147e72776854b8a2</data>
    </edge>
    <edge source="LINEARITY" target="SIMPLE_LINEAR_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Linearity is a property of the relationship between the response and the explanatory variable in the simple linear regression model</data>
      <data key="d6">25fce1af816975003128126b5cfea73b</data>
    </edge>
    <edge source="LINEARITY" target="Y">
      <data key="d4">1.0</data>
      <data key="d5">The linearity assumption applies to the relationship between the response variable Y and the explanatory variables X</data>
      <data key="d6">3cbe71f7649e84cd67cb3fa0d3e632cf</data>
    </edge>
    <edge source="LINEARITY" target="RESIDUAL_PLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Residual plots can be used to check for violations of the linearity assumption in a statistical model</data>
      <data key="d6">7cd6069e88e81548a237fa937adfecc6</data>
    </edge>
    <edge source="LINEARITY" target="NON_LINEAR_TRANSFORMATIONS">
      <data key="d4">1.0</data>
      <data key="d5">Non-linear transformations can help meet the assumption of linearity</data>
      <data key="d6">c03eb12d07d48f9e94260f08dae10cdf</data>
    </edge>
    <edge source="POLYNOMIAL_REGRESSION" target="SIMPLE_LINEAR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Polynomial regression is an extension of simple linear models, allowing for more complex relationships between variables</data>
      <data key="d6">b8ec334f8c87bf1d9cb6043fa1a64214</data>
    </edge>
    <edge source="POLYNOMIAL_REGRESSION" target="LINEAR_MODELS">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis and algorithmic modeling, POLYNOMIAL_REGRESSION and LINEAR_MODELS are closely related concepts. Linear models are fundamental statistical tools used to analyze the relationship between a response variable and one or more explanatory variables. When the relationship between these variables is not linear but can be described by a polynomial function, linear models are extended to incorporate polynomial regression. This extension allows for the modeling of non-linear relationships by including polynomials of the explanatory variables. In essence, POLYNOMIAL_REGRESSION is a specific subtype of LINEAR_MODELS, designed to capture more complex patterns in data where a straight-line relationship does not adequately represent the underlying dynamics.</data>
      <data key="d6">0328e428a30c44572676dd571dd1e9bd,87ba4f416a28aabc3b396908f5913b54</data>
    </edge>
    <edge source="POLYNOMIAL_REGRESSION" target="QUADRATIC_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Quadratic regression is a specific case of polynomial regression where the model includes a squared term of the independent variable</data>
      <data key="d6">87ba4f416a28aabc3b396908f5913b54</data>
    </edge>
    <edge source="POLYNOMIAL_REGRESSION" target="PLR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">plr.model is an instance of polynomial regression, specifically a quadratic regression model</data>
      <data key="d6">084dadebfca8bcb6377c205c45bee295</data>
    </edge>
    <edge source="POLYNOMIAL_REGRESSION" target="BETA1">
      <data key="d4">1.0</data>
      <data key="d5">Beta1 is a parameter in the polynomial regression model</data>
      <data key="d6">11452a08471d93959558de2ece9a69af</data>
    </edge>
    <edge source="POLYNOMIAL_REGRESSION" target="BETA2">
      <data key="d4">1.0</data>
      <data key="d5">Beta2 is a parameter in the polynomial regression model</data>
      <data key="d6">11452a08471d93959558de2ece9a69af</data>
    </edge>
    <edge source="POLYNOMIAL_REGRESSION" target="LINEAR_MODEL">
      <data key="d4">2.0</data>
      <data key="d5">The POLYNOMIAL_REGRESSION model, a variant of the LINEAR_MODEL family, is characterized by its unique approach to modeling relationships between a response variable and one or more predictor variables. Unlike simple linear models that assume a straight-line relationship, POLYNOMIAL_REGRESSION incorporates higher-degree polynomials of the explanatory variables, allowing for the representation of more complex, non-linear relationships. Despite the non-linear nature of these relationships, the model remains linear in the parameters, meaning that the coefficients associated with the polynomial terms are estimated using linear methods. This feature enables the model to maintain the benefits of linear models, such as simplicity and interpretability, while also capturing the nuances of non-linear data patterns.</data>
      <data key="d6">188219b9e5b6b6368360840921877de9,b9af17718641389ba07f53be13f31f8c</data>
    </edge>
    <edge source="LOG_TRANSFORMED_PREDICTOR" target="LINEAR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The log-transformed predictor is used in the linear model</data>
      <data key="d6">656dce234514b9db38b5b5616557c1e9</data>
    </edge>
    <edge source="GOOD_MODEL" target="MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The criteria for a good model apply to the model</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="RESIDUAL_ANALYSIS" target="LINEAR_MODELS">
      <data key="d4">1.0</data>
      <data key="d5">Residual analysis is a critical step in assessing the adequacy of model assumptions in linear models</data>
      <data key="d6">0328e428a30c44572676dd571dd1e9bd</data>
    </edge>
    <edge source="RESIDUAL_ANALYSIS" target="RESIDUAL_PLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Residual plots are a tool used in residual analysis to visually inspect the residuals for patterns that might indicate violations of model assumptions</data>
      <data key="d6">0328e428a30c44572676dd571dd1e9bd</data>
    </edge>
    <edge source="RESIDUAL_ANALYSIS" target="Q_Q_PLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Q-Q plots are used in residual analysis to compare the distribution of the residuals to a theoretical distribution, helping to assess the normality of the residuals</data>
      <data key="d6">0328e428a30c44572676dd571dd1e9bd</data>
    </edge>
    <edge source="RESIDUAL_ANALYSIS" target="NULL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">Residual analysis is used to create a null plot, which helps in assessing whether the model assumptions are satisfied</data>
      <data key="d6">aa13c33a7e61206e6021e2736002ca9a</data>
    </edge>
    <edge source="RESIDUAL_ANALYSIS" target="Q_Q_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">Q-Q plots are used in residual analysis to check the normality assumption of the errors in a linear model. Deviations from the straight line indicate departures from normality.</data>
      <data key="d6">3bfc9b92571973e54c8095302acc1aaa</data>
    </edge>
    <edge source="RESIDUAL_ANALYSIS" target="LINEAR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Residual analysis is applied to linear models to check the assumptions of linearity, homoscedasticity, independence, and normality of errors. It helps to evaluate the validity of the model.</data>
      <data key="d6">3bfc9b92571973e54c8095302acc1aaa</data>
    </edge>
    <edge source="CHAPTER 2" target="RESIDUAL ANALYSIS">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 2 is summarized before the discussion of residual analysis</data>
      <data key="d6">752d1285c8b1e15a2e175515f77ddd5a</data>
    </edge>
    <edge source="RESIDUAL ANALYSIS" target="DEFINITIONS">
      <data key="d4">1.0</data>
      <data key="d5">Residual analysis includes definitions provided in Subsection 3.1</data>
      <data key="d6">752d1285c8b1e15a2e175515f77ddd5a</data>
    </edge>
    <edge source="RESIDUAL ANALYSIS" target="MODEL ASSUMPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">Residual analysis includes a discussion of model assumptions in Subsection 3.2</data>
      <data key="d6">752d1285c8b1e15a2e175515f77ddd5a</data>
    </edge>
    <edge source="RESIDUAL ANALYSIS" target="RESIDUAL PLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Residual analysis includes an explanation of residual plots in Subsection 3.3</data>
      <data key="d6">752d1285c8b1e15a2e175515f77ddd5a</data>
    </edge>
    <edge source="RESIDUAL ANALYSIS" target="ASSESSING NORMALITY">
      <data key="d4">1.0</data>
      <data key="d5">Residual analysis includes methods for assessing normality in Subsection 3.4</data>
      <data key="d6">752d1285c8b1e15a2e175515f77ddd5a</data>
    </edge>
    <edge source="RESIDUAL ANALYSIS" target="SUMMARY OF CHAPTER 3">
      <data key="d4">1.0</data>
      <data key="d5">Residual analysis is followed by a summary of Chapter 3 in Subsection 3.5</data>
      <data key="d6">752d1285c8b1e15a2e175515f77ddd5a</data>
    </edge>
    <edge source="NON-LINEAR TRANSFORMATIONS" target="THE LOG-TRANSFORMATION">
      <data key="d4">1.0</data>
      <data key="d5">Non-linear transformations include the log-transformation discussed in Subsection 4.1</data>
      <data key="d6">752d1285c8b1e15a2e175515f77ddd5a</data>
    </edge>
    <edge source="CHAPTER_3" target="NON_LINEAR_TRANSFORMATIONS">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 3 is likely discussed before Chapter 4, which covers non-linear transformations, indicating a possible progression in the document's structure</data>
      <data key="d6">9846990771550ccdb865e49ecb96e2a3</data>
    </edge>
    <edge source="NON_LINEAR_TRANSFORMATIONS" target="LOG_TRANSFORMATION">
      <data key="d4">1.0</data>
      <data key="d5">Non-linear transformations include the log-transformation, which is a specific method for changing the scale of data</data>
      <data key="d6">9846990771550ccdb865e49ecb96e2a3</data>
    </edge>
    <edge source="NON_LINEAR_TRANSFORMATIONS" target="MAMMALS_DATASET">
      <data key="d4">1.0</data>
      <data key="d5">Non-linear transformations are likely applied to the mammals dataset as an example of their use in data analysis</data>
      <data key="d6">9846990771550ccdb865e49ecb96e2a3</data>
    </edge>
    <edge source="NON_LINEAR_TRANSFORMATIONS" target="TREES_DATASET">
      <data key="d4">1.0</data>
      <data key="d5">Non-linear transformations are likely applied to the trees dataset as an example of their use in data analysis</data>
      <data key="d6">9846990771550ccdb865e49ecb96e2a3</data>
    </edge>
    <edge source="NON_LINEAR_TRANSFORMATIONS" target="CHAPTER_4">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 4 discusses non-linear transformations, including specific examples and datasets</data>
      <data key="d6">9846990771550ccdb865e49ecb96e2a3</data>
    </edge>
    <edge source="NON_LINEAR_TRANSFORMATIONS" target="REMEDIAL_ACTIONS">
      <data key="d4">1.0</data>
      <data key="d5">Non-linear transformations are one of the remedial actions that can be taken to address violations of the modelling assumptions</data>
      <data key="d6">e361ac139c268d5c3f3623f920e68af2</data>
    </edge>
    <edge source="NON_LINEAR_TRANSFORMATIONS" target="MODEL_ASSUMPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">Non-linear transformations can be used to address violations of the model assumptions</data>
      <data key="d6">c03eb12d07d48f9e94260f08dae10cdf</data>
    </edge>
    <edge source="NON_LINEAR_TRANSFORMATIONS" target="HETEROSCEDASTICITY">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, NON_LINEAR_TRANSFORMATIONS play a crucial role in addressing issues related to HETEROSCEDASTICITY within regression models. Heteroscedasticity refers to the condition where the variance of the error terms, or residuals, is not constant across all levels of the independent variables. This violation of one of the key assumptions of linear regression models can lead to inefficient and biased estimates of the regression coefficients, potentially undermining the validity of statistical inferences.

To mitigate the effects of heteroscedasticity, NON_LINEAR_TRANSFORMATIONS are employed. These transformations, which can include logarithmic, square root, or power transformations, among others, are applied to the dependent variable or the independent variables in the regression model. The purpose of these transformations is to stabilize the variance of the error terms, thereby improving the model's adherence to the assumption of homoscedasticity. By doing so, the accuracy and reliability of the regression analysis are enhanced, allowing for more robust conclusions to be drawn about the relationships between the variables under study.</data>
      <data key="d6">c03eb12d07d48f9e94260f08dae10cdf,d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="NON_LINEAR_TRANSFORMATIONS" target="UNUSUAL_OBSERVATIONS">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, particularly within regression models, NON_LINEAR_TRANSFORMATIONS play a crucial role. These transformations are applied to the data to mitigate the impact of UNUSUAL_OBSERVATIONS, which can significantly skew the results of linear models. By adjusting the scale or distribution of the data, non-linear transformations help in ensuring that the regression analysis is not disproportionately influenced by outliers or extreme values. This process enhances the robustness and accuracy of the model, making it more reliable for predictive and inferential purposes.</data>
      <data key="d6">c03eb12d07d48f9e94260f08dae10cdf,d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="NON_LINEAR_TRANSFORMATIONS" target="LOG_TRANSFORM">
      <data key="d4">1.0</data>
      <data key="d5">The log-transform is a type of non-linear transformation</data>
      <data key="d6">c03eb12d07d48f9e94260f08dae10cdf</data>
    </edge>
    <edge source="NON_LINEAR_TRANSFORMATIONS" target="DUNN_AND_SMYTH_BOOK">
      <data key="d4">1.0</data>
      <data key="d5">The book by Dunn and Smyth discusses the use of non-linear transformations in regression models</data>
      <data key="d6">d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="NON_LINEAR_TRANSFORMATIONS" target="LINEAR_MODELS">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, NON_LINEAR_TRANSFORMATIONS play a crucial role in enhancing the effectiveness of LINEAR_MODELS. These transformations are applied to address violations of model assumptions, specifically non-linearity and heteroscedasticity, which are common issues in data that can lead to inaccurate model predictions. By adjusting the data through non-linear transformations, the models can better meet their underlying assumptions, ensuring more reliable and robust results. Additionally, these transformations help in reducing the undue influence of unusual observations, thereby improving the overall stability and generalizability of the linear models. Through the strategic use of non-linear transformations, analysts can refine LINEAR_MODELS to better capture the complex relationships within the data, leading to more insightful and accurate statistical inferences.</data>
      <data key="d6">07951ffe6787af44aa60c90c69e62f83,d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="NON_LINEAR_TRANSFORMATIONS" target="LOG_TRANSFORMED_VARIABLES">
      <data key="d4">1.0</data>
      <data key="d5">Non-linear transformations often involve log-transformed variables to address model assumptions</data>
      <data key="d6">d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="LOG_TRANSFORMATION" target="VARIANCE">
      <data key="d4">1.0</data>
      <data key="d5">Log-transforming the response may stabilise the variance, making it more consistent across different levels of the mean</data>
      <data key="d6">66f7fae9d896ff2b3fd40186cc833503</data>
    </edge>
    <edge source="LOG_TRANSFORMATION" target="RIGHT_SKEWED_DISTRIBUTION">
      <data key="d4">1.0</data>
      <data key="d5">Log-transforming the variable tends to make the distribution more symmetrical, especially when the variable has a right skewed distribution</data>
      <data key="d6">66f7fae9d896ff2b3fd40186cc833503</data>
    </edge>
    <edge source="LOG_TRANSFORMATION" target="POSITIVE_RESPONSE_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">Applying a log-transformation to a positive response variable ensures that predictions from the model will always be positive</data>
      <data key="d6">66f7fae9d896ff2b3fd40186cc833503</data>
    </edge>
    <edge source="LOG_TRANSFORMATION" target="HETEROSCEDASTICITY">
      <data key="d4">1.0</data>
      <data key="d5">Log transformation is a specific type of transformation that can be used to address heteroscedasticity in linear models</data>
      <data key="d6">07951ffe6787af44aa60c90c69e62f83</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="CHAPTER_4">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 4 likely uses the mammals dataset as an example to illustrate the application of non-linear transformations</data>
      <data key="d6">9846990771550ccdb865e49ecb96e2a3</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="BODY_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">The mammals dataset contains the average body weight in kg for 62 species of land mammal</data>
      <data key="d6">66f7fae9d896ff2b3fd40186cc833503</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="BRAIN_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">The mammals dataset contains the average brain weight in g for 62 species of land mammal</data>
      <data key="d6">66f7fae9d896ff2b3fd40186cc833503</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="SCATTERPLOT">
      <data key="d4">2.0</data>
      <data key="d5">The scatterplot, derived from the MAMMALS_DATASET, serves as a graphical representation to explore the relationship between the variables BRAIN and BODY. This visual tool allows for the identification of patterns, trends, and potential correlations within the data, providing insights into how the brain size of mammals relates to their body size. The scatterplot facilitates a deeper understanding of the structure and relations within the mammalian community by illustrating the distribution of data points and any discernible structure that may exist between these two variables.</data>
      <data key="d6">66f7fae9d896ff2b3fd40186cc833503,f9d6c3504b8f8b5c25550076e45f8270</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="AFRICAN_ELEPHANT">
      <data key="d4">2.0</data>
      <data key="d5">The MAMMALS_DATASET, a comprehensive collection of data on various mammalian species, prominently features the AFRICAN_ELEPHANT. This majestic creature stands out within the dataset due to its remarkably high body weight and brain weight, both of which are significantly above average when compared to other species in the dataset. The African elephant's substantial size and brain mass are notable characteristics that contribute to its unique position within the mammalian community, reflecting its evolutionary adaptations and ecological role in its natural habitat.</data>
      <data key="d6">66f7fae9d896ff2b3fd40186cc833503,f9d6c3504b8f8b5c25550076e45f8270</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="ASIAN_ELEPHANT">
      <data key="d4">2.0</data>
      <data key="d5">The MAMMALS_DATASET, a comprehensive collection of data on various mammalian species, prominently features the ASIAN_ELEPHANT. This majestic creature stands out within the dataset due to its remarkably high body weight and brain weight, both of which are significantly above average when compared to other species in the dataset. The ASIAN_ELEPHANT's substantial size and brain mass highlight its unique position in the mammalian community, reflecting its evolutionary adaptations and ecological role in its natural habitat.</data>
      <data key="d6">66f7fae9d896ff2b3fd40186cc833503,f9d6c3504b8f8b5c25550076e45f8270</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="HUMAN">
      <data key="d4">2.0</data>
      <data key="d5">The MAMMALS_DATASET, a comprehensive collection of various species, notably includes the HUMAN species. Humans within this dataset are characterized by their average body weight, which is not particularly distinctive among mammals. However, what sets humans apart is their unusually high brain weight relative to their body size, a feature that significantly contributes to their cognitive capabilities and distinguishes them from other species in the dataset. Despite having an average body weight, the high brain weight in humans makes them a standout species within the MAMMALS_DATASET, highlighting the unique evolutionary adaptations that have led to their advanced intelligence and complex social structures.</data>
      <data key="d6">66f7fae9d896ff2b3fd40186cc833503,f9d6c3504b8f8b5c25550076e45f8270</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="FIGURE_4_1">
      <data key="d4">1.0</data>
      <data key="d5">Figure 4.1 is a visual representation of the mammals dataset, specifically showing the scatterplot of brain weight against body weight</data>
      <data key="d6">f9d6c3504b8f8b5c25550076e45f8270</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="LOG_TRANSFORM">
      <data key="d4">1.0</data>
      <data key="d5">Log transformation is applied to the body weight variable in the mammals dataset to reduce skewness and the influence of unusual observations</data>
      <data key="d6">f9d6c3504b8f8b5c25550076e45f8270</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="FIGURE_4_2">
      <data key="d4">1.0</data>
      <data key="d5">Figure 4.2 is a visual representation of the mammals dataset, specifically showing the histograms of body weight and log(body weight)</data>
      <data key="d6">f9d6c3504b8f8b5c25550076e45f8270</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="LOG_BRAIN_WEIGHT">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the MAMMALS_DATASET, Log Brain Weight, specifically defined as the logarithm of the average brain weight, serves as a significant variable. This variable is prominently utilized in regression analysis, highlighting its importance in understanding the relationships and patterns within the dataset. The transformation of brain weight into its logarithmic form not only simplifies the analysis but also helps in achieving a more linear relationship with other variables, making it a crucial component in statistical modeling.</data>
      <data key="d6">bd05fe6a05f9a13d33c4f1b5a771ada5,c47968226557bc2eb5aec5bb7994fd0e</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="LOG_BODY_WEIGHT">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the MAMMALS_DATASET, Log body weight, specifically defined as the logarithm of the average body weight, serves as a significant variable. This variable is prominently utilized in regression analysis, highlighting its importance in understanding the relationships and patterns within the dataset. The transformation of body weight into its logarithmic form not only simplifies the analysis but also helps in stabilizing the variance and achieving a more linear relationship with other variables in the dataset. This approach is particularly beneficial in statistical modeling, where the goal is to identify and interpret the underlying structure and relationships within the community of interest, in this case, mammals.</data>
      <data key="d6">bd05fe6a05f9a13d33c4f1b5a771ada5,c47968226557bc2eb5aec5bb7994fd0e</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">The mammals dataset is used to fit a regression model of the logarithm of average brain weight on the logarithm of average body weight.</data>
      <data key="d6">f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="MASS_PACKAGE">
      <data key="d4">1.0</data>
      <data key="d5">The mammals dataset is part of the MASS package, which provides the data for the regression analysis.</data>
      <data key="d6">f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="HUMAN_OBSERVATION">
      <data key="d4">1.0</data>
      <data key="d5">Human observation is part of the mammals dataset</data>
      <data key="d6">c47968226557bc2eb5aec5bb7994fd0e</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="WATER_OPOSSUM_OBSERVATION">
      <data key="d4">1.0</data>
      <data key="d5">Water opposum observation is part of the mammals dataset</data>
      <data key="d6">c47968226557bc2eb5aec5bb7994fd0e</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="RHESUS_MONKEY_OBSERVATION">
      <data key="d4">1.0</data>
      <data key="d5">Rhesus monkey observation is part of the mammals dataset</data>
      <data key="d6">c47968226557bc2eb5aec5bb7994fd0e</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="RESIDUALS_VS_LEVERAGE_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The residuals vs leverage plot is applied to the mammals dataset to identify influential data points</data>
      <data key="d6">9e2ebbb113c00fa43f0af3c0696baf95</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="ELEPHANT_SPECIES">
      <data key="d4">1.0</data>
      <data key="d5">The mammals dataset includes elephant species, which can be highly influential</data>
      <data key="d6">83bb91cf725e5116ca2f5748fddccfae</data>
    </edge>
    <edge source="MAMMALS_DATASET" target="LOG_TRANSFORMED_VARIABLES">
      <data key="d4">1.0</data>
      <data key="d5">Log-transformed variables can be used in the mammals dataset to reduce the influence of elephant species</data>
      <data key="d6">83bb91cf725e5116ca2f5748fddccfae</data>
    </edge>
    <edge source="TREES_DATASET" target="CHAPTER_4">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 4 likely uses the trees dataset as an example to illustrate the application of non-linear transformations</data>
      <data key="d6">9846990771550ccdb865e49ecb96e2a3</data>
    </edge>
    <edge source="TREES_DATASET" target="HEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Height is a variable in the trees dataset used for predicting the volume of timber</data>
      <data key="d6">efeeb664622c1ee594e6a08a8322ffe3</data>
    </edge>
    <edge source="TREES_DATASET" target="GIRTH">
      <data key="d4">1.0</data>
      <data key="d5">Girth is a variable in the trees dataset used for predicting the volume of timber</data>
      <data key="d6">efeeb664622c1ee594e6a08a8322ffe3</data>
    </edge>
    <edge source="TREES_DATASET" target="VOLUME">
      <data key="d4">1.0</data>
      <data key="d5">Volume is the dependent variable in the trees dataset, to be predicted from height and girth</data>
      <data key="d6">efeeb664622c1ee594e6a08a8322ffe3</data>
    </edge>
    <edge source="TREES_DATASET" target="LOGTREES">
      <data key="d4">1.0</data>
      <data key="d5">The trees dataset is transformed into the logTrees data frame by applying a logarithmic transformation to the variables</data>
      <data key="d6">a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="TREES_DATASET" target="LOG_VOLUME">
      <data key="d4">1.0</data>
      <data key="d5">The trees dataset contains the dependent variable log-volume</data>
      <data key="d6">d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="TREES_DATASET" target="LOG_DIAMETER">
      <data key="d4">1.0</data>
      <data key="d5">The trees dataset contains the independent variable log-diameter</data>
      <data key="d6">d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="TREES_DATASET" target="LOG_HEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">The trees dataset contains the independent variable log-height</data>
      <data key="d6">d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATION" target="CHAPTER_5">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 5 discusses least squares estimation, which is a method for estimating model parameters</data>
      <data key="d6">9846990771550ccdb865e49ecb96e2a3</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATION" target="LEAST_SQUARES_ESTIMATE">
      <data key="d4">1.0</data>
      <data key="d5">Least squares estimation involves calculating the least squares estimate, which is a specific method for estimating model parameters</data>
      <data key="d6">9846990771550ccdb865e49ecb96e2a3</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATION" target="SIMPLE_LINEAR_REGRESSION">
      <data key="d4">2.0</data>
      <data key="d5">Least squares estimation, a pivotal technique in statistical analysis, is employed in simple linear regression to determine the line of best fit. This method aims to minimize the sum of the squares of the residuals, thereby providing an optimal model that closely represents the relationship between the variables under study. Simple linear regression, as a statistical tool, is specifically designed to model the relationship between two variables, where one variable is dependent and the other is independent. The use of least squares estimation in this context ensures that the model accurately reflects the data, making it a reliable method for predictive analysis and understanding the underlying structure of the relationship between the variables.</data>
      <data key="d6">9846990771550ccdb865e49ecb96e2a3,cf6e59c3746d399dc8baf5064f78ac57</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATION" target="DERIVATION_OF_LSE">
      <data key="d4">1.0</data>
      <data key="d5">The least squares estimate is derived through a process that involves calculus and linear algebra, which is explained in the derivation of the LSE</data>
      <data key="d6">cf6e59c3746d399dc8baf5064f78ac57</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATION" target="REGRESSION_PLANE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, LEAST_SQUARES_ESTIMATION is a method employed to determine the REGRESSION_PLANE by minimizing the sum of squared distances along the z-axis. This process, known as least squares estimation, effectively reduces the sum of squared residuals, providing an optimal fit for the regression plane. The regression plane, in turn, is the result of this least squares estimation, ensuring that the model accurately represents the data by minimizing the discrepancies between observed and predicted values.</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376,b1690cb1a67892245c0665e5099e322d</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATION" target="DESIGN_MATRIX">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis, LEAST_SQUARES_ESTIMATION is a method employed to estimate the parameter vector (\u03b2) in a linear model. This estimation process heavily relies on the DESIGN_MATRIX (X), which plays a crucial role in the algorithm. The design matrix is a fundamental component in least squares estimation, as it is used in conjunction with the algorithm to fit models and derive accurate parameter estimates. The LEAST_SQUARES_ESTIMATION technique utilizes the information contained within the DESIGN_MATRIX to minimize the sum of the squared residuals, thereby providing the best fit for the linear model under consideration.</data>
      <data key="d6">6616e10c85e86291147e72776854b8a2,9f335f1ecb85a1427df926df8bb1e89f,b9af17718641389ba07f53be13f31f8c</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATION" target="PARAMETER_VECTOR">
      <data key="d4">4.0</data>
      <data key="d5">In the context of statistical analysis, LEAST_SQUARES_ESTIMATION is a pivotal method employed to estimate the PARAMETER_VECTOR in linear models. This estimation process is grounded in the minimization of the sum of the squares of the residuals, effectively reducing the discrepancies between the observed data points and the values predicted by the model. The PARAMETER_VECTOR, denoted as &#946;, is a crucial component in this algorithm, serving as the set of coefficients that the least squares estimation algorithm optimizes. By fitting models through this technique, the method ensures that the parameter vector is adjusted to best represent the relationship between the variables as described by the design matrix and the observed data. This approach is widely utilized in various fields for its ability to provide robust and accurate estimations of model parameters.</data>
      <data key="d6">07951ffe6787af44aa60c90c69e62f83,6616e10c85e86291147e72776854b8a2,9f335f1ecb85a1427df926df8bb1e89f,b9af17718641389ba07f53be13f31f8c</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATION" target="SYSTEMATIC_COMPONENT">
      <data key="d4">1.0</data>
      <data key="d5">Least squares estimation fits models where the systematic component is linear in the parameters</data>
      <data key="d6">9f335f1ecb85a1427df926df8bb1e89f</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATION" target="LINEAR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Least squares estimation is used to fit linear models</data>
      <data key="d6">9f335f1ecb85a1427df926df8bb1e89f</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATION" target="QUADRATIC_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Least squares estimation is used in quadratic regression to determine the parameters that minimize the sum of squared distances between the observations and the parabola</data>
      <data key="d6">87ba4f416a28aabc3b396908f5913b54</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATION" target="PARAMETERS">
      <data key="d4">1.0</data>
      <data key="d5">Least squares estimation is a method for determining the values of parameters (such as &#946;0, &#946;1, and &#946;2) in a statistical model that best fit the data</data>
      <data key="d6">87ba4f416a28aabc3b396908f5913b54</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATION" target="MODEL_REPARAMETERISATIONS">
      <data key="d4">1.0</data>
      <data key="d5">Least squares estimation provides the mathematical framework for model reparameterisations in linear models</data>
      <data key="d6">07951ffe6787af44aa60c90c69e62f83</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATE" target="EXERCISE_22A">
      <data key="d4">1.0</data>
      <data key="d5">Exercise 22a introduced the model for which the least squares estimate is given</data>
      <data key="d6">096afa471635bc59c3bfa9af4d04d625</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATE" target="MU">
      <data key="d4">1.0</data>
      <data key="d5">The least squares estimate is for the parameter &#181; = (&#181;A, &#181;B, &#181;C)</data>
      <data key="d6">096afa471635bc59c3bfa9af4d04d625</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATE" target="SA">
      <data key="d4">1.0</data>
      <data key="d5">SA is used in the calculation of the least squares estimate for &#181;A</data>
      <data key="d6">096afa471635bc59c3bfa9af4d04d625</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATE" target="SB">
      <data key="d4">1.0</data>
      <data key="d5">SB is used in the calculation of the least squares estimate for &#181;B</data>
      <data key="d6">096afa471635bc59c3bfa9af4d04d625</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATE" target="SC">
      <data key="d4">1.0</data>
      <data key="d5">SC is used in the calculation of the least squares estimate for &#181;C</data>
      <data key="d6">096afa471635bc59c3bfa9af4d04d625</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATE" target="NA">
      <data key="d4">1.0</data>
      <data key="d5">nA is used in the calculation of the least squares estimate for &#181;A</data>
      <data key="d6">096afa471635bc59c3bfa9af4d04d625</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATE" target="NB">
      <data key="d4">1.0</data>
      <data key="d5">nB is used in the calculation of the least squares estimate for &#181;B</data>
      <data key="d6">096afa471635bc59c3bfa9af4d04d625</data>
    </edge>
    <edge source="LEAST_SQUARES_ESTIMATE" target="NC">
      <data key="d4">1.0</data>
      <data key="d5">nC is used in the calculation of the least squares estimate for &#181;C</data>
      <data key="d6">096afa471635bc59c3bfa9af4d04d625</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="DERIVATION_OF_LSE">
      <data key="d4">1.0</data>
      <data key="d5">The derivation of the least squares estimate is relevant to simple linear regression, as it is used to find the parameters of the model</data>
      <data key="d6">cf6e59c3746d399dc8baf5064f78ac57</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="SUMMARY_OF_CHAPTER_5">
      <data key="d4">1.0</data>
      <data key="d5">Simple linear regression is a topic covered in the summary of Chapter 5, which provides an overview of the key points related to least squares estimation</data>
      <data key="d6">cf6e59c3746d399dc8baf5064f78ac57</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="ST117">
      <data key="d4">2.0</data>
      <data key="d5">ST117, a comprehensive statistics course, introduced the concept of SIMPLE_LINEAR_REGRESSION as an integral part of its curriculum. SIMPLE_LINEAR_REGRESSION, also known as the simple linear regression model, represents the simplest form of a linear model. This model is designed to analyze the relationship between a single independent variable and a dependent variable, providing a foundational understanding of how variables are related in a linear context. Through ST117, students gain insights into the application of SIMPLE_LINEAR_REGRESSION, learning how to estimate parameters using least squares estimators and conduct residual analysis to assess the model's fit and reliability. This introduction to SIMPLE_LINEAR_REGRESSION in ST117 serves as a stepping stone for more complex statistical analyses, equipping students with the necessary skills to understand and interpret data in various fields.</data>
      <data key="d6">28eee75e95bbbaf143368c3289585670,7b32c106246576bb451a5a3985914351</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="ST121">
      <data key="d4">2.0</data>
      <data key="d5">ST121, a course in the curriculum, introduced the concept of SIMPLE_LINEAR_REGRESSION, specifically focusing on the simple linear regression model. This model is noted as the simplest form of a linear model, providing foundational knowledge in statistical analysis and regression techniques. Through ST121, learners gain an understanding of how SIMPLE_LINEAR_REGRESSION operates, enabling them to analyze and predict outcomes based on a single independent variable.</data>
      <data key="d6">28eee75e95bbbaf143368c3289585670,7b32c106246576bb451a5a3985914351</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="LINEAR_ALGEBRA">
      <data key="d4">1.0</data>
      <data key="d5">Simple linear regression models can be understood and analyzed using tools from linear algebra</data>
      <data key="d6">7b32c106246576bb451a5a3985914351</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="LINEAR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">A simple linear regression is a specific type of linear model</data>
      <data key="d6">28eee75e95bbbaf143368c3289585670</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="R">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, R, a powerful programming language and software environment for data analysis and graphics, is extensively utilized to fit simple linear regression models. The simple linear regression model, a fundamental statistical tool, is adept at exploring the relationship between a single dependent variable and one independent variable. R offers a variety of functions specifically designed to facilitate the fitting of these models, enabling users to estimate parameters, conduct hypothesis tests, and perform residual analysis with ease. This capability of R makes it an indispensable tool for data scientists and statisticians seeking to understand and predict the linear relationship between variables in various fields such as economics, biology, and social sciences.</data>
      <data key="d6">25fce1af816975003128126b5cfea73b,28eee75e95bbbaf143368c3289585670</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="DIAMOND_DATA">
      <data key="d4">1.0</data>
      <data key="d5">The diamond data can be analyzed using simple linear regression to understand the relationship between the price and weight of diamonds</data>
      <data key="d6">28eee75e95bbbaf143368c3289585670</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="OUTCOME_VARIABLE_Y">
      <data key="d4">1.0</data>
      <data key="d5">Simple linear regression involves the outcome variable Y, which is the response variable in the model</data>
      <data key="d6">e47d573a10e64a657e58218df64d8920</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="EXPLANATORY_VARIABLE_X">
      <data key="d4">1.0</data>
      <data key="d5">Simple linear regression involves the explanatory variable X, which is used to explain the variation in Y</data>
      <data key="d6">e47d573a10e64a657e58218df64d8920</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="STATISTICAL_INDEPENDENCE">
      <data key="d4">1.0</data>
      <data key="d5">Statistical independence is a concept that might be relevant when considering the assumptions of simple linear regression, particularly in the context of the independence of errors.</data>
      <data key="d6">4683e58cf41e5f5d415a63ddb2fe0cac</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="OBSERVATIONS">
      <data key="d4">1.0</data>
      <data key="d5">Simple linear regression uses a set of paired observations (x1, y1), (x2, y2), ..., (xn, yn) to model the relationship between the explanatory and response variables.</data>
      <data key="d6">4683e58cf41e5f5d415a63ddb2fe0cac</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="SOLITAIRE_RING_EXAMPLE">
      <data key="d4">1.0</data>
      <data key="d5">The Solitaire ring example illustrates the application of simple linear regression, where the price of the ring (yj) is the response variable and the weight of the diamond (xj) is the explanatory variable.</data>
      <data key="d6">4683e58cf41e5f5d415a63ddb2fe0cac</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="R2">
      <data key="d4">1.0</data>
      <data key="d5">R2 is a measure used in simple linear regression to assess the goodness of fit of the model</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="REGRESSION_PLANE">
      <data key="d4">1.0</data>
      <data key="d5">Simple linear regression is extended to a regression plane in R3 for multiple regression analysis</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="EPSILON">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the SIMPLE_LINEAR_REGRESSION model, the error term, denoted as EPSILON (&#949;), plays a crucial role. It signifies the unexplained variation in the dependent variable Y, essentially capturing the portion of the variability in Y that cannot be attributed to the explanatory variables in the model. More specifically, the error term &#949;j for each observation j, represents the deviation of the observed Y values from their expected values, highlighting the difference between the actual data points and the values predicted by the regression line. This discrepancy is an inherent component of the model, reflecting the randomness and unpredictability in the data that the linear relationship does not account for. Understanding and analyzing the error terms is essential for assessing the goodness of fit of the regression model and for making accurate predictions.</data>
      <data key="d6">512d9ffebe309a6f944ebce1ae2ff2a3,74a5a0e8ae0f846240c782cc1a30f82f</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="BETA_0">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the SIMPLE_LINEAR_REGRESSION model, BETA_0 (\u03b20) serves as a crucial parameter known as the intercept. This parameter is pivotal as it specifies the value of the dependent variable Y when all predictor variables, particularly X, are at a value of zero. BETA_0 represents the baseline or starting point of the regression line on the Y-axis, providing the expected value of Y in the absence of any influence from the predictor variables. This foundational component of the model helps in understanding the behavior of Y independently of X, offering insights into the inherent characteristics or conditions of the system being studied when no predictors are active.</data>
      <data key="d6">512d9ffebe309a6f944ebce1ae2ff2a3,74a5a0e8ae0f846240c782cc1a30f82f</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="BETA_1">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, the SIMPLE_LINEAR_REGRESSION is a fundamental technique used to analyze the relationship between two continuous variables, where one variable is the predictor (X) and the other is the response (Y). BETA_1, denoted as \u03b21, plays a crucial role in this model as it represents the slope parameter. This parameter quantifies the change in the response variable Y for a one-unit change in the predictor variable X, assuming all other variables are held constant. However, it's important to note that the interpretation of BETA_1 as the slope is specific to the simple linear regression model and does not directly apply when considering the relationship between X1 and Y in the presence of another predictor variable X2, as this would imply a more complex model, such as multiple linear regression. In the simple linear regression framework, BETA_1 is a key measure of the linear association between X and Y.</data>
      <data key="d6">512d9ffebe309a6f944ebce1ae2ff2a3,74a5a0e8ae0f846240c782cc1a30f82f</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="X1">
      <data key="d4">1.0</data>
      <data key="d5">X1 (price) is the predictor variable in the simple linear regression model</data>
      <data key="d6">512d9ffebe309a6f944ebce1ae2ff2a3</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="Y">
      <data key="d4">2.0</data>
      <data key="d5">In the context of simple linear regression, Y, specifically identified as sales volume, serves as the dependent variable. The values of Y are predicted based on the values of the independent variable X, illustrating the relationship between sales volume and another factor, X, within the model. This statistical technique allows for the analysis and prediction of sales volume given changes in the independent variable, providing insights into the structure and behavior of the community of interest.</data>
      <data key="d6">512d9ffebe309a6f944ebce1ae2ff2a3,74a5a0e8ae0f846240c782cc1a30f82f</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="PRICE">
      <data key="d4">1.0</data>
      <data key="d5">The simple linear regression model uses price as the only predictor variable to estimate the relationship between price and sales. The relationship strength is moderate, as the model only considers one variable.</data>
      <data key="d6">426434b67f6a287852ab66b82ca873cf</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="MULTIPLE_LINEAR_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">The simple linear regression model is a simplified version of the multiple linear regression model, which includes only price as a predictor variable. The relationship strength is moderate, as the multiple linear regression model provides a more comprehensive analysis by including additional variables.</data>
      <data key="d6">426434b67f6a287852ab66b82ca873cf</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="INTERCEPT">
      <data key="d4">1.0</data>
      <data key="d5">The intercept is a parameter in the simple linear regression model, but its interpretation is limited due to the dataset's range of price values. The relationship strength is low, as the intercept's relevance is restricted in this context.</data>
      <data key="d6">426434b67f6a287852ab66b82ca873cf</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="X">
      <data key="d4">1.0</data>
      <data key="d5">In simple linear regression, X is the independent variable whose values are used to predict the values of the dependent variable Y.</data>
      <data key="d6">74a5a0e8ae0f846240c782cc1a30f82f</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="SIGMA_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">&#963;^2 is the variance of the error term &#1013;j in the simple linear regression model, which determines the spread of the observed Y values around their expected values.</data>
      <data key="d6">74a5a0e8ae0f846240c782cc1a30f82f</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="HEIGHTS">
      <data key="d4">1.0</data>
      <data key="d5">Heights are the response variable in the simple linear regression model</data>
      <data key="d6">25fce1af816975003128126b5cfea73b</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="LOG2_DIAMETERS">
      <data key="d4">1.0</data>
      <data key="d5">Log2-diameters are the explanatory variable in the simple linear regression model</data>
      <data key="d6">25fce1af816975003128126b5cfea73b</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="NULL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">Simple linear regression models can produce null plots when the model assumptions are met</data>
      <data key="d6">aa13c33a7e61206e6021e2736002ca9a</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="TRANSFORM">
      <data key="d4">1.0</data>
      <data key="d5">A simple linear regression model can be improved by applying a transform to the explanatory variable</data>
      <data key="d6">15c7b5750483a382ce59751008e86751</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Simple linear regression is an example where the quantities used in the derivation of the least squares estimator are illustrated</data>
      <data key="d6">a4a817bb79d6ae8812c808ca41d47f43</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="LEVERAGES">
      <data key="d4">1.0</data>
      <data key="d5">The formula for calculating leverages in simple linear regression is provided in Exercise 21, which is relevant to understanding the influence of data points in regression analysis.</data>
      <data key="d6">f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="SALES_VOLUME">
      <data key="d4">1.0</data>
      <data key="d5">Simple linear regression is used to model the relationship between sales volume and price.</data>
      <data key="d6">e079b7c92d5c0b009ff02040eb652bc6</data>
    </edge>
    <edge source="SIMPLE_LINEAR_REGRESSION" target="BRAND">
      <data key="d4">1.0</data>
      <data key="d5">The Brand variable is not included in the simple linear regression model, which only considers the price variable</data>
      <data key="d6">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </edge>
    <edge source="CHAPTER_5" target="SUMMARY_OF_CHAPTER_5">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 5 is summarized in the Summary of Chapter 5</data>
      <data key="d6">9133320d451c1c1eddf1438064663b17</data>
    </edge>
    <edge source="INVERTIBLE_LINEAR_TRANSFORMATIONS" target="SUMMARY_OF_CHAPTER_5">
      <data key="d4">1.0</data>
      <data key="d5">Invertible linear transformations are discussed in the summary of Chapter 5, which includes various examples and applications of least squares estimation</data>
      <data key="d6">cf6e59c3746d399dc8baf5064f78ac57</data>
    </edge>
    <edge source="INVERTIBLE_LINEAR_TRANSFORMATIONS" target="BETA">
      <data key="d4">1.0</data>
      <data key="d5">Invertible linear transformations can change the interpretation of the parameter vector Beta</data>
      <data key="d6">255685e281cc5a9edf073c700f425a6b</data>
    </edge>
    <edge source="MAXIMUM_LIKELIHOOD_ESTIMATION" target="LIKELIHOOD_FUNCTION_OF_NORMAL_LINEAR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The likelihood function of a normal linear model is used in maximum likelihood estimation to find the parameters that maximize the likelihood of the observed data</data>
      <data key="d6">cf6e59c3746d399dc8baf5064f78ac57</data>
    </edge>
    <edge source="CHAPTER_6" target="LIKELIHOOD_FUNCTION">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 6 discusses the likelihood function of a normal linear model</data>
      <data key="d6">9133320d451c1c1eddf1438064663b17</data>
    </edge>
    <edge source="CHAPTER_6" target="MLE_PARAMETER_VECTOR">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 6 discusses the MLE for the parameter vector</data>
      <data key="d6">9133320d451c1c1eddf1438064663b17</data>
    </edge>
    <edge source="CHAPTER_6" target="SAMPLING_DISTRIBUTION_LEAST_SQUARES">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 6 discusses the sampling distribution of the least squares estimator</data>
      <data key="d6">9133320d451c1c1eddf1438064663b17</data>
    </edge>
    <edge source="CHAPTER_6" target="UNBIASED_ESTIMATOR_ERROR_VARIANCE">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 6 discusses the unbiased estimator for the error variance</data>
      <data key="d6">9133320d451c1c1eddf1438064663b17</data>
    </edge>
    <edge source="CHAPTER_6" target="SUMMARY_OF_CHAPTER_6">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 6 is summarized in the Summary of Chapter 6</data>
      <data key="d6">9133320d451c1c1eddf1438064663b17</data>
    </edge>
    <edge source="LIKELIHOOD_FUNCTION" target="MLE">
      <data key="d4">1.0</data>
      <data key="d5">The likelihood function is maximized to derive the MLE for the parameter vector &#946;</data>
      <data key="d6">d738df7d83784c8a41b3948271c537b6</data>
    </edge>
    <edge source="LIKELIHOOD_FUNCTION" target="MULTIVARIATE_NORMAL_DISTRIBUTION">
      <data key="d4">1.0</data>
      <data key="d5">The multivariate normal distribution is used to derive the likelihood function in the context of the normal linear model</data>
      <data key="d6">f483798b15ef305e7826fd7142379e03</data>
    </edge>
    <edge source="LIKELIHOOD_FUNCTION" target="Y">
      <data key="d4">1.0</data>
      <data key="d5">Y is the vector of observed values that the likelihood function is calculated for</data>
      <data key="d6">f483798b15ef305e7826fd7142379e03</data>
    </edge>
    <edge source="LIKELIHOOD_FUNCTION" target="BETA">
      <data key="d4">1.0</data>
      <data key="d5">Beta (&#946;) is a parameter in the normal linear model that the likelihood function is a function of</data>
      <data key="d6">f483798b15ef305e7826fd7142379e03</data>
    </edge>
    <edge source="LIKELIHOOD_FUNCTION" target="SIGMA_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">Sigma squared (&#963;^2) is a parameter in the normal linear model that the likelihood function is a function of</data>
      <data key="d6">f483798b15ef305e7826fd7142379e03</data>
    </edge>
    <edge source="LIKELIHOOD_FUNCTION" target="IN">
      <data key="d4">1.0</data>
      <data key="d5">In is part of the covariance matrix of the normal linear model that the likelihood function is derived from</data>
      <data key="d6">f483798b15ef305e7826fd7142379e03</data>
    </edge>
    <edge source="CHAPTER_7" target="HAT_MATRIX_PROPERTIES">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 7 discusses the properties of the hat matrix</data>
      <data key="d6">9133320d451c1c1eddf1438064663b17</data>
    </edge>
    <edge source="CHAPTER_7" target="RESIDUALS_FITTED_VALUES_PROPERTIES">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 7 discusses the properties of the residuals and of the fitted values</data>
      <data key="d6">9133320d451c1c1eddf1438064663b17</data>
    </edge>
    <edge source="HAT_MATRIX" target="PROPERTIES_HAT_MATRIX">
      <data key="d4">1.0</data>
      <data key="d5">The hat matrix has specific properties that are important for understanding its role in regression analysis</data>
      <data key="d6">1d52aaeb960f9787b6229e57738f8e47</data>
    </edge>
    <edge source="HAT_MATRIX" target="RESIDUALS">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, particularly within the realm of regression models, the HAT_MATRIX, denoted as H, plays a pivotal role in the computation of RESIDUALS, symbolized as b\u03f5. The hat matrix is a key component in the process of determining how well the regression model fits the data. Specifically, it is utilized in the calculation of residuals through the formula (In - H)y, where In represents the identity matrix. This formula helps in assessing the discrepancies between the observed data points and the values predicted by the model, providing insights into the model's accuracy and reliability. The residuals, therefore, serve as indicators of the model's performance, with the hat matrix facilitating their computation by quantifying the influence of each data point on the fitted values.</data>
      <data key="d6">1d52aaeb960f9787b6229e57738f8e47,3fb977ccba63e267d2e7dd4de6479ce1</data>
    </edge>
    <edge source="HAT_MATRIX" target="FITTED_VALUES">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, particularly within the realm of linear regression, the HAT_MATRIX, also known as the influence matrix, plays a pivotal role. This matrix, denoted as H, is a key component in the computation of FITTED_VALUES, or yb, which are the predicted values of the dependent variable based on the linear model. The HAT_MATRIX essentially maps the vector of observed values to the vector of fitted values, facilitating the understanding of how the model adjusts the data to fit the regression line. This process is fundamental in assessing the goodness of fit and the impact of individual data points on the regression model.</data>
      <data key="d6">1d52aaeb960f9787b6229e57738f8e47,3fb977ccba63e267d2e7dd4de6479ce1</data>
    </edge>
    <edge source="HAT_MATRIX" target="SUMMARY_CHAPTER_7">
      <data key="d4">1.0</data>
      <data key="d5">The hat matrix is a key concept discussed in the summary of Chapter 7</data>
      <data key="d6">1d52aaeb960f9787b6229e57738f8e47</data>
    </edge>
    <edge source="HAT_MATRIX" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">The hat matrix is used in the calculation of the least squares estimator Beta hat (&#946;b)</data>
      <data key="d6">9923e77ac6b3de95cb5026bc5e7fe8c0</data>
    </edge>
    <edge source="HAT_MATRIX" target="LINEAR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The hat matrix (H) is a key component in the linear model, used to project observed values to fitted values</data>
      <data key="d6">3fb977ccba63e267d2e7dd4de6479ce1</data>
    </edge>
    <edge source="HAT_MATRIX" target="YB">
      <data key="d4">1.0</data>
      <data key="d5">The hat matrix (H) is used to transform the observed response values (Y) into fitted values (Yb)</data>
      <data key="d6">6ee02b38ae842fd5eac9a11c4fd6659f</data>
    </edge>
    <edge source="HAT_MATRIX" target="LEVERAGE">
      <data key="d4">2.0</data>
      <data key="d5">The Hat Matrix, a pivotal concept in statistical analysis, particularly in the context of linear regression, is intricately associated with the calculation of leverages, often referred to as hat-values, for data points within a dataset. This matrix plays a crucial role in identifying the influence of individual observations on the fitted values in regression analysis. Specifically, the Hat Matrix enables the computation of the leverage (hii) for each observation, which quantifies the potential impact of a data point on the regression model's predictions. By examining these leverages, data scientists and statisticians can discern the structure and relations within the community of interest, pinpointing any potential outliers or influential points that may disproportionately affect the model's outcomes. This detailed analysis is fundamental in ensuring the robustness and reliability of statistical models, particularly when dealing with complex datasets.</data>
      <data key="d6">7e05f1b457a496c8b3630e7044fc5981,bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </edge>
    <edge source="HAT_MATRIX" target="LEVERAGES">
      <data key="d4">1.0</data>
      <data key="d5">Leverages are derived from the hat matrix, which is used in regression analysis to understand the influence of each data point on the fitted regression model.</data>
      <data key="d6">f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </edge>
    <edge source="HAT_MATRIX" target="ROBUST_REGRESSION">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, Robust Regression emerges as a sophisticated method for addressing the challenge of influential points within a dataset. Contrary to traditional regression techniques that heavily rely on the Hat Matrix to detect and potentially overemphasize the impact of outliers, Robust Regression offers a more nuanced approach. While it does incorporate the use of the Hat Matrix to identify influential points, its primary strength lies in its ability to mitigate the undue influence these points may have on the regression analysis. By doing so, Robust Regression ensures that the model's parameters are estimated in a way that is less sensitive to extreme values, thereby providing a more reliable and robust estimation of the relationships between variables. This method is particularly valuable in scenarios where the presence of outliers could skew the results of a standard regression analysis, making it a preferred choice for data scientists and statisticians seeking to enhance the robustness and accuracy of their models.</data>
      <data key="d6">7e05f1b457a496c8b3630e7044fc5981,83bb91cf725e5116ca2f5748fddccfae</data>
    </edge>
    <edge source="HAT_MATRIX" target="YBI">
      <data key="d4">1.0</data>
      <data key="d5">The hat matrix is used in the calculation of ybi, the estimated value of the response variable</data>
      <data key="d6">83bb91cf725e5116ca2f5748fddccfae</data>
    </edge>
    <edge source="HAT_MATRIX" target="COOKS_DISTANCE">
      <data key="d4">1.0</data>
      <data key="d5">The hat matrix is indirectly related to Cook's distance as it is used to calculate the leverage (hii) which is part of the formula for Cook's distance</data>
      <data key="d6">7e05f1b457a496c8b3630e7044fc5981</data>
    </edge>
    <edge source="RESIDUALS" target="FITTED_VALUES">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, particularly within the realm of linear regression and model fitting, the entities RESIDUALS and FITTED_VALUES play pivotal roles. The RESIDUALS are computed as the difference between the observed values and the FITTED_VALUES, which are the predicted values from the model. This calculation is fundamental in assessing the accuracy and fit of the model. The RESIDUALS serve as a critical diagnostic tool, allowing analysts to evaluate how well the model's predictions align with the actual data points. By analyzing the residuals, one can identify patterns or systematic deviations that might indicate shortcomings in the model's assumptions or suggest the presence of outliers. This process is essential for refining models and ensuring they accurately represent the underlying data structure.</data>
      <data key="d6">1d52aaeb960f9787b6229e57738f8e47,60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="RESIDUALS" target="LINE_OF_BEST_FIT">
      <data key="d4">1.0</data>
      <data key="d5">Residuals are calculated based on the difference between observed and predicted sales values from the line of best fit</data>
      <data key="d6">f16299fc00a7a69bdf983dce826b4918</data>
    </edge>
    <edge source="RESIDUALS" target="REGRESSION_PLANE">
      <data key="d4">1.0</data>
      <data key="d5">Residuals are the signed distances along the z-axis between the datapoints and the regression plane</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376</data>
    </edge>
    <edge source="RESIDUALS" target="YJ">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, RESIDUALS play a crucial role in assessing the accuracy of a model's predictions. They are calculated as the difference between the observed response values Yj and their fitted values, which are the predicted values based on the model. This calculation helps in understanding how well the model fits the data by quantifying the discrepancies between the actual and predicted values. The entity YJ represents the observed response values in the dataset, which are compared against the model's predictions to compute the residuals. This process is fundamental in residual analysis, aiding in the identification of patterns, outliers, and potential improvements to the model.</data>
      <data key="d6">5cc49d301d9cd1f8e20b92ab9d8346b0,d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="RESIDUALS" target="PARAMETER_ESTIMATES">
      <data key="d4">1.0</data>
      <data key="d5">Residuals are minimized by choosing appropriate parameter estimates, aiming to reduce the model's error</data>
      <data key="d6">22093a562f5f05dc9891b45ab9bcbea8</data>
    </edge>
    <edge source="RESIDUALS" target="RESIDUAL_SUM_OF_SQUARES">
      <data key="d4">1.0</data>
      <data key="d5">Residuals contribute to the calculation of the residual sum of squares, which is a measure of the model's overall error</data>
      <data key="d6">22093a562f5f05dc9891b45ab9bcbea8</data>
    </edge>
    <edge source="RESIDUALS" target="EPSILON">
      <data key="d4">1.0</data>
      <data key="d5">Residuals are used as estimates of the errors &#1013;1, ..., &#1013;n in the regression model</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6</data>
    </edge>
    <edge source="RESIDUALS" target="RESIDUAL_PLOTS">
      <data key="d4">3.0</data>
      <data key="d5">Residual plots, a critical tool in statistical analysis, are employed to visualize residuals, which are the differences between observed and predicted values. These plots serve a dual purpose: they help in assessing the fit of a regression model and also in checking for any violations of model assumptions. By using residuals as estimates of the errors, analysts can determine if the model adequately captures the underlying structure of the data. This is essential for ensuring that the model's predictions are reliable and that the assumptions of the statistical model, such as homoscedasticity and normality, are not being violated. Residual plots provide a graphical representation that aids in identifying patterns or trends that might indicate a poor model fit or the presence of outliers, thereby facilitating a more thorough evaluation of the model's performance.</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6,7cd6069e88e81548a237fa937adfecc6,a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="RESIDUALS" target="ERRORS">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, RESIDUALS and ERRORS are closely related concepts. RESIDUALS are the estimates of the ERRORS in a statistical model. More specifically, RESIDUALS are calculated as the differences between the observed values and the values predicted by the model. These discrepancies, often referred to as ERRORS, highlight the extent to which the model's predictions deviate from the actual observed data points. This information is crucial for assessing the accuracy and reliability of the model, as well as for identifying any patterns or systematic biases that may exist in the data.</data>
      <data key="d6">7cd6069e88e81548a237fa937adfecc6,e361ac139c268d5c3f3623f920e68af2</data>
    </edge>
    <edge source="RESIDUALS" target="SMOOTHED_RESIDUAL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The smoothed residual plot is a graphical representation of the residuals from the regression model</data>
      <data key="d6">674b8d5bb1f830d0fb944942514d1a16</data>
    </edge>
    <edge source="RESIDUALS" target="SMOOTHING_CURVE">
      <data key="d4">1.0</data>
      <data key="d5">The smoothing curve is a non-parametric estimate of the mean of the residuals</data>
      <data key="d6">674b8d5bb1f830d0fb944942514d1a16</data>
    </edge>
    <edge source="RESIDUALS" target="VARIANCE_ASSUMPTION">
      <data key="d4">1.0</data>
      <data key="d5">The constant variance assumption is based on the residuals</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="RESIDUALS" target="ROUNDING">
      <data key="d4">1.0</data>
      <data key="d5">Rounding can affect the residuals</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="RESIDUALS" target="HOMOSCEDASTICITY">
      <data key="d4">1.0</data>
      <data key="d5">Homoscedasticity is a property of the residuals, indicating that the variance is the same for all values of the independent variable</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="RESIDUALS" target="MULTIPLE_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Residuals are used to assess the fit of the multiple regression model</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="RESIDUALS" target="Y_HAT_J">
      <data key="d4">1.0</data>
      <data key="d5">Residuals are calculated as the difference between the observed values Yj and the fitted values Y_hat_j</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="RESIDUALS" target="RESIDSS">
      <data key="d4">1.0</data>
      <data key="d5">ResidSS is calculated using the residuals</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="RESIDUALS" target="IN">
      <data key="d4">1.0</data>
      <data key="d5">The identity matrix (In) is used in the calculation of residuals (b&#1013;) as (In - H)y</data>
      <data key="d6">3fb977ccba63e267d2e7dd4de6479ce1</data>
    </edge>
    <edge source="RESIDUALS" target="Y">
      <data key="d4">1.0</data>
      <data key="d5">Residuals are calculated by subtracting the fitted values from the observed response vector Y</data>
      <data key="d6">0b650eb2f1dcd603b64fec3c4b5cd24b</data>
    </edge>
    <edge source="RESIDUALS" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">The least squares estimator Beta hat (&#946;b) is used to calculate the fitted values, which in turn are used to calculate the residuals</data>
      <data key="d6">0b650eb2f1dcd603b64fec3c4b5cd24b</data>
    </edge>
    <edge source="RESIDUALS" target="UNUSUAL_OBSERVATIONS">
      <data key="d4">1.0</data>
      <data key="d5">The magnitude of the residuals helps determine the influence of data points on the fitted model</data>
      <data key="d6">e593096f3805c2686423cb91ea276fe6</data>
    </edge>
    <edge source="FITTED_VALUES" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Fitted values are calculated using the estimated parameters Beta hat</data>
      <data key="d6">01d5ee79489582b4135fc96f676b24a0</data>
    </edge>
    <edge source="FITTED_VALUES" target="SIMPLE_REGRESSION_LINE">
      <data key="d4">1.0</data>
      <data key="d5">The simple regression line is used to calculate the fitted values by substituting the explanatory variable values into the model equation</data>
      <data key="d6">22093a562f5f05dc9891b45ab9bcbea8</data>
    </edge>
    <edge source="FITTED_VALUES" target="NULL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">Fitted values are plotted against residuals in a null plot to check for patterns that might indicate violations of model assumptions</data>
      <data key="d6">aa13c33a7e61206e6021e2736002ca9a</data>
    </edge>
    <edge source="FITTED_VALUES" target="SMOOTHED_RESIDUAL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The smoothed residual plot uses the fitted values from the regression model</data>
      <data key="d6">674b8d5bb1f830d0fb944942514d1a16</data>
    </edge>
    <edge source="FITTED_VALUES" target="SMOOTHING_CURVE">
      <data key="d4">1.0</data>
      <data key="d5">The smoothing curve is a function of the fitted values</data>
      <data key="d6">674b8d5bb1f830d0fb944942514d1a16</data>
    </edge>
    <edge source="FITTED_VALUES" target="RESPONSE_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">Fitted values are the values predicted by the model for the response variable</data>
      <data key="d6">e361ac139c268d5c3f3623f920e68af2</data>
    </edge>
    <edge source="FITTED_VALUES" target="RESIDUAL_PLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Residual plots include a plot of residuals versus fitted values to check for patterns that might indicate a poor fit</data>
      <data key="d6">a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="FITTED_VALUES" target="Y">
      <data key="d4">1.0</data>
      <data key="d5">Fitted values are the predicted values of Y based on the linear model</data>
      <data key="d6">0b650eb2f1dcd603b64fec3c4b5cd24b</data>
    </edge>
    <edge source="FITTED_VALUES" target="H">
      <data key="d4">1.0</data>
      <data key="d5">H is used in the calculation of the fitted values (yb) in the regression model</data>
      <data key="d6">e593096f3805c2686423cb91ea276fe6</data>
    </edge>
    <edge source="FITTED_VALUES" target="UNUSUAL_OBSERVATIONS">
      <data key="d4">1.0</data>
      <data key="d5">The fitted values, along with the residuals, determine the influence of data points on the fitted model</data>
      <data key="d6">e593096f3805c2686423cb91ea276fe6</data>
    </edge>
    <edge source="UNUSUAL_OBSERVATIONS" target="LEVERAGES">
      <data key="d4">2.0</data>
      <data key="d5">In the context of regression analysis, UNUSUAL_OBSERVATIONS refer to data points that deviate significantly from the general pattern of the data. Among these, LEVERAGES are particularly noteworthy. LEVERAGES are a specific subset of unusual observations that are characterized by their high leverage values. These values indicate the potential influence a data point has on the regression line due to its position in the predictor space. The influence of a data point is determined not only by its leverage but also by the magnitude of its residual, which measures the difference between the observed and predicted values. Thus, in regression analysis, LEVERAGES, along with the size of the residuals, play a crucial role in assessing the impact of individual data points on the overall model.</data>
      <data key="d6">1d52aaeb960f9787b6229e57738f8e47,e593096f3805c2686423cb91ea276fe6</data>
    </edge>
    <edge source="UNUSUAL_OBSERVATIONS" target="OUTLIERS">
      <data key="d4">1.0</data>
      <data key="d5">Outliers are a type of unusual observation in regression analysis</data>
      <data key="d6">1d52aaeb960f9787b6229e57738f8e47</data>
    </edge>
    <edge source="UNUSUAL_OBSERVATIONS" target="INFLUENCE">
      <data key="d4">1.0</data>
      <data key="d5">Influence is a type of unusual observation in regression analysis</data>
      <data key="d6">1d52aaeb960f9787b6229e57738f8e47</data>
    </edge>
    <edge source="UNUSUAL_OBSERVATIONS" target="TRANSFORMATIONS">
      <data key="d4">1.0</data>
      <data key="d5">Transformations can reduce the influence of unusual observations in linear models</data>
      <data key="d6">07951ffe6787af44aa60c90c69e62f83</data>
    </edge>
    <edge source="UNUSUAL_OBSERVATIONS" target="IN">
      <data key="d4">1.0</data>
      <data key="d5">In is used in the calculation of the influence of data points on the fitted model</data>
      <data key="d6">e593096f3805c2686423cb91ea276fe6</data>
    </edge>
    <edge source="UNUSUAL_OBSERVATIONS" target="REGRESSION_OUTLIERS">
      <data key="d4">1.0</data>
      <data key="d5">Regression outliers are a type of unusual observations that have a large positive or negative residual</data>
      <data key="d6">e593096f3805c2686423cb91ea276fe6</data>
    </edge>
    <edge source="UNUSUAL_OBSERVATIONS" target="COOKS_DISTANCE">
      <data key="d4">1.0</data>
      <data key="d5">Cook's distance is used to identify influential data points, which are a type of unusual observations</data>
      <data key="d6">e593096f3805c2686423cb91ea276fe6</data>
    </edge>
    <edge source="LEVERAGES" target="HII">
      <data key="d4">1.0</data>
      <data key="d5">The concept of leverages is represented by the leverage (hii) of the data points</data>
      <data key="d6">6ee02b38ae842fd5eac9a11c4fd6659f</data>
    </edge>
    <edge source="LEVERAGES" target="CAR_PACKAGE">
      <data key="d4">1.0</data>
      <data key="d5">The car package provides functions to calculate and plot leverages for a given model, as demonstrated with the mammals dataset.</data>
      <data key="d6">f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </edge>
    <edge source="LEVERAGES" target="INDEX_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">An index plot is used to display the leverages for the regression model, as shown in Figure 8.1.</data>
      <data key="d6">f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </edge>
    <edge source="LEVERAGES" target="RULE_OF_THUMB_THRESHOLD">
      <data key="d4">1.0</data>
      <data key="d5">The rule of thumb threshold is used to identify data points with high leverage in the regression model.</data>
      <data key="d6">f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </edge>
    <edge source="OUTLIERS" target="REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Outliers in the context of regression analysis refer to response values that do not fit the current model, potentially indicating unusual observations.</data>
      <data key="d6">f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </edge>
    <edge source="OUTLIERS" target="DATASET">
      <data key="d4">1.0</data>
      <data key="d5">Outliers are present in the dataset and are expected in a reasonably sized dataset</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="INFLUENCE" target="INFLUENTIAL_DATA_POINTS">
      <data key="d4">1.0</data>
      <data key="d5">Influential data points are those that have a high degree of influence on the regression model</data>
      <data key="d6">1d52aaeb960f9787b6229e57738f8e47</data>
    </edge>
    <edge source="INFLUENCE" target="CHAPTER 8">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 8 discusses the concept of influence in statistical models, including how to identify and handle influential data points</data>
      <data key="d6">617760dc9b9682075b10899cc8473dd5</data>
    </edge>
    <edge source="INFLUENCE" target="INFLUENTIAL_OBSERVATION">
      <data key="d4">1.0</data>
      <data key="d5">Influence is related to the concept of an influential observation</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="INFLUENTIAL_DATA_POINTS" target="HUMAN_OBSERVATION">
      <data key="d4">1.0</data>
      <data key="d5">The Human observation is an example of an influential data point that requires careful consideration</data>
      <data key="d6">323899f01972255cd3278bccee20d5d8</data>
    </edge>
    <edge source="INFLUENTIAL_DATA_POINTS" target="FITTED_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Influential data points can significantly impact the fitted model, potentially skewing the results</data>
      <data key="d6">83bb91cf725e5116ca2f5748fddccfae</data>
    </edge>
    <edge source="CHAPTER 8" target="SUMMARY OF CHAPTER 8">
      <data key="d4">1.0</data>
      <data key="d5">The summary of Chapter 8 provides a recap of the key concepts and findings related to influence in statistical models</data>
      <data key="d6">617760dc9b9682075b10899cc8473dd5</data>
    </edge>
    <edge source="CHAPTER 9" target="CATEGORICAL PREDICTOR VARIABLES">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 9 focuses on the use and analysis of categorical predictor variables in statistical models</data>
      <data key="d6">617760dc9b9682075b10899cc8473dd5</data>
    </edge>
    <edge source="CHAPTER 9" target="RETAIL DATA">
      <data key="d4">1.0</data>
      <data key="d5">Retail data is used as an example in Chapter 9 to demonstrate the inclusion of brand as a predictor variable in statistical models</data>
      <data key="d6">617760dc9b9682075b10899cc8473dd5</data>
    </edge>
    <edge source="CHAPTER 9" target="BRAND">
      <data key="d4">1.0</data>
      <data key="d5">Brand is discussed in Chapter 9 as a categorical predictor variable, particularly in the context of retail data</data>
      <data key="d6">617760dc9b9682075b10899cc8473dd5</data>
    </edge>
    <edge source="CHAPTER 9" target="ALTERNATIVE PARAMETERISATIONS">
      <data key="d4">1.0</data>
      <data key="d5">Alternative parameterisations are discussed in Chapter 9 as methods for representing categorical predictor variables in statistical models</data>
      <data key="d6">617760dc9b9682075b10899cc8473dd5</data>
    </edge>
    <edge source="CHAPTER 9" target="MODEL WITH INTERACTION">
      <data key="d4">1.0</data>
      <data key="d5">A model with an interaction is discussed in Chapter 9 as a type of statistical model that includes interaction terms between predictor variables</data>
      <data key="d6">617760dc9b9682075b10899cc8473dd5</data>
    </edge>
    <edge source="CHAPTER 9" target="SUMMARY OF CHAPTER 9">
      <data key="d4">1.0</data>
      <data key="d5">The summary of Chapter 9 provides a recap of the key concepts and findings related to categorical predictor variables in statistical models</data>
      <data key="d6">617760dc9b9682075b10899cc8473dd5</data>
    </edge>
    <edge source="BRAND" target="MULTIPLE_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Brand is a categorical variable that may be included in multiple regression as a factor influencing the response variable sales</data>
      <data key="d6">336546bc73cbe1828a0cc1a45faf8f5a</data>
    </edge>
    <edge source="BRAND" target="RETAIL_DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The Retail dataset includes the categorical predictor variable brand</data>
      <data key="d6">39aef0392258a09378ce45d8b03a268a</data>
    </edge>
    <edge source="BRAND" target="AVERAGE_RESPONSE">
      <data key="d4">1.0</data>
      <data key="d5">The average response can be modeled to be dependent on the value that the categorical predictor brand takes</data>
      <data key="d6">39aef0392258a09378ce45d8b03a268a</data>
    </edge>
    <edge source="BRAND" target="BRAND_MODEL1">
      <data key="d4">1.0</data>
      <data key="d5">Brand is a predictor variable in the model Brand.model1</data>
      <data key="d6">ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </edge>
    <edge source="BRAND" target="SALES">
      <data key="d4">3.0</data>
      <data key="d5">In the context of the statistical analysis, the entity "BRAND" significantly influences the entity "SALES". The sales data is categorized by brand, indicating that each brand has a unique impact on sales figures. This is reflected in the regression model where different intercepts are assigned for each brand, highlighting the distinct contribution of each brand to the sales. The "SALES" is the dependent variable in the model equations, clearly showing that the sales of the product are directly influenced by the "BRAND" variable. This relationship underscores the importance of brand in shaping sales outcomes, as variations in brand can lead to significant differences in sales performance.</data>
      <data key="d6">06d5666e6bfdda828b48adba883b4a61,86ece4718d27d1a6c6a1f448cc850e2b,b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </edge>
    <edge source="BRAND" target="FIGURE_9_3">
      <data key="d4">1.0</data>
      <data key="d5">Figure 9.3 codes points by brand, showing the relationship between sales and price for each brand</data>
      <data key="d6">b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </edge>
    <edge source="BRAND" target="FIGURE_9_2">
      <data key="d4">1.0</data>
      <data key="d5">Figure 9.2 reveals the systematic pattern of sales and price for each brand</data>
      <data key="d6">b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </edge>
    <edge source="BRAND" target="X">
      <data key="d4">1.0</data>
      <data key="d5">X contains the brand information as dummy variables for brandA, brandB, and brandC</data>
      <data key="d6">e800735d6b2a244875f5e0d292de1527</data>
    </edge>
    <edge source="BRAND" target="BRANDA">
      <data key="d4">1.0</data>
      <data key="d5">BrandA is one of the categories of the Brand variable</data>
      <data key="d6">06d5666e6bfdda828b48adba883b4a61</data>
    </edge>
    <edge source="BRAND" target="BRANDB">
      <data key="d4">1.0</data>
      <data key="d5">BrandB is one of the categories of the Brand variable</data>
      <data key="d6">06d5666e6bfdda828b48adba883b4a61</data>
    </edge>
    <edge source="BRAND" target="BRANDC">
      <data key="d4">1.0</data>
      <data key="d5">BrandC is one of the categories of the Brand variable</data>
      <data key="d6">06d5666e6bfdda828b48adba883b4a61</data>
    </edge>
    <edge source="BRAND" target="PRICE">
      <data key="d4">1.0</data>
      <data key="d5">Price is a variable that is associated with the Brand variable in the model equations</data>
      <data key="d6">06d5666e6bfdda828b48adba883b4a61</data>
    </edge>
    <edge source="BRAND" target="C_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">c_price is derived from the Price variable and is associated with the Brand variable in the model equations</data>
      <data key="d6">06d5666e6bfdda828b48adba883b4a61</data>
    </edge>
    <edge source="BRAND" target="MU">
      <data key="d4">1.0</data>
      <data key="d5">Mu is a parameter in the model equations that is specific to each category of the Brand variable</data>
      <data key="d6">06d5666e6bfdda828b48adba883b4a61</data>
    </edge>
    <edge source="BRAND" target="BETA">
      <data key="d4">1.0</data>
      <data key="d5">Beta is a parameter in the model equations that represents the effect of the price deviation on sales, which is associated with the Brand variable</data>
      <data key="d6">06d5666e6bfdda828b48adba883b4a61</data>
    </edge>
    <edge source="BRAND" target="EPSILON">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon is the error term in the model equations, which is associated with the Brand variable</data>
      <data key="d6">06d5666e6bfdda828b48adba883b4a61</data>
    </edge>
    <edge source="BRAND" target="BRAND_C">
      <data key="d4">1.0</data>
      <data key="d5">Brand C is a specific category within the Brand variable, used as the reference category in the model equations</data>
      <data key="d6">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </edge>
    <edge source="BRAND" target="DESIGN_MATRIX">
      <data key="d4">1.0</data>
      <data key="d5">Brand is a category variable included in the design matrix, with dummy variables representing the different brands</data>
      <data key="d6">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </edge>
    <edge source="BRAND" target="PARAMETER_ESTIMATES">
      <data key="d4">1.0</data>
      <data key="d5">The parameter estimates for the Brand variable correspond to the differences in sales between the different brands and the reference category</data>
      <data key="d6">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </edge>
    <edge source="BRAND" target="PARAMETERISATION">
      <data key="d4">1.0</data>
      <data key="d5">The parameterisation of the model depends on the choice of reference category for the Brand variable</data>
      <data key="d6">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </edge>
    <edge source="BRAND" target="RETAIL_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Brand is a variable in the Retail data, used as a predictor in the model</data>
      <data key="d6">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </edge>
    <edge source="BRAND" target="ROUNDING_ERRORS">
      <data key="d4">1.0</data>
      <data key="d5">Rounding errors can affect the accuracy of the parameter estimates for the Brand variable</data>
      <data key="d6">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </edge>
    <edge source="BRAND" target="REFERENCE_CATEGORY">
      <data key="d4">1.0</data>
      <data key="d5">The reference category for the Brand variable is chosen using the relevel command</data>
      <data key="d6">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </edge>
    <edge source="BRAND" target="RELEVEL">
      <data key="d4">1.0</data>
      <data key="d5">The relevel command is used to change the reference category of the Brand variable</data>
      <data key="d6">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </edge>
    <edge source="BRAND" target="BRAND_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The Brand variable is included as a predictor in the brand.model fitted using the lm() function</data>
      <data key="d6">86ece4718d27d1a6c6a1f448cc850e2b</data>
    </edge>
    <edge source="BRAND" target="INTERACTION">
      <data key="d4">2.0</data>
      <data key="d5">The BRAND entity significantly interacts with the INTERACTION, particularly with price, to influence sales volume. This interaction reveals that the effect of price on sales volume is not uniform but varies by brand. This variability suggests that the relationship between price and sales volume is contingent upon the specific brand in question, indicating that different brands may respond differently to price changes. This nuanced understanding is crucial for tailoring pricing strategies to maximize sales volume within the context of each brand's unique market dynamics.</data>
      <data key="d6">86ece4718d27d1a6c6a1f448cc850e2b,b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </edge>
    <edge source="BRAND" target="SALES_VOLUME">
      <data key="d4">1.0</data>
      <data key="d5">Brand affects sales volume, as seen in the regression model</data>
      <data key="d6">b6870535f3975c49d45e62fbe475f198</data>
    </edge>
    <edge source="PARAMETERISATIONS" target="THE_T_STATISTIC">
      <data key="d4">1.0</data>
      <data key="d5">Parameterisations may be related to the T-statistic as different parameterisations can affect the calculation or interpretation of the T-statistic</data>
      <data key="d6">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </edge>
    <edge source="MODEL_WITH_INTERACTION" target="THE_T_STATISTIC">
      <data key="d4">1.0</data>
      <data key="d5">A model with an interaction may be related to the T-statistic as interaction terms can influence the T-statistic in hypothesis testing</data>
      <data key="d6">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </edge>
    <edge source="SUMMARY_OF_CHAPTER_9" target="THE_T_STATISTIC">
      <data key="d4">1.0</data>
      <data key="d5">Summary of Chapter 9 may be related to the T-statistic as it could provide context or background information relevant to the T-statistic</data>
      <data key="d6">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </edge>
    <edge source="SUMMARY_OF_CHAPTER_9" target="CHAPTER_9">
      <data key="d4">1.0</data>
      <data key="d5">The summary of Chapter 9 provides an overview of the content covered in the chapter, including the introduction of categorical predictors</data>
      <data key="d6">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </edge>
    <edge source="THE_T_STATISTIC" target="USEFUL_RESULTS_FOR_MULTIVARIATE_NORMAL">
      <data key="d4">1.0</data>
      <data key="d5">Useful results for the multivariate normal distribution are directly related to the T-statistic as the T-statistic is often derived from the multivariate normal distribution</data>
      <data key="d6">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </edge>
    <edge source="THE_T_STATISTIC" target="DISTRIBUTIONAL_PROPERTIES_OF_BETA_HAT_AND_S_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">Distributional properties of &#946;b and s^2 are directly related to the T-statistic as the T-statistic is calculated using these properties</data>
      <data key="d6">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </edge>
    <edge source="THE_T_STATISTIC" target="SIMPLE_EXAMPLE_IN_R">
      <data key="d4">1.0</data>
      <data key="d5">A simple example in R is related to the T-statistic as it demonstrates how to calculate or use the T-statistic in practice</data>
      <data key="d6">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </edge>
    <edge source="THE_T_STATISTIC" target="SUMMARY_OF_CHAPTER_10">
      <data key="d4">1.0</data>
      <data key="d5">Summary of Chapter 10 may be related to the T-statistic as it could summarize the key points about the T-statistic discussed in the chapter</data>
      <data key="d6">ef68dd0317c62e3cdde00395f7a21bd7</data>
    </edge>
    <edge source="CHAPTER_10" target="CHAPTER_11">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 10 is followed by Chapter 11 in the document, suggesting a sequential relationship</data>
      <data key="d6">0ba6a4dc0ac5f8b86cc2a22fd51b9517</data>
    </edge>
    <edge source="CHAPTER_11" target="CHAPTER_12">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the document under analysis, CHAPTER_11 and CHAPTER_12 are identified as sequential chapters, with CHAPTER_11 directly preceding CHAPTER_12. This sequential arrangement implies a structured progression of content, where CHAPTER_11 serves as a foundational or prerequisite chapter to CHAPTER_12. It is hypothesized that the material covered in CHAPTER_11 provides essential background information and concepts that are critical for understanding the topics discussed in CHAPTER_12. This relationship underscores the importance of a linear reading approach, where the reader is encouraged to progress through the chapters in order to fully grasp the interconnected themes and ideas presented in the document.</data>
      <data key="d6">0ba6a4dc0ac5f8b86cc2a22fd51b9517,f087dce67c830cc3152c8d9cbb76cdb8</data>
    </edge>
    <edge source="CHAPTER_12" target="CHAPTER_13">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 12 is followed by Chapter 13, suggesting a progression in topics from t-tests for normal linear models to the F-test and ANOVA, which may build upon or complement each other</data>
      <data key="d6">f087dce67c830cc3152c8d9cbb76cdb8</data>
    </edge>
    <edge source="CHAPTER_12" target="T_TEST">
      <data key="d4">1.0</data>
      <data key="d5">The t-test is a central topic of Chapter 12, which discusses its application in normal linear models, including hypothesis testing and limitations</data>
      <data key="d6">f087dce67c830cc3152c8d9cbb76cdb8</data>
    </edge>
    <edge source="CHAPTER_13" target="F_TEST">
      <data key="d4">1.0</data>
      <data key="d5">The F-test is a topic covered in Chapter 13, likely in the context of ANOVA and its application in comparing variances between groups</data>
      <data key="d6">f087dce67c830cc3152c8d9cbb76cdb8</data>
    </edge>
    <edge source="CHAPTER_13" target="ANOVA">
      <data key="d4">1.0</data>
      <data key="d5">ANOVA is a key focus of Chapter 13, which explores its use in statistical analysis, particularly in comparing means across multiple groups</data>
      <data key="d6">f087dce67c830cc3152c8d9cbb76cdb8</data>
    </edge>
    <edge source="T_TEST" target="R">
      <data key="d4">1.0</data>
      <data key="d5">R is used in the simple example of the t-test in Chapter 12, demonstrating its application in statistical testing</data>
      <data key="d6">f087dce67c830cc3152c8d9cbb76cdb8</data>
    </edge>
    <edge source="T_TEST" target="F_TEST">
      <data key="d4">1.0</data>
      <data key="d5">The t-test is a simpler version of the F-test, used when comparing two groups. The F-test is more general and can be used to compare multiple groups.</data>
      <data key="d6">59ad428bf172e7866861ea44cbe198e2</data>
    </edge>
    <edge source="F_TEST" target="ANOVA">
      <data key="d4">1.0</data>
      <data key="d5">The F-test is a key component of ANOVA, used to compare multiple group means and determine if there are significant differences among them.</data>
      <data key="d6">59ad428bf172e7866861ea44cbe198e2</data>
    </edge>
    <edge source="F_TEST" target="NESTED_LINEAR_MODELS">
      <data key="d4">1.0</data>
      <data key="d5">Nested linear models are used in the context of the F-test to compare models and determine if a more complex model significantly improves the fit over a simpler model.</data>
      <data key="d6">59ad428bf172e7866861ea44cbe198e2</data>
    </edge>
    <edge source="F_TEST" target="EXISTENCE_OF_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">The F-test is used to test for the existence of regression, determining if there is a significant relationship between the dependent variable and the independent variables.</data>
      <data key="d6">59ad428bf172e7866861ea44cbe198e2</data>
    </edge>
    <edge source="F_TEST" target="DISTRIBUTION_OF_F_STATISTIC">
      <data key="d4">1.0</data>
      <data key="d5">The distribution of the F-statistic is used in the F-test to determine the significance of the test results, comparing the observed F-statistic to the critical value from the F-distribution.</data>
      <data key="d6">59ad428bf172e7866861ea44cbe198e2</data>
    </edge>
    <edge source="ANOVA" target="TOTAL_SUM_OF_SQUARES">
      <data key="d4">1.0</data>
      <data key="d5">The total sum of squares is decomposed in ANOVA to determine the explained and unexplained variability, which is used to calculate the F-statistic.</data>
      <data key="d6">59ad428bf172e7866861ea44cbe198e2</data>
    </edge>
    <edge source="ANOVA" target="ANOVA_TABLE">
      <data key="d4">1.0</data>
      <data key="d5">The ANOVA table is a result of the ANOVA process, summarizing the sources of variation, degrees of freedom, sums of squares, mean squares, and F-statistics.</data>
      <data key="d6">59ad428bf172e7866861ea44cbe198e2</data>
    </edge>
    <edge source="ANOVA" target="ILLUSTRATION_IN_R">
      <data key="d4">1.0</data>
      <data key="d5">Illustration in R provides practical examples of how ANOVA and the F-test are applied using the R programming language.</data>
      <data key="d6">59ad428bf172e7866861ea44cbe198e2</data>
    </edge>
    <edge source="R" target="NORMAL_LINEAR_MODELS">
      <data key="d4">1.0</data>
      <data key="d5">R is used to fit normal linear models, with well-established functions for this purpose</data>
      <data key="d6">7b32c106246576bb451a5a3985914351</data>
    </edge>
    <edge source="R" target="LM_FUNCTION">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis and algorithmic understanding, the software environment R offers a powerful tool for data scientists and statisticians alike. Specifically, R provides the lm() function, which is a fundamental component for fitting linear models. This function enables users to estimate the relationships between variables using least squares estimators, making it an essential tool for conducting linear regression analysis. Through the lm() function, R facilitates the identification of patterns and structures within datasets, allowing for the interpretation of complex statistical models and the generation of insights into the community of interest. The lm() function in R is not only a means for fitting linear models but also a gateway to conducting residual analysis, which is crucial for assessing the goodness of fit and the assumptions underlying the model. Overall, R's lm() function stands as a versatile and indispensable tool in the realm of statistical inference and data analysis.</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b,b99ecc2f79f56198a8c2adbdff95d576,f18bacb0a2fbfea44dd6326224184216</data>
    </edge>
    <edge source="R" target="SUMMARY_FUNCTION">
      <data key="d4">1.0</data>
      <data key="d5">R provides the summary() function for summarizing fitted models</data>
      <data key="d6">b99ecc2f79f56198a8c2adbdff95d576</data>
    </edge>
    <edge source="R" target="USINGR_PACKAGE">
      <data key="d4">1.0</data>
      <data key="d5">R can load the UsingR package to access additional datasets</data>
      <data key="d6">b99ecc2f79f56198a8c2adbdff95d576</data>
    </edge>
    <edge source="R" target="I_FUNCTION">
      <data key="d4">1.0</data>
      <data key="d5">R provides the I() function to inhibit the conversion of an expression into an operator</data>
      <data key="d6">f18bacb0a2fbfea44dd6326224184216</data>
    </edge>
    <edge source="R" target="COEF_FUNCTION">
      <data key="d4">1.0</data>
      <data key="d5">R provides the coef() function to extract model coefficients</data>
      <data key="d6">f18bacb0a2fbfea44dd6326224184216</data>
    </edge>
    <edge source="R" target="LM">
      <data key="d4">1.0</data>
      <data key="d5">R is the software used to run the lm() function</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="R" target="MLR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">R is the software used to create the mlr.model</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="R" target="X">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, R, a powerful programming language for statistical computing and graphics, utilizes the design matrix X for analytical purposes. Specifically, X is employed within the model.matrix output for brand.model1, indicating its role in the structure and representation of the model's data. Additionally, R leverages the design matrix X to fit a linear regression model, a fundamental statistical technique used for understanding the relationship between one or more independent variables and a dependent variable. This dual usage of X highlights its significance in the algorithmic analysis conducted by R, where it serves as a critical component in both the preparation of data for modeling and the fitting of the linear regression model itself.</data>
      <data key="d6">119bc73ddf8eebadfb8eae272fa323a7,e800735d6b2a244875f5e0d292de1527</data>
    </edge>
    <edge source="R" target="BETA">
      <data key="d4">1.0</data>
      <data key="d5">R estimates the parameter vector Beta when fitting the linear regression model</data>
      <data key="d6">119bc73ddf8eebadfb8eae272fa323a7</data>
    </edge>
    <edge source="R" target="EPSILON">
      <data key="d4">1.0</data>
      <data key="d5">R assumes that the error terms Epsilon are independently and identically distributed with mean zero and constant variance</data>
      <data key="d6">119bc73ddf8eebadfb8eae272fa323a7</data>
    </edge>
    <edge source="R" target="PLR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">R is the software environment used to fit the quadratic regression model plr.model</data>
      <data key="d6">084dadebfca8bcb6377c205c45bee295</data>
    </edge>
    <edge source="R" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">R is used to calculate the estimates Beta hat</data>
      <data key="d6">01d5ee79489582b4135fc96f676b24a0</data>
    </edge>
    <edge source="R" target="YB">
      <data key="d4">1.0</data>
      <data key="d5">In R, the command fitted(model) returns the fitted values YB of a model</data>
      <data key="d6">5cc49d301d9cd1f8e20b92ab9d8346b0</data>
    </edge>
    <edge source="R" target="RESIDUALS_COMMAND">
      <data key="d4">1.0</data>
      <data key="d5">R provides the command residuals(model) to obtain the residuals from a fitted model</data>
      <data key="d6">e6f79ceb0df54119a4dc71b2162ac50b</data>
    </edge>
    <edge source="R" target="SIMULATED_DATA">
      <data key="d4">1.0</data>
      <data key="d5">R is used to generate and analyze simulated data</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="R" target="NORMAL_Q_Q_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">In R, the normal Q-Q plot for the standardised residuals of the fitted model can be produced using the plot function with the argument which = 2.</data>
      <data key="d6">eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </edge>
    <edge source="R" target="BRAND_MODEL1">
      <data key="d4">1.0</data>
      <data key="d5">R is the software used to implement the model Brand.model1</data>
      <data key="d6">ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </edge>
    <edge source="R" target="BRAND.MODEL1">
      <data key="d4">1.0</data>
      <data key="d5">R is used to verify the design matrix and generate the model.matrix output for brand.model1</data>
      <data key="d6">e800735d6b2a244875f5e0d292de1527</data>
    </edge>
    <edge source="R" target="BRAND_MODEL3">
      <data key="d4">1.0</data>
      <data key="d5">R is the software environment used to fit the linear model Brand_model3</data>
      <data key="d6">6a6f85d0a6e46196ab3a901fcc82a720</data>
    </edge>
    <edge source="R" target="DESIGN_MATRIX">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, R, a powerful programming language and software environment for data analysis and graphics, utilizes the DESIGN_MATRIX to fit a linear model specifically for Retail data. This design matrix plays a crucial role in the algorithmic analysis by providing the necessary structure that allows R to estimate the parameters of the linear model. Through this process, R can identify the relationships between the variables in the Retail dataset, enabling a deeper understanding of the underlying patterns and trends within the retail industry. The use of the design matrix in R facilitates the application of least squares estimators, which are fundamental in determining the best fit for the linear model by minimizing the sum of the squared residuals. This comprehensive approach ensures that the model accurately represents the data, making it a valuable tool for statistical inference and predictive analytics in the retail sector.</data>
      <data key="d6">06199787dd7f75f7338dd24d4f3dc26e,48971100deb5bb374a41c1f2b7b2a86a</data>
    </edge>
    <edge source="R" target="SAMPLE_MEANS">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entity "R" is utilized to generate parameter estimates, which are then compared with the sample means of sales volume for stores belonging to three distinct brands: Brand A, Brand B, and Brand C. This comparison, facilitated by "R", serves as a crucial step in understanding the sales performance of each brand's stores. By juxtaposing the parameter estimates against the sample means, insights into the sales dynamics and potential differences among the brands can be gleaned, providing a comprehensive overview of the market scenario for these entities.</data>
      <data key="d6">06199787dd7f75f7338dd24d4f3dc26e,48971100deb5bb374a41c1f2b7b2a86a</data>
    </edge>
    <edge source="R" target="PARAMETER_ESTIMATES">
      <data key="d4">1.0</data>
      <data key="d5">R calculates the parameter estimates based on the model and data</data>
      <data key="d6">06199787dd7f75f7338dd24d4f3dc26e</data>
    </edge>
    <edge source="R" target="RETAIL_DATA">
      <data key="d4">1.0</data>
      <data key="d5">R is used to fit the model to the Retail data</data>
      <data key="d6">a1fc936df848a0fbc791e4bcc9b527b6</data>
    </edge>
    <edge source="R" target="CATEGORICAL_PREDICTORS">
      <data key="d4">1.0</data>
      <data key="d5">R can be used to incorporate categorical predictors into a linear model. This involves using functions in R to create dummy variables and fit the linear model.</data>
      <data key="d6">d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </edge>
    <edge source="ANOVA_TABLE" target="F_STATISTIC_DISTRIBUTION">
      <data key="d4">1.0</data>
      <data key="d5">The ANOVA table is used to calculate the F-statistic, which follows a specific distribution under the null hypothesis</data>
      <data key="d6">638b52a0671088c9aa208790411ab898</data>
    </edge>
    <edge source="ANOVA_TABLE" target="ILLUSTRATION_IN_R">
      <data key="d4">1.0</data>
      <data key="d5">Illustration in R can be used to demonstrate the creation and interpretation of an ANOVA table</data>
      <data key="d6">638b52a0671088c9aa208790411ab898</data>
    </edge>
    <edge source="ANOVA_TABLE" target="SUMMARY_OF_CHAPTER_13">
      <data key="d4">1.0</data>
      <data key="d5">The summary of Chapter 13 likely includes a discussion of the ANOVA table and its role in regression analysis</data>
      <data key="d6">638b52a0671088c9aa208790411ab898</data>
    </edge>
    <edge source="ANOVA_TABLE" target="TEST_FOR_NON-LINEARITY">
      <data key="d4">1.0</data>
      <data key="d5">A test for non-linearity can be performed using the residuals from an ANOVA table, to check if the relationship between variables is linear</data>
      <data key="d6">638b52a0671088c9aa208790411ab898</data>
    </edge>
    <edge source="F_STATISTIC_DISTRIBUTION" target="SEQUENTIAL_ANOVA">
      <data key="d4">1.0</data>
      <data key="d5">The distribution of the F-statistic is used in sequential ANOVA to test the significance of each factor added to the model</data>
      <data key="d6">638b52a0671088c9aa208790411ab898</data>
    </edge>
    <edge source="SEQUENTIAL_ANOVA" target="FACTORS">
      <data key="d4">1.0</data>
      <data key="d5">Sequential ANOVA involves the sequential addition of factors to the model, assessing their individual contributions</data>
      <data key="d6">638b52a0671088c9aa208790411ab898</data>
    </edge>
    <edge source="FACTORS" target="CHAPTER 14">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 14 includes a section on factors, which are elements considered in the context of the chapter</data>
      <data key="d6">322cf1f37d33953af834b695b7f08b72</data>
    </edge>
    <edge source="CHAPTER 14" target="TEST FOR NON-LINEARITY">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 14 includes a test for non-linearity, which is a procedure to assess the linearity of a model or data</data>
      <data key="d6">322cf1f37d33953af834b695b7f08b72</data>
    </edge>
    <edge source="CHAPTER 14" target="SUMMARY OF CHAPTER 14">
      <data key="d4">1.0</data>
      <data key="d5">The summary of Chapter 14 provides an overview of the topics covered in the chapter, which includes factors and the test for non-linearity</data>
      <data key="d6">322cf1f37d33953af834b695b7f08b72</data>
    </edge>
    <edge source="CHAPTER 15" target="BIAS AND VARIANCE">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 15 discusses bias and variance, concepts related to model performance and error</data>
      <data key="d6">322cf1f37d33953af834b695b7f08b72</data>
    </edge>
    <edge source="CHAPTER 15" target="THE MODEL HIERARCHY">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 15 includes a section on the model hierarchy, which is a structure or classification of models</data>
      <data key="d6">322cf1f37d33953af834b695b7f08b72</data>
    </edge>
    <edge source="CHAPTER 15" target="MODEL SELECTION STATISTICS">
      <data key="d4">2.0</data>
      <data key="d5">Chapter 15, titled "Model Selection Statistics," delves into the critical aspect of choosing among various models by utilizing specific metrics or criteria. This chapter emphasizes the importance of model selection statistics as a method for comparing and selecting models, providing readers with a comprehensive understanding of how to evaluate different models based on statistical measures. The focus is on equipping data scientists and statisticians with the tools they need to make informed decisions about which model best fits their data and research objectives.</data>
      <data key="d6">322cf1f37d33953af834b695b7f08b72,ad35c05a2497a4e16a014d64483842a8</data>
    </edge>
    <edge source="CHAPTER 15" target="VARIABLE SELECTION">
      <data key="d4">2.0</data>
      <data key="d5">Chapter 15, titled "VARIABLE SELECTION," delves into the critical process of variable selection within statistical modeling. This chapter emphasizes the importance of identifying and selecting the most relevant variables to ensure the model's efficiency and accuracy. The discussion in Chapter 15 highlights variable selection as a strategic approach to enhance model performance by focusing on variables that significantly contribute to the predictive power of the model, while eliminating those that do not add substantial value. This process is crucial for understanding the underlying structure of the data and for making informed decisions in model building.</data>
      <data key="d6">322cf1f37d33953af834b695b7f08b72,ad35c05a2497a4e16a014d64483842a8</data>
    </edge>
    <edge source="CHAPTER 15" target="MULTICOLLINEARITY">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 15 addresses multicollinearity as a potential issue in multiple regression models</data>
      <data key="d6">ad35c05a2497a4e16a014d64483842a8</data>
    </edge>
    <edge source="CHAPTER 15" target="SUMMARY OF CHAPTER 15">
      <data key="d4">1.0</data>
      <data key="d5">The summary of Chapter 15 provides an overview of the topics covered in the chapter</data>
      <data key="d6">ad35c05a2497a4e16a014d64483842a8</data>
    </edge>
    <edge source="CHAPTER 16" target="GENERAL LINEAR MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 16 discusses the General Linear Model as a broad statistical model</data>
      <data key="d6">ad35c05a2497a4e16a014d64483842a8</data>
    </edge>
    <edge source="CHAPTER 16" target="GENERALISED LEAST SQUARES ESTIMATOR">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 16 covers the generalised least squares estimator as a method for estimating parameters in the presence of non-independent errors</data>
      <data key="d6">ad35c05a2497a4e16a014d64483842a8</data>
    </edge>
    <edge source="CHAPTER 16" target="WEIGHTED REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 16 addresses weighted regression as a technique for regression analysis with weighted data points</data>
      <data key="d6">ad35c05a2497a4e16a014d64483842a8</data>
    </edge>
    <edge source="CHAPTER 16" target="SERIALLY CORRELATED ERRORS">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 16 discusses serially correlated errors as a potential issue in regression models</data>
      <data key="d6">ad35c05a2497a4e16a014d64483842a8</data>
    </edge>
    <edge source="GLM" target="BINARY_RESPONSE_DATA">
      <data key="d4">1.0</data>
      <data key="d5">GLMs can be used for modeling binary response data, providing a framework for analyzing data with binary outcomes</data>
      <data key="d6">54768ad5bc5877b6bf042aac07fb64d9</data>
    </edge>
    <edge source="GLM" target="HYPOTHESIS_TESTING">
      <data key="d4">1.0</data>
      <data key="d5">Hypothesis testing is used in the context of GLMs to test the significance of model parameters and the overall fit of the model</data>
      <data key="d6">54768ad5bc5877b6bf042aac07fb64d9</data>
    </edge>
    <edge source="GLM" target="SUMMARY_OF_CHAPTER_17">
      <data key="d4">1.0</data>
      <data key="d5">Chapter 17's summary includes a review of the concepts related to GLMs, providing a comprehensive overview of the topic</data>
      <data key="d6">54768ad5bc5877b6bf042aac07fb64d9</data>
    </edge>
    <edge source="GLM" target="APPENDIX_A">
      <data key="d4">1.0</data>
      <data key="d5">Linear algebra and multivariable calculus, covered in Appendix A, provide the mathematical foundation for understanding GLMs</data>
      <data key="d6">54768ad5bc5877b6bf042aac07fb64d9</data>
    </edge>
    <edge source="GLM" target="APPENDIX_B">
      <data key="d4">1.0</data>
      <data key="d5">Generalized expectation, discussed in Appendix B, is a theoretical concept that underpins the formulation of GLMs</data>
      <data key="d6">54768ad5bc5877b6bf042aac07fb64d9</data>
    </edge>
    <edge source="GLM" target="APPENDIX_C">
      <data key="d4">1.0</data>
      <data key="d5">The distribution of s^2, explained in Appendix C, is relevant to the estimation of variance components in GLMs</data>
      <data key="d6">54768ad5bc5877b6bf042aac07fb64d9</data>
    </edge>
    <edge source="GLM" target="APPENDIX_D">
      <data key="d4">1.0</data>
      <data key="d5">M-estimators and robust regression, covered in Appendix D, are alternative methods to GLMs for estimating model parameters in the presence of outliers</data>
      <data key="d6">54768ad5bc5877b6bf042aac07fb64d9</data>
    </edge>
    <edge source="APPENDIX_B" target="APPENDIX_C">
      <data key="d4">1.0</data>
      <data key="d5">Appendix B and Appendix C are both sections in a document, covering different statistical concepts</data>
      <data key="d6">acead09befa8eb465dd2e8c2d93a43c5</data>
    </edge>
    <edge source="APPENDIX_C" target="APPENDIX_D">
      <data key="d4">1.0</data>
      <data key="d5">Appendix C and Appendix D are both sections in a document, covering different statistical concepts</data>
      <data key="d6">acead09befa8eb465dd2e8c2d93a43c5</data>
    </edge>
    <edge source="APPENDIX_D" target="APPENDIX_E">
      <data key="d4">1.0</data>
      <data key="d5">Appendix D and Appendix E are both sections in a document, covering different statistical concepts</data>
      <data key="d6">acead09befa8eb465dd2e8c2d93a43c5</data>
    </edge>
    <edge source="APPENDIX_E" target="APPENDIX_F">
      <data key="d4">1.0</data>
      <data key="d5">Appendix E and Appendix F are both sections in a document, covering different statistical concepts</data>
      <data key="d6">acead09befa8eb465dd2e8c2d93a43c5</data>
    </edge>
    <edge source="APPENDIX_F" target="APPENDIX_G">
      <data key="d4">1.0</data>
      <data key="d5">Appendix F and Appendix G are both sections in a document, covering different statistical concepts</data>
      <data key="d6">acead09befa8eb465dd2e8c2d93a43c5</data>
    </edge>
    <edge source="APPENDIX_G" target="APPENDIX_H">
      <data key="d4">1.0</data>
      <data key="d5">Appendix G and Appendix H are both sections in a document, covering different statistical concepts</data>
      <data key="d6">acead09befa8eb465dd2e8c2d93a43c5</data>
    </edge>
    <edge source="PDF_VERSION" target="RIGHTS">
      <data key="d4">1.0</data>
      <data key="d5">The PDF version of the lecture notes is subject to the rights reserved, meaning it should not be distributed or uploaded to the internet</data>
      <data key="d6">d1cc8e172b83ff69b921cef864fc09f5</data>
    </edge>
    <edge source="RIGHTS" target="TYPO_REPORTING">
      <data key="d4">1.0</data>
      <data key="d5">The procedure for reporting typos is an exception to the rights reserved, allowing for feedback on the lecture notes</data>
      <data key="d6">d1cc8e172b83ff69b921cef864fc09f5</data>
    </edge>
    <edge source="TYPO_REPORTING" target="DIVERSITY_NOTE">
      <data key="d4">1.0</data>
      <data key="d5">The diversity note is related to the typo reporting procedure as it encourages the inclusion of more diverse sources in future versions of the lecture notes</data>
      <data key="d6">d1cc8e172b83ff69b921cef864fc09f5</data>
    </edge>
    <edge source="DIVERSITY_NOTE" target="NORMAL_LINEAR_MODELS">
      <data key="d4">1.0</data>
      <data key="d5">The concept of normal linear models is indirectly related to the diversity note as it pertains to the content of the lecture notes that could be augmented with more diverse sources</data>
      <data key="d6">d1cc8e172b83ff69b921cef864fc09f5</data>
    </edge>
    <edge source="NORMAL_LINEAR_MODELS" target="LECTURE_NOTES">
      <data key="d4">1.0</data>
      <data key="d5">The lecture notes discuss normal linear models, which are statistical models used to describe relationships between variables</data>
      <data key="d6">7b32c106246576bb451a5a3985914351</data>
    </edge>
    <edge source="NORMAL_LINEAR_MODELS" target="EXPLANATORY_VARIABLES">
      <data key="d4">1.0</data>
      <data key="d5">Normal linear models use explanatory variables to explain the variation in the outcome variable</data>
      <data key="d6">7b32c106246576bb451a5a3985914351</data>
    </edge>
    <edge source="NORMAL_LINEAR_MODELS" target="OUTCOME_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">Normal linear models predict or explain the outcome variable using one or more explanatory variables</data>
      <data key="d6">7b32c106246576bb451a5a3985914351</data>
    </edge>
    <edge source="NORMAL_LINEAR_MODELS" target="LINEAR_ALGEBRA">
      <data key="d4">1.0</data>
      <data key="d5">Normal linear models are analyzed using tools from linear algebra, which is common for machine learning algorithms</data>
      <data key="d6">7b32c106246576bb451a5a3985914351</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLES" target="OBSERVATIONAL_STUDIES">
      <data key="d4">1.0</data>
      <data key="d5">In observational studies, explanatory variables are considered non-random and the model is developed conditional on their values</data>
      <data key="d6">22478e53f29f16e3eab9d167fea52b22</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLES" target="RESPONSE_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">Explanatory variables are used to explain or predict the response variable in statistical models</data>
      <data key="d6">22478e53f29f16e3eab9d167fea52b22</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLES" target="MULTIPLE_LINEAR_REGRESSION">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the EXPLANATORY_VARIABLES, such as price and local advertising, play a crucial role in the MULTIPLE_LINEAR_REGRESSION model. This model is employed to predict the RESPONSE_VARIABLE, which in this scenario is sales. The explanatory variables are used to explain the variations in sales, and the relationship strength between these variables and sales is high. The MULTIPLE_LINEAR_REGRESSION model considers multiple variables simultaneously, providing a more accurate and comprehensive prediction of sales compared to models that consider only a single variable. This approach allows for a deeper understanding of the factors influencing sales and enables more precise predictions, making it a valuable tool in business analytics and decision-making processes.</data>
      <data key="d6">2ced3e26eaed2dfcd8e4caf49737cab4,426434b67f6a287852ab66b82ca873cf</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLES" target="DATASET">
      <data key="d4">1.0</data>
      <data key="d5">Explanatory variables are part of the dataset</data>
      <data key="d6">2ced3e26eaed2dfcd8e4caf49737cab4</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLES" target="YN">
      <data key="d4">1.0</data>
      <data key="d5">The means of the response variables Y1, ..., Yn depend on the values taken by the explanatory variables</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLES" target="TREES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Explanatory variables, such as log-diameter and log-height, are used in the trees.model to predict the dependent variable</data>
      <data key="d6">a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="LINEAR_ALGEBRA" target="VECTORS">
      <data key="d4">1.0</data>
      <data key="d5">In linear algebra, vectors are denoted by small letters, which can sometimes lead to notational issues when dealing with both random variables and their realizations</data>
      <data key="d6">22478e53f29f16e3eab9d167fea52b22</data>
    </edge>
    <edge source="LINEAR_ALGEBRA" target="MATRICES">
      <data key="d4">1.0</data>
      <data key="d5">In linear algebra, matrices are denoted by capital letters, which can sometimes lead to notational issues when dealing with both random variables and their realizations</data>
      <data key="d6">22478e53f29f16e3eab9d167fea52b22</data>
    </edge>
    <edge source="LINEAR_ALGEBRA" target="MULTIPLE_LINEAR_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Linear algebra provides the mathematical tools needed for multiple linear regression</data>
      <data key="d6">2ced3e26eaed2dfcd8e4caf49737cab4</data>
    </edge>
    <edge source="LINEAR_MODEL" target="MULTIPLE_LINEAR_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Multiple linear regression is a specific type of linear model</data>
      <data key="d6">28eee75e95bbbaf143368c3289585670</data>
    </edge>
    <edge source="LINEAR_MODEL" target="RESPONSE_VARIABLE_Y">
      <data key="d4">1.0</data>
      <data key="d5">Response variable Y is a key component of a linear model, representing the variable of interest</data>
      <data key="d6">7594eee7e77beb023d1cd64aec64920d</data>
    </edge>
    <edge source="LINEAR_MODEL" target="EXPLANATORY_VARIABLES_X">
      <data key="d4">1.0</data>
      <data key="d5">Explanatory variables X are used in a linear model to explain or predict the response variable</data>
      <data key="d6">7594eee7e77beb023d1cd64aec64920d</data>
    </edge>
    <edge source="LINEAR_MODEL" target="CONDITIONAL_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">A linear model can be considered a conditional model, developed based on the values taken by the explanatory variables</data>
      <data key="d6">7594eee7e77beb023d1cd64aec64920d</data>
    </edge>
    <edge source="LINEAR_MODEL" target="X">
      <data key="d4">1.0</data>
      <data key="d5">The relationship between the response variable and the explanatory variable X is part of a linear model, where "linear" refers to linearity in the parameters</data>
      <data key="d6">c9a01b92d11585f6549f62e8bd78d652</data>
    </edge>
    <edge source="LINEAR_MODEL" target="WEISBERG_S_2014">
      <data key="d4">1.0</data>
      <data key="d5">The linear model is described in Weisberg, S. (2014)</data>
      <data key="d6">656dce234514b9db38b5b5616557c1e9</data>
    </edge>
    <edge source="LINEAR_MODEL" target="BETA_0">
      <data key="d4">2.0</data>
      <data key="d5">Beta_0 is a parameter in the linear model</data>
      <data key="d6">656dce234514b9db38b5b5616557c1e9,9611ea31ff53888971694cdefe806f64</data>
    </edge>
    <edge source="LINEAR_MODEL" target="BETA_1">
      <data key="d4">2.0</data>
      <data key="d5">Beta_1 is a parameter in the linear model</data>
      <data key="d6">656dce234514b9db38b5b5616557c1e9,9611ea31ff53888971694cdefe806f64</data>
    </edge>
    <edge source="LINEAR_MODEL" target="EPSILON_J">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon_j is the error term in the linear model</data>
      <data key="d6">656dce234514b9db38b5b5616557c1e9</data>
    </edge>
    <edge source="LINEAR_MODEL" target="PARAMETER_VECTOR_BETA">
      <data key="d4">1.0</data>
      <data key="d5">A linear model can be defined using the parameter vector &#946;, which contains the coefficients that determine the relationship between variables in the model</data>
      <data key="d6">ae1e66f5b64284090abc285c1d4389f5</data>
    </edge>
    <edge source="LINEAR_MODEL" target="TRANSFORMED_EXPLANATORY_VARIABLES">
      <data key="d4">1.0</data>
      <data key="d5">Linear models can use transformed explanatory variables, such as logarithms, to capture non-linear relationships</data>
      <data key="d6">188219b9e5b6b6368360840921877de9</data>
    </edge>
    <edge source="LINEAR_MODEL" target="ERRORS">
      <data key="d4">1.0</data>
      <data key="d5">The linear model assumes that the errors are independent, have constant variance, and follow a normal distribution. These assumptions are checked using residual analysis.</data>
      <data key="d6">3bfc9b92571973e54c8095302acc1aaa</data>
    </edge>
    <edge source="LINEAR_MODEL" target="BETA_2">
      <data key="d4">1.0</data>
      <data key="d5">Beta_2 is a parameter in the linear model</data>
      <data key="d6">9611ea31ff53888971694cdefe806f64</data>
    </edge>
    <edge source="LINEAR_MODEL" target="EPSILON">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon is the error term in the linear model</data>
      <data key="d6">9611ea31ff53888971694cdefe806f64</data>
    </edge>
    <edge source="LINEAR_MODEL" target="SIGMA_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">Sigma squared (&#963;^2) is an important parameter in the linear model, for which an unbiased estimate can be computed</data>
      <data key="d6">3fb977ccba63e267d2e7dd4de6479ce1</data>
    </edge>
    <edge source="LINEAR_MODEL" target="CATEGORICAL_PREDICTORS">
      <data key="d4">1.0</data>
      <data key="d5">Categorical predictors can be incorporated into a linear model to analyze their effect on the response variable. This involves creating dummy variables for each level of the categorical predictor and including them in the model as explanatory variables.</data>
      <data key="d6">d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </edge>
    <edge source="LINEAR_MODEL" target="T_STATISTIC">
      <data key="d4">1.0</data>
      <data key="d5">The T-statistic is used in a linear model to test hypotheses about the parameters of the model. It is calculated by dividing the estimated parameter by its standard error, and its distribution under the null hypothesis is used to determine the significance of the parameter.</data>
      <data key="d6">d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </edge>
    <edge source="MULTIPLE_LINEAR_REGRESSION" target="EPSILON">
      <data key="d4">1.0</data>
      <data key="d5">The error term (&#1013;) is part of the multiple linear regression model, representing the unexplained variation in Y</data>
      <data key="d6">512d9ffebe309a6f944ebce1ae2ff2a3</data>
    </edge>
    <edge source="MULTIPLE_LINEAR_REGRESSION" target="BETA_0">
      <data key="d4">1.0</data>
      <data key="d5">The intercept (&#946;0) is part of the multiple linear regression model, representing the expected value of Y when all predictor variables are zero</data>
      <data key="d6">512d9ffebe309a6f944ebce1ae2ff2a3</data>
    </edge>
    <edge source="MULTIPLE_LINEAR_REGRESSION" target="BETA_1">
      <data key="d4">1.0</data>
      <data key="d5">The coefficient (&#946;1) is part of the multiple linear regression model, representing the change in Y for a one unit increase in X1, given a fixed value of X2</data>
      <data key="d6">512d9ffebe309a6f944ebce1ae2ff2a3</data>
    </edge>
    <edge source="MULTIPLE_LINEAR_REGRESSION" target="BETA_2">
      <data key="d4">1.0</data>
      <data key="d5">The coefficient (&#946;2) is part of the multiple linear regression model, representing the change in Y for a one unit increase in X2, assuming X1 is held constant</data>
      <data key="d6">512d9ffebe309a6f944ebce1ae2ff2a3</data>
    </edge>
    <edge source="MULTIPLE_LINEAR_REGRESSION" target="X1">
      <data key="d4">1.0</data>
      <data key="d5">X1 (price) is one of the predictor variables in the multiple linear regression model</data>
      <data key="d6">512d9ffebe309a6f944ebce1ae2ff2a3</data>
    </edge>
    <edge source="MULTIPLE_LINEAR_REGRESSION" target="X2">
      <data key="d4">1.0</data>
      <data key="d5">X2 (advert) is one of the predictor variables in the multiple linear regression model</data>
      <data key="d6">512d9ffebe309a6f944ebce1ae2ff2a3</data>
    </edge>
    <edge source="MULTIPLE_LINEAR_REGRESSION" target="Y">
      <data key="d4">1.0</data>
      <data key="d5">Y (sales volume) is the dependent variable in the multiple linear regression model</data>
      <data key="d6">512d9ffebe309a6f944ebce1ae2ff2a3</data>
    </edge>
    <edge source="MULTIPLE_LINEAR_REGRESSION" target="ADVERTISING_BUDGET">
      <data key="d4">1.0</data>
      <data key="d5">The multiple linear regression model includes the local advertising budget as a predictor variable, adjusting for its influence on sales. The relationship strength is high, as the model considers the effect of advertising on sales in addition to price.</data>
      <data key="d6">426434b67f6a287852ab66b82ca873cf</data>
    </edge>
    <edge source="MULTIPLE_LINEAR_REGRESSION" target="PRICE">
      <data key="d4">1.0</data>
      <data key="d5">The multiple linear regression model includes price as one of the predictor variables, adjusting for the influence of local advertising on sales. The relationship strength is high, as the model provides a more accurate estimate of the effect of price on sales when controlling for other variables.</data>
      <data key="d6">426434b67f6a287852ab66b82ca873cf</data>
    </edge>
    <edge source="MULTIPLE_LINEAR_REGRESSION" target="PARTIAL_REGRESSION_COEFFICIENTS">
      <data key="d4">1.0</data>
      <data key="d5">Partial regression coefficients are parameters in the multiple linear regression model that represent the expected change in sales for a unit change in an explanatory variable, adjusting for the effects of other variables. The relationship strength is high, as these coefficients are central to the model's predictive power.</data>
      <data key="d6">426434b67f6a287852ab66b82ca873cf</data>
    </edge>
    <edge source="DIAMOND_DATA" target="SINGAPORE_DOLLARS">
      <data key="d4">1.0</data>
      <data key="d5">The price of the diamonds in the diamond data is measured in Singapore Dollars</data>
      <data key="d6">28eee75e95bbbaf143368c3289585670</data>
    </edge>
    <edge source="DIAMOND_DATA" target="CARAT">
      <data key="d4">1.0</data>
      <data key="d5">The weight of the diamonds in the diamond data is measured in carats</data>
      <data key="d6">28eee75e95bbbaf143368c3289585670</data>
    </edge>
    <edge source="DIAMOND_DATA" target="SCATTERPLOT">
      <data key="d4">1.0</data>
      <data key="d5">A scatterplot can be used to visualize the relationship between the price and weight of diamonds in the diamond data</data>
      <data key="d6">28eee75e95bbbaf143368c3289585670</data>
    </edge>
    <edge source="DIAMOND_DATA" target="MODEL_1_1">
      <data key="d4">1.0</data>
      <data key="d5">Diamond data is used to fit model (1.1), which describes the relationship between diamond price and carat weight</data>
      <data key="d6">e47d573a10e64a657e58218df64d8920</data>
    </edge>
    <edge source="DIAMOND_DATA" target="MODEL_1_2">
      <data key="d4">1.0</data>
      <data key="d5">Diamond data is also relevant to model (1.2), which is a reparameterisation of model (1.1)</data>
      <data key="d6">e47d573a10e64a657e58218df64d8920</data>
    </edge>
    <edge source="DIAMOND_DATA" target="REGRESSION_THROUGH_ORIGIN">
      <data key="d4">1.0</data>
      <data key="d5">Diamond data is used to fit a regression through the origin model, as part of Exercise 3</data>
      <data key="d6">e47d573a10e64a657e58218df64d8920</data>
    </edge>
    <edge source="SINGAPORE_DOLLARS" target="RING_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">Singapore Dollars (S$) is the currency used to measure the dependent variable, ring price, in the linear regression model.</data>
      <data key="d6">3e7eef51f3109d60697f3299b541b726</data>
    </edge>
    <edge source="SINGAPORE_DOLLARS" target="SLOPE">
      <data key="d4">1.0</data>
      <data key="d5">The slope in the linear regression model is measured in Singapore Dollars (S$) per carat of diamond weight. This relationship indicates the currency unit used for the price of Solitaire rings.</data>
      <data key="d6">f0e0c5b2deaaf9fc2bc8b63d9ab989b1</data>
    </edge>
    <edge source="CARAT" target="DIAMOND_DATASET">
      <data key="d4">1.0</data>
      <data key="d5">Carat is a variable in the diamond dataset</data>
      <data key="d6">b99ecc2f79f56198a8c2adbdff95d576</data>
    </edge>
    <edge source="CARAT" target="SLR_DIAMOND">
      <data key="d4">1.0</data>
      <data key="d5">Carat is the predictor variable in the SLR.diamond model</data>
      <data key="d6">b99ecc2f79f56198a8c2adbdff95d576</data>
    </edge>
    <edge source="CARAT" target="SLR.DIAMOND">
      <data key="d4">1.0</data>
      <data key="d5">The carat variable is the predictor variable in the SLR.diamond model</data>
      <data key="d6">2b01334fb633566ba368a764ad579fce</data>
    </edge>
    <edge source="CARAT" target="SLOPE">
      <data key="d4">1.0</data>
      <data key="d5">Slope is related to Carat as the slope is the coefficient associated with the explanatory variable carat</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="CARAT" target="SLOPE_COEFFICIENT">
      <data key="d4">1.0</data>
      <data key="d5">The slope coefficient is associated with the unit of weight for diamonds, indicating the change in price for every additional 0.1 carat</data>
      <data key="d6">e1ad57124a08c0e123deda212ea03c32</data>
    </edge>
    <edge source="CARAT" target="DIAMOND">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the diamond industry, the entities "CARAT" and "DIAMOND" are intricately linked. The carat, a unit of measurement, is pivotal in quantifying the weight of a diamond. This weight, in turn, significantly influences the price of diamond jewelry, such as a solitaire ring, where heavier diamonds command higher prices. Within a statistical analysis framework, the carat is utilized as a predictor variable in a linear model, indicating its importance in understanding and predicting the value of diamonds. This highlights the direct correlation between a diamond's weight and its market value, with the carat serving as a crucial factor in both the physical and economic characteristics of diamonds.</data>
      <data key="d6">35e06960dba699ce0d56fc1e98bdbe96,e1ad57124a08c0e123deda212ea03c32</data>
    </edge>
    <edge source="CARAT" target="MEAN">
      <data key="d4">1.0</data>
      <data key="d5">mean(carat) is used to center the carat variable in the linear model</data>
      <data key="d6">35e06960dba699ce0d56fc1e98bdbe96</data>
    </edge>
    <edge source="SCATTERPLOT" target="DIAMOND_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">A scatterplot visually illustrates the relationship between diamond weight and ring price, showing how the price of a ring depends on the weight of its diamond.</data>
      <data key="d6">3e7eef51f3109d60697f3299b541b726</data>
    </edge>
    <edge source="SCATTERPLOT" target="RING_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">A scatterplot visually illustrates the relationship between diamond weight and ring price, showing how the price of a ring depends on the weight of its diamond.</data>
      <data key="d6">3e7eef51f3109d60697f3299b541b726</data>
    </edge>
    <edge source="SCATTERPLOT" target="LINE_OF_BEST_FIT">
      <data key="d4">3.0</data>
      <data key="d5">The scatterplot, featuring points that represent the relationship between the price of a ring and the weight of its diamond, is enriched by the inclusion of a line of best fit. This line, calculated using methods like least squares, is specifically added to the scatterplot to illustrate the linearity of the relationship between the two variables. The line of best fit visually captures the trend in the data, providing a clear indication of how the price of a ring changes with the weight of its diamond.</data>
      <data key="d6">2e5e1bdaa9fcc7b3391d277fd6bb247a,3e7eef51f3109d60697f3299b541b726,ae1e66f5b64284090abc285c1d4389f5</data>
    </edge>
    <edge source="SCATTERPLOT" target="FIGURE_1_3">
      <data key="d4">1.0</data>
      <data key="d5">Figure 1.3 shows two scatterplots that depict the relationship between diamond weight and ring price</data>
      <data key="d6">c5b269ff5c94db7ebd2cb9f7be16f171</data>
    </edge>
    <edge source="SCATTERPLOT" target="RESIDUAL">
      <data key="d4">1.0</data>
      <data key="d5">The residuals are illustrated by the red lines in the scatterplot</data>
      <data key="d6">2e5e1bdaa9fcc7b3391d277fd6bb247a</data>
    </edge>
    <edge source="SCATTERPLOT" target="ANSCOMBE_QUARTET">
      <data key="d4">1.0</data>
      <data key="d5">Anscombe's Quartet shows that datasets with the same summary statistics can have very different scatterplots</data>
      <data key="d6">9f335f1ecb85a1427df926df8bb1e89f</data>
    </edge>
    <edge source="SCATTERPLOT" target="X">
      <data key="d4">2.0</data>
      <data key="d5">The scatterplot, featuring X as one of its principal variables, visually illustrates the relationship between X and another variable, Y. X, positioned along one of the axes, plays a pivotal role in uncovering patterns and trends within the data. Through the scatterplot, one can discern the nature of the association between X and Y, whether it be linear, curvilinear, or nonexistent, thereby providing insights into the potential influence of X on Y within the context of the dataset under examination.</data>
      <data key="d6">11452a08471d93959558de2ece9a69af,2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="SCATTERPLOT" target="Y">
      <data key="d4">2.0</data>
      <data key="d5">The scatterplot, featuring entities X and Y, visually illustrates the relationship between these two variables. Y, as one of the pivotal variables, is plotted on the scatterplot, enabling the observation of potential patterns or trends that exist between X and Y. This graphical representation serves as a fundamental tool in statistical analysis, providing insights into the nature of the association between the variables, whether it be linear, non-linear, or no relationship at all.</data>
      <data key="d6">11452a08471d93959558de2ece9a69af,2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="SCATTERPLOT" target="DATASET_1">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 1 is visualized using a scatterplot that shows the relationship between x-values and y-values</data>
      <data key="d6">84ffe1b8496dc660c47248c9f7b5bdea</data>
    </edge>
    <edge source="SCATTERPLOT" target="DATASET_2">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 2 is visualized using a scatterplot that reveals a poor fit for a simple linear regression model</data>
      <data key="d6">84ffe1b8496dc660c47248c9f7b5bdea</data>
    </edge>
    <edge source="SCATTERPLOT" target="DATASET_3">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 3 is visualized using a scatterplot that highlights an outlier affecting the fitted line</data>
      <data key="d6">84ffe1b8496dc660c47248c9f7b5bdea</data>
    </edge>
    <edge source="SCATTERPLOT" target="DATASET_4">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 4 is visualized using a scatterplot that shows the dependence of the slope on a single unusual data point</data>
      <data key="d6">84ffe1b8496dc660c47248c9f7b5bdea</data>
    </edge>
    <edge source="SCATTERPLOT" target="PARABOLA">
      <data key="d4">1.0</data>
      <data key="d5">Parabola is the curve of best fit shown in the scatterplot</data>
      <data key="d6">11452a08471d93959558de2ece9a69af</data>
    </edge>
    <edge source="SCATTERPLOT" target="QUADRATIC_CURVE">
      <data key="d4">1.0</data>
      <data key="d5">Scatterplot shows the quadratic curve of best fit</data>
      <data key="d6">11452a08471d93959558de2ece9a69af</data>
    </edge>
    <edge source="SCATTERPLOT" target="WESTERN_RED_CEDAR_DATA">
      <data key="d4">1.0</data>
      <data key="d5">A scatterplot can be created using the Western red cedar data to visualize the relationship between variables such as height and diameter</data>
      <data key="d6">ae1e66f5b64284090abc285c1d4389f5</data>
    </edge>
    <edge source="SCATTERPLOT" target="LOGARITHMIC_CURVE_OF_BEST_FIT">
      <data key="d4">1.0</data>
      <data key="d5">A logarithmic curve of best fit can be added to a scatterplot to represent the relationship between a log-transformed variable and another variable</data>
      <data key="d6">ae1e66f5b64284090abc285c1d4389f5</data>
    </edge>
    <edge source="SCATTERPLOT" target="NULL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">A scatterplot is often paired with a null plot to visually assess the fit of a model</data>
      <data key="d6">aa13c33a7e61206e6021e2736002ca9a</data>
    </edge>
    <edge source="SCATTERPLOT" target="REGRESSION_LINE">
      <data key="d4">5.0</data>
      <data key="d5">In the context of statistical analysis, the entities of interest are the SCATTERPLOT and the REGRESSION_LINE. A scatterplot serves as a visual tool to represent the relationship between two variables, typically an explanatory variable and a response variable. Points are plotted on a two-dimensional graph, with each point representing an observation. The scatterplot is instrumental in visualizing the fitted regression line, which is a line of best fit that is drawn to illustrate the relationship between the explanatory variable and the response variable. The regression line is crucial for predicting the response variable based on the explanatory variable. It is added to the scatterplot to provide a clear illustration of the relationship between the variables, enabling the assessment of the strength and direction of the association. The regression line is fitted to the data points in the scatterplot, ensuring that it minimizes the sum of the squared distances from the points to the line, a method known as least squares estimation. This process facilitates a deeper understanding of the data structure and the underlying patterns within the community of interest.</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742,674b8d5bb1f830d0fb944942514d1a16,82cfcd5865cffe55e965a50745656e60,b9ec8a6c7960cc6196ec94fd976f05b0,ef24ca5edd06893b737e6a1c8a9825f6</data>
    </edge>
    <edge source="SCATTERPLOT" target="WELL_FITTING_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">A well-fitting model is illustrated by a scatterplot where the observations are scattered relatively evenly on either side of the regression line</data>
      <data key="d6">674b8d5bb1f830d0fb944942514d1a16</data>
    </edge>
    <edge source="SCATTERPLOT" target="RESPONSE_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">A scatterplot displays the relationship between the response variable and the explanatory variable</data>
      <data key="d6">b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </edge>
    <edge source="SCATTERPLOT" target="EXPLANATORY_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">A scatterplot displays the relationship between the response variable and the explanatory variable</data>
      <data key="d6">b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </edge>
    <edge source="SCATTERPLOT" target="RESIDUAL_PLOT">
      <data key="d4">4.0</data>
      <data key="d5">In the context of statistical analysis, the entities of interest are the SCATTERPLOT and the RESIDUAL_PLOT. A scatterplot is a graphical representation that displays the relationship between two variables, often used as a preliminary tool to visualize potential correlations or patterns in the data. It is a fundamental component in the analysis of regression models, as it allows for the visual inspection of the relationship between the independent and dependent variables.

The residual plot, on the other hand, is a specialized graph that is derived from the scatterplot. It is created by plotting the residuals (the differences between the observed and predicted values) against the predicted values or one of the independent variables. This plot is crucial for checking the assumptions of a regression model, such as linearity, homoscedasticity (constant variance of errors), and independence of errors. By analyzing the residual plot, one can identify if the model is adequately capturing the relationship between the variables or if there are any systematic patterns that suggest a need for model adjustments.

Together, the scatterplot and residual plot serve as powerful tools for data analysis. They are used in conjunction to not only assess the fit of a model but also to understand the underlying structure and relationships within the data. By examining these plots, data scientists can make informed decisions about the appropriateness of the model, the need for transformations, or the presence of outliers that might affect the model's predictive accuracy.</data>
      <data key="d6">15c7b5750483a382ce59751008e86751,312309b45c59e1c84695ac3c7e202742,521acf88540d5897188c9ec65b17e6a6,82cfcd5865cffe55e965a50745656e60</data>
    </edge>
    <edge source="SCATTERPLOT" target="OBSERVATIONS">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis, the scatterplot serves as a visual representation of the relationship between the response and explanatory variables. This is achieved by plotting the observations on the scatterplot, which effectively illustrates the connection between the variables under study. The scatterplot is directly derived from the observations, providing a graphical summary that aids in understanding the structure and patterns within the data. Through this visual tool, one can identify trends, outliers, and potential correlations within the community of interest, enhancing the interpretability of the statistical analysis.</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742,82cfcd5865cffe55e965a50745656e60,b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </edge>
    <edge source="SCATTERPLOT" target="NON_MONOTONIC_NON_LINEAR_RELATIONSHIP">
      <data key="d4">1.0</data>
      <data key="d5">The non-monotonic non-linear relationship is visualized in the scatterplot</data>
      <data key="d6">82cfcd5865cffe55e965a50745656e60</data>
    </edge>
    <edge source="SCATTERPLOT" target="FITTED_REGRESSION_LINE">
      <data key="d4">1.0</data>
      <data key="d5">A scatterplot of the data can include a fitted regression line, which is a visual representation of the relationship between the independent and dependent variables as estimated by the linear regression model.</data>
      <data key="d6">23fc620f1238c6a1b5c5e3a08e149c53</data>
    </edge>
    <edge source="SCATTERPLOT" target="SMOOTHING_CURVE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis and data visualization, the entities of interest are the SCATTERPLOT and the SMOOTHING_CURVE. A scatterplot is a graphical representation used to display the relationship between two variables, typically plotted along two axes. The SCATTERPLOT provides a visual depiction of the data points, allowing for the identification of patterns, clusters, or outliers within the dataset.

To enhance the interpretability of the SCATTERPLOT, a SMOOTHING_CURVE can be added. This curve is fitted to the data points in the scatterplot, serving as a visual aid to help discern the overall pattern or trend that the raw data points might suggest. The SMOOTHING_CURVE is particularly useful in revealing underlying trends or patterns that might not be immediately apparent from the scatterplot alone. By fitting the curve to the data, it can smooth out the variability in the data points, providing a clearer view of the relationship between the variables being studied.

In summary, the SCATTERPLOT and the SMOOTHING_CURVE are complementary tools in statistical analysis. The SCATTERPLOT offers a raw, unadulterated view of the data, while the SMOOTHING_CURVE provides a refined perspective, aiding in the identification of trends and patterns within the data.</data>
      <data key="d6">23fc620f1238c6a1b5c5e3a08e149c53,312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="SCATTERPLOT" target="DATA">
      <data key="d4">1.0</data>
      <data key="d5">The scatterplot is created from the data</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="SCATTERPLOT" target="REPEATED_VALUES">
      <data key="d4">1.0</data>
      <data key="d5">Repeated values of the explanatory variable can affect the appearance and interpretation of a scatterplot</data>
      <data key="d6">521acf88540d5897188c9ec65b17e6a6</data>
    </edge>
    <edge source="SCATTERPLOT" target="RETAIL_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Scatterplot is a graphical representation of the relationship between sales and price for each brand in the retail data</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="DIAMOND_RING_PRICING" target="LINEAR_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Linear regression is the statistical method used in the study of diamond ring pricing to analyze the relationship between diamond weight and ring price.</data>
      <data key="d6">3e7eef51f3109d60697f3299b541b726</data>
    </edge>
    <edge source="DIAMOND_RING_PRICING" target="JOURNAL_OF_STATISTICS_EDUCATION">
      <data key="d4">1.0</data>
      <data key="d5">The Journal of Statistics Education published a study on diamond ring pricing using linear regression.</data>
      <data key="d6">3e7eef51f3109d60697f3299b541b726</data>
    </edge>
    <edge source="LINEAR_REGRESSION" target="DATASET_1">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 1 is analyzed using linear regression to understand the relationship between x-values and y-values</data>
      <data key="d6">84ffe1b8496dc660c47248c9f7b5bdea</data>
    </edge>
    <edge source="LINEAR_REGRESSION" target="DATASET_2">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 2 is analyzed using linear regression, but the model is not suitable due to the curved relationship</data>
      <data key="d6">84ffe1b8496dc660c47248c9f7b5bdea</data>
    </edge>
    <edge source="LINEAR_REGRESSION" target="DATASET_3">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 3 is analyzed using linear regression, but an outlier significantly affects the fitted line</data>
      <data key="d6">84ffe1b8496dc660c47248c9f7b5bdea</data>
    </edge>
    <edge source="LINEAR_REGRESSION" target="DATASET_4">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 4 is analyzed using linear regression, but the model's validity is questionable due to the unique x-value</data>
      <data key="d6">84ffe1b8496dc660c47248c9f7b5bdea</data>
    </edge>
    <edge source="DIAMOND_WEIGHT" target="RING_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">Diamond weight is the independent variable that influences the dependent variable, ring price, in the linear regression model.</data>
      <data key="d6">3e7eef51f3109d60697f3299b541b726</data>
    </edge>
    <edge source="DIAMOND_WEIGHT" target="SOLITAIRE_RING">
      <data key="d4">1.0</data>
      <data key="d5">The price of a Solitaire ring is influenced by the weight of its diamond. The relationship is described by a linear regression model where the weight of the diamond is a predictor variable and the price of the ring is the response variable.</data>
      <data key="d6">f0e0c5b2deaaf9fc2bc8b63d9ab989b1</data>
    </edge>
    <edge source="LINE_OF_BEST_FIT" target="WEIGHT">
      <data key="d4">2.0</data>
      <data key="d5">In the context of a linear regression analysis, the line of best fit, symbolically represented as Price = &#946;0 + &#946;1 * Weight + &#951;, is determined by the relationship between two key variables: Weight and Price. Weight, one of the entities in focus, serves as a crucial variable in defining this line of best fit. This statistical model aims to predict Price based on the weight of an object, where &#946;0 is the intercept, &#946;1 is the slope coefficient associated with Weight, and &#951; represents the error term. The model assumes a linear relationship between Weight and Price, enabling the estimation of Price given the weight of an object.</data>
      <data key="d6">6f10cac870c690419e5351e8a6aeae9e,d4bbb6beb0dd5c40d2941af71b7c1776</data>
    </edge>
    <edge source="LINE_OF_BEST_FIT" target="PRICE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the linear regression model being analyzed, the line of best fit, often determined through the method of least squares, serves a pivotal role in understanding the relationship between the entities of interest. Specifically, the line of best fit is utilized to predict the value of the dependent variable, which in this scenario is referred to as the price. This indicates that price is not merely an independent variable, as one description suggests, but rather the outcome or dependent variable that is being forecasted based on other factors within the model. The line of best fit, therefore, represents the optimal linear relationship that minimizes the sum of the squared differences between the observed prices and the predicted prices, providing a clear indication of how price is expected to change in response to variations in the independent variables. This predictive capability is crucial for understanding and forecasting sales, as it allows for the estimation of price points that are likely to influence sales volumes, thereby informing strategic decisions in pricing and sales forecasting.</data>
      <data key="d6">d4bbb6beb0dd5c40d2941af71b7c1776,f16299fc00a7a69bdf983dce826b4918</data>
    </edge>
    <edge source="LINE_OF_BEST_FIT" target="BETA_0">
      <data key="d4">4.0</data>
      <data key="d5">The entities in focus are the "LINE_OF_BEST_FIT" and "BETA_0". BETA_0 is a crucial parameter that defines the intercept of the LINE_OF_BEST_FIT. This parameter, also referred to as the intercept parameter, plays a significant role in the structure of the line of best fit. The LINE_OF_BEST_FIT, in turn, incorporates BETA_0 as its intercept, indicating the point where the line crosses the y-axis when the independent variable is zero. This relationship between BETA_0 and the LINE_OF_BEST_FIT is fundamental in statistical analysis, particularly in the context of linear regression models, where BETA_0 helps in understanding the baseline prediction before the influence of any independent variables.</data>
      <data key="d6">6f10cac870c690419e5351e8a6aeae9e,b99ecc2f79f56198a8c2adbdff95d576,d4bbb6beb0dd5c40d2941af71b7c1776,f16299fc00a7a69bdf983dce826b4918</data>
    </edge>
    <edge source="LINE_OF_BEST_FIT" target="BETA_1">
      <data key="d4">4.0</data>
      <data key="d5">In the context of statistical analysis, the entities of interest are the LINE_OF_BEST_FIT and BETA_1. BETA_1 is a critical parameter that defines the slope of the LINE_OF_BEST_FIT. This line of best fit, which includes BETA_1 as its slope parameter, is a fundamental component in linear regression analysis. It represents the linear relationship between two variables, where BETA_1 quantifies the change in the dependent variable for a unit change in the independent variable. The LINE_OF_BEST_FIT is determined through methods such as least squares estimation, aiming to minimize the sum of the squared residuals. This line serves as a predictive model, allowing for the estimation of the dependent variable based on the independent variable, with BETA_1 playing a pivotal role in capturing the strength and direction of the relationship.</data>
      <data key="d6">6f10cac870c690419e5351e8a6aeae9e,b99ecc2f79f56198a8c2adbdff95d576,d4bbb6beb0dd5c40d2941af71b7c1776,f16299fc00a7a69bdf983dce826b4918</data>
    </edge>
    <edge source="LINE_OF_BEST_FIT" target="WEIGHT_DEVIATIONS">
      <data key="d4">1.0</data>
      <data key="d5">The line of best fit is a model that represents the relationship between the weight deviations from the mean and the price of Solitaire rings</data>
      <data key="d6">c5b269ff5c94db7ebd2cb9f7be16f171</data>
    </edge>
    <edge source="LINE_OF_BEST_FIT" target="SALES">
      <data key="d4">1.0</data>
      <data key="d5">Sales is the dependent variable that is modeled by the line of best fit</data>
      <data key="d6">f16299fc00a7a69bdf983dce826b4918</data>
    </edge>
    <edge source="LINE_OF_BEST_FIT" target="EPSILON">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon represents the error term in the line of best fit</data>
      <data key="d6">f16299fc00a7a69bdf983dce826b4918</data>
    </edge>
    <edge source="LINE_OF_BEST_FIT" target="RESIDUAL">
      <data key="d4">1.0</data>
      <data key="d5">The residuals are used to find the line of best fit by minimising the sum of squared residuals</data>
      <data key="d6">2e5e1bdaa9fcc7b3391d277fd6bb247a</data>
    </edge>
    <edge source="WEIGHT" target="PRICE">
      <data key="d4">3.0</data>
      <data key="d5">In the context of the diamond market, the weight of a diamond significantly influences its price, as evidenced by a simple linear regression model. This model identifies weight as a crucial predictor variable that directly impacts the price of a solitaire ring. The relationship between weight and price is such that an increase in weight leads to a corresponding increase in price, highlighting the importance of this variable in determining the final cost of a solitaire ring.</data>
      <data key="d6">6f10cac870c690419e5351e8a6aeae9e,d4bbb6beb0dd5c40d2941af71b7c1776,f18bacb0a2fbfea44dd6326224184216</data>
    </edge>
    <edge source="WEIGHT" target="DETERMINISTIC_PART">
      <data key="d4">1.0</data>
      <data key="d5">Weight is a variable that contributes to the deterministic part of the relationship between price and weight</data>
      <data key="d6">d4bbb6beb0dd5c40d2941af71b7c1776</data>
    </edge>
    <edge source="WEIGHT" target="RANDOM_ERROR">
      <data key="d4">1.0</data>
      <data key="d5">Weight is a variable that, along with the random error, contributes to the scatter of observations around the line of best fit</data>
      <data key="d6">d4bbb6beb0dd5c40d2941af71b7c1776</data>
    </edge>
    <edge source="WEIGHT" target="DIAMOND">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the entities mentioned, the weight of a diamond stands out as a pivotal attribute that significantly influences its price. This characteristic is currently under analysis to better understand its relationship with the diamond's cost, highlighting the importance of weight in determining the value of diamonds.</data>
      <data key="d6">c5b269ff5c94db7ebd2cb9f7be16f171,f18bacb0a2fbfea44dd6326224184216</data>
    </edge>
    <edge source="WEIGHT" target="AVG_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Avg_weight is the average of the weight of diamonds in the dataset, used in the model equation</data>
      <data key="d6">f18bacb0a2fbfea44dd6326224184216</data>
    </edge>
    <edge source="WEIGHT" target="INTERCEPT">
      <data key="d4">1.0</data>
      <data key="d5">Weight is used to calculate the intercept, which represents the predicted price of a Solitaire ring with a diamond of average weight</data>
      <data key="d6">f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </edge>
    <edge source="WEIGHT" target="AVERAGE_DIAMOND_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">The average diamond weight is a reference point for analyzing the weight of diamonds</data>
      <data key="d6">c5b269ff5c94db7ebd2cb9f7be16f171</data>
    </edge>
    <edge source="WEIGHT" target="REGRESSION_LINE">
      <data key="d4">1.0</data>
      <data key="d5">The regression line is a model that represents the relationship between the weight of diamonds and the price of Solitaire rings</data>
      <data key="d6">c5b269ff5c94db7ebd2cb9f7be16f171</data>
    </edge>
    <edge source="PRICE" target="DETERMINISTIC_PART">
      <data key="d4">1.0</data>
      <data key="d5">Price is a variable that is influenced by the deterministic part of the relationship between price and weight</data>
      <data key="d6">d4bbb6beb0dd5c40d2941af71b7c1776</data>
    </edge>
    <edge source="PRICE" target="RANDOM_ERROR">
      <data key="d4">1.0</data>
      <data key="d5">Price is a variable that, along with the random error, contributes to the scatter of observations around the line of best fit</data>
      <data key="d6">d4bbb6beb0dd5c40d2941af71b7c1776</data>
    </edge>
    <edge source="PRICE" target="BETA_0">
      <data key="d4">1.0</data>
      <data key="d5">Beta_0 is the intercept parameter that determines the value of Price when Weight is zero</data>
      <data key="d6">6f10cac870c690419e5351e8a6aeae9e</data>
    </edge>
    <edge source="PRICE" target="BETA_1">
      <data key="d4">3.0</data>
      <data key="d5">In the context of the linear regression model being analyzed, PRICE serves as the explanatory variable, closely tied to the coefficient known as BETA_1. BETA_1, in this scenario, functions as the slope parameter, illustrating the impact of PRICE on the dependent variable, which is likely sales volume. This coefficient quantifies the change in PRICE for a unit change in another variable, possibly Weight, though the direct relationship with Weight is not explicitly confirmed in the given descriptions. The descriptions collectively emphasize the pivotal role of BETA_1 in determining how PRICE modifications affect the outcome of interest, making it a crucial element in understanding the dynamics of the model.</data>
      <data key="d6">250ee5d766c64e7975bcc427b4bf9074,6f10cac870c690419e5351e8a6aeae9e,a828fd17fc38e902484872c88a6b242c</data>
    </edge>
    <edge source="PRICE" target="EPSILON">
      <data key="d4">1.0</data>
      <data key="d5">&#1013; is the random error term that represents the deviation of the observed Price from the systematic component</data>
      <data key="d6">6f10cac870c690419e5351e8a6aeae9e</data>
    </edge>
    <edge source="PRICE" target="DIAMOND_DATASET">
      <data key="d4">1.0</data>
      <data key="d5">Price is a variable in the diamond dataset</data>
      <data key="d6">b99ecc2f79f56198a8c2adbdff95d576</data>
    </edge>
    <edge source="PRICE" target="SLR_DIAMOND">
      <data key="d4">1.0</data>
      <data key="d5">Price is the response variable in the SLR.diamond model</data>
      <data key="d6">b99ecc2f79f56198a8c2adbdff95d576</data>
    </edge>
    <edge source="PRICE" target="SLR.DIAMOND">
      <data key="d4">1.0</data>
      <data key="d5">The price variable is the response variable in the SLR.diamond model</data>
      <data key="d6">2b01334fb633566ba368a764ad579fce</data>
    </edge>
    <edge source="PRICE" target="SLOPE_COEFFICIENT">
      <data key="d4">1.0</data>
      <data key="d5">The slope coefficient affects the price of a Solitaire ring, with an increase of 0.1 carat leading to a S$ 372 increase in price</data>
      <data key="d6">e1ad57124a08c0e123deda212ea03c32</data>
    </edge>
    <edge source="PRICE" target="INTERCEPT">
      <data key="d4">3.0</data>
      <data key="d5">In the context of a regression model analyzing the price of Solitaire rings in relation to diamond weight, the intercept represents the estimated price of a ring when the diamond weight is at an average level, approximately valued at S$ 500. This interpretation provides a practical baseline for price estimation. However, it is noted that the concept of the intercept being the price when the diamond weight is zero is theoretically nonsensical, yet it serves as a foundational point in the model. When dealing with price values significantly larger than zero, the interpretation of the intercept necessitates extrapolation, offering insights into the baseline price before the influence of diamond weight is considered. The entities "PRICE" and "INTERCEPT" are central to understanding the structure and relations within the regression model, particularly in the context of Solitaire ring pricing.</data>
      <data key="d6">2ced3e26eaed2dfcd8e4caf49737cab4,e1ad57124a08c0e123deda212ea03c32,f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </edge>
    <edge source="PRICE" target="SOLITAIRE_RING">
      <data key="d4">2.0</data>
      <data key="d5">The PRICE of a SOLITAIRE_RING is intricately linked to the weight of the diamond it features. This relationship is being analyzed to understand how the cost of Solitaire rings fluctuates in response to variations in diamond weight. The PRICE of a Solitaire ring is not solely dependent on the diamond's weight but is also influenced by other factors. However, the weight of the diamond plays a significant role in determining the overall PRICE of the SOLITAIRE_RING. This analysis aims to provide insights into the structure of pricing within the context of diamond weight, offering a comprehensive understanding of the market dynamics for Solitaire rings.</data>
      <data key="d6">c5b269ff5c94db7ebd2cb9f7be16f171,e1ad57124a08c0e123deda212ea03c32</data>
    </edge>
    <edge source="PRICE" target="S_372">
      <data key="d4">1.0</data>
      <data key="d5">S$ 372,- is the predicted increase in price for a diamond ring with a diamond that is 0.1 carat heavier</data>
      <data key="d6">f18bacb0a2fbfea44dd6326224184216</data>
    </edge>
    <edge source="PRICE" target="REGRESSION_LINE">
      <data key="d4">1.0</data>
      <data key="d5">The regression line is a model that represents the relationship between the weight of diamonds and the price of Solitaire rings</data>
      <data key="d6">c5b269ff5c94db7ebd2cb9f7be16f171</data>
    </edge>
    <edge source="PRICE" target="SIMPLE_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Price is the explanatory variable in simple regression, which is analyzed in relation to the response variable sales</data>
      <data key="d6">336546bc73cbe1828a0cc1a45faf8f5a</data>
    </edge>
    <edge source="PRICE" target="MULTIPLE_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Price is one of the explanatory variables in multiple regression, which is analyzed in relation to the response variable sales</data>
      <data key="d6">336546bc73cbe1828a0cc1a45faf8f5a</data>
    </edge>
    <edge source="PRICE" target="SLR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The price variable is used as the predictor variable in the simple linear regression model</data>
      <data key="d6">2e5e1bdaa9fcc7b3391d277fd6bb247a</data>
    </edge>
    <edge source="PRICE" target="SALES">
      <data key="d4">7.0</data>
      <data key="d5">In the context of the statistical analysis conducted on the entities PRICE and SALES, it is evident that PRICE acts as the independent variable, significantly influencing SALES, which is the dependent variable. The relationship between PRICE and SALES is quantified through the regression model, where the effect of PRICE on SALES is consistent across all three store brands, as indicated by the Beta coefficient. This coefficient highlights the magnitude and direction of the relationship, revealing that a change in PRICE leads to a corresponding change in SALES volume. Notably, the model includes a negative coefficient, suggesting an inverse relationship between the PRICE of the product and the SALES volume. This implies that as the PRICE increases, SALES volume tends to decrease, and vice versa. The analysis accounts for the influence of PRICE on SALES volumes, providing insights into how pricing strategies can impact sales performance across different store brands.</data>
      <data key="d6">1820d10ee0f23f34b3ea88ba475bc52d,250ee5d766c64e7975bcc427b4bf9074,7037e0369bfdaad5a730cabb2b44831c,8326c645426789920a99ed373725fa0e,906eb7d6b49fa360e7e5b65c56cd4d76,a1fc936df848a0fbc791e4bcc9b527b6,b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </edge>
    <edge source="PRICE" target="DATAPPOINTS">
      <data key="d4">1.0</data>
      <data key="d5">Price is one of the coordinates (x-axis) of the datapoints in the 3D graphical representation</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376</data>
    </edge>
    <edge source="PRICE" target="REGRESSION_PLANE">
      <data key="d4">1.0</data>
      <data key="d5">Price is one of the explanatory variables in the regression analysis, and the regression plane describes the relationship between price and sales</data>
      <data key="d6">b1690cb1a67892245c0665e5099e322d</data>
    </edge>
    <edge source="PRICE" target="OBSERVED_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Price is one of the explanatory variables in the observed data</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="PRICE" target="DATASET">
      <data key="d4">1.0</data>
      <data key="d5">Price is a variable in the dataset</data>
      <data key="d6">2ced3e26eaed2dfcd8e4caf49737cab4</data>
    </edge>
    <edge source="PRICE" target="SALES_VOLUME">
      <data key="d4">3.0</data>
      <data key="d5">In the context of the provided data, PRICE and SALES_VOLUME are two key entities under examination. PRICE significantly influences SALES_VOLUME, as evidenced by the linear regression model employed in the analysis. PRICE acts as an independent predictor variable, enabling the prediction of SALES_VOLUME. The regression model highlights the relationship between PRICE and SALES_VOLUME, demonstrating that changes in PRICE can have a direct impact on the SALES_VOLUME of the product or service under study. This statistical relationship is crucial for understanding the market dynamics and can be leveraged for strategic decision-making in pricing strategies.</data>
      <data key="d6">7a605c3b689bec7ab2c46df9c123e3f3,b6870535f3975c49d45e62fbe475f198,e079b7c92d5c0b009ff02040eb652bc6</data>
    </edge>
    <edge source="PRICE" target="BRAND_MODEL1">
      <data key="d4">1.0</data>
      <data key="d5">Price is a predictor variable in the model Brand.model1</data>
      <data key="d6">ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </edge>
    <edge source="PRICE" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Price is an independent variable whose effect on sales is quantified by the estimated coefficient for Beta (&#946;)</data>
      <data key="d6">248924760a2bfbc82501fd6b11cfa0aa</data>
    </edge>
    <edge source="PRICE" target="X">
      <data key="d4">1.0</data>
      <data key="d5">X includes the price variable as a numerical predictor in the regression model</data>
      <data key="d6">e800735d6b2a244875f5e0d292de1527</data>
    </edge>
    <edge source="PRICE" target="C_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">price is the original variable that c_pricej is derived from by subtracting the sample mean</data>
      <data key="d6">1b523d1edabe381403fc470a9b8d47fa</data>
    </edge>
    <edge source="PRICE" target="BRAND_MODEL2">
      <data key="d4">1.0</data>
      <data key="d5">Price is an independent variable in Brand.model2, affecting sales</data>
      <data key="d6">93da9813e10a119798de6982977f1239</data>
    </edge>
    <edge source="PRICE" target="AVERAGE_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">The average price is a specific value of the price variable used as a reference point for interpreting the brand variable coefficients</data>
      <data key="d6">9854704301b8df256ca1013b8d53dfac</data>
    </edge>
    <edge source="PRICE" target="BETA">
      <data key="d4">1.0</data>
      <data key="d5">Beta (&#946;) is the coefficient of the price variable (pricej) in the model</data>
      <data key="d6">1d141ab04db553f78a313e430e54abb5</data>
    </edge>
    <edge source="PRICE" target="DESIGN_MATRIX_X">
      <data key="d4">1.0</data>
      <data key="d5">Price is an independent variable in the linear regression model, included in the design matrix X</data>
      <data key="d6">c103c6d096d52868eda26d991194b5f2</data>
    </edge>
    <edge source="PRICE" target="RETAIL_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Price is a variable in the retail data used as a predictor in the regression models</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="PRICE" target="PARALLEL_LINES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Price is an explanatory variable in the parallel lines model</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="PRICE" target="BRAND_A">
      <data key="d4">1.0</data>
      <data key="d5">Brand A stores' sales are affected by price changes, as indicated by the sales model 210.379 - 0.657 * pricei</data>
      <data key="d6">3dd24a54028976ba54304ec7169bb74b</data>
    </edge>
    <edge source="PRICE" target="BRAND_B">
      <data key="d4">1.0</data>
      <data key="d5">Brand B stores' sales are affected by price changes, as indicated by the sales model 225.252 - 0.803 * pricei</data>
      <data key="d6">3dd24a54028976ba54304ec7169bb74b</data>
    </edge>
    <edge source="PRICE" target="BRAND_C">
      <data key="d4">1.0</data>
      <data key="d5">Brand C stores' sales are affected by price changes, as indicated by the sales model 162.879 - 0.231 * pricei</data>
      <data key="d6">3dd24a54028976ba54304ec7169bb74b</data>
    </edge>
    <edge source="PRICE" target="INTERACTION">
      <data key="d4">1.0</data>
      <data key="d5">The effect of price on sales volume can change depending on the brand, indicating an interaction between price and brand</data>
      <data key="d6">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </edge>
    <edge source="BETA_0" target="DETERMINISTIC_PART">
      <data key="d4">1.0</data>
      <data key="d5">Beta_0 is a parameter that contributes to the deterministic part of the relationship between price and weight</data>
      <data key="d6">d4bbb6beb0dd5c40d2941af71b7c1776</data>
    </edge>
    <edge source="BETA_0" target="SALES">
      <data key="d4">1.0</data>
      <data key="d5">Beta_0 is the intercept term in the model, contributing to the baseline sales volume</data>
      <data key="d6">250ee5d766c64e7975bcc427b4bf9074</data>
    </edge>
    <edge source="BETA_0" target="INTERCEPT">
      <data key="d4">1.0</data>
      <data key="d5">The intercept (&#946;0) is the same as the parameter Beta_0 in the linear regression model</data>
      <data key="d6">a828fd17fc38e902484872c88a6b242c</data>
    </edge>
    <edge source="BETA_0" target="Y">
      <data key="d4">3.0</data>
      <data key="d5">In the context of the quadratic regression model under analysis, BETA_0, also denoted as Beta_0 (\u03b20), plays a pivotal role as the intercept parameter. This parameter is crucial as it represents the expected value of Y when all the explanatory variables are set to zero. BETA_0 is an integral part of the systematic component of the model, which is responsible for determining the mean of Y. Essentially, BETA_0 contributes to the calculation of Y by anchoring the regression line at a specific point when the values of the explanatory variables are at their baseline, zero level. This foundational parameter is essential for understanding the baseline behavior of Y in the absence of any influence from the explanatory variables.</data>
      <data key="d6">2d5cdecc342ddacd2c090f1838430cee,9a27580975988e83f6e3a0d9010893b5,a828fd17fc38e902484872c88a6b242c</data>
    </edge>
    <edge source="BETA_0" target="YJ">
      <data key="d4">2.0</data>
      <data key="d5">In the context of a linear regression analysis, BETA_0, denoted as &#946;0, serves as the crucial intercept term in the regression equation that models the relationship with the dependent variable YJ. YJ, in turn, is determined through the application of this linear regression model, where BETA_0 plays a foundational role as the starting point for estimating the values of YJ. This relationship underscores the significance of BETA_0 in establishing the baseline from which the effects of other variables on YJ can be measured and understood.</data>
      <data key="d6">75dc4d8cb195a1f969d9e9496631086b,7ad4ccec4c7bb3702aed71c17dc6b96f</data>
    </edge>
    <edge source="BETA_0" target="YN">
      <data key="d4">1.0</data>
      <data key="d5">Yn is related to BETA_0 as part of the linear regression model for the nth observation</data>
      <data key="d6">8f1d95acff56e1633dceb775fa713174</data>
    </edge>
    <edge source="BETA_0" target="LINEAR_PREDICTOR">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, BETA_0, also denoted as Beta_0 (\u03b20), plays a crucial role as the intercept parameter within the linear predictor. This parameter is fundamental to the linear predictor, as it contributes significantly to the intercept of the mean function of the response variable. As a component of the linear predictor, BETA_0 helps in establishing the baseline value from which the effects of other predictors are measured, ensuring that the model accurately reflects the structure and relationships within the data.</data>
      <data key="d6">67e4c1866b0c6e162e6e3317949e8da9,6a47154bf457c25f22c3cf9f649c5db0</data>
    </edge>
    <edge source="BETA_0" target="QUADRATIC_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Beta_0 (&#946;0) is one of the parameters in the quadratic regression model. It is estimated using least squares estimation</data>
      <data key="d6">e5131a1158e58f1b7b44b21ced7b6f60</data>
    </edge>
    <edge source="BETA_0" target="YI">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the statistical model under consideration, BETA_0, also denoted as Beta_0 or \u03b20, serves a pivotal role as the intercept. This parameter is crucial in the regression model as it establishes the baseline prediction for the response variable YI. Specifically, BETA_0 contributes to estimating the expected value of the log-transformed response variable YI when all the explanatory variables in the model are set to zero. This foundational component of the model helps in understanding the starting point of the relationship between the response variable and the set of explanatory variables, providing a critical point of reference for interpreting the effects of these variables on YI.</data>
      <data key="d6">0fbc9037ca9a440e79e9ac05664b9b3d,90b7e0427699cc1bb461e37939935138</data>
    </edge>
    <edge source="BETA_0" target="YB">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, BETA_0, also denoted as \u03b20, plays a crucial role as a component in the formula utilized to predict the response variable YB. Specifically, BETA_0 is integrated into the calculation to determine YB, which represents the back-transformed prediction for the variable Y. This indicates that BETA_0 is a foundational parameter in the model, contributing to the estimation of YB through a process that likely involves adjustments for the scale or distribution of the data, given the back-transformation step. The relationship between BETA_0 and YB highlights the importance of understanding the underlying structure and relations within the model to accurately interpret the predictions.</data>
      <data key="d6">21e429490eeefe7d9c245058fd48ca68,995fb26a0261f824952fa7b2fac3382e</data>
    </edge>
    <edge source="BETA_0" target="LOG_BRAIN_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Beta_0 is the intercept of the line of best fit when log(brain weight) is regressed against log(body weight)</data>
      <data key="d6">e2422d8b80004aab4ea74d5209587861</data>
    </edge>
    <edge source="BETA_0" target="BRAIN_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Beta_0 is part of the model used to predict brain weight</data>
      <data key="d6">efeeb664622c1ee594e6a08a8322ffe3</data>
    </edge>
    <edge source="BETA_0" target="S">
      <data key="d4">3.0</data>
      <data key="d5">In the context of simple linear regression analysis, BETA_0, denoted as \u03b20, plays a pivotal role as a parameter within the function S(\u03b2). This function is central to the calculation of the sum of squared differences. BETA_0 is specifically utilized in the minimization process of S(\u03b2), which is a critical step in determining the least squares estimates. This statistical procedure ensures that the model's predictions are as close as possible to the observed data points, thereby optimizing the fit of the linear regression model.</data>
      <data key="d6">10ac76f99674a01ca0f4a55586dea07e,416494d940a9f505da9853caca26fe63,50a56c34050fb7f7709300a51399b150</data>
    </edge>
    <edge source="BETA_0" target="S_BETA">
      <data key="d4">1.0</data>
      <data key="d5">Beta_0 is a parameter in the linear regression model that contributes to the calculation of S(&#946;)</data>
      <data key="d6">8f7a05b6d231105a6194eebdb2df372e</data>
    </edge>
    <edge source="BETA_0" target="BETA_HAT_0">
      <data key="d4">1.0</data>
      <data key="d5">Beta_0 is the true parameter that Beta_hat_0 estimates in the original model</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="BETA_0" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Beta_0 is the intercept parameter that Beta hat (&#946;b) estimates in the simple linear regression model</data>
      <data key="d6">d14413709de2897231aaa83be3aa346f</data>
    </edge>
    <edge source="BETA_0" target="BETA_0_HAT">
      <data key="d4">1.0</data>
      <data key="d5">BETA_0 (&#946;0) is the true parameter that BETA_0_HAT (b&#946;0) estimates</data>
      <data key="d6">69ffba28a61d98d8d18f91c24b74dd4a</data>
    </edge>
    <edge source="BETA_1" target="DETERMINISTIC_PART">
      <data key="d4">1.0</data>
      <data key="d5">Beta_1 is a parameter that contributes to the deterministic part of the relationship between price and weight</data>
      <data key="d6">d4bbb6beb0dd5c40d2941af71b7c1776</data>
    </edge>
    <edge source="BETA_1" target="Y">
      <data key="d4">3.0</data>
      <data key="d5">In the context of a quadratic regression model, BETA_1 (&#946;1) serves as a crucial coefficient. It represents the average change in the dependent variable Y for a one unit increase in the independent variable X1, assuming all other variables are held constant, particularly X2. BETA_1 is an integral part of the systematic component of the model, directly influencing the mean of Y. This coefficient is pivotal in understanding the linear relationship between X1 and Y, contributing significantly to the calculation of Y within the model's framework.</data>
      <data key="d6">2d5cdecc342ddacd2c090f1838430cee,9a27580975988e83f6e3a0d9010893b5,a828fd17fc38e902484872c88a6b242c</data>
    </edge>
    <edge source="BETA_1" target="X1">
      <data key="d4">1.0</data>
      <data key="d5">X1 is the explanatory variable associated with the coefficient Beta_1 in the linear regression model</data>
      <data key="d6">a828fd17fc38e902484872c88a6b242c</data>
    </edge>
    <edge source="BETA_1" target="YN">
      <data key="d4">1.0</data>
      <data key="d5">Yn is related to BETA_1 as part of the linear regression model for the nth observation</data>
      <data key="d6">8f1d95acff56e1633dceb775fa713174</data>
    </edge>
    <edge source="BETA_1" target="LINEAR_PREDICTOR">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, BETA_1, denoted as \u03b21, plays a pivotal role as the coefficient of the variable X1 within the linear predictor. This entity, BETA_1, is integral to the linear predictor, as it significantly influences the slope of the mean function of the response variable. By contributing to the slope, BETA_1 helps in quantifying the relationship between X1 and the response, enabling a deeper understanding of how changes in X1 are associated with alterations in the mean response. As a component of the linear predictor, BETA_1 is crucial for estimating the expected value of the response given the predictors, making it a key parameter in linear regression analysis.</data>
      <data key="d6">67e4c1866b0c6e162e6e3317949e8da9,6a47154bf457c25f22c3cf9f649c5db0</data>
    </edge>
    <edge source="BETA_1" target="YJ">
      <data key="d4">1.0</data>
      <data key="d5">Yj is calculated using Beta_1 as the coefficient of the first explanatory variable</data>
      <data key="d6">7ad4ccec4c7bb3702aed71c17dc6b96f</data>
    </edge>
    <edge source="BETA_1" target="QUADRATIC_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Beta_1 (&#946;1) is one of the parameters in the quadratic regression model. It is estimated using least squares estimation</data>
      <data key="d6">e5131a1158e58f1b7b44b21ced7b6f60</data>
    </edge>
    <edge source="BETA_1" target="YI">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the statistical model under analysis, BETA_1, denoted as \u03b21, serves as a crucial slope parameter. This parameter is specifically linked to the explanatory variable X1, playing a pivotal role in the regression model by dictating the extent to which the log-transformed response variable YI is affected for each unitary increase in X1. BETA_1's influence is thus pivotal in predicting YI, as it quantifies the relationship between X1 and the log-transformed YI, enabling a deeper understanding of how changes in X1 correspond to variations in YI.</data>
      <data key="d6">0fbc9037ca9a440e79e9ac05664b9b3d,90b7e0427699cc1bb461e37939935138</data>
    </edge>
    <edge source="BETA_1" target="YB">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, BETA_1, also denoted as \u03b21, plays a pivotal role as the coefficient that quantifies the impact of the independent variable X1 on the predicted response variable YB. This coefficient is integral to the calculation of YB, which represents the back-transformed prediction for the dependent variable Y. The effect of BETA_1 becomes evident when there are changes in X1, illustrating the direct relationship between the predictor and the response variable. This relationship is crucial for understanding how alterations in X1 influence the back-transformed predictions of Y, providing insights into the structure and behavior of the community of interest within the algorithmic analysis framework.</data>
      <data key="d6">21e429490eeefe7d9c245058fd48ca68,995fb26a0261f824952fa7b2fac3382e</data>
    </edge>
    <edge source="BETA_1" target="LOG_BRAIN_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Beta_1 is the slope of the line of best fit when log(brain weight) is regressed against log(body weight)</data>
      <data key="d6">e2422d8b80004aab4ea74d5209587861</data>
    </edge>
    <edge source="BETA_1" target="BRAIN_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Beta_1 is part of the model used to predict brain weight, with a value of 0.75</data>
      <data key="d6">efeeb664622c1ee594e6a08a8322ffe3</data>
    </edge>
    <edge source="BETA_1" target="S">
      <data key="d4">3.0</data>
      <data key="d5">In the context of simple linear regression analysis, BETA_1, denoted as \u03b21, plays a pivotal role as a parameter within the function S(\u03b2). This function is central to the calculation of the sum of squared differences. BETA_1 is specifically utilized in the minimization process of S(\u03b2), which is a critical step in determining the least squares estimates. This statistical procedure ensures that the model's predictions are as close as possible to the observed data points, thereby optimizing the fit of the linear regression model.</data>
      <data key="d6">10ac76f99674a01ca0f4a55586dea07e,416494d940a9f505da9853caca26fe63,50a56c34050fb7f7709300a51399b150</data>
    </edge>
    <edge source="BETA_1" target="S_BETA">
      <data key="d4">1.0</data>
      <data key="d5">Beta_1 is a parameter in the linear regression model that contributes to the calculation of S(&#946;)</data>
      <data key="d6">8f7a05b6d231105a6194eebdb2df372e</data>
    </edge>
    <edge source="BETA_1" target="BETA_HAT_1">
      <data key="d4">1.0</data>
      <data key="d5">Beta_1 is the true parameter that Beta_hat_1 estimates in the original model</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="BETA_1" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Beta_1 is the coefficient of the predictor that Beta hat (&#946;b) estimates in the simple linear regression model</data>
      <data key="d6">d14413709de2897231aaa83be3aa346f</data>
    </edge>
    <edge source="BETA_1" target="BETA_1_HAT">
      <data key="d4">1.0</data>
      <data key="d5">BETA_1 (&#946;1) is the true parameter that BETA_1_HAT (b&#946;1) estimates</data>
      <data key="d6">69ffba28a61d98d8d18f91c24b74dd4a</data>
    </edge>
    <edge source="EPSILON" target="RANDOM_ERROR">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon is the random error term that contributes to the scatter of observations around the line of best fit</data>
      <data key="d6">d4bbb6beb0dd5c40d2941af71b7c1776</data>
    </edge>
    <edge source="EPSILON" target="SIGMA_SQUARED">
      <data key="d4">7.0</data>
      <data key="d5">In the context of statistical modeling, particularly within linear and quadratic regression analyses, EPSILON represents the error term, which is assumed to follow a normal distribution. This error term's variance is denoted by SIGMA_SQUARED (\u03c3^2), a crucial parameter that quantifies the spread of the errors around the line of best fit. SIGMA_SQUARED is an integral component of the variance-covariance matrix, reflecting the variability in EPSILON. It is noteworthy that the descriptions provided are consistent in identifying SIGMA_SQUARED as the variance of the error term EPSILON, emphasizing its role in determining the precision of the regression model's predictions.</data>
      <data key="d6">2d5cdecc342ddacd2c090f1838430cee,6f10cac870c690419e5351e8a6aeae9e,75dc4d8cb195a1f969d9e9496631086b,7ad4ccec4c7bb3702aed71c17dc6b96f,7fc5b8303ab530821bf2140ba6a8a889,8f1d95acff56e1633dceb775fa713174,dd7e7d54883ca0f687568a738b95d4d0</data>
    </edge>
    <edge source="EPSILON" target="MODEL_EQUATION">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon is the error term in the model equation (1.2)</data>
      <data key="d6">f18bacb0a2fbfea44dd6326224184216</data>
    </edge>
    <edge source="EPSILON" target="SALES">
      <data key="d4">6.0</data>
      <data key="d5">In the context of the sales volume model for SALES, EPSILON, denoted as \u03f5j, represents the error term specific to the jth store. This error term encapsulates the random variation in sales that is not explained by the model, embodying the unexplained variation in sales which is not accounted for by the systematic component of the model. Essentially, EPSILON (\u03f5j) signifies the deviation of the observed sales at the jth store from the expected sales, highlighting the random error component that affects sales figures. This error term is crucial for understanding the residual analysis in the sales model, as it quantifies the portion of sales variability that remains unexplained after all known predictors have been accounted for.</data>
      <data key="d6">1b523d1edabe381403fc470a9b8d47fa,250ee5d766c64e7975bcc427b4bf9074,48971100deb5bb374a41c1f2b7b2a86a,8326c645426789920a99ed373725fa0e,906eb7d6b49fa360e7e5b65c56cd4d76,a1fc936df848a0fbc791e4bcc9b527b6</data>
    </edge>
    <edge source="EPSILON" target="Y">
      <data key="d4">14.0</data>
      <data key="d5">In the context of statistical modeling, particularly within linear and quadratic regression frameworks, EPSILON (denoted as \u03f5) plays a pivotal role as the error term. It represents the discrepancy between the observed values of the dependent variable Y and the values predicted by the model. This discrepancy, or unexplained variation, is crucial for understanding the accuracy of the model's predictions and the underlying data's structure.

EPSILON (\u03f5) is an essential component in the linear regression model, where it contributes to the observed values of Y, the dependent variable. The relationship between Y and EPSILON is encapsulated in the model equation Y = X\u03b2 + \u03f5, where X\u03b2 represents the linear combination of the independent variable(s) and the coefficients, and \u03f5 signifies the error term. This error term, \u03f5, encapsulates the variability in Y that cannot be explained by the model's predictors.

In the quadratic regression model, EPSILON also serves as the error term, contributing to the calculation of Y. This highlights its versatility in different types of regression analyses. The error term, \u03f5, is not only a residual but also a key factor in the distribution of Y around its mean. The variance of Y is equal to the variance of \u03f5, indicating that the spread of the observed values of Y is directly influenced by the distribution of the error term.

EPSILON (\u03f5) is further described as the error term that represents the difference between the observed sales values (Y) and the predicted values, emphasizing its role in real-world applications, such as sales forecasting. This error term, \u03f5j, is specific to each observation and represents the unexplained variation in Y around the straight line model, contributing to the variability of Y around the expected value.

In summary, EPSILON (\u03f5) is the error term in both linear and quadratic regression models, playing a critical role in the calculation and understanding of the dependent variable Y. It represents the unexplained variation in Y, affecting the distribution of Y around its mean and contributing to the overall variability of the response variable. Y, the dependent or response variable, includes the error vector EPSILON, which is integral to the model's accuracy and the interpretation of the data.</data>
      <data key="d6">248924760a2bfbc82501fd6b11cfa0aa,2d5cdecc342ddacd2c090f1838430cee,3cbe71f7649e84cd67cb3fa0d3e632cf,69ffba28a61d98d8d18f91c24b74dd4a,6c66e9414880964ee899ceb0f16d22e9,7ad4ccec4c7bb3702aed71c17dc6b96f,7d074208b1259e7d84f9f870d3828bb6,82932abd152e0b84a1c26a2daa4c08df,8f1d95acff56e1633dceb775fa713174,9a27580975988e83f6e3a0d9010893b5,a828fd17fc38e902484872c88a6b242c,dd7e7d54883ca0f687568a738b95d4d0,e41cc40f061f487b1ea0f256d4a963e4,f2300d613896880cbb7c255a4d858315</data>
    </edge>
    <edge source="EPSILON" target="X">
      <data key="d4">7.0</data>
      <data key="d5">In the context of a linear regression model, the entities EPSILON (\u03f5) and X play pivotal roles. X, referred to as the design matrix or the matrix of explanatory variables, is a fundamental component in the model equation. It is used in conjunction with the parameters Beta (\u03b2) and the error term EPSILON (\u03f5) to form the linear regression model equation, which is typically represented as Y = X\u03b2 + \u03f5. Here, Y denotes the response variable that is determined by the interaction of X, the parameters, and the errors. The design matrix X, alongside the parameters, is crucial for modeling the error term EPSILON, ensuring that the linear regression model accurately captures the relationships and variability within the data.</data>
      <data key="d6">119bc73ddf8eebadfb8eae272fa323a7,3cbe71f7649e84cd67cb3fa0d3e632cf,75dc4d8cb195a1f969d9e9496631086b,7ad4ccec4c7bb3702aed71c17dc6b96f,aeddef300427d211c74c6008b5b6b328,dd7e7d54883ca0f687568a738b95d4d0,e4f14e6785c6d7b7469e695aaeb170d0</data>
    </edge>
    <edge source="EPSILON" target="BETA">
      <data key="d4">5.0</data>
      <data key="d5">In the context of a linear regression model, EPSILON and BETA are integral components that together determine the response variable Y. BETA, denoted by the Greek letter &#946;, represents the coefficients of the model, which quantify the relationship between the predictor variables X and the response variable Y. These coefficients are estimated through methods such as least squares, aiming to minimize the sum of the squared differences between the observed and predicted values of Y. On the other hand, EPSILON, symbolized by the Greek letter &#1013;, signifies the error terms in the model. These errors encapsulate the variability in Y that cannot be explained by the predictor variables X. The errors are assumed to be independent and identically distributed, typically with a mean of zero and a constant variance, facilitating the use of statistical inference to make predictions and draw conclusions about the population from which the data are drawn. Together, BETA and EPSILON play pivotal roles in the linear model equation Y = X&#946; + &#1013;, where X represents the matrix of predictor variables. This equation is central to understanding and interpreting the relationships and structure within the community of interest, allowing for the estimation of Y given the values of X and the parameters &#946; and &#1013;.</data>
      <data key="d6">01d5ee79489582b4135fc96f676b24a0,119bc73ddf8eebadfb8eae272fa323a7,3cbe71f7649e84cd67cb3fa0d3e632cf,7ad4ccec4c7bb3702aed71c17dc6b96f,dd7e7d54883ca0f687568a738b95d4d0</data>
    </edge>
    <edge source="EPSILON" target="NN">
      <data key="d4">4.0</data>
      <data key="d5">EPSILON and NN are intricately connected within the context of statistical modeling. EPSILON, denoted as \u03f5, is the error term in a model and is assumed to follow a multivariate normal distribution, which is represented by NN. This relationship signifies that the variability in EPSILON can be described by a multivariate normal distribution, NN, implying that the errors are normally distributed across multiple dimensions. This assumption is crucial for various statistical analyses, particularly in regression models, where the normality of error terms is a fundamental requirement for the validity of least squares estimators and residual analysis. The adherence of EPSILON to the multivariate normal distribution NN facilitates a robust statistical inference, enabling accurate predictions and estimations within the model.</data>
      <data key="d6">75dc4d8cb195a1f969d9e9496631086b,8f1d95acff56e1633dceb775fa713174,dd7e7d54883ca0f687568a738b95d4d0,e41cc40f061f487b1ea0f256d4a963e4</data>
    </edge>
    <edge source="EPSILON" target="IN">
      <data key="d4">4.0</data>
      <data key="d5">In the context of a linear regression model, EPSILON, which represents the error term, is closely associated with IN, a matrix that plays a pivotal role in the model's structure. Specifically, IN is an integral component of the variance-covariance matrix that characterizes the distribution of EPSILON. This relationship is further elucidated by the fact that the covariance matrix of EPSILON is given by \u03c3^2In, where \u03c3^2 denotes the variance of the error term and IN is the identity matrix of appropriate dimension. This formulation implies that the errors are homoscedastic, meaning they have constant variance, and are uncorrelated, as the off-diagonal elements of the covariance matrix are zero. The variance-covariance matrix of the error term \u03f5, which is another notation for EPSILON, is also explicitly stated to be \u03c3^2In, reinforcing the central role of IN in defining the statistical properties of the errors in the linear regression framework.</data>
      <data key="d6">75dc4d8cb195a1f969d9e9496631086b,7ad4ccec4c7bb3702aed71c17dc6b96f,8f1d95acff56e1633dceb775fa713174,dd7e7d54883ca0f687568a738b95d4d0</data>
    </edge>
    <edge source="EPSILON" target="EPSILONJ">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon is the column vector containing all Epsilonj values</data>
      <data key="d6">b5d0a103e1f34a00aef67fedd0e8c693</data>
    </edge>
    <edge source="EPSILON" target="VARIANCE_FUNCTION">
      <data key="d4">1.0</data>
      <data key="d5">&#1013;'s variance contributes to the constant variance function of the response Y</data>
      <data key="d6">67e4c1866b0c6e162e6e3317949e8da9</data>
    </edge>
    <edge source="EPSILON" target="YJ">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, particularly linear regression analysis, EPSILON (\u03f5j) represents the error term that is intrinsic to the jth unit of observation. This error term, EPSILON, encapsulates the variability in the data that cannot be explained by the linear combination of the predictor variables, xj, and the coefficients, Beta. YJ, the dependent variable for the jth observation, is formulated as a linear combination of Beta and xj, with the addition of EPSILON, thereby accounting for the residual variability. This inclusion of the error term in YJ is crucial for conducting residual analysis and assessing the goodness-of-fit of the model, ensuring that the model accurately represents the underlying data structure and relationships.</data>
      <data key="d6">6c1684ed2a4840576c6b0f4d1a3a482f,7fc5b8303ab530821bf2140ba6a8a889</data>
    </edge>
    <edge source="EPSILON" target="N">
      <data key="d4">3.0</data>
      <data key="d5">In the context of a quadratic regression model, EPSILON represents the error term, which is assumed to follow a normal distribution characterized by a mean of 0 and a variance of \u03c3^2, denoted as N(0, \u03c3^2). This error term is crucial for understanding the discrepancies between the observed data points and the values predicted by the model. N, in this scenario, refers to the dimension or length of the error term vector EPSILON, indicating the number of observations or data points in the dataset. The normal distribution assumption for EPSILON ensures that the least squares estimators used in the regression analysis are unbiased and efficient, facilitating a robust statistical inference.</data>
      <data key="d6">2d5cdecc342ddacd2c090f1838430cee,6c1684ed2a4840576c6b0f4d1a3a482f,e4f14e6785c6d7b7469e695aaeb170d0</data>
    </edge>
    <edge source="EPSILON" target="YI">
      <data key="d4">1.0</data>
      <data key="d5">Yi includes the error term Epsilon (&#1013;i) for the ith unit of observation in the quadratic regression model</data>
      <data key="d6">6c1684ed2a4840576c6b0f4d1a3a482f</data>
    </edge>
    <edge source="EPSILON" target="DESIGN_MATRIX">
      <data key="d4">1.0</data>
      <data key="d5">The design matrix X, along with the vector of parameters Beta, is used to predict the response vector Y in the linear model. The difference between the predicted values and the actual observed values is captured by the vector of random errors Epsilon.</data>
      <data key="d6">e7494d6cfc3e38a4d2f3f6b21ef6445d</data>
    </edge>
    <edge source="EPSILON" target="INDEPENDENCE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entities EPSILON and INDEPENDENCE are closely related. The errors, denoted as \u03f51, ..., \u03f5n (also referred to as epsilon or \u03f5), are assumed to be independent. This independence of errors is a fundamental requirement for the independence assumption, which is a critical condition in various statistical models, particularly in regression analysis. The independence assumption ensures that the errors do not influence each other, which is essential for the validity of statistical inferences and the reliability of model predictions. This independence of the errors, EPSILON, is a key aspect of the INDEPENDENCE assumption, ensuring that each error term is unrelated to the others, thereby maintaining the integrity of the statistical model.</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6,3cbe71f7649e84cd67cb3fa0d3e632cf</data>
    </edge>
    <edge source="EPSILON" target="HOMOSCEDASTICITY">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, EPSILON refers to the errors in a model, typically denoted as \u03f51, ..., \u03f5n. These errors are pivotal in understanding the variability and the fit of the model to the data. A key assumption in many statistical models, particularly in linear regression, is that these errors, EPSILON, have constant variance. This property is known as HOMOSCEDASTICITY. HOMOSCEDASTICITY is crucial as it ensures that the least squares estimators are unbiased and have minimum variance, making them the best linear unbiased estimators (BLUE). The assumption of HOMOSCEDASTICITY applies directly to the errors, EPSILON, implying that the variability of the errors does not change across the range of the predictor variables. This ensures that the model's predictions are reliable and that the standard errors of the estimates are accurate, facilitating valid hypothesis testing and confidence interval estimation.</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6,3cbe71f7649e84cd67cb3fa0d3e632cf</data>
    </edge>
    <edge source="EPSILON" target="NORMALITY">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, EPSILON refers to the errors (\u03f51, ..., \u03f5n) that are inherent in the data. These errors play a crucial role in understanding the variability and uncertainty in the observed data points. The assumption of NORMALITY is directly associated with these errors, stipulating that they are distributed according to a normal distribution. This normality assumption is fundamental for various statistical tests and models, as it ensures that the least squares estimators provide unbiased and efficient parameter estimates. The normal distribution of errors is a key requirement for conducting residual analysis and making valid inferences about the population from which the data is sampled. This assumption facilitates the use of parametric tests and linear regression models, which rely on the properties of the normal distribution to estimate and interpret the relationships between variables accurately.</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6,3cbe71f7649e84cd67cb3fa0d3e632cf</data>
    </edge>
    <edge source="EPSILON" target="YN">
      <data key="d4">1.0</data>
      <data key="d5">The response variables Y1, ..., Yn are influenced by the errors &#1013;1, ..., &#1013;n in the regression model</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6</data>
    </edge>
    <edge source="EPSILON" target="HETEROSCEDASTICITY">
      <data key="d4">1.0</data>
      <data key="d5">Heteroscedasticity is a violation of the assumption that the errors &#1013;1, ..., &#1013;n have constant variance</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6</data>
    </edge>
    <edge source="EPSILON" target="IDENTICALLY_DISTRIBUTED">
      <data key="d4">1.0</data>
      <data key="d5">The errors &#1013;1, ..., &#1013;n are assumed to be identically distributed, although this is not true for the response variables</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6</data>
    </edge>
    <edge source="EPSILON" target="LOG_BRAIN_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">epsilon (&#1013;) represents the error term in the linear regression model when log(brain weight) is regressed against log(body weight)</data>
      <data key="d6">e2422d8b80004aab4ea74d5209587861</data>
    </edge>
    <edge source="EPSILON" target="S">
      <data key="d4">1.0</data>
      <data key="d5">&#1013;j is the error term for the jth observation, which is part of the simple linear regression model used in the calculation of S(&#946;)</data>
      <data key="d6">50a56c34050fb7f7709300a51399b150</data>
    </edge>
    <edge source="EPSILON" target="ALPHA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon (&#1013;) is the error term in the linear regression model that Alpha hat (&#945;b) accounts for</data>
      <data key="d6">f5716ce115458c0652124734ca344806</data>
    </edge>
    <edge source="EPSILON" target="ALPHA">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon_j is the error term in the model parameterised by Alpha</data>
      <data key="d6">5609007c6229060ffc85d8056a7fefde</data>
    </edge>
    <edge source="EPSILON" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon (&#1013;) is the error term in the linear model, which affects the distribution of Beta hat (&#946;b)</data>
      <data key="d6">b70a75a6412b2e5c44af50734844f4be</data>
    </edge>
    <edge source="EPSILON" target="DI">
      <data key="d4">1.0</data>
      <data key="d5">epsilon (&#1013;) is part of the error term that contributes to the calculation of the Cook's distance Di</data>
      <data key="d6">98d6982108f2d42fe0437bff8c666e17</data>
    </edge>
    <edge source="EPSILON" target="BRAND_A">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon (&#1013;j) is the error term for the jth store when it is of Brand A</data>
      <data key="d6">1d141ab04db553f78a313e430e54abb5</data>
    </edge>
    <edge source="EPSILON" target="BRAND_B">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon (&#1013;j) is the error term for the jth store when it is of Brand B</data>
      <data key="d6">1d141ab04db553f78a313e430e54abb5</data>
    </edge>
    <edge source="EPSILON" target="BRAND_C">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon (&#1013;j) is the error term for the jth store when it is of Brand C</data>
      <data key="d6">1d141ab04db553f78a313e430e54abb5</data>
    </edge>
    <edge source="EPSILON" target="SALES_VOLUME">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the regression model analyzing sales data, EPSILON plays a crucial role as the error term. It quantifies the unexplained variation in SALES_VOLUME, capturing the discrepancy between the observed sales volume at each store and the sales volume that the model predicts based on the available explanatory variables. This discrepancy, or deviation, is essential for understanding the limitations of the model and for conducting a thorough residual analysis to assess model fit and identify potential outliers or patterns in the data that the model does not capture. EPSILON, therefore, serves as a critical component in the statistical inference process, providing insights into the structure and relations within the community of interest, which in this case, is the group of stores being analyzed.</data>
      <data key="d6">06199787dd7f75f7338dd24d4f3dc26e,b6870535f3975c49d45e62fbe475f198</data>
    </edge>
    <edge source="EPSILON" target="RETAIL_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon is the error term in the regression models, representing the unexplained variation in sales in the retail data</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="EPSILON" target="PARALLEL_LINES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon is the error term in the parallel lines model</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="RANDOM_ERROR" target="MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The model includes a random error component that represents the variability not explained by the systematic component</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="Y">
      <data key="d4">4.0</data>
      <data key="d5">In the context of statistical analysis, the entities SIGMA_SQUARED and Y are intricately related. SIGMA_SQUARED, denoted as \u03c3^2, plays a pivotal role as the variance of the error term \u03f5, which contributes to the overall variability in the data. This variance, \u03c3^2, is also identified as the variance of the response vector Y, indicating that the error term's variability is directly responsible for the dispersion observed in Y. Furthermore, \u03c3^2 is a component of the covariance matrix of the vector Y, suggesting its importance in understanding the relationships and structure within the data. Thus, the variance of Y is fundamentally tied to \u03c3^2, encapsulating the error term's influence on the response vector's variability.</data>
      <data key="d6">7ad4ccec4c7bb3702aed71c17dc6b96f,9005147593b2f27b9e2a5eede3601bdc,9a27580975988e83f6e3a0d9010893b5,e41cc40f061f487b1ea0f256d4a963e4</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="N">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, SIGMA_SQUARED (\u03c3^2) serves as the variance parameter for the distribution N. This indicates the spread or dispersion of the data points in the distribution. Specifically, in the normal distribution N, SIGMA_SQUARED plays a crucial role in defining the width of the distribution curve, where a larger value of \u03c3^2 implies a wider spread of data points around the mean, and a smaller value suggests a tighter clustering of data around the mean. This parameter is essential for understanding the structure and characteristics of the community of interest, as it helps in quantifying the variability within the data.</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12,b5d0a103e1f34a00aef67fedd0e8c693</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="VARIANCE_FUNCTION">
      <data key="d4">1.0</data>
      <data key="d5">&#963;^2 is the constant variance of the response Y given the values of the explanatory variables</data>
      <data key="d6">67e4c1866b0c6e162e6e3317949e8da9</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="X">
      <data key="d4">1.0</data>
      <data key="d5">The design matrix X is used in the covariance matrix of the linear regression model, which includes sigma squared (&#963;^2)</data>
      <data key="d6">e4f14e6785c6d7b7469e695aaeb170d0</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="E">
      <data key="d4">1.0</data>
      <data key="d5">&#963;^2 is the variance of the normal distribution of log(Y), which is used to calculate the expected value of Y</data>
      <data key="d6">34fceaaf7d835828b5ee2327325c37f8</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="MLE">
      <data key="d4">1.0</data>
      <data key="d5">&#963;^2 is the variance of the error term in the linear regression model, for which the MLE is derived</data>
      <data key="d6">d738df7d83784c8a41b3948271c537b6</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="LOG_LIKELIHOOD">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, SIGMA_SQUARED (\u03c3^2) and LOG_LIKELIHOOD are two pivotal components. SIGMA_SQUARED represents the variance of the error term, a crucial parameter in the log-likelihood function. This function, in turn, is maximized for a given value of \u03c3^2 by the vector \u03b2, which minimizes the residual sum of squares function S(\u03b2). This relationship underscores the interdependence between the variance of the error term and the log-likelihood function, highlighting how the optimization of one directly influences the other. The process of maximizing the log-likelihood function for a given \u03c3^2 by minimizing the residual sum of squares is a fundamental aspect of statistical inference, particularly in the context of regression analysis.</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12,ad799500572246a07f983a3b92c0e61f</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="L">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, SIGMA_SQUARED, denoted as \u03c3^2, plays a crucial role as a parameter within the likelihood function, L(\u03b2, \u03c3^2|y). This parameter, SIGMA_SQUARED, is integral to the formulation of L, as it helps quantify the variability or dispersion of the data points around the model predictions. The likelihood function, L, is a fundamental component in statistical inference, particularly in the estimation of model parameters, where it measures the goodness of fit of a set of parameters given the observed data, y. The presence of SIGMA_SQUARED within L indicates its importance in assessing the model's ability to accurately predict the data, considering the inherent randomness or noise in the observations.</data>
      <data key="d6">e7edd8b2874a350779ae20f1ecdf4733,f632f01188d2c6e3091a965580cb4600</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="BETA_HAT">
      <data key="d4">7.0</data>
      <data key="d5">In the context of statistical modeling, SIGMA_SQUARED (\u03c3^2) plays a dual yet interconnected role in relation to BETA_HAT (\u03b2b). Firstly, \u03c3^2 is identified as a pivotal parameter within the model, directly influencing the calculation of \u03b2b through the Maximum Likelihood Estimation (MLE) method. This highlights its significance in estimating the parameters of the model. Secondly, \u03c3^2 is recognized as the variance of the error term, a critical component that shapes the distribution of \u03b2b. This variance not only affects the precision of \u03b2b but also contributes to the covariance matrix of its distribution, thereby impacting the overall structure and reliability of the statistical model. Thus, SIGMA_SQUARED is central to understanding the variability and uncertainty associated with BETA_HAT, making it a key element in statistical inference and model analysis.</data>
      <data key="d6">2de7a36b32bf79c8f32612c8aaa9daa8,3d357cfa3ef0d00f49cf4acaeac1c9d1,45f31b040576e9f3b4def6d0466cc016,542f546c5a131196e4701fb33c9b1dee,9dddcd96af7b557e578b3f5f36efacd7,b70a75a6412b2e5c44af50734844f4be,d14413709de2897231aaa83be3aa346f</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="VARIANCE_BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">SIGMA_SQUARED (&#963;^2) is part of the variance of the least squares estimator BETA_HAT</data>
      <data key="d6">69ffba28a61d98d8d18f91c24b74dd4a</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="SIGMA_SQUARED_MLE">
      <data key="d4">1.0</data>
      <data key="d5">Sigma squared (&#963;^2) is the true error variance that Sigma squared MLE (&#963;b^2_MLE) estimates</data>
      <data key="d6">09caa54ca1372d152e47051be4d44ede</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="Z">
      <data key="d4">1.0</data>
      <data key="d5">Z is used in estimating the variance, which is related to the error variance (&#963;^2)</data>
      <data key="d6">09caa54ca1372d152e47051be4d44ede</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="S_HAT_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">&#963;^2 is the true variance that s^2(Y) estimates in the linear regression model</data>
      <data key="d6">6648f0d6deed51fb4fb25e6992a71ddf</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="S2">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, SIGMA_SQUARED, denoted as \u03c3^2, represents the error variance, a fundamental parameter that quantifies the dispersion of data points around the mean. S2, or S squared (s^2), serves as an unbiased estimator for SIGMA_SQUARED, meaning it is designed to accurately estimate the true variance of the population without systematic bias. This relationship between SIGMA_SQUARED and S2 is crucial in understanding the reliability and precision of statistical models, particularly in the realm of linear regression and residual analysis, where estimating the error variance is essential for assessing model fit and making valid inferences.</data>
      <data key="d6">9923e77ac6b3de95cb5026bc5e7fe8c0,fc5b725f3c662c5471af20efdcc2dbff</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="SIGMA_HAT_SQUARED_MLE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, SIGMA_SQUARED (\u03c3^2) represents the true variance of the error term in a model. This true variance is a fundamental parameter that quantifies the dispersion of the error terms around the model's predictions. SIGMA_HAT_SQUARED_MLE (\u03c3b^2) MLE, on the other hand, is the Maximum Likelihood Estimate (MLE) of SIGMA_SQUARED. It is an attempt to estimate the true variance based on the observed data. However, it is important to note that while the MLE estimate is often used for its desirable properties, such as consistency and asymptotic efficiency, it may be biased in finite samples. This means that the estimate may not exactly equal the true variance, especially in smaller datasets, leading to potential underestimation or overestimation of the true variance. Despite this bias, the MLE estimate remains a crucial tool in statistical inference and model fitting, providing a point estimate for the variance of the error term that can be used in hypothesis testing, confidence interval construction, and prediction intervals.</data>
      <data key="d6">2673d078d29f2af78fab9b6eacd15e37,45f31b040576e9f3b4def6d0466cc016</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="S_SQUARED">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, SIGMA_SQUARED (\u03c3^2) and S_SQUARED (s^2) are pivotal concepts. SIGMA_SQUARED represents the true variance that exists in the population, which is often unknown and serves as the target for estimation. S_SQUARED, on the other hand, is the unbiased estimate of this true variance. It is derived from the sample data and is designed to provide an accurate reflection of the population's variance without systematic bias. This means that on average, over many samples, S_SQUARED will equal SIGMA_SQUARED, making it a reliable estimator in statistical inference, particularly when dealing with the error term in regression analysis or other statistical models.</data>
      <data key="d6">2673d078d29f2af78fab9b6eacd15e37,45f31b040576e9f3b4def6d0466cc016</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="YC">
      <data key="d4">4.0</data>
      <data key="d5">In the context of statistical analysis, Sigma squared (&#963;^2) plays a significant role in understanding the variability and interrelation within the data of YC. It is a crucial component of the covariance matrix of YC, which provides insights into the linear dependence between the variables in the distribution of YC. Additionally, Sigma squared (&#963;^2) is also an integral part of the variance of YC, indicating the dispersion of the values in YC from their mean. This variance component is essential for assessing the spread and variability of YC, contributing to a comprehensive understanding of the data's structure and relationships.</data>
      <data key="d6">1da117a2f92b2db00290d2a0bfc06beb,679722cf8ce5ce5aee4e379528470efe,74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="EB">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis, SIGMA_SQUARED (\u03c3^2) plays a significant role in understanding the variability and interrelation within the entity EB. Specifically, SIGMA_SQUARED is an integral component of the covariance matrix of EB, which provides insights into the linear dependence between EB and other variables in the dataset. Moreover, \u03c3^2 directly influences the variance of EB, indicating its importance in quantifying the dispersion of EB values around their mean. This relationship underscores the significance of SIGMA_SQUARED in assessing the uncertainty and variability associated with EB, making it a critical parameter in statistical inference and model estimation.</data>
      <data key="d6">5a0d392715f06d5e873f45ae06aa729a,74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="S">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, SIGMA_SQUARED (\u03c3^2) represents the error variance in the regression model. It is a fundamental parameter that quantifies the dispersion of the observed data points around the regression line. S, on the other hand, is an unbiased estimate of SIGMA_SQUARED (\u03c3^2), serving as a practical measure to approximate the error variance. This estimation is crucial for understanding the reliability and accuracy of the regression model, as it provides insights into the variability of the data and the goodness of fit of the model. By leveraging S as an estimate, data scientists can perform residual analysis, hypothesis testing, and make informed decisions regarding model selection and prediction intervals.</data>
      <data key="d6">90b7e0427699cc1bb461e37939935138,c47968226557bc2eb5aec5bb7994fd0e</data>
    </edge>
    <edge source="SIGMA_SQUARED" target="DI">
      <data key="d4">1.0</data>
      <data key="d5">Di is indirectly related to Sigma squared (&#963;^2), the true error variance</data>
      <data key="d6">0443ab5e20a4f6b2f243c989ef6c723a</data>
    </edge>
    <edge source="LM_FUNCTION" target="SLR_DIAMOND">
      <data key="d4">1.0</data>
      <data key="d5">lm() is used to fit the SLR.diamond model</data>
      <data key="d6">b99ecc2f79f56198a8c2adbdff95d576</data>
    </edge>
    <edge source="LM_FUNCTION" target="BRAND_MODEL4">
      <data key="d4">1.0</data>
      <data key="d5">brand.model4 is created by the lm() function in R</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="SUMMARY_FUNCTION" target="SLR_DIAMOND">
      <data key="d4">1.0</data>
      <data key="d5">summary() is used to provide a summary of the SLR.diamond model</data>
      <data key="d6">b99ecc2f79f56198a8c2adbdff95d576</data>
    </edge>
    <edge source="USINGR_PACKAGE" target="DIAMOND_DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The UsingR package contains the diamond dataset</data>
      <data key="d6">b99ecc2f79f56198a8c2adbdff95d576</data>
    </edge>
    <edge source="DIAMOND_DATASET" target="SLR_DIAMOND2">
      <data key="d4">1.0</data>
      <data key="d5">The diamond dataset is used to fit the linear model slr_diamond2</data>
      <data key="d6">f18bacb0a2fbfea44dd6326224184216</data>
    </edge>
    <edge source="DIAMOND_DATASET" target="SIMPLE_LINEAR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The diamond dataset is suitable for exploratory analysis and modeling, including the application of simple linear models</data>
      <data key="d6">b8ec334f8c87bf1d9cb6043fa1a64214</data>
    </edge>
    <edge source="USINGR" target="INSTALL.PACKAGES">
      <data key="d4">1.0</data>
      <data key="d5">The UsingR package is installed using the install.packages function</data>
      <data key="d6">2b01334fb633566ba368a764ad579fce</data>
    </edge>
    <edge source="USINGR" target="LIBRARY">
      <data key="d4">1.0</data>
      <data key="d5">The UsingR package is loaded into the R session using the library function</data>
      <data key="d6">2b01334fb633566ba368a764ad579fce</data>
    </edge>
    <edge source="SLR.DIAMOND" target="LM">
      <data key="d4">1.0</data>
      <data key="d5">The SLR.diamond model is created by fitting the price of diamonds as a function of their carat weight using the lm function</data>
      <data key="d6">2b01334fb633566ba368a764ad579fce</data>
    </edge>
    <edge source="SLR.DIAMOND" target="DIAMOND">
      <data key="d4">1.0</data>
      <data key="d5">The diamond dataset is used as the data source for the SLR.diamond model</data>
      <data key="d6">2b01334fb633566ba368a764ad579fce</data>
    </edge>
    <edge source="SLR.DIAMOND" target="SUMMARY">
      <data key="d4">1.0</data>
      <data key="d5">The summary function is used to provide a statistical summary of the SLR.diamond model</data>
      <data key="d6">2b01334fb633566ba368a764ad579fce</data>
    </edge>
    <edge source="LM" target="DIAMOND">
      <data key="d4">1.0</data>
      <data key="d5">lm() is used to fit a linear model to the diamond dataset</data>
      <data key="d6">35e06960dba699ce0d56fc1e98bdbe96</data>
    </edge>
    <edge source="LM" target="I">
      <data key="d4">1.0</data>
      <data key="d5">I() is used in the formula argument of lm() to evaluate the expression before using it as an explanatory variable</data>
      <data key="d6">35e06960dba699ce0d56fc1e98bdbe96</data>
    </edge>
    <edge source="LM" target="MODEL">
      <data key="d4">1.0</data>
      <data key="d5">lm() function is used to create the model</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="LM" target="PLR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">lm() is the function in R used to fit the quadratic regression model plr.model</data>
      <data key="d6">084dadebfca8bcb6377c205c45bee295</data>
    </edge>
    <edge source="LM" target="TREE_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">lm function is used to create the tree.model</data>
      <data key="d6">25fce1af816975003128126b5cfea73b</data>
    </edge>
    <edge source="DIAMOND" target="SOLITAIRE_RING">
      <data key="d4">3.0</data>
      <data key="d5">The Solitaire_ring is a piece of jewelry that prominently features a single Diamond as its main stone. Diamonds, known for their brilliance and durability, are the essential component of Solitaire rings, serving as the focal point of the design. In the context provided, the Diamond is specifically highlighted as a crucial element of the Solitaire ring, emphasizing its significance in the composition and aesthetic appeal of this type of ring. The Solitaire ring's design is centered around showcasing the solitary Diamond, making it a symbol of simplicity and elegance in the world of jewelry.</data>
      <data key="d6">c5b269ff5c94db7ebd2cb9f7be16f171,e1ad57124a08c0e123deda212ea03c32,f18bacb0a2fbfea44dd6326224184216</data>
    </edge>
    <edge source="DIAMOND" target="INTERCEPT">
      <data key="d4">1.0</data>
      <data key="d5">Diamond weight is a factor in determining the intercept, which represents the predicted price of a Solitaire ring with a diamond of average weight</data>
      <data key="d6">f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </edge>
    <edge source="RESIDUAL_STANDARD_ERROR" target="MULTIPLE_R_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">Residual standard error is related to Multiple R-squared as both are measures of model fit, with R-squared indicating the proportion of variance explained by the model</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="RESIDUAL_STANDARD_ERROR" target="ADJUSTED_R_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">Residual standard error is related to Adjusted R-squared as both are measures of model fit, with Adjusted R-squared adjusting for the number of predictors</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="RESIDUAL_STANDARD_ERROR" target="F_STATISTIC">
      <data key="d4">1.0</data>
      <data key="d5">Residual standard error is related to F-statistic as both are used to assess the significance of the model</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="RESIDUAL_STANDARD_ERROR" target="P_VALUE">
      <data key="d4">1.0</data>
      <data key="d5">Residual standard error is related to P-value as both are used to assess the significance of the model</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="MULTIPLE_R_SQUARED" target="ADJUSTED_R_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">Multiple R-squared is related to Adjusted R-squared as Adjusted R-squared adjusts for the number of predictors in the model</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="MULTIPLE_R_SQUARED" target="F_STATISTIC">
      <data key="d4">1.0</data>
      <data key="d5">Multiple R-squared is related to F-statistic as both are used to assess the significance of the model</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="MULTIPLE_R_SQUARED" target="P_VALUE">
      <data key="d4">1.0</data>
      <data key="d5">Multiple R-squared is related to P-value as both are used to assess the significance of the model</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="ADJUSTED_R_SQUARED" target="F_STATISTIC">
      <data key="d4">1.0</data>
      <data key="d5">Adjusted R-squared is related to F-statistic as both are used to assess the significance of the model</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="ADJUSTED_R_SQUARED" target="P_VALUE">
      <data key="d4">1.0</data>
      <data key="d5">Adjusted R-squared is related to P-value as both are used to assess the significance of the model</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="F_STATISTIC" target="P_VALUE">
      <data key="d4">1.0</data>
      <data key="d5">F-statistic is related to P-value as the P-value is calculated based on the F-statistic to determine the significance of the model</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="ESTIMATE" target="INTERCEPT">
      <data key="d4">1.0</data>
      <data key="d5">Estimate is related to Intercept as the Estimate column lists the intercept of the line of best fit</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="ESTIMATE" target="SLOPE">
      <data key="d4">1.0</data>
      <data key="d5">Estimate is related to Slope as the Estimate column lists the slope of the line of best fit</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="INTERCEPT" target="SOLITAIRE_RING_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">Intercept is related to Solitaire Ring Price as it represents the expected average price of a Solitaire ring with a diamond of zero weight</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="INTERCEPT" target="SOLITAIRE_RING">
      <data key="d4">2.0</data>
      <data key="d5">In the context of a linear regression model analyzing the price of diamond jewelry, the intercept plays a significant role. Specifically, the intercept represents the baseline price estimate for a solitaire ring when the weight of the diamond is zero. This value, often referred to as the INTERCEPT in statistical models, is crucial for understanding the starting point of the price estimation for SOLITAIRE_RING. However, it's important to note that while the intercept provides a mathematical baseline, its practical application in real-world scenarios may be limited due to the naive assumption that a solitaire ring could have a price when the diamond weight is zero, which is not a realistic situation. Thus, while the intercept offers a foundational value for price prediction, its interpretation should be approached with an understanding of its theoretical nature and potential disconnect from practical reality.</data>
      <data key="d6">f0e0c5b2deaaf9fc2bc8b63d9ab989b1,f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </edge>
    <edge source="INTERCEPT" target="NEGATIVE_AVERAGE_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">A negative average price is a nonsensical concept that may arise from the interpretation of the intercept in the model</data>
      <data key="d6">e1ad57124a08c0e123deda212ea03c32</data>
    </edge>
    <edge source="INTERCEPT" target="SLR_DIAMOND2">
      <data key="d4">1.0</data>
      <data key="d5">The intercept parameter is estimated in the slr.diamond2 model</data>
      <data key="d6">35e06960dba699ce0d56fc1e98bdbe96</data>
    </edge>
    <edge source="INTERCEPT" target="AVERAGE_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Average weight is the specific carat weight (0.204 carats) used in the interpretation of the intercept</data>
      <data key="d6">f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </edge>
    <edge source="INTERCEPT" target="SAMPLE_MEAN_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">Sample mean price is equal to the estimated intercept, indicating the average price of Solitaire rings in the dataset</data>
      <data key="d6">f8a3c7ad2423fe8e91b33ca812ddfeff</data>
    </edge>
    <edge source="INTERCEPT" target="SLR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The intercept is a parameter in the simple linear regression model</data>
      <data key="d6">2e5e1bdaa9fcc7b3391d277fd6bb247a</data>
    </edge>
    <edge source="INTERCEPT" target="TREE_MODEL">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the statistical model referred to as TREE_MODEL, the INTERCEPT plays a crucial role as a parameter. This parameter, known as the intercept, signifies the baseline or starting point in the model, specifically representing the expected value of the Height variable when all the predictor variables are at their minimum, zero. This foundational value is pivotal in understanding the baseline prediction of the model before the influence of any predictor variables. The INTERCEPT's value thus anchors the TREE_MODEL, providing a point of reference from which the effects of the predictor variables can be measured and understood.</data>
      <data key="d6">25fce1af816975003128126b5cfea73b,c619949b08fc2b7edf3a7635b46dc147</data>
    </edge>
    <edge source="INTERCEPT" target="TREES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The intercept is one of the parameters estimated by the trees.model</data>
      <data key="d6">a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="INTERCEPT" target="PARALLEL_LINES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Intercept is a parameter in the parallel lines model</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="SLOPE" target="SOLITAIRE_RING_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">Slope is related to Solitaire Ring Price as it indicates the average price increase for every additional carat of weight</data>
      <data key="d6">9fc9b618723695c0c593043162a4084b</data>
    </edge>
    <edge source="SLOPE" target="SOLITAIRE_RING">
      <data key="d4">1.0</data>
      <data key="d5">The slope in the linear regression model represents the change in the price of a Solitaire ring for every additional carat of diamond weight. This relationship is used to estimate the price increase for additional diamond weight.</data>
      <data key="d6">f0e0c5b2deaaf9fc2bc8b63d9ab989b1</data>
    </edge>
    <edge source="SLOPE" target="ROUNDING">
      <data key="d4">1.0</data>
      <data key="d5">The process of rounding the slope coefficient in a regression model involves adjusting the numerical value to a more sensible representation in the given context. This relationship is important for practical interpretation of the model.</data>
      <data key="d6">f0e0c5b2deaaf9fc2bc8b63d9ab989b1</data>
    </edge>
    <edge source="SLOPE" target="SLR_DIAMOND2">
      <data key="d4">1.0</data>
      <data key="d5">The slope parameter is estimated in the slr.diamond2 model</data>
      <data key="d6">35e06960dba699ce0d56fc1e98bdbe96</data>
    </edge>
    <edge source="SOLITAIRE_RING" target="SAMPLE_MEAN_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">The sample mean price of Solitaire rings is a statistical measure that is equal to the estimated intercept of the regression line</data>
      <data key="d6">c5b269ff5c94db7ebd2cb9f7be16f171</data>
    </edge>
    <edge source="ROUNDING" target="REGRESSION_LINE">
      <data key="d4">1.0</data>
      <data key="d5">Rounding can affect the accuracy of the fitted regression line, but the line still provides a good description of the relationship between the explanatory and response variables</data>
      <data key="d6">ef24ca5edd06893b737e6a1c8a9825f6</data>
    </edge>
    <edge source="SLOPE_COEFFICIENT" target="LOG2">
      <data key="d4">1.0</data>
      <data key="d5">The log2 transformation of the predictor variable (Dbh) affects the interpretation of the slope coefficient in Tree.model. A doubling of the diameter of a Western red cedar tree is associated with an increase in its height of around 83 decimeters on average.</data>
      <data key="d6">c619949b08fc2b7edf3a7635b46dc147</data>
    </edge>
    <edge source="SLOPE_COEFFICIENT" target="TREE_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The slope coefficient is a parameter in Tree.model, representing the change in Height for a one-unit increase in the log2-transformed Dbh.</data>
      <data key="d6">c619949b08fc2b7edf3a7635b46dc147</data>
    </edge>
    <edge source="ALPHA_0" target="MODEL_EQUATION">
      <data key="d4">1.0</data>
      <data key="d5">Alpha_0 is the intercept in the model equation (1.2)</data>
      <data key="d6">f18bacb0a2fbfea44dd6326224184216</data>
    </edge>
    <edge source="ALPHA_0" target="Y">
      <data key="d4">1.0</data>
      <data key="d5">Alpha_0 (&#945;0) is the intercept parameter that determines the starting point of the straight line model</data>
      <data key="d6">f2300d613896880cbb7c255a4d858315</data>
    </edge>
    <edge source="ALPHA_0" target="ALPHA_HAT_0">
      <data key="d4">1.0</data>
      <data key="d5">Alpha_0 is the true parameter that Alpha_hat_0 estimates in the reparameterised model</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="ALPHA_1" target="MODEL_EQUATION">
      <data key="d4">1.0</data>
      <data key="d5">Alpha_1 is the coefficient for the predictor variable (weight - avg_weight) in the model equation (1.2)</data>
      <data key="d6">f18bacb0a2fbfea44dd6326224184216</data>
    </edge>
    <edge source="ALPHA_1" target="Y">
      <data key="d4">1.0</data>
      <data key="d5">Alpha_1 (&#945;1) is the slope parameter that determines the rate of change of Y with respect to X in the straight line model</data>
      <data key="d6">f2300d613896880cbb7c255a4d858315</data>
    </edge>
    <edge source="ALPHA_1" target="SXX">
      <data key="d4">1.0</data>
      <data key="d5">Sxx is used in the calculation of Alpha_1 (&#945;1) in the straight line model</data>
      <data key="d6">f2300d613896880cbb7c255a4d858315</data>
    </edge>
    <edge source="ALPHA_1" target="SXY">
      <data key="d4">1.0</data>
      <data key="d5">Sxy is used in the calculation of Alpha_1 (&#945;1) in the straight line model</data>
      <data key="d6">f2300d613896880cbb7c255a4d858315</data>
    </edge>
    <edge source="ALPHA_1" target="X_BAR">
      <data key="d4">1.0</data>
      <data key="d5">Bar X (x&#175;) is used in the calculation of Alpha_1 (&#945;1) in the straight line model</data>
      <data key="d6">f2300d613896880cbb7c255a4d858315</data>
    </edge>
    <edge source="ALPHA_1" target="Y_BAR">
      <data key="d4">1.0</data>
      <data key="d5">Bar Y (y&#175;) is used in the calculation of Alpha_1 (&#945;1) in the straight line model</data>
      <data key="d6">f2300d613896880cbb7c255a4d858315</data>
    </edge>
    <edge source="ALPHA_1" target="ALPHA_HAT_1">
      <data key="d4">1.0</data>
      <data key="d5">Alpha_1 is the true parameter that Alpha_hat_1 estimates in the reparameterised model</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="COEF_FUNCTION" target="BRAND_MODEL4">
      <data key="d4">1.0</data>
      <data key="d5">The coef() function is used to extract coefficients from brand.model4</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="COEF_FUNCTION" target="ROUND_FUNCTION">
      <data key="d4">1.0</data>
      <data key="d5">The round() function is used to round the coefficients extracted by the coef() function</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="SLR_DIAMOND2" target="COEF">
      <data key="d4">1.0</data>
      <data key="d5">coef() is used to extract the coefficients from the slr.diamond2 model</data>
      <data key="d6">35e06960dba699ce0d56fc1e98bdbe96</data>
    </edge>
    <edge source="SLR_DIAMOND2" target="MEAN">
      <data key="d4">1.0</data>
      <data key="d5">mean(carat) is used in the formula of the slr.diamond2 model to center the carat variable</data>
      <data key="d6">35e06960dba699ce0d56fc1e98bdbe96</data>
    </edge>
    <edge source="I" target="N">
      <data key="d4">1.0</data>
      <data key="d5">I ranges from 1 to n, indicating the total number of observations</data>
      <data key="d6">bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </edge>
    <edge source="COEF" target="MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Model is used as input for the coef() function to extract coefficients</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="COEF" target="BETA0">
      <data key="d4">1.0</data>
      <data key="d5">coef() function extracts the value of Beta0 (&#946;0)</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="COEF" target="BETA1">
      <data key="d4">1.0</data>
      <data key="d5">coef() function extracts the value of Beta1 (&#946;1)</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="COEF" target="BETA2">
      <data key="d4">1.0</data>
      <data key="d5">coef() function extracts the value of Beta2 (&#946;2)</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="COEF" target="PLR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The coefficients of the quadratic regression model plr.model are extracted using the coef() function</data>
      <data key="d6">084dadebfca8bcb6377c205c45bee295</data>
    </edge>
    <edge source="COEF" target="TREE_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Tree.model is used as input for the coef function to extract the coefficients</data>
      <data key="d6">25fce1af816975003128126b5cfea73b</data>
    </edge>
    <edge source="COEF" target="TREES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The coef() function is used to extract the coefficients of the trees.model</data>
      <data key="d6">a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="COEF" target="TREES.MODEL">
      <data key="d4">1.0</data>
      <data key="d5">trees.model is the linear model from which the coefficients are extracted by the coef function</data>
      <data key="d6">9a28a6420fca4405488ca35762f9dc28</data>
    </edge>
    <edge source="COEF" target="BRAND_MODEL3">
      <data key="d4">1.0</data>
      <data key="d5">Brand_model3 is the model from which the coefficients are extracted using the coef() function</data>
      <data key="d6">6a6f85d0a6e46196ab3a901fcc82a720</data>
    </edge>
    <edge source="REGRESSION_LINE" target="ANSCOMBE_QUARTET">
      <data key="d4">1.0</data>
      <data key="d5">Anscombe's Quartet has the same regression line for all four datasets, despite having different scatterplots</data>
      <data key="d6">9f335f1ecb85a1427df926df8bb1e89f</data>
    </edge>
    <edge source="REGRESSION_LINE" target="X">
      <data key="d4">1.0</data>
      <data key="d5">X is one of the variables used to fit the regression line</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="REGRESSION_LINE" target="Y">
      <data key="d4">1.0</data>
      <data key="d5">Y is the variable that is predicted by the regression line</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="REGRESSION_LINE" target="LINEARITY_ASSUMPTION">
      <data key="d4">1.0</data>
      <data key="d5">The linearity assumption is checked by examining the regression line and residual plots to ensure the relationship is linear</data>
      <data key="d6">b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </edge>
    <edge source="REGRESSION_LINE" target="EXPLANATORY_VARIABLE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, the REGRESSION_LINE is a fundamental concept that represents the relationship between one or more EXPLANATORY_VARIABLEs and a response variable. The EXPLANATORY_VARIABLE plays a crucial role in the REGRESSION_LINE, as it is utilized to explain and predict the variation in the response variable. By incorporating the explanatory variable into the regression model, one can estimate the impact of changes in the explanatory variable on the response variable, thereby facilitating a deeper understanding of the underlying relationship between the two variables. This process is pivotal in statistical analysis, as it allows for the identification of significant predictors and the quantification of their effects on the response variable, enhancing the predictive power and explanatory capacity of the model.</data>
      <data key="d6">82cfcd5865cffe55e965a50745656e60,ef24ca5edd06893b737e6a1c8a9825f6</data>
    </edge>
    <edge source="REGRESSION_LINE" target="RESPONSE_VARIABLE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, the entity known as the REGRESSION_LINE is employed to predict the RESPONSE_VARIABLE. The RESPONSE_VARIABLE, in this scenario, is identified as the dependent variable, which means its variation is sought to be explained by the explanatory variable(s) through the REGRESSION_LINE. This line serves as a crucial component in the model, as it facilitates the prediction of the RESPONSE_VARIABLE based on the values of the explanatory variable(s). The relationship between the REGRESSION_LINE and the RESPONSE_VARIABLE is fundamental to understanding the structure and behavior of the data within the model.</data>
      <data key="d6">82cfcd5865cffe55e965a50745656e60,ef24ca5edd06893b737e6a1c8a9825f6</data>
    </edge>
    <edge source="REGRESSION_LINE" target="DATA">
      <data key="d4">1.0</data>
      <data key="d5">The regression line is calculated from the data</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="REGRESSION_LINE" target="OBSERVATIONS">
      <data key="d4">1.0</data>
      <data key="d5">The regression line is calculated from the observations</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="REGRESSION_LINE" target="RESIDUAL_PLOT">
      <data key="d4">2.0</data>
      <data key="d5">The REGRESSION_LINE is a statistical tool used to model the relationship between a dependent variable and one or more independent variables. It is a line that best fits the data points in a scatter plot, minimizing the sum of the squared differences between the observed values and the values predicted by the line. The RESIDUAL_PLOT is a graphical representation that helps to assess the fit of the REGRESSION_LINE. It does so by plotting the residuals, which are the differences between the observed values and the values predicted by the regression line. The residuals are calculated by subtracting the predicted values from the observed values. A residual plot that shows a random scatter of points around the horizontal axis indicates a good fit of the regression line to the data, while patterns in the residual plot may suggest that the regression line does not adequately capture the relationship between the variables.</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742,ef24ca5edd06893b737e6a1c8a9825f6</data>
    </edge>
    <edge source="REGRESSION_LINE" target="PREDICTOR_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">The regression line is calculated from the predictor variable</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="REGRESSION_LINE" target="SALES_VOLUME">
      <data key="d4">1.0</data>
      <data key="d5">The regression line is used to visualize the relationship between sales volume and price, and to predict sales volume based on price.</data>
      <data key="d6">7a605c3b689bec7ab2c46df9c123e3f3</data>
    </edge>
    <edge source="REGRESSION_LINE" target="INTERACTION">
      <data key="d4">1.0</data>
      <data key="d5">The non-parallel regression lines for different brands indicate an interaction between brand and price</data>
      <data key="d6">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </edge>
    <edge source="REGRESSION_LINES" target="PARAMETERISATION">
      <data key="d4">1.0</data>
      <data key="d5">Regression lines are affected by the choice of parameterisation, which can alter the interpretation of the model</data>
      <data key="d6">9854704301b8df256ca1013b8d53dfac</data>
    </edge>
    <edge source="PARAMETERISATION" target="DIAMONDS_EXAMPLE">
      <data key="d4">1.0</data>
      <data key="d5">Parameterisation is illustrated through the diamonds example, showing how reparameterisation can improve interpretability</data>
      <data key="d6">9854704301b8df256ca1013b8d53dfac</data>
    </edge>
    <edge source="PARAMETERISATION" target="BRAND_C">
      <data key="d4">1.0</data>
      <data key="d5">Changing the reference category of the brand variable to Brand C affects the parameterisation of the model</data>
      <data key="d6">a1fc936df848a0fbc791e4bcc9b527b6</data>
    </edge>
    <edge source="REPARAMETERISATIONS" target="MATERIAL_COVERED">
      <data key="d4">1.0</data>
      <data key="d5">Re-parameterisations are a concept that will be covered in the material of the module, requiring students to develop skills in moving between mathematical descriptions, R implementation, and interpretation in the application context.</data>
      <data key="d6">ccced70c40ef9105ac2f7a9bfd151125</data>
    </edge>
    <edge source="MATERIAL_COVERED" target="SKILLS">
      <data key="d4">1.0</data>
      <data key="d5">The material covered so far exemplifies the skills needed to move confidently between mathematical descriptions, R implementation, and interpretation in the application context.</data>
      <data key="d6">ccced70c40ef9105ac2f7a9bfd151125</data>
    </edge>
    <edge source="SKILLS" target="EXERCISES">
      <data key="d4">1.0</data>
      <data key="d5">The skills required for the module are practiced through the exercises provided, which help students assimilate and practice the material discussed so far.</data>
      <data key="d6">ccced70c40ef9105ac2f7a9bfd151125</data>
    </edge>
    <edge source="EXERCISES" target="EXERCISE_1">
      <data key="d4">1.0</data>
      <data key="d5">Exercise 1 is one of the exercises provided to help students assimilate and practice the material discussed so far.</data>
      <data key="d6">ccced70c40ef9105ac2f7a9bfd151125</data>
    </edge>
    <edge source="EXERCISES" target="EXERCISE_2">
      <data key="d4">1.0</data>
      <data key="d5">Exercise 2 is one of the exercises provided to help students assimilate and practice the material discussed so far.</data>
      <data key="d6">ccced70c40ef9105ac2f7a9bfd151125</data>
    </edge>
    <edge source="EXERCISES" target="EXERCISE_3">
      <data key="d4">1.0</data>
      <data key="d5">Exercise 3 is one of the exercises provided to help students assimilate and practice the material discussed so far, specifically addressing the issue of negative price predictions for Solitaire rings with very small diamonds.</data>
      <data key="d6">ccced70c40ef9105ac2f7a9bfd151125</data>
    </edge>
    <edge source="EXERCISE_3" target="MODEL_1_1">
      <data key="d4">1.0</data>
      <data key="d5">Model (1.1) is compared to a regression through the origin model in Exercise 3 to assess the fit of the models</data>
      <data key="d6">e47d573a10e64a657e58218df64d8920</data>
    </edge>
    <edge source="MODEL_1_1" target="REPARAMETERISATION">
      <data key="d4">1.0</data>
      <data key="d5">Model (1.1) can be reparameterised into model (1.2), leading to the same predictions</data>
      <data key="d6">e47d573a10e64a657e58218df64d8920</data>
    </edge>
    <edge source="REPARAMETERISATION" target="BETA">
      <data key="d4">2.0</data>
      <data key="d5">Reparameterisation, a crucial technique in statistical modeling, significantly enhances the interpretability of the parameter Beta, particularly within the context of a linear model. Although reparameterisation does not alter the estimate for Beta, which is the parameter associated with price, it facilitates a clearer understanding of its role and impact in the model. This process is essential for refining the analysis and ensuring that the statistical inferences drawn from the model are both accurate and meaningful.</data>
      <data key="d6">9854704301b8df256ca1013b8d53dfac,9fc2b1e8b2b61b557f88eb9e9c708597</data>
    </edge>
    <edge source="REPARAMETERISATION" target="BRAND_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">Reparameterisation allows for the interpretation of the estimated coefficients for the brand variable without assuming a specific price</data>
      <data key="d6">9854704301b8df256ca1013b8d53dfac</data>
    </edge>
    <edge source="REGRESSION_THROUGH_ORIGIN" target="R_COMMAND">
      <data key="d4">1.0</data>
      <data key="d5">The R command is used to fit the regression through the origin model to the diamond data</data>
      <data key="d6">e47d573a10e64a657e58218df64d8920</data>
    </edge>
    <edge source="Y" target="LINEAR_REGRESSION_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Y is the response variable in the linear regression model, which is the outcome being predicted</data>
      <data key="d6">070499b11a2fc1530fd2751d0920ad31</data>
    </edge>
    <edge source="Y" target="BETA_2">
      <data key="d4">2.0</data>
      <data key="d5">In the context of a quadratic regression model, BETA_2, denoted as \u03b22, plays a significant role in the calculation of Y. Specifically, BETA_2 is the coefficient associated with the quadratic term of the independent variable X2. This coefficient quantifies the average change in Y that results from a one-unit increase in X2, provided that the value of X1, another independent variable in the model, remains constant. This relationship highlights the non-linear influence of X2 on Y, capturing the curvature in the association between the two variables. Thus, BETA_2 is crucial for understanding how changes in X2, when squared, affect the dependent variable Y within the quadratic regression framework.</data>
      <data key="d6">2d5cdecc342ddacd2c090f1838430cee,a828fd17fc38e902484872c88a6b242c</data>
    </edge>
    <edge source="Y" target="X">
      <data key="d4">12.0</data>
      <data key="d5">In the context of statistical modeling, the entities Y and X are central to understanding the relationships and structures within a given dataset. Y, often referred to as the response or dependent variable, is influenced by X, the predictor or explanatory variable. The relationship between Y and X is primarily explored through linear regression models, where Y is a function of X and the parameter vector Beta. This systematic component, \u03b20 + \u03b21xj, describes how the mean of Y depends on X.

X, serving as the design matrix, plays a pivotal role in calculating Y in the linear regression framework. It is the matrix that determines the mean of the vector Y, and its influence on Y can be linear or non-linear. In the case of non-linear relationships, the polynomial regression model is employed to capture the influence of X on Y. This model extends the linear relationship by incorporating higher-order terms of X.

Furthermore, the straight line model, a specific case of linear regression, highlights the direct influence of X on Y. In this model, X is the predictor variable that affects Y in a linear fashion. However, the relationship can also be quadratic, as seen in the quadratic regression model, where X's influence on Y is described through a second-degree polynomial.

The linear model Y = X\u03b2 + \u03f5 encapsulates the essence of the relationship between Y and X, where \u03b2 represents the coefficients that quantify the effect of X on Y, and \u03f5 denotes the error term, accounting for the variability in Y that is not explained by X. This model is fundamental in regression analysis, where Y and X are analyzed together to understand the underlying structure of the data.

In summary, Y and X are variables in the dataset that are intricately linked through various regression models, ranging from simple linear relationships to more complex polynomial and quadratic models. The design matrix X and the response variable Y are analyzed together to uncover the systematic and stochastic components of their relationship, providing insights into the community of interest within the context of algorithmic analysis.</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0,2d5cdecc342ddacd2c090f1838430cee,3cbe71f7649e84cd67cb3fa0d3e632cf,6c66e9414880964ee899ceb0f16d22e9,7ad4ccec4c7bb3702aed71c17dc6b96f,9005147593b2f27b9e2a5eede3601bdc,9a27580975988e83f6e3a0d9010893b5,c9a01b92d11585f6549f62e8bd78d652,dd7e7d54883ca0f687568a738b95d4d0,e41cc40f061f487b1ea0f256d4a963e4,e4f14e6785c6d7b7469e695aaeb170d0,f2300d613896880cbb7c255a4d858315</data>
    </edge>
    <edge source="Y" target="Y_GIVEN_X">
      <data key="d4">1.0</data>
      <data key="d5">The conditional expectation of Y given X is equal to &#946;0 + &#946;1x, which is the systematic component of the model</data>
      <data key="d6">9a27580975988e83f6e3a0d9010893b5</data>
    </edge>
    <edge source="Y" target="Y_VECTOR">
      <data key="d4">1.0</data>
      <data key="d5">Y is represented as a column vector Y in the matrix formulation of the linear regression model</data>
      <data key="d6">9a27580975988e83f6e3a0d9010893b5</data>
    </edge>
    <edge source="Y" target="X_MATRIX">
      <data key="d4">1.0</data>
      <data key="d5">X is the matrix in the linear regression model that, when multiplied by the parameter vector &#946;, gives the systematic component of Y</data>
      <data key="d6">9a27580975988e83f6e3a0d9010893b5</data>
    </edge>
    <edge source="Y" target="BETA">
      <data key="d4">8.0</data>
      <data key="d5">In the context of a linear regression model, Y serves as the dependent or response variable, influenced by the parameter vector Beta (denoted as \u03b2). The relationship between Y and Beta is central to the model, where Y is calculated as a linear combination of the predictor variables X, scaled by the elements of Beta. This relationship is mathematically represented as Y = X\u03b2 + \u03f5, where \u03f5 represents the error term. The parameter vector Beta determines the mean of the vector Y through the mean function E(Y) = X\u03b2, effectively capturing the systematic component of Y when multiplied by X. Thus, Y and Beta are intrinsically linked in the linear regression framework, with Y's values being a function of the linear transformation of X by Beta.</data>
      <data key="d6">3cbe71f7649e84cd67cb3fa0d3e632cf,7ad4ccec4c7bb3702aed71c17dc6b96f,7d074208b1259e7d84f9f870d3828bb6,8f1d95acff56e1633dceb775fa713174,9005147593b2f27b9e2a5eede3601bdc,9a27580975988e83f6e3a0d9010893b5,dd7e7d54883ca0f687568a738b95d4d0,e41cc40f061f487b1ea0f256d4a963e4</data>
    </edge>
    <edge source="Y" target="EPSILON_VECTOR">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon is the vector of errors that, when added to the systematic component X&#946;, gives the observed values of Y</data>
      <data key="d6">9a27580975988e83f6e3a0d9010893b5</data>
    </edge>
    <edge source="Y" target="YJ">
      <data key="d4">3.0</data>
      <data key="d5">Y, often represented in its vector form, is a significant entity that encapsulates a series of values, specifically the YJ values. YJ is an integral component of Y, contributing to the vector's composition. The vector Y can be understood as the representation of a set of equations, where Yj, one of the many elements, plays a crucial role. This set of equations, when expressed in vector form, provides a comprehensive view of the relationships and structures within the data, making Y a pivotal element in statistical analysis and algorithmic interpretation.</data>
      <data key="d6">7fc5b8303ab530821bf2140ba6a8a889,b5d0a103e1f34a00aef67fedd0e8c693</data>
    </edge>
    <edge source="Y" target="LINEAR_PREDICTOR">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, Y serves as the response variable, playing a pivotal role in the analysis. Y's mean value is intricately linked to the linear predictor, a function that encapsulates the relationship between the response variable and one or more predictor variables. This linear predictor is a fundamental component in determining the expected value of Y, reflecting the linear combination of the predictor variables' effects. As such, Y is modeled as a direct function of the linear predictor, highlighting the dependency of its mean on the values predicted by this linear combination. This relationship is central to understanding the structure and behavior of the community of interest within the algorithmic analysis framework.</data>
      <data key="d6">67e4c1866b0c6e162e6e3317949e8da9,6a47154bf457c25f22c3cf9f649c5db0</data>
    </edge>
    <edge source="Y" target="VARIANCE_FUNCTION">
      <data key="d4">1.0</data>
      <data key="d5">Y's variance given the values of the explanatory variables is constant and equal to the variance of the error term &#1013;</data>
      <data key="d6">67e4c1866b0c6e162e6e3317949e8da9</data>
    </edge>
    <edge source="Y" target="NN">
      <data key="d4">2.0</data>
      <data key="d5">Y follows a multivariate normal distribution Nn with mean X&#946; and covariance matrix &#963;^2In</data>
      <data key="d6">67e4c1866b0c6e162e6e3317949e8da9,9005147593b2f27b9e2a5eede3601bdc</data>
    </edge>
    <edge source="Y" target="Y1">
      <data key="d4">1.0</data>
      <data key="d5">Y1 is one of the components of the vector Y</data>
      <data key="d6">9005147593b2f27b9e2a5eede3601bdc</data>
    </edge>
    <edge source="Y" target="YN">
      <data key="d4">1.0</data>
      <data key="d5">Yn is one of the components of the vector Y</data>
      <data key="d6">9005147593b2f27b9e2a5eede3601bdc</data>
    </edge>
    <edge source="Y" target="IN">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis, the entities Y and IN are intricately related through the covariance matrix. Specifically, IN is a component of the covariance matrix associated with the vector Y. This covariance matrix is pivotal in understanding the variability and interdependence of the elements within Y. It is determined that the covariance matrix of the response vector Y is characterized by IN, highlighting its foundational role in the structure of Y's variability. Moreover, the presence of an error term influences Y's covariance matrix, leading to its representation as &#963;^2IN. This formulation encapsulates the combined effect of the inherent variability in Y and the error term, where &#963;^2 denotes the variance of the error. Thus, the covariance matrix of Y, influenced by IN and the error term, provides a comprehensive view of the statistical relationships within the vector Y.</data>
      <data key="d6">7ad4ccec4c7bb3702aed71c17dc6b96f,9005147593b2f27b9e2a5eede3601bdc,e41cc40f061f487b1ea0f256d4a963e4</data>
    </edge>
    <edge source="Y" target="E_Y">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, E(Y) represents the expected value of the response variable Y, which is also referred to as the mean function of the response vector Y. This indicates that E(Y) is a fundamental component in understanding the central tendency of the distribution of Y, providing insights into the average behavior of the variable within the community of interest. As a measure of central location, E(Y) plays a crucial role in various statistical models, including linear regression, where it helps in predicting the mean response for a given set of explanatory variables.</data>
      <data key="d6">7fc5b8303ab530821bf2140ba6a8a889,e41cc40f061f487b1ea0f256d4a963e4</data>
    </edge>
    <edge source="Y" target="Y">
      <data key="d4">1.0</data>
      <data key="d5">y are the observed values of the response variable Y</data>
      <data key="d6">7fc5b8303ab530821bf2140ba6a8a889</data>
    </edge>
    <edge source="Y" target="X_BETA">
      <data key="d4">1.0</data>
      <data key="d5">X&#946; is the linear combination of the design matrix X and the parameter vector Beta that contributes to the expected value of Y</data>
      <data key="d6">7fc5b8303ab530821bf2140ba6a8a889</data>
    </edge>
    <edge source="Y" target="N">
      <data key="d4">1.0</data>
      <data key="d5">The dimension n is the length of the vector Y, which is the number of units of observation</data>
      <data key="d6">e4f14e6785c6d7b7469e695aaeb170d0</data>
    </edge>
    <edge source="Y" target="JTH_UNIT_OBSERVATION">
      <data key="d4">1.0</data>
      <data key="d5">The jth unit of observation is associated with the response vector Y</data>
      <data key="d6">e41cc40f061f487b1ea0f256d4a963e4</data>
    </edge>
    <edge source="Y" target="VAR_Y">
      <data key="d4">1.0</data>
      <data key="d5">Var(Y) is the variance-covariance matrix of the response vector Y</data>
      <data key="d6">e41cc40f061f487b1ea0f256d4a963e4</data>
    </edge>
    <edge source="Y" target="PLR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Y is the response variable in the quadratic regression model plr.model</data>
      <data key="d6">084dadebfca8bcb6377c205c45bee295</data>
    </edge>
    <edge source="Y" target="QUADRATIC_CURVE">
      <data key="d4">1.0</data>
      <data key="d5">Quadratic curve represents the relationship between X and Y</data>
      <data key="d6">11452a08471d93959558de2ece9a69af</data>
    </edge>
    <edge source="Y" target="DESIGN_MATRIX">
      <data key="d4">1.0</data>
      <data key="d5">The design matrix X is used in conjunction with the vector of parameters Beta to predict the response vector Y in the linear model. The relationship is given by the equation Y = X Beta + Epsilon.</data>
      <data key="d6">e7494d6cfc3e38a4d2f3f6b21ef6445d</data>
    </edge>
    <edge source="Y" target="LOG_Y">
      <data key="d4">1.0</data>
      <data key="d5">Y is transformed into log(Y) by taking the natural logarithm</data>
      <data key="d6">21e429490eeefe7d9c245058fd48ca68</data>
    </edge>
    <edge source="Y" target="S">
      <data key="d4">6.0</data>
      <data key="d5">Y is a vector of observed values that plays a pivotal role in the statistical analysis involving the function S(&#946;). This function, S(&#946;), is calculated with respect to Y, and it is used to determine the least squares estimates, which are essential for minimizing the sum of squared differences between the observed values and the predicted values. Y's influence extends to the calculation of residuals, which are the differences between the observed values and the values predicted by the model. The function S(&#946;) is also referred to as the sum of squared differences function, and its calculation is directly dependent on the vector Y, highlighting Y's significance in the context of algorithmic analysis and statistical inference.</data>
      <data key="d6">10ac76f99674a01ca0f4a55586dea07e,2167274129d4cfa74a002c4cc39df8a8,255685e281cc5a9edf073c700f425a6b,416494d940a9f505da9853caca26fe63,50a56c34050fb7f7709300a51399b150,eac62cdd5518e1269fed150639331c2c</data>
    </edge>
    <edge source="Y" target="BETA_HAT">
      <data key="d4">21.0</data>
      <data key="d5">In the context of statistical analysis, Y represents the vector of observed values that are central to the calculation of BETA_HAT (\u03b2b). BETA_HAT (\u03b2b) is the least squares estimate derived from Y, aiming to fit the observed data optimally. This estimation process involves using the method of least squares to determine the best fit line for the data points represented in Y. It is noteworthy that Y is the foundational data set from which BETA_HAT (\u03b2b) is estimated, ensuring that the estimator accurately reflects the structure and relationships within the observed values. Additionally, the Maximum Likelihood Estimation (MLE) method is also mentioned as a technique used in the calculation of BETA_HAT (\u03b2b) from Y, providing a robust approach to parameter estimation. Overall, Y and BETA_HAT (\u03b2b) are intricately linked, with Y serving as the empirical data that BETA_HAT (\u03b2b) is calculated from to model and understand the underlying statistical relationships.</data>
      <data key="d6">0ac60299320c55d642b3e38440c25f90,2673d078d29f2af78fab9b6eacd15e37,28cf5ff0c09fa5c0390267bb9aa3ce47,2de7a36b32bf79c8f32612c8aaa9daa8,2f2523c52c6d2869fb19f77b66ce8259,3d357cfa3ef0d00f49cf4acaeac1c9d1,45f31b040576e9f3b4def6d0466cc016,542f546c5a131196e4701fb33c9b1dee,56ff186fc629e1e42f2759fc4b984199,69ffba28a61d98d8d18f91c24b74dd4a,6b55b41598d5264f8dc6b72769748722,7955aae3fd4ca51b9ef8843e13c1f517,82932abd152e0b84a1c26a2daa4c08df,9d300fc83afb3261af61b2ab9721cadc,9dddcd96af7b557e578b3f5f36efacd7,9fc2b1e8b2b61b557f88eb9e9c708597,a4a817bb79d6ae8812c808ca41d47f43,aa195e72eb5285a4bcae9c856af30a87,b70a75a6412b2e5c44af50734844f4be,f9b615b879f72501f338f8983d4cac3d,fc5b725f3c662c5471af20efdcc2dbff</data>
    </edge>
    <edge source="Y" target="S_BETA">
      <data key="d4">1.0</data>
      <data key="d5">Y is a vector used in the expression of S(&#946;)</data>
      <data key="d6">21ec28dfe2b2c18030d541d63e51f45e</data>
    </edge>
    <edge source="Y" target="XTY">
      <data key="d4">1.0</data>
      <data key="d5">Vector Y is multiplied by matrix XT to form vector XTY</data>
      <data key="d6">254a8a17b1be06702934341e3bf41e85</data>
    </edge>
    <edge source="Y" target="ALPHA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Y is the vector of observed values that Alpha hat (&#945;b) is calculated from</data>
      <data key="d6">f5716ce115458c0652124734ca344806</data>
    </edge>
    <edge source="Y" target="GAMMA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Y is the vector of observed values that Gamma hat (&#947;b) is calculated from</data>
      <data key="d6">d94760a5f9f6ea115fcc18024035a627</data>
    </edge>
    <edge source="Y" target="ALPHA">
      <data key="d4">1.0</data>
      <data key="d5">Y_j is the dependent variable in the model parameterised by Alpha</data>
      <data key="d6">5609007c6229060ffc85d8056a7fefde</data>
    </edge>
    <edge source="Y" target="X_ALPHA">
      <data key="d4">1.0</data>
      <data key="d5">Matrix X_alpha and vector Y are related through the reparameterised model equation, where Y is a function of X_alpha and the parameter vector Alpha</data>
      <data key="d6">6c66e9414880964ee899ceb0f16d22e9</data>
    </edge>
    <edge source="Y" target="EPSILON_BJ">
      <data key="d4">1.0</data>
      <data key="d5">Y is the vector of observed values from which the residuals (&#1013;bj) are calculated</data>
      <data key="d6">255685e281cc5a9edf073c700f425a6b</data>
    </edge>
    <edge source="Y" target="L">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, Y serves as the observed data that is integral to the calculation of the likelihood function, denoted as L(&#946;, &#963;^2|y). This function plays a pivotal role in estimating the parameters &#946; and &#963;^2, which are crucial for understanding the underlying structure and relationships within the data Y. The likelihood function L, therefore, acts as a bridge between the observed data Y and the parameters of interest, facilitating the process of statistical inference and model fitting.</data>
      <data key="d6">e7edd8b2874a350779ae20f1ecdf4733,f632f01188d2c6e3091a965580cb4600</data>
    </edge>
    <edge source="Y" target="SIGMA_SQUARED_MLE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, Y represents the vector of observed values that is utilized in the computation of the maximum likelihood estimate (MLE) for the error variance, denoted as SIGMA_SQUARED_MLE (\u03c3b^2_MLE). This process involves leveraging Y to estimate the variability of the errors in a statistical model, where SIGMA_SQUARED_MLE serves as the MLE for the error variance. The use of Y in this calculation is pivotal for understanding the precision of the model's predictions and the reliability of the estimated parameters.</data>
      <data key="d6">09caa54ca1372d152e47051be4d44ede,69ffba28a61d98d8d18f91c24b74dd4a</data>
    </edge>
    <edge source="Y" target="S_HAT_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">Y is the response vector used in the calculation of the unbiased estimator s^2(Y) for the error variance</data>
      <data key="d6">6648f0d6deed51fb4fb25e6992a71ddf</data>
    </edge>
    <edge source="Y" target="S2">
      <data key="d4">1.0</data>
      <data key="d5">Y is the vector of observed values used in the calculation of the unbiased estimator s^2</data>
      <data key="d6">9923e77ac6b3de95cb5026bc5e7fe8c0</data>
    </edge>
    <edge source="Y" target="YC">
      <data key="d4">3.0</data>
      <data key="d5">Y and YC are related entities in the context of statistical analysis. YC, also denoted as Yc, is derived from Y through the application of the hat matrix H. This transformation is used to calculate the fitted values in a regression model. The hat matrix, H, plays a crucial role in this process by mapping the vector of observed response values, Y, to the vector of predicted response values, Yc. This transformation is fundamental in understanding the structure and relations within the data, particularly in the context of linear regression analysis. The fitted values, Yc, are essential for assessing the goodness of fit of the model and for making predictions based on the model.</data>
      <data key="d6">679722cf8ce5ce5aee4e379528470efe,74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6</data>
    </edge>
    <edge source="Y" target="EB">
      <data key="d4">5.0</data>
      <data key="d5">In the context of statistical analysis, the entities Y and EB are intricately related. EB is derived from Y through the application of a specific matrix transformation, denoted as (In - H), where In represents the identity matrix. This transformation is pivotal in calculating EB from Y, highlighting a direct method of transformation between the two entities. The relationship between EB and Y is further elucidated by the covariance matrix Cov(Eb, Y), which is not equal to the zero matrix. This indicates a non-trivial relationship between the vector of residuals EB and the vector of response variables Y. Specifically, the covariance matrix is calculated as (In - H)&#963;^2In, where &#963;^2 represents the variance, providing a comprehensive measure of the dispersion of the residuals around the mean and the interrelation between EB and Y. This detailed statistical analysis underscores the structured relationship between the entities Y and EB within the algorithmic framework, revealing the underlying structure and dynamics of the community of interest.</data>
      <data key="d6">2685edb9e8031c8ea725c43a40af22a8,5a0d392715f06d5e873f45ae06aa729a,74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6</data>
    </edge>
    <edge source="Y" target="YB">
      <data key="d4">1.0</data>
      <data key="d5">Yb is calculated from the observed response values (Y)</data>
      <data key="d6">6ee02b38ae842fd5eac9a11c4fd6659f</data>
    </edge>
    <edge source="Y" target="SALES">
      <data key="d4">1.0</data>
      <data key="d5">Y is the vector of observed sales values</data>
      <data key="d6">248924760a2bfbc82501fd6b11cfa0aa</data>
    </edge>
    <edge source="Y" target="S_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">Y is the vector of observed values from which S squared (s^2) is estimated</data>
      <data key="d6">0ac60299320c55d642b3e38440c25f90</data>
    </edge>
    <edge source="X" target="LINEAR_REGRESSION_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">X is the explanatory variable in the linear regression model, which is used to explain the variation in Y</data>
      <data key="d6">070499b11a2fc1530fd2751d0920ad31</data>
    </edge>
    <edge source="X" target="BETA">
      <data key="d4">8.0</data>
      <data key="d5">In the context of the linear regression model, the design matrix X plays a pivotal role in estimating the parameters, collectively denoted as Beta (&#946;). X, the matrix of explanatory variables, is intricately linked with Beta, as it is utilized in the calculation of the parameters &#946;0, &#946;1, ..., &#946;q through the method of least squares estimation. This matrix X, when multiplied by the parameter vector Beta, facilitates the computation of the mean function of the response vector Y. Specifically, in the linear model Y = X&#946; + &#1013;, where &#1013; represents the error term, X serves as the design matrix that, in conjunction with Beta, is used to predict the values of Y. It is through this relationship that X and Beta contribute to the understanding and prediction of the price predictor within the linear regression framework.</data>
      <data key="d6">119bc73ddf8eebadfb8eae272fa323a7,3cbe71f7649e84cd67cb3fa0d3e632cf,6c1684ed2a4840576c6b0f4d1a3a482f,7ad4ccec4c7bb3702aed71c17dc6b96f,8f1d95acff56e1633dceb775fa713174,aeddef300427d211c74c6008b5b6b328,dd7e7d54883ca0f687568a738b95d4d0,e4f14e6785c6d7b7469e695aaeb170d0</data>
    </edge>
    <edge source="X" target="YJ">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, X serves as the matrix of explanatory variables, which are integral to the determination of YJ. X's role is pivotal as it contains the values that YJ, the dependent variable, is contingent upon. This relationship is further elucidated through the application of a straight line model, where X is utilized in the calculation of YJ, highlighting its significance in understanding the linear relationship between the variables.</data>
      <data key="d6">1303b66694a101878ca530c0b41cf5ef,7fc5b8303ab530821bf2140ba6a8a889</data>
    </edge>
    <edge source="X" target="X_BETA">
      <data key="d4">2.0</data>
      <data key="d5">The design matrix X, in conjunction with the parameter vector Beta (X_BETA), plays a pivotal role in statistical modeling. Specifically, X is utilized to calculate the mean function of the response vector Y, which is achieved by multiplying X with Beta to form the product X&#946;. This operation is fundamental in linear regression analysis, where X encapsulates the independent variables, and Beta represents the coefficients that quantify the relationship between these variables and the response Y. The product X&#946; thus provides a linear prediction of the mean response for given values of the independent variables, facilitating the understanding and prediction of the response variable's behavior within the context of the model.</data>
      <data key="d6">7fc5b8303ab530821bf2140ba6a8a889,e4f14e6785c6d7b7469e695aaeb170d0</data>
    </edge>
    <edge source="X" target="P">
      <data key="d4">1.0</data>
      <data key="d5">The dimension p is the number of columns in the design matrix X, including the intercept and explanatory variables</data>
      <data key="d6">e4f14e6785c6d7b7469e695aaeb170d0</data>
    </edge>
    <edge source="X" target="NN">
      <data key="d4">1.0</data>
      <data key="d5">The design matrix X is used in the distribution of the response vector Y, which is assumed to be Nn(X&#946;, &#963;^2In)</data>
      <data key="d6">e4f14e6785c6d7b7469e695aaeb170d0</data>
    </edge>
    <edge source="X" target="KTH_EXPLANATORY_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">The kth explanatory variable is a column in the design matrix X</data>
      <data key="d6">e41cc40f061f487b1ea0f256d4a963e4</data>
    </edge>
    <edge source="X" target="PLR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">X is the predictor variable used in the quadratic regression model plr.model</data>
      <data key="d6">084dadebfca8bcb6377c205c45bee295</data>
    </edge>
    <edge source="X" target="QUADRATIC_CURVE">
      <data key="d4">1.0</data>
      <data key="d5">Quadratic curve represents the relationship between X and Y</data>
      <data key="d6">11452a08471d93959558de2ece9a69af</data>
    </edge>
    <edge source="X" target="BETA1">
      <data key="d4">1.0</data>
      <data key="d5">X is the variable whose linear term is represented by Beta1 in the quadratic regression model</data>
      <data key="d6">11452a08471d93959558de2ece9a69af</data>
    </edge>
    <edge source="X" target="BETA2">
      <data key="d4">1.0</data>
      <data key="d5">X is the variable whose quadratic term is represented by Beta2 in the quadratic regression model</data>
      <data key="d6">11452a08471d93959558de2ece9a69af</data>
    </edge>
    <edge source="X" target="EXPECTED_CHANGE">
      <data key="d4">1.0</data>
      <data key="d5">The expected change in the response due to a unit change in the explanatory variable X depends on the current value x of X</data>
      <data key="d6">c9a01b92d11585f6549f62e8bd78d652</data>
    </edge>
    <edge source="X" target="PLOT">
      <data key="d4">1.0</data>
      <data key="d5">A plot is usually helpful when there is only one explanatory variable X, to visualize the relationship between the expected response and X</data>
      <data key="d6">c9a01b92d11585f6549f62e8bd78d652</data>
    </edge>
    <edge source="X" target="QUADRATIC_RELATIONSHIP">
      <data key="d4">1.0</data>
      <data key="d5">The explanatory variable X has a quadratic relationship with the expected response, as described by the systematic component of the fitted model</data>
      <data key="d6">c9a01b92d11585f6549f62e8bd78d652</data>
    </edge>
    <edge source="X" target="SYSTEMATIC_COMPONENT">
      <data key="d4">1.0</data>
      <data key="d5">The systematic component of the fitted model describes the relationship between the response variable and the explanatory variable X</data>
      <data key="d6">c9a01b92d11585f6549f62e8bd78d652</data>
    </edge>
    <edge source="X" target="POLYNOMIAL_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The relationship between the response variable and the explanatory variable X is described by a polynomial model of order 2</data>
      <data key="d6">c9a01b92d11585f6549f62e8bd78d652</data>
    </edge>
    <edge source="X" target="XJ">
      <data key="d4">1.0</data>
      <data key="d5">Matrix X contains the values of the explanatory variable xj for all units of observation</data>
      <data key="d6">6c1684ed2a4840576c6b0f4d1a3a482f</data>
    </edge>
    <edge source="X" target="BETA_HAT">
      <data key="d4">18.0</data>
      <data key="d5">Matrix X, also known as the design matrix, plays a pivotal role in the statistical analysis involving the calculation of Beta hat (&#946;&#770;), which represents the estimated coefficient vector. X contains the predictor variables that define the linear relationship between the independent variables and Beta hat (&#946;&#770;). It is assumed to be of full rank, ensuring that it can adequately influence the calculation of Beta hat (&#946;&#770;) without multicollinearity issues. X, alongside the response variable Y, is utilized in the matrix operations XTX and XTY, which are fundamental in the least squares estimation process. This method aims to minimize the sum of the squared residuals, leading to the optimal estimation of Beta hat (&#946;&#770;). Furthermore, X is integral in the function S(&#946;), indicating its involvement in more complex statistical models beyond simple linear regression. The design matrix X is thus a critical component in the least squares and maximum likelihood estimation techniques, serving as the basis for calculating the estimated parameter vector Beta hat (&#946;&#770;).</data>
      <data key="d6">248924760a2bfbc82501fd6b11cfa0aa,255685e281cc5a9edf073c700f425a6b,2673d078d29f2af78fab9b6eacd15e37,28cf5ff0c09fa5c0390267bb9aa3ce47,2b6d31b6bff4eae3a4809451c4fb9fa6,2de7a36b32bf79c8f32612c8aaa9daa8,2f2523c52c6d2869fb19f77b66ce8259,3d357cfa3ef0d00f49cf4acaeac1c9d1,45f31b040576e9f3b4def6d0466cc016,69ffba28a61d98d8d18f91c24b74dd4a,6b55b41598d5264f8dc6b72769748722,7955aae3fd4ca51b9ef8843e13c1f517,aa195e72eb5285a4bcae9c856af30a87,b70a75a6412b2e5c44af50734844f4be,e7edd8b2874a350779ae20f1ecdf4733,f632f01188d2c6e3091a965580cb4600,f9b615b879f72501f338f8983d4cac3d,fc5b725f3c662c5471af20efdcc2dbff</data>
    </edge>
    <edge source="X" target="YB">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis, X serves a pivotal role as the design matrix, which is utilized in the computation of the vector of fitted values, denoted as YB. Specifically, YB is calculated through the multiplication of X with Beta hat (&#946;b), a key parameter estimate in regression analysis. This process underscores the fundamental relationship between X and YB, where X's values are leveraged to predict or estimate YB, reflecting the linear relationship between the independent variables represented by X and the dependent variable represented by YB.</data>
      <data key="d6">2b6d31b6bff4eae3a4809451c4fb9fa6,5cc49d301d9cd1f8e20b92ab9d8346b0,82932abd152e0b84a1c26a2daa4c08df</data>
    </edge>
    <edge source="X" target="EPSILON_BJ">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, Epsilon bj (&#949;bj) and the design matrix X are integral components of a complex model. The design matrix X, which encapsulates the explanatory variables of the model, plays a pivotal role in the calculation of Epsilon bj (&#949;bj). Specifically, X is utilized to compute the fitted values, which are subsequently employed to determine the residuals, represented by Epsilon bj (&#949;bj). This process is fundamental in assessing the accuracy of the model's predictions and understanding the discrepancies between observed and predicted values. Through the application of X in the calculation of residuals, insights into the model's performance and the underlying data structure can be gleaned, facilitating a deeper understanding of the relationships within the community of interest.</data>
      <data key="d6">255685e281cc5a9edf073c700f425a6b,5cc49d301d9cd1f8e20b92ab9d8346b0</data>
    </edge>
    <edge source="X" target="S">
      <data key="d4">6.0</data>
      <data key="d5">Matrix X, consisting of predictor variables, plays a pivotal role in the statistical analysis involving the function S(&#946;). X is utilized in the computation of S(&#946;), which represents the sum of squared errors, to predict values through the product X&#946;. This matrix is also central to the calculation of residuals, which are derived from the difference between observed and predicted values. Furthermore, X influences the least squares estimates, which aim to minimize the sum of squared differences between observed and predicted values. In essence, X is the vector of predictor variables that significantly impacts the function S(&#946;) and the least squares estimation process, contributing to the accuracy of predictions and the assessment of model fit.</data>
      <data key="d6">10ac76f99674a01ca0f4a55586dea07e,2167274129d4cfa74a002c4cc39df8a8,255685e281cc5a9edf073c700f425a6b,416494d940a9f505da9853caca26fe63,50a56c34050fb7f7709300a51399b150,eac62cdd5518e1269fed150639331c2c</data>
    </edge>
    <edge source="X" target="S_BETA">
      <data key="d4">1.0</data>
      <data key="d5">X is a matrix used in the expression of S(&#946;)</data>
      <data key="d6">21ec28dfe2b2c18030d541d63e51f45e</data>
    </edge>
    <edge source="X" target="NORMAL_EQUATIONS">
      <data key="d4">1.0</data>
      <data key="d5">X is the matrix used in the normal equations</data>
      <data key="d6">21ec28dfe2b2c18030d541d63e51f45e</data>
    </edge>
    <edge source="X" target="RANK">
      <data key="d4">1.0</data>
      <data key="d5">The rank of X is equal to p, indicating that X has full column rank</data>
      <data key="d6">9d300fc83afb3261af61b2ab9721cadc</data>
    </edge>
    <edge source="X" target="XT">
      <data key="d4">2.0</data>
      <data key="d5">Matrix X, a fundamental entity in the context of linear algebra and statistical analysis, plays a pivotal role in the transformation of data structures. Specifically, Matrix X is transposed to form matrix XT, a process that involves flipping the matrix over its diagonal, switching the row and column indices of the elements. This operation is crucial in various applications, including but not limited to, solving systems of linear equations, performing least squares regression, and conducting principal component analysis. The relationship between X and XT is symbiotic, as X is used in the calculation of XT, the transpose of X, highlighting the interdependence of these matrices in algorithmic analysis and data manipulation tasks.</data>
      <data key="d6">1303b66694a101878ca530c0b41cf5ef,254a8a17b1be06702934341e3bf41e85</data>
    </edge>
    <edge source="X" target="X1">
      <data key="d4">1.0</data>
      <data key="d5">X1 to Xn are the observed values that form the second column of the matrix X</data>
      <data key="d6">56ff186fc629e1e42f2759fc4b984199</data>
    </edge>
    <edge source="X" target="XTX">
      <data key="d4">2.0</data>
      <data key="d5">X, a significant entity in the realm of statistical analysis, plays a pivotal role in the computation of XTX, a matrix that encapsulates the product of the transpose of X and X itself. This matrix, XTX, is not merely a mathematical construct but a cornerstone in the least squares estimation process, a fundamental technique in linear regression analysis. The least squares estimation, which relies heavily on XTX, is utilized to determine the best fit line for a set of data points, minimizing the sum of the squares of the residuals. In this context, XTX serves as a key matrix, facilitating the calculation of coefficients that define the relationship between variables in a dataset. The utilization of X in the calculation of XTX underscores its importance in statistical inference, where the properties of XTX are analyzed to understand the structure and relationships within the community of interest.</data>
      <data key="d6">1303b66694a101878ca530c0b41cf5ef,56ff186fc629e1e42f2759fc4b984199</data>
    </edge>
    <edge source="X" target="ALPHA0">
      <data key="d4">1.0</data>
      <data key="d5">X is used in the calculation of Alpha0 in the straight line model</data>
      <data key="d6">1303b66694a101878ca530c0b41cf5ef</data>
    </edge>
    <edge source="X" target="ALPHA1">
      <data key="d4">1.0</data>
      <data key="d5">X is used in the calculation of Alpha1 in the straight line model</data>
      <data key="d6">1303b66694a101878ca530c0b41cf5ef</data>
    </edge>
    <edge source="X" target="X_BAR">
      <data key="d4">1.0</data>
      <data key="d5">X is used in the calculation of X_bar, the mean of the xj values</data>
      <data key="d6">1303b66694a101878ca530c0b41cf5ef</data>
    </edge>
    <edge source="X" target="SXX">
      <data key="d4">1.0</data>
      <data key="d5">X is used in the calculation of Sxx, the variance of the xj values</data>
      <data key="d6">1303b66694a101878ca530c0b41cf5ef</data>
    </edge>
    <edge source="X" target="SXY">
      <data key="d4">1.0</data>
      <data key="d5">X is used in the calculation of Sxy, the covariance between xj and yj values</data>
      <data key="d6">1303b66694a101878ca530c0b41cf5ef</data>
    </edge>
    <edge source="X" target="Y_BAR">
      <data key="d4">1.0</data>
      <data key="d5">X is used in the calculation of Y_bar, the mean of the yj values</data>
      <data key="d6">1303b66694a101878ca530c0b41cf5ef</data>
    </edge>
    <edge source="X" target="XTY">
      <data key="d4">1.0</data>
      <data key="d5">X is used in the calculation of XTY, the matrix product of the transpose of X and Y</data>
      <data key="d6">1303b66694a101878ca530c0b41cf5ef</data>
    </edge>
    <edge source="X" target="ALPHA_HAT">
      <data key="d4">2.0</data>
      <data key="d5">X serves as the design matrix in the statistical analysis context, playing a pivotal role in the computation of ALPHA_HAT, which represents the least squares estimate for the model parameters. This matrix is crucial for the calculation of ALPHA_HAT (\u03b1b), facilitating the estimation process in linear regression models. Through the application of least squares estimators, X enables the determination of the best-fitting parameters for the model, ensuring that the squared residuals are minimized. This process is fundamental in statistical inference, allowing for the identification of relationships within the data and the prediction of outcomes based on the model's parameters.</data>
      <data key="d6">1303b66694a101878ca530c0b41cf5ef,f5716ce115458c0652124734ca344806</data>
    </edge>
    <edge source="X" target="Z">
      <data key="d4">1.0</data>
      <data key="d5">X is transformed by A^-1 to obtain the design matrix Z in the transformed model</data>
      <data key="d6">d94760a5f9f6ea115fcc18024035a627</data>
    </edge>
    <edge source="X" target="X_ALPHA">
      <data key="d4">1.0</data>
      <data key="d5">Matrix X is transformed to X_alpha using matrix A to obtain the design matrix for the reparameterised model</data>
      <data key="d6">6c66e9414880964ee899ceb0f16d22e9</data>
    </edge>
    <edge source="X" target="SIGMA_SQUARED_MLE">
      <data key="d4">1.0</data>
      <data key="d5">X is the matrix of predictor variables used in the calculation of Sigma squared MLE (&#963;b^2_MLE)</data>
      <data key="d6">09caa54ca1372d152e47051be4d44ede</data>
    </edge>
    <edge source="X" target="S2">
      <data key="d4">1.0</data>
      <data key="d5">X is the matrix of predictor variables used in the calculation of the unbiased estimator s^2</data>
      <data key="d6">9923e77ac6b3de95cb5026bc5e7fe8c0</data>
    </edge>
    <edge source="X" target="H">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis and algorithmic modeling, the entities X and H are intricately related. Matrix H is derived from matrix X through a specific mathematical operation, highlighting its dependency on the design matrix X. The formula applied to generate H from X is given by H = X(XTX)^-1XT, which is a fundamental computation in the realm of linear regression analysis. This formula represents the calculation of the hat matrix, also known as the projection matrix, in regression analysis. The hat matrix plays a crucial role in determining the fitted values of the dependent variable and is pivotal in the process of residual analysis and understanding the leverage of data points in the model. Thus, H's derivation from X underscores the significance of matrix X in shaping the structure and outcomes of the statistical model under consideration.</data>
      <data key="d6">46629f2efc6c82e81265a131b4bab2ee,7d074208b1259e7d84f9f870d3828bb6</data>
    </edge>
    <edge source="X" target="HX">
      <data key="d4">1.0</data>
      <data key="d5">HX is the result of the matrix operation X(X'X)^-1X', which simplifies to X, indicating that HX = X</data>
      <data key="d6">1da117a2f92b2db00290d2a0bfc06beb</data>
    </edge>
    <edge source="X" target="YC">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, X and YC are two entities that share a significant relationship. X is a crucial variable used in the calculation of YC, specifically through the formula YC = X&#946;, where &#946; represents the regression coefficients. This indicates that X has a direct influence on the value of YC, making it a key factor in the determination of YC's outcome.

Furthermore, YC is indirectly derived from X through the mechanism of the hat matrix H. The hat matrix, often denoted as H, plays a pivotal role in linear regression analysis as it projects the vector of observed values into the space of fitted values. In this scenario, the hat matrix facilitates the transformation of X into YC, highlighting the indirect yet essential connection between the two entities.

Overall, X and YC are intricately linked in the realm of statistical modeling, with X serving as a fundamental input for the calculation of YC, and the hat matrix H acting as the conduit for this transformation. This relationship underscores the importance of understanding the underlying statistical mechanisms that govern the interaction between these entities, particularly in the context of regression analysis.</data>
      <data key="d6">1da117a2f92b2db00290d2a0bfc06beb,7d074208b1259e7d84f9f870d3828bb6</data>
    </edge>
    <edge source="X" target="EB">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, EB is a variable that is indirectly derived from entity X through a complex relationship defined by the matrix (In - H). This relationship indicates that EB is influenced by the design matrix X, suggesting a structured connection between the two entities. Further, EB is also shaped by the calculation X&#946;, where &#946; represents the coefficients that quantify the influence of X on EB. This implies that EB is a function of X, modified by the matrix (In - H) and the coefficients &#946;, reflecting a multi-faceted relationship between X and EB in the algorithmic analysis.</data>
      <data key="d6">5a0d392715f06d5e873f45ae06aa729a,7d074208b1259e7d84f9f870d3828bb6</data>
    </edge>
    <edge source="X" target="HII">
      <data key="d4">1.0</data>
      <data key="d5">The values of the explanatory variables (X) in the design matrix influence the leverage (hii) of the data points</data>
      <data key="d6">6ee02b38ae842fd5eac9a11c4fd6659f</data>
    </edge>
    <edge source="X" target="SALESJ">
      <data key="d4">1.0</data>
      <data key="d5">X is the design matrix used in the model for Salesj</data>
      <data key="d6">228bdca7843406def245d755e8df49f6</data>
    </edge>
    <edge source="X" target="N">
      <data key="d4">1.0</data>
      <data key="d5">n is the number of rows in the design matrix X</data>
      <data key="d6">228bdca7843406def245d755e8df49f6</data>
    </edge>
    <edge source="X" target="SALES">
      <data key="d4">1.0</data>
      <data key="d5">The Sales vector is modeled as a linear combination of the columns of the design matrix X</data>
      <data key="d6">aeddef300427d211c74c6008b5b6b328</data>
    </edge>
    <edge source="X" target="MU_A">
      <data key="d4">1.0</data>
      <data key="d5">Mu_A (&#181;A) is one of the parameters that the design matrix X is used to estimate in the linear regression model</data>
      <data key="d6">aeddef300427d211c74c6008b5b6b328</data>
    </edge>
    <edge source="X" target="MU_B">
      <data key="d4">1.0</data>
      <data key="d5">Mu_B (&#181;B) is one of the parameters that the design matrix X is used to estimate in the linear regression model</data>
      <data key="d6">aeddef300427d211c74c6008b5b6b328</data>
    </edge>
    <edge source="X" target="MU_C">
      <data key="d4">1.0</data>
      <data key="d5">Mu_C (&#181;C) is one of the parameters that the design matrix X is used to estimate in the linear regression model</data>
      <data key="d6">aeddef300427d211c74c6008b5b6b328</data>
    </edge>
    <edge source="X" target="BRANDA">
      <data key="d4">1.0</data>
      <data key="d5">X includes the dummy variable brandA to represent brand A in the dataset</data>
      <data key="d6">e800735d6b2a244875f5e0d292de1527</data>
    </edge>
    <edge source="X" target="BRANDB">
      <data key="d4">1.0</data>
      <data key="d5">X includes the dummy variable brandB to represent brand B in the dataset</data>
      <data key="d6">e800735d6b2a244875f5e0d292de1527</data>
    </edge>
    <edge source="X" target="BRANDC">
      <data key="d4">1.0</data>
      <data key="d5">X includes the dummy variable brandC to represent brand C in the dataset</data>
      <data key="d6">e800735d6b2a244875f5e0d292de1527</data>
    </edge>
    <edge source="OBSERVATIONS" target="XJ">
      <data key="d4">1.0</data>
      <data key="d5">Xj is the measurement of the explanatory variable X for the jth unit of observation, which is part of the paired observations used in simple linear regression.</data>
      <data key="d6">4683e58cf41e5f5d415a63ddb2fe0cac</data>
    </edge>
    <edge source="OBSERVATIONS" target="YJ">
      <data key="d4">1.0</data>
      <data key="d5">Yj is the measurement of the response variable Y for the jth unit of observation, which is part of the paired observations used in simple linear regression.</data>
      <data key="d6">4683e58cf41e5f5d415a63ddb2fe0cac</data>
    </edge>
    <edge source="OBSERVATIONS" target="DATASET_1">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 1 has a specific number of observations</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="OBSERVATIONS" target="DATASET_2">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 2 has a specific number of observations</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="OBSERVATIONS" target="RESIDUAL_PLOT">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis, the entities "OBSERVATIONS" and "RESIDUAL_PLOT" play integral roles in assessing the fit of a model. The observations, which are the data points collected from the community of interest, serve as the foundation for calculating residuals. Residuals, in this context, are the differences between the observed values and the values predicted by the model. These residuals are then plotted in what is known as a residual plot. The residual plot is a graphical representation that helps in evaluating the goodness of fit of the model. It visually displays the distribution of residuals, allowing for an analysis of patterns that may indicate issues with the model's assumptions. By examining the residual plot, one can identify whether the model is adequately capturing the underlying structure of the data or if there are systematic errors that need to be addressed. This process is crucial for refining models and ensuring they accurately represent the relationships within the data.</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742,82cfcd5865cffe55e965a50745656e60,b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </edge>
    <edge source="OBSERVATIONS" target="DATA">
      <data key="d4">1.0</data>
      <data key="d5">The data is composed of individual observations</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="XJ" target="YJ">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis, XJ and YJ are identified as key variables within a dataset. XJ serves as the explanatory variable, playing a crucial role in influencing the dependent variable, YJ. It is highlighted that YJ is a function of XJ, specifically for the jth unit of observation, indicating a direct relationship between the two variables. Moreover, XJ is not the sole explanatory variable but is one among others that collectively contribute to the determination of YJ's value. This suggests a multifaceted relationship where XJ, alongside other explanatory variables, shapes the outcome of YJ, making it essential to consider the entire set of explanatory variables for a comprehensive understanding of YJ's variability.</data>
      <data key="d6">6c1684ed2a4840576c6b0f4d1a3a482f,7fc5b8303ab530821bf2140ba6a8a889,d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="XJ" target="S_BETA">
      <data key="d4">1.0</data>
      <data key="d5">Xj is used in the calculation of S(&#946;) as part of the sum of squared residuals</data>
      <data key="d6">8f7a05b6d231105a6194eebdb2df372e</data>
    </edge>
    <edge source="XJ" target="BETA_0_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Xj is used in the calculation of Beta_0 hat (b&#946;0) as part of the least squares estimation</data>
      <data key="d6">3fdeeb7593174f5e8a9cff55a7cd92e3</data>
    </edge>
    <edge source="XJ" target="BETA_1_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Xj is used in the calculation of Beta_1 hat (b&#946;1) as part of the least squares estimation</data>
      <data key="d6">3fdeeb7593174f5e8a9cff55a7cd92e3</data>
    </edge>
    <edge source="XJ" target="SXY">
      <data key="d4">2.0</data>
      <data key="d5">XJ plays a crucial role in the statistical analysis of the relationship between two variables, x and y. Specifically, XJ is utilized in the computation of SXY, which represents the covariance between X and Y. This covariance measure is fundamental in understanding the degree to which two variables linearly vary together. By incorporating XJ into the calculation of SXY, one can gain insights into the structure and interdependencies within the data, facilitating a deeper understanding of the statistical relationship between the variables of interest.</data>
      <data key="d6">f5716ce115458c0652124734ca344806,f9e7b2eac9f82681301da3d1e2f23328</data>
    </edge>
    <edge source="XJ" target="SXX">
      <data key="d4">1.0</data>
      <data key="d5">Xj is used in the calculation of Sxx, the variance of X</data>
      <data key="d6">f9e7b2eac9f82681301da3d1e2f23328</data>
    </edge>
    <edge source="XJ" target="ALPHA">
      <data key="d4">1.0</data>
      <data key="d5">x_j is the independent variable in the model parameterised by Alpha</data>
      <data key="d6">5609007c6229060ffc85d8056a7fefde</data>
    </edge>
    <edge source="XJ" target="C">
      <data key="d4">1.0</data>
      <data key="d5">C is the constant by which we multiply all observations of the explanatory variable Xj</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="XJ" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Xj is the predictor value that contributes to the calculation of Beta hat (&#946;b) in the simple linear regression model</data>
      <data key="d6">d14413709de2897231aaa83be3aa346f</data>
    </edge>
    <edge source="XJ" target="X_BAR">
      <data key="d4">2.0</data>
      <data key="d5">Xj values are used in the calculation of X_bar, the mean of the x valuesX_bar is the mean of all Xj values, used in the calculation of the variance</data>
      <data key="d6">90b7e0427699cc1bb461e37939935138</data>
    </edge>
    <edge source="YJ" target="X1">
      <data key="d4">1.0</data>
      <data key="d5">Variable X1 is one of the predictors used to explain the response variable Yj</data>
      <data key="d6">75dc4d8cb195a1f969d9e9496631086b</data>
    </edge>
    <edge source="YJ" target="XK">
      <data key="d4">1.0</data>
      <data key="d5">Variable Xk is one of the predictors used to explain the response variable Yj</data>
      <data key="d6">75dc4d8cb195a1f969d9e9496631086b</data>
    </edge>
    <edge source="YJ" target="XJM">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, YJ serves as the response variable, which is the outcome of interest. XJM, on the other hand, is identified as one of the explanatory variables that contribute to the understanding and prediction of YJ's value. The relationship between YJ and XJM is such that the value of XJM helps in explaining the variability in YJ, implying a direct influence of XJM on YJ within the statistical model. This suggests that changes in XJM could potentially be associated with changes in YJ, making XJM a crucial factor in the algorithmic analysis of the data set involving YJ.</data>
      <data key="d6">75dc4d8cb195a1f969d9e9496631086b,b5d0a103e1f34a00aef67fedd0e8c693</data>
    </edge>
    <edge source="YJ" target="BETA_M">
      <data key="d4">1.0</data>
      <data key="d5">&#946;m is the coefficient for the mth explanatory variable in the regression equation for Yj</data>
      <data key="d6">75dc4d8cb195a1f969d9e9496631086b</data>
    </edge>
    <edge source="YJ" target="XN">
      <data key="d4">1.0</data>
      <data key="d5">XN is used in the calculation of YJ in the multiple regression model</data>
      <data key="d6">b5d0a103e1f34a00aef67fedd0e8c693</data>
    </edge>
    <edge source="YJ" target="BETA0">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the statistical model being analyzed, YJ is characterized as a dependent variable that is modeled as a linear function. This linear relationship is defined with respect to BETA0, which plays a crucial role as the intercept term in the equation for YJ. BETA0, the intercept parameter, is foundational in establishing the baseline value of YJ when all other independent variables are zero. This relationship underscores the significance of BETA0 in understanding the structure and behavior of YJ within the model.</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12,b5d0a103e1f34a00aef67fedd0e8c693</data>
    </edge>
    <edge source="YJ" target="BETA1">
      <data key="d4">2.0</data>
      <data key="d5">In the context of a statistical model, YJ is the dependent variable that is being modeled as a linear function of the explanatory variables. Specifically, the relationship between YJ and the first explanatory variable is quantified by the coefficient Beta1, which is one of the slope parameters in the equation. Beta1 measures the change in YJ for a one-unit change in the first explanatory variable, holding all other variables constant. This linear relationship implies that YJ can be predicted or explained to some extent by the first explanatory variable, with Beta1 indicating the strength and direction of this association.</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12,b5d0a103e1f34a00aef67fedd0e8c693</data>
    </edge>
    <edge source="YJ" target="BETAK">
      <data key="d4">2.0</data>
      <data key="d5">In the context of a statistical model, YJ is identified as the dependent variable, which is mathematically represented as a linear function of the explanatory variable associated with the coefficient BETAK. BETAK, denoted as Beta_k, is specifically characterized as one of the slope parameters in the equation that models YJ. This implies that the relationship between YJ and the explanatory variable linked to BETAK is linear, with BETAK quantifying the change in YJ for a unit change in the explanatory variable, holding all other factors constant. This linear relationship is fundamental to understanding the structure and dynamics of the model, providing insights into how variations in the explanatory variable influence YJ.</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12,b5d0a103e1f34a00aef67fedd0e8c693</data>
    </edge>
    <edge source="YJ" target="EPSILONJ">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical modeling, YJ and EPSILONJ are two key components of a linear regression analysis. YJ, the observed value for the jth unit of observation, is modeled as a linear function, incorporating various independent variables that contribute to its value. However, it is important to note that YJ does not solely consist of these linear components; it also includes an error term, denoted as EPSILONJ. This error term, EPSILONJ, encapsulates the residual variation in YJ that cannot be explained by the linear model. It represents the difference between the observed value YJ and the value predicted by the linear function, highlighting the unpredicted or random variations in the data. Thus, YJ is a composite of the systematic linear effects and the stochastic error component, EPSILONJ, providing a comprehensive view of the jth unit of observation within the statistical model.</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12,b5d0a103e1f34a00aef67fedd0e8c693,d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="YJ" target="BETA">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, YJ represents the dependent variable, which is the outcome of interest. The expected value of YJ is determined by a function of the parameters encapsulated in BETA, denoted as (\u03b20, \u03b21, ..., \u03b2q). BETA is the vector of coefficients that, when multiplied by the explanatory variables, contributes to the prediction of YJ. This relationship is central to the model equation, where the explanatory variables, when weighted by the coefficients in BETA, help in understanding and predicting the behavior of YJ. The model assumes a linear relationship between the explanatory variables and YJ, with BETA serving as the bridge to quantify this relationship.</data>
      <data key="d6">6c1684ed2a4840576c6b0f4d1a3a482f,7fc5b8303ab530821bf2140ba6a8a889</data>
    </edge>
    <edge source="YJ" target="BETA_2">
      <data key="d4">1.0</data>
      <data key="d5">Yj is calculated using Beta_2 as the coefficient of the second explanatory variable</data>
      <data key="d6">7ad4ccec4c7bb3702aed71c17dc6b96f</data>
    </edge>
    <edge source="YJ" target="BETA_K">
      <data key="d4">1.0</data>
      <data key="d5">Yj is calculated using Beta_k as the coefficient of the kth explanatory variable</data>
      <data key="d6">7ad4ccec4c7bb3702aed71c17dc6b96f</data>
    </edge>
    <edge source="YJ" target="XJ1">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, YJ is the dependent variable that is being modeled. Specifically, YJ is calculated as a linear function of XJ1, which serves as the first explanatory or predictor variable in this model. This relationship indicates that changes in XJ1 are expected to have a direct impact on the value of YJ, following the principles of linear regression analysis. The use of XJ1 as the primary predictor suggests that the model aims to understand and predict variations in YJ based on the variations observed in XJ1.</data>
      <data key="d6">7ad4ccec4c7bb3702aed71c17dc6b96f,87b717ba065d6d7c7431af284137eb12</data>
    </edge>
    <edge source="YJ" target="XJ2">
      <data key="d4">2.0</data>
      <data key="d5">YJ is a dependent variable that is modeled as a linear function of XJ2, which serves as the second explanatory or predictor variable in the context of the given data. This relationship indicates that changes in XJ2 are expected to have a linear impact on the value of YJ, suggesting a direct correlation where YJ can be estimated based on the value of XJ2 through a linear regression model. The use of XJ2 as the second predictor variable implies that there might be other explanatory variables involved in the model, but within the provided descriptions, XJ2 is highlighted as having a significant role in determining the value of YJ.</data>
      <data key="d6">7ad4ccec4c7bb3702aed71c17dc6b96f,87b717ba065d6d7c7431af284137eb12</data>
    </edge>
    <edge source="YJ" target="XJK">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, YJ is determined through a linear relationship with XJK, where XJK represents the value of the kth explanatory or predictor variable. This implies that YJ can be estimated or predicted based on the linear function of XJK, highlighting a direct association between the two entities. The relationship suggests that changes in XJK could potentially influence the value of YJ, adhering to the principles of linear regression analysis.</data>
      <data key="d6">7ad4ccec4c7bb3702aed71c17dc6b96f,87b717ba065d6d7c7431af284137eb12</data>
    </edge>
    <edge source="YJ" target="EPSILON_J">
      <data key="d4">1.0</data>
      <data key="d5">Yj includes the error term Epsilon_j</data>
      <data key="d6">7ad4ccec4c7bb3702aed71c17dc6b96f</data>
    </edge>
    <edge source="YJ" target="EPSILON_BJ">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, YJ and EPSILON_BJ are two key components involved in the assessment of model fit. YJ represents the observed value for the jth observation, which is a crucial datum point in understanding the actual response of the system under study. EPSILON_BJ, denoted as \u03f5bj, is the residual for the jth observation, calculated as the difference between the observed value YJ and the fitted value Ybj. This residual, \u03f5bj, quantifies the discrepancy between the observed data and the values predicted by the model, providing insights into the model's accuracy and the presence of any systematic errors. By examining EPSILON_BJ, one can assess the goodness of fit of the model and identify potential outliers or influential points that may affect the overall model performance.</data>
      <data key="d6">5cc49d301d9cd1f8e20b92ab9d8346b0,e6f79ceb0df54119a4dc71b2162ac50b</data>
    </edge>
    <edge source="YJ" target="YBJ">
      <data key="d4">1.0</data>
      <data key="d5">Yj is the observed value that is compared to the fitted value Ybj to calculate the residual &#1013;bj</data>
      <data key="d6">e6f79ceb0df54119a4dc71b2162ac50b</data>
    </edge>
    <edge source="YJ" target="S_BETA">
      <data key="d4">1.0</data>
      <data key="d5">Yj is used in the calculation of S(&#946;) as part of the sum of squared residuals</data>
      <data key="d6">8f7a05b6d231105a6194eebdb2df372e</data>
    </edge>
    <edge source="YJ" target="BETA_0_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Yj is used in the calculation of Beta_0 hat (b&#946;0) as part of the least squares estimation</data>
      <data key="d6">3fdeeb7593174f5e8a9cff55a7cd92e3</data>
    </edge>
    <edge source="YJ" target="BETA_1_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Yj is used in the calculation of Beta_1 hat (b&#946;1) as part of the least squares estimation</data>
      <data key="d6">3fdeeb7593174f5e8a9cff55a7cd92e3</data>
    </edge>
    <edge source="YJ" target="SXY">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, YJ plays a pivotal role in the computation of SXY, which represents the covariance between the variables X and Y. This covariance measure, SXY, is crucial for understanding the linear relationship and the direction of the association between X and Y. YJ contributes directly to the calculation of SXY, thereby influencing the insights derived about the interdependence of X and Y within the dataset. This relationship underscores the importance of YJ in statistical inference, particularly when assessing the strength and direction of the association between the two variables.</data>
      <data key="d6">f5716ce115458c0652124734ca344806,f9e7b2eac9f82681301da3d1e2f23328</data>
    </edge>
    <edge source="YJ" target="SXX">
      <data key="d4">1.0</data>
      <data key="d5">Yj is indirectly related to Sxx through its relationship with Xj and Y_bar</data>
      <data key="d6">f9e7b2eac9f82681301da3d1e2f23328</data>
    </edge>
    <edge source="YJ" target="BETA2">
      <data key="d4">1.0</data>
      <data key="d5">Yj is modeled as a linear function of Beta2, one of the slope parameters</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12</data>
    </edge>
    <edge source="YJ" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Yj is one of the observed values that contribute to the calculation of Beta hat (&#946;b)</data>
      <data key="d6">d14413709de2897231aaa83be3aa346f</data>
    </edge>
    <edge source="VARIABLES" target="DATA">
      <data key="d4">1.0</data>
      <data key="d5">Variables are quantifiable features in the system of interest, and data are the realized values obtained when these variables are measured on specific units of observation.</data>
      <data key="d6">4683e58cf41e5f5d415a63ddb2fe0cac</data>
    </edge>
    <edge source="DATA" target="FEATURES">
      <data key="d4">1.0</data>
      <data key="d5">The data contains the features that are relevant for analysis</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="DATA" target="RESIDUAL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The residual plot is created from the data</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="DATA" target="NORMAL_Q_Q_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">Data are plotted on the normal Q-Q plot, either on the vertical or horizontal axis, to assess their conformity to a normal distribution.</data>
      <data key="d6">eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </edge>
    <edge source="RANDOM_VARIABLE" target="APPLIED_STATISTICS">
      <data key="d4">1.0</data>
      <data key="d5">The concept of a random variable is important in applied statistics, as it helps to distinguish between the theoretical construct of a variable and the actual values observed in data.</data>
      <data key="d6">4683e58cf41e5f5d415a63ddb2fe0cac</data>
    </edge>
    <edge source="RANDOM_VARIABLE" target="UNITS_OF_OBSERVATION">
      <data key="d4">1.0</data>
      <data key="d5">Units of observation are the basis for defining the outcomes of a random variable in a statistical study</data>
      <data key="d6">7594eee7e77beb023d1cd64aec64920d</data>
    </edge>
    <edge source="RANDOM_VARIABLE" target="REALISATION">
      <data key="d4">1.0</data>
      <data key="d5">Random variable and its realisation are related concepts, where the realisation is the observed value of the random variable</data>
      <data key="d6">7594eee7e77beb023d1cd64aec64920d</data>
    </edge>
    <edge source="APPLIED_STATISTICS" target="THEORY_UNDERNEATH">
      <data key="d4">1.0</data>
      <data key="d5">Applied statistics relies on the theory underneath for its methods and models, although it may use less consistent terminology and notation</data>
      <data key="d6">7594eee7e77beb023d1cd64aec64920d</data>
    </edge>
    <edge source="APPLIED_STATISTICS" target="PRECISION_OF_LANGUAGE_AND_NOTATION">
      <data key="d4">1.0</data>
      <data key="d5">Precision of language and notation is important in applied statistics, especially at the undergraduate level, to maintain clarity and consistency</data>
      <data key="d6">7594eee7e77beb023d1cd64aec64920d</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLES_X" target="DESIGNED_EXPERIMENT">
      <data key="d4">1.0</data>
      <data key="d5">In a designed experiment, the values of the explanatory variables are typically chosen by the experimenter</data>
      <data key="d6">7594eee7e77beb023d1cd64aec64920d</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLES_X" target="OBSERVATIONAL_STUDIES">
      <data key="d4">1.0</data>
      <data key="d5">In observational studies, the values of the explanatory variables are observed along with the response variable</data>
      <data key="d6">7594eee7e77beb023d1cd64aec64920d</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLES_X" target="NONRANDOM_VARIABLES">
      <data key="d4">1.0</data>
      <data key="d5">Explanatory variables X are treated as nonrandom variables in statistical models, meaning their values are considered fixed</data>
      <data key="d6">7594eee7e77beb023d1cd64aec64920d</data>
    </edge>
    <edge source="OBSERVATIONAL_STUDIES" target="DATA_MEASUREMENT">
      <data key="d4">1.0</data>
      <data key="d5">Data in observational studies is measured or observed at the same time as the response variable</data>
      <data key="d6">22478e53f29f16e3eab9d167fea52b22</data>
    </edge>
    <edge source="RESPONSE_VARIABLE" target="EXPLANATORY_VARIABLE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, the explanatory variable, denoted as "EXPLANATORY_VARIABLE" (x), plays a pivotal role in predicting or explaining the response variable, referred to as "RESPONSE_VARIABLE" (Y). This relationship is fundamental to the model equation, where the explanatory variable's values are utilized to forecast or elucidate the behavior of the response variable. The explanatory variable is intricately linked to the response variable, serving as a key predictor in the analysis. This connection is essential for understanding the underlying structure and relationships within the data, enabling more accurate predictions and insights into the community of interest.</data>
      <data key="d6">b9af17718641389ba07f53be13f31f8c,b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </edge>
    <edge source="RESPONSE_VARIABLE" target="ERROR_TERM">
      <data key="d4">1.0</data>
      <data key="d5">The response variable (Y) is modeled as a function of the explanatory variable and the parameters of the model, with the error term (&#1013;) representing the difference between the observed values and the values predicted by the model.</data>
      <data key="d6">b9af17718641389ba07f53be13f31f8c</data>
    </edge>
    <edge source="RESPONSE_VARIABLE" target="MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The model aims to predict or explain the response variable</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="RESPONSE_VARIABLE" target="PARAMETER_ESTIMATION">
      <data key="d4">1.0</data>
      <data key="d5">The response variable is the target of parameter estimation, which aims to understand its relationship with predictor variables</data>
      <data key="d6">768c516c8b27fb9800427e848f02fc33</data>
    </edge>
    <edge source="RESPONSE_VARIABLE" target="VARIANCE">
      <data key="d4">1.0</data>
      <data key="d5">The variance of the response variable is a measure of its spread or dispersion</data>
      <data key="d6">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </edge>
    <edge source="RESPONSE_VARIABLE" target="PREDICTOR_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">The response variable is predicted by the predictor variable</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="RESPONSE_VARIABLE" target="ERRORS">
      <data key="d4">1.0</data>
      <data key="d5">Errors are the discrepancies between the observed values of the response variable and the values predicted by the model</data>
      <data key="d6">e361ac139c268d5c3f3623f920e68af2</data>
    </edge>
    <edge source="PROBABILITY_THEORY" target="RANDOM_VARIABLES">
      <data key="d4">1.0</data>
      <data key="d5">In probability theory, random variables are denoted by capital letters to distinguish them from their realizations</data>
      <data key="d6">22478e53f29f16e3eab9d167fea52b22</data>
    </edge>
    <edge source="VECTORS" target="SIMPLE_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Vectors are used in simple regression to represent the data points for the response and explanatory variables</data>
      <data key="d6">336546bc73cbe1828a0cc1a45faf8f5a</data>
    </edge>
    <edge source="SIMULATED_DATASET" target="RETAIL_STORES">
      <data key="d4">1.0</data>
      <data key="d5">The simulated dataset consists of data from 96 retail stores</data>
      <data key="d6">22478e53f29f16e3eab9d167fea52b22</data>
    </edge>
    <edge source="RETAIL_STORES" target="PRODUCT_SALES">
      <data key="d4">1.0</data>
      <data key="d5">For each of the 96 retail stores, the data includes information on product sales</data>
      <data key="d6">22478e53f29f16e3eab9d167fea52b22</data>
    </edge>
    <edge source="SIMPLE_REGRESSION" target="MULTIPLE_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Simple regression is a special case of multiple regression, which uses one explanatory variable to predict the response variable</data>
      <data key="d6">22478e53f29f16e3eab9d167fea52b22</data>
    </edge>
    <edge source="SIMPLE_REGRESSION" target="SALES">
      <data key="d4">1.0</data>
      <data key="d5">Sales is the response variable in simple regression, which is analyzed in relation to one explanatory variable</data>
      <data key="d6">336546bc73cbe1828a0cc1a45faf8f5a</data>
    </edge>
    <edge source="MULTIPLE_REGRESSION" target="MATRIX">
      <data key="d4">1.0</data>
      <data key="d5">Matrices are used in multiple regression to organize and manipulate the data, including the response and multiple explanatory variables</data>
      <data key="d6">336546bc73cbe1828a0cc1a45faf8f5a</data>
    </edge>
    <edge source="MULTIPLE_REGRESSION" target="SALES">
      <data key="d4">1.0</data>
      <data key="d5">Sales is the response variable in multiple regression, which is analyzed in relation to multiple explanatory variables</data>
      <data key="d6">336546bc73cbe1828a0cc1a45faf8f5a</data>
    </edge>
    <edge source="MULTIPLE_REGRESSION" target="ADVERT">
      <data key="d4">1.0</data>
      <data key="d5">Advert is one of the explanatory variables in multiple regression, which is analyzed in relation to the response variable sales</data>
      <data key="d6">336546bc73cbe1828a0cc1a45faf8f5a</data>
    </edge>
    <edge source="MULTIPLE_REGRESSION" target="NULL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">Multiple regression models can also produce null plots when the model assumptions are met</data>
      <data key="d6">aa13c33a7e61206e6021e2736002ca9a</data>
    </edge>
    <edge source="MULTIPLE_REGRESSION" target="TREES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The trees.model is an example of a multiple regression model that includes more than one explanatory variable</data>
      <data key="d6">a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="MATRIX" target="BRAND_MODEL3">
      <data key="d4">1.0</data>
      <data key="d5">Matrix is used as input data for the linear model Brand_model3</data>
      <data key="d6">6a6f85d0a6e46196ab3a901fcc82a720</data>
    </edge>
    <edge source="SALES" target="SLR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The sales variable is used as the response variable in the simple linear regression model</data>
      <data key="d6">2e5e1bdaa9fcc7b3391d277fd6bb247a</data>
    </edge>
    <edge source="SALES" target="ADVERT">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the entities SALES and ADVERT, it is observed that the sales volume (SALES) is significantly influenced by the amount spent on local advertising (ADVERT). This relationship is substantiated by the positive coefficient in the statistical model, indicating that an increase in advertising expenditure leads to a corresponding increase in sales volume. This suggests a direct correlation where higher investments in local advertising campaigns contribute to enhanced product sales, highlighting the effectiveness of advertising strategies in boosting market performance.</data>
      <data key="d6">250ee5d766c64e7975bcc427b4bf9074,2e5e1bdaa9fcc7b3391d277fd6bb247a</data>
    </edge>
    <edge source="SALES" target="R_2">
      <data key="d4">1.0</data>
      <data key="d5">R^2 is the space in which the simple linear regression model is represented, with sales as the dependent variable</data>
      <data key="d6">250ee5d766c64e7975bcc427b4bf9074</data>
    </edge>
    <edge source="SALES" target="R_3">
      <data key="d4">1.0</data>
      <data key="d5">R^3 is the space in which the extended linear regression model is represented, with sales as the dependent variable</data>
      <data key="d6">250ee5d766c64e7975bcc427b4bf9074</data>
    </edge>
    <edge source="SALES" target="DATAPPOINTS">
      <data key="d4">1.0</data>
      <data key="d5">Sales is one of the coordinates (z-axis) of the datapoints in the 3D graphical representation</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376</data>
    </edge>
    <edge source="SALES" target="REGRESSION_PLANE">
      <data key="d4">1.0</data>
      <data key="d5">Sales is the response variable in the regression analysis, and the regression plane describes the relationship between sales and the explanatory variables</data>
      <data key="d6">b1690cb1a67892245c0665e5099e322d</data>
    </edge>
    <edge source="SALES" target="OBSERVED_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Sales volume (sales) is the response variable in the observed data</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="SALES" target="ADVERTISING_BUDGET">
      <data key="d4">1.0</data>
      <data key="d5">An increase in the local advertising budget is associated with an increase in sales. The relationship strength is high, as a &#163;1000 increase in the advertising budget is associated with a 544 unit increase in sales.</data>
      <data key="d6">426434b67f6a287852ab66b82ca873cf</data>
    </edge>
    <edge source="SALES" target="BRAND_A">
      <data key="d4">9.0</data>
      <data key="d5">In the context of the sales figures and performance of Brand A stores, a comprehensive statistical analysis has been conducted to understand the relationship between sales volumes and various factors. Brand A is a significant entity in the sales comparison, with its sales figures being a focal point for the CEO's interest. The sales model for Brand A stores is characterized by a specific relationship between sales and price, estimated as Sales = 210.379 - 0.657 * pricei, highlighting the inverse relationship between the price of the product and sales volume.

Moreover, the sales volumes at the jth store of Brand A are influenced by the category of the brand, with Mu serving as the intercept in the sales model. This indicates that the baseline sales volume for Brand A stores is adjusted by the brand's category. The sales model for Brand A stores is further specified as Salesj = &#956;B + &#946; pricej + &#966;j, where the jth store is of Brand A. This model takes into account the unique characteristics of Brand A stores, including the specific intercept (&#956;A) that differentiates its sales pattern from other brands.

The sales volume of Brand A stores is notably influenced by the price of the product, with a steeper gradient for the fitted regression line compared to Brand C. This suggests that changes in price have a more pronounced effect on the sales volume for Brand A stores. The dependent variable, Salesj, is modeled distinctly for Brand A stores, reflecting the unique sales dynamics associated with this brand.

In summary, Brand A's sales are intricately linked to the price of the product, the brand's category, and the specific intercept that characterizes its sales model. The CEO's interest in Brand A's sales volumes underscores the brand's significance in the market, while the statistical models highlight the nuanced relationship between sales, price, and brand category for Brand A stores.</data>
      <data key="d6">1820d10ee0f23f34b3ea88ba475bc52d,1b523d1edabe381403fc470a9b8d47fa,1d141ab04db553f78a313e430e54abb5,3dd24a54028976ba54304ec7169bb74b,7037e0369bfdaad5a730cabb2b44831c,8326c645426789920a99ed373725fa0e,906eb7d6b49fa360e7e5b65c56cd4d76,a1fc936df848a0fbc791e4bcc9b527b6,c103c6d096d52868eda26d991194b5f2</data>
    </edge>
    <edge source="SALES" target="BRAND_B">
      <data key="d4">8.0</data>
      <data key="d5">In the context of the sales analysis for Brand B stores, the sales model is characterized by a significant relationship between sales volumes and product prices. Specifically, sales are estimated using the formula 225.252 - 0.803 * pricei, highlighting the inverse relationship between sales and price. This model suggests that as the price of the product increases, sales decrease at a rate of 0.803 units for every unit increase in price.

Moreover, the sales at the jth store of Brand B are influenced by the brand's category, which adds a specific intercept, Alpha B, to the sales model. This indicates that Brand B's sales are modeled as &#956;C + &#946; pricej + &#981;j when the jth store is of Brand B, where &#956;C represents the base sales volume, &#946; is the coefficient for price, and &#981;j is the error term. This model takes into account the unique characteristics of Brand B stores, which affect sales volumes differently compared to other brands.

Brand B's sales volumes are also compared to those of Brand A, providing insights into the competitive landscape. The sales variable, Salesj, is the dependent variable for stores of Brand B, and its value is influenced by whether the store is of Brand B, with a specific intercept, &#956;\u02c6B, that differs from other brands. This specific intercept, &#956;B, is crucial in modeling sales for Brand B stores, reflecting the unique sales dynamics of the brand.

Furthermore, the sales volume of Brand B stores is influenced by the price of the product, with a steeper gradient for the fitted regression line than Brand C. This indicates that Brand B is more sensitive to price changes than Brand C, with sales volumes declining more rapidly as prices increase. This sensitivity to price changes is a critical factor in understanding the sales performance of Brand B stores and developing effective pricing strategies.</data>
      <data key="d6">1820d10ee0f23f34b3ea88ba475bc52d,1b523d1edabe381403fc470a9b8d47fa,1d141ab04db553f78a313e430e54abb5,3dd24a54028976ba54304ec7169bb74b,7037e0369bfdaad5a730cabb2b44831c,8326c645426789920a99ed373725fa0e,906eb7d6b49fa360e7e5b65c56cd4d76,a1fc936df848a0fbc791e4bcc9b527b6</data>
    </edge>
    <edge source="SALES" target="BRAND_C">
      <data key="d4">8.0</data>
      <data key="d5">In the context of the sales model for Brand C stores, SALES (Salesj) is the dependent variable, illustrating the relationship between sales volumes and the price of the product. The sales for Brand C are estimated using the formula 162.879 - 0.231 * pricei, which indicates a less steep gradient for the fitted regression line compared to Brands A and B, suggesting that Brand C's sales are less sensitive to price changes. The sales model for Brand C stores is further refined by adding a specific intercept, \u00b5C, to account for the unique characteristics of Brand C. This intercept, \u00b5\u02c6C, is adjusted if the jth store is of Brand C, reflecting the influence of Brand C's category on sales at the store level. Additionally, the sales for Brand C stores are modeled as \u00b5C + \u03b2 pricej + \u03f5j, where \u03b2 represents the coefficient for price, and \u03f5j accounts for any store-specific effects. Comparisons are made between Brand C's sales volumes and those of Brand A, providing insights into the competitive landscape. Overall, the sales of Brand C stores are influenced by the price of the product, the brand's category, and store-specific factors, all of which contribute to the unique sales dynamics of Brand C.</data>
      <data key="d6">1820d10ee0f23f34b3ea88ba475bc52d,1b523d1edabe381403fc470a9b8d47fa,1d141ab04db553f78a313e430e54abb5,3dd24a54028976ba54304ec7169bb74b,7037e0369bfdaad5a730cabb2b44831c,8326c645426789920a99ed373725fa0e,906eb7d6b49fa360e7e5b65c56cd4d76,a1fc936df848a0fbc791e4bcc9b527b6</data>
    </edge>
    <edge source="SALES" target="BETA">
      <data key="d4">4.0</data>
      <data key="d5">In the context of the statistical analysis of sales data, BETA is a crucial parameter that quantifies the relationship between the price of a product and the sales volume at the jth store. Specifically, BETA serves as the coefficient for the price variable (c_pricej) in the model designed to predict SALESj. This parameter is pivotal in understanding how changes in price influence sales volume. In the reparameterised model, BETA continues to represent the effect of price on sales, providing insights into the sensitivity of sales to price fluctuations at the store level.</data>
      <data key="d6">1b523d1edabe381403fc470a9b8d47fa,8326c645426789920a99ed373725fa0e,906eb7d6b49fa360e7e5b65c56cd4d76,a1fc936df848a0fbc791e4bcc9b527b6</data>
    </edge>
    <edge source="SALES" target="BRAND_MODEL1">
      <data key="d4">2.0</data>
      <data key="d5">In the context of algorithmic analysis, the entities SALES and BRAND_MODEL1 are intricately related. SALES, serving as the dependent variable, is the primary focus of interest, representing the outcome that is being predicted or explained. BRAND_MODEL1, on the other hand, is a statistical model designed to estimate SALES. This model takes into account factors such as brand and price, suggesting that these variables are considered as independent variables that might influence the sales figures. The model, Brand.model1, aims to establish a relationship between the brand, price, and the resulting sales, providing insights into how these factors collectively impact sales performance.</data>
      <data key="d6">8326c645426789920a99ed373725fa0e,ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </edge>
    <edge source="SALES" target="FIGURE_9_3">
      <data key="d4">1.0</data>
      <data key="d5">Figure 9.3 shows the scatterplot of sales against price</data>
      <data key="d6">b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </edge>
    <edge source="SALES" target="FIGURE_9_2">
      <data key="d4">1.0</data>
      <data key="d5">Figure 9.2 shows the scatterplot of sales against price, revealing a systematic pattern</data>
      <data key="d6">b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </edge>
    <edge source="SALES" target="MU_HAT_A">
      <data key="d4">1.0</data>
      <data key="d5">The sales variable is influenced by the estimated intercept for Brand A (&#181;&#710;A) in the regression model</data>
      <data key="d6">7037e0369bfdaad5a730cabb2b44831c</data>
    </edge>
    <edge source="SALES" target="MU_HAT_B">
      <data key="d4">1.0</data>
      <data key="d5">The sales variable is influenced by the estimated intercept for Brand B (&#181;&#710;B) in the regression model</data>
      <data key="d6">7037e0369bfdaad5a730cabb2b44831c</data>
    </edge>
    <edge source="SALES" target="MU_HAT_C">
      <data key="d4">1.0</data>
      <data key="d5">The sales variable is influenced by the estimated intercept for Brand C (&#181;&#710;C) in the regression model</data>
      <data key="d6">7037e0369bfdaad5a730cabb2b44831c</data>
    </edge>
    <edge source="SALES" target="BETA_HAT">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the statistical analysis, SALES is identified as the dependent variable, which is significantly influenced by the estimated coefficient, BETA_HAT (\u03b2\u02c6), particularly in relation to the price variable. This indicates that changes in price have a quantifiable effect on SALES, as captured by the regression model. The coefficient BETA_HAT (\u03b2\u02c6) serves as a measure of this influence, providing insights into the strength and direction of the relationship between price adjustments and sales performance.</data>
      <data key="d6">248924760a2bfbc82501fd6b11cfa0aa,7037e0369bfdaad5a730cabb2b44831c</data>
    </edge>
    <edge source="SALES" target="C_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">c_pricej is used as a predictor in the model equations for Salesj</data>
      <data key="d6">1b523d1edabe381403fc470a9b8d47fa</data>
    </edge>
    <edge source="SALES" target="MU_A">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the sales data for stores of Brand A, referred to as SALES, is being modeled using a linear regression framework. The model incorporates an intercept parameter, denoted as \u00b5A, which is specific to Brand A and plays a crucial role in the model for Salesj, where j represents the individual stores. This intercept parameter, \u00b5A, is a fundamental component of the model as it establishes the baseline sales value for Brand A stores before considering any other explanatory variables. The model assumes that the sales for each store (Salesj) can be estimated by adding a store-specific effect, \u03f5j, to the intercept parameter \u00b5A, thereby capturing the unique characteristics of each store within the Brand A community. This statistical approach allows for a nuanced understanding of the sales performance across different stores under the Brand A umbrella, providing insights into the factors that influence sales beyond the common baseline established by \u00b5A.</data>
      <data key="d6">1b523d1edabe381403fc470a9b8d47fa,48971100deb5bb374a41c1f2b7b2a86a</data>
    </edge>
    <edge source="SALES" target="MU_B">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the sales data for stores of Brand B (MU_B) is being modeled using a linear regression framework. The model for Salesj, which represents the sales at a particular store j of Brand B (SALES), is structured around the intercept parameter \u00b5B. This parameter, \u00b5B, is crucial as it serves as the baseline sales value for Brand B stores in the absence of any other influencing factors. The term \u03f5j represents the deviation or variation in sales at store j from the baseline sales value \u00b5B, capturing the unique characteristics or conditions specific to that store. This model allows for the identification of trends and patterns in sales data for Brand B stores, providing insights into factors that influence sales performance across different locations.</data>
      <data key="d6">1b523d1edabe381403fc470a9b8d47fa,48971100deb5bb374a41c1f2b7b2a86a</data>
    </edge>
    <edge source="SALES" target="MU_C">
      <data key="d4">2.0</data>
      <data key="d5">In the context of algorithmic analysis, particularly focusing on the statistical modeling of sales data for Brand C's stores, the entity "SALES" is associated with the sales figures, while "MU_C" (\u00b5C) represents the intercept parameter specific to Brand C in the linear regression model. This model, which aims to predict Salesj (the sales for the jth store), is formulated as Salesj = \u00b5C + \u03f5j, where \u00b5C is the baseline sales value for Brand C, and \u03f5j accounts for the additional sales variation attributed to the jth store. This statistical model is crucial for understanding the baseline performance of Brand C's stores and the unique contributions of each store to the overall sales figures.</data>
      <data key="d6">1b523d1edabe381403fc470a9b8d47fa,48971100deb5bb374a41c1f2b7b2a86a</data>
    </edge>
    <edge source="SALES" target="BRAND_MODEL2">
      <data key="d4">1.0</data>
      <data key="d5">Sales is the dependent variable in Brand.model2, influenced by brand and price</data>
      <data key="d6">93da9813e10a119798de6982977f1239</data>
    </edge>
    <edge source="SALES" target="MU">
      <data key="d4">2.0</data>
      <data key="d5">In the context of algorithmic analysis, particularly focusing on the statistical modeling of sales data, the entity "MU" plays a significant role. MU is identified as the base intercept for sales at the jth store, providing a foundational value from which sales figures are extrapolated. This parameter is crucial in understanding the baseline sales volume, as it is reparameterized within the model to better reflect the inherent structure and trends within the sales data. This reparameterization allows for a more accurate and nuanced interpretation of sales patterns, ensuring that the model can effectively capture the underlying dynamics of sales performance across different stores. The entity "SALES" is directly associated with these analyses, serving as the dependent variable that MU helps to predict and explain. Through the use of sophisticated statistical techniques, such as linear regression and residual analysis, the relationship between MU and SALES can be thoroughly explored, offering insights into the factors that influence sales volume at various stores.</data>
      <data key="d6">906eb7d6b49fa360e7e5b65c56cd4d76,a1fc936df848a0fbc791e4bcc9b527b6</data>
    </edge>
    <edge source="SALES" target="ALPHA_B">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis for sales data, Alpha B plays a dual yet interconnected role. Firstly, Alpha B is a critical component in calculating the intercept for sales at the jth store, specifically when the store is associated with Brand B. This calculation is pivotal for understanding the baseline sales performance of Brand B stores. Secondly, Alpha B quantifies the disparity in sales volume between Brand B and Brand A, offering insights into the comparative sales performance of these two brands. This dual interpretation of Alpha B enriches the analysis by providing a nuanced understanding of sales dynamics within the retail environment, particularly in relation to brand-specific sales trends.</data>
      <data key="d6">906eb7d6b49fa360e7e5b65c56cd4d76,a1fc936df848a0fbc791e4bcc9b527b6</data>
    </edge>
    <edge source="SALES" target="ALPHA_C">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis for sales data, Alpha C (ALPHA_C) plays a significant role. It is characterized as a parameter that, when added to Mu, determines the intercept for sales at the jth store, specifically for stores that are of Brand C. This indicates that Alpha C helps in establishing the baseline sales volume for Brand C stores. Additionally, Alpha C signifies the difference in sales volume between Brand C and Brand A, providing insights into comparative sales performance among different brands within the same market. This parameter is crucial for understanding and predicting sales trends, particularly in relation to brand-specific effects on sales outcomes.</data>
      <data key="d6">906eb7d6b49fa360e7e5b65c56cd4d76,a1fc936df848a0fbc791e4bcc9b527b6</data>
    </edge>
    <edge source="SALES" target="DESIGN_MATRIX_X">
      <data key="d4">1.0</data>
      <data key="d5">Sales is the dependent variable in the linear regression model, which is modeled using the design matrix X</data>
      <data key="d6">c103c6d096d52868eda26d991194b5f2</data>
    </edge>
    <edge source="SALES" target="RETAIL_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Sales is the dependent variable in the regression models, which is predicted based on price and brand</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="SALES" target="PARALLEL_LINES_MODEL">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the parallel lines model, SALES serves as the response variable, indicating the volume of sales for different brands. This model posits that the impact of price on SALES volume is uniform across all brands, suggesting that changes in price affect sales in a consistent manner irrespective of the brand. This uniformity in response is a key assumption of the parallel lines model, which simplifies the analysis by treating the effect of price as a constant factor for all brands involved.</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac,1820d10ee0f23f34b3ea88ba475bc52d</data>
    </edge>
    <edge source="SALES" target="BRAND_MODEL4">
      <data key="d4">1.0</data>
      <data key="d5">Sales is the response variable in the regression model fitted by brand.model4</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="SALES" target="INTERACTION_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">In the interaction model, the effect of price on sales volume differs between brands</data>
      <data key="d6">1820d10ee0f23f34b3ea88ba475bc52d</data>
    </edge>
    <edge source="ADVERT" target="BETA_2">
      <data key="d4">2.0</data>
      <data key="d5">In the context of a linear regression analysis, the entity ADVERT serves as an explanatory variable, which is used to predict changes in the sales volume. The coefficient BETA_2 is directly associated with ADVERT, acting as the quantifier for the effect of advertising on sales. This coefficient measures the magnitude and direction of the impact that advertising has on the dependent variable, sales volume, within the model. Specifically, BETA_2 indicates how much the sales volume is expected to change for a unit increase in advertising expenditure, holding all other factors constant. This relationship is crucial for understanding the dynamics between advertising efforts and their outcomes in terms of sales, providing insights into the effectiveness of marketing strategies.</data>
      <data key="d6">250ee5d766c64e7975bcc427b4bf9074,a828fd17fc38e902484872c88a6b242c</data>
    </edge>
    <edge source="ADVERT" target="FIGURE_1_6">
      <data key="d4">1.0</data>
      <data key="d5">Figure 1.6 shows the relationship between the sales volume and the local advertising budget</data>
      <data key="d6">250ee5d766c64e7975bcc427b4bf9074</data>
    </edge>
    <edge source="ADVERT" target="DATAPPOINTS">
      <data key="d4">1.0</data>
      <data key="d5">Advert is one of the coordinates (y-axis) of the datapoints in the 3D graphical representation</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376</data>
    </edge>
    <edge source="ADVERT" target="REGRESSION_PLANE">
      <data key="d4">1.0</data>
      <data key="d5">Advert is one of the explanatory variables in the regression analysis, and the regression plane describes the relationship between advert and sales</data>
      <data key="d6">b1690cb1a67892245c0665e5099e322d</data>
    </edge>
    <edge source="ADVERT" target="OBSERVED_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Advertising budget (advert) is one of the explanatory variables in the observed data</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="ADVERT" target="STATISTICAL_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Advert was considered as an independent variable in a more complex model but is not discussed in the context of simple linear regression.</data>
      <data key="d6">e079b7c92d5c0b009ff02040eb652bc6</data>
    </edge>
    <edge source="RESIDUAL" target="INFLUENTIAL_DATA_POINT">
      <data key="d4">1.0</data>
      <data key="d5">Residuals (ri) are used to identify influential data points by comparing their absolute values to thresholds</data>
      <data key="d6">09391efd3b8c510205098b548bc8dc74</data>
    </edge>
    <edge source="SLR_MODEL" target="COEFFICIENT">
      <data key="d4">1.0</data>
      <data key="d5">The coefficient is a parameter in the simple linear regression model</data>
      <data key="d6">2e5e1bdaa9fcc7b3391d277fd6bb247a</data>
    </edge>
    <edge source="BETA_2" target="X2">
      <data key="d4">1.0</data>
      <data key="d5">X2 is the explanatory variable associated with the coefficient Beta_2 in the linear regression model</data>
      <data key="d6">a828fd17fc38e902484872c88a6b242c</data>
    </edge>
    <edge source="BETA_2" target="QUADRATIC_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">BETA_2 (&#946;2) is one of the parameters in the quadratic regression model. It is estimated using least squares estimation</data>
      <data key="d6">e5131a1158e58f1b7b44b21ced7b6f60</data>
    </edge>
    <edge source="BETA_2" target="YI">
      <data key="d4">1.0</data>
      <data key="d5">The parameter Beta_2 (&#946;2) is associated with the explanatory variable X2 and affects the change in the log-transformed response variable Yi per unit change in X2.</data>
      <data key="d6">0fbc9037ca9a440e79e9ac05664b9b3d</data>
    </edge>
    <edge source="BETA_2" target="YB">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the statistical model under analysis, BETA_2, also denoted as &#946;2, plays a significant role as the coefficient that quantifies the impact of the predictor variable X2 on the predicted response variable YB. Despite X2 being fixed in this scenario, BETA_2 remains crucial in the calculation of YB. YB, in turn, represents the back-transformed prediction for the response variable Y, integrating the effect size specified by BETA_2. This relationship underscores the importance of BETA_2 in understanding the structure and dynamics of the model, particularly in how changes in X2, if variable, would theoretically influence the predicted values of YB.</data>
      <data key="d6">21e429490eeefe7d9c245058fd48ca68,995fb26a0261f824952fa7b2fac3382e</data>
    </edge>
    <edge source="R2" target="OBSERVED_DATA">
      <data key="d4">1.0</data>
      <data key="d5">When there is one explanatory variable, the observed data can be represented as points in R2</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="R2" target="DI">
      <data key="d4">1.0</data>
      <data key="d5">r2 is part of the variable representing the squared standardised residual for the ith observation that contributes to the Cook's distance Di</data>
      <data key="d6">98d6982108f2d42fe0437bff8c666e17</data>
    </edge>
    <edge source="R3" target="REGRESSION_PLANE">
      <data key="d4">1.0</data>
      <data key="d5">R3 is the space in which the regression plane is visualized in a 3D graphical representation</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376</data>
    </edge>
    <edge source="R3" target="OBSERVED_DATA">
      <data key="d4">1.0</data>
      <data key="d5">When there are two explanatory variables, the observed data can be represented as points in R3</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="R3" target="PLANE">
      <data key="d4">1.0</data>
      <data key="d5">Points in R3 are summarized using a plane</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="GRAPHICAL_ILLUSTRATION" target="HTML_VERSION">
      <data key="d4">1.0</data>
      <data key="d5">Graphical illustration is used in the HTML version of the document to provide a visual representation of the data and fitted model</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376</data>
    </edge>
    <edge source="GRAPHICAL_ILLUSTRATION" target="3D_GRAPHICAL_REPRESENTATION">
      <data key="d4">1.0</data>
      <data key="d5">3D graphical representation is a type of graphical illustration that shows the data and fitted model in three dimensions</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376</data>
    </edge>
    <edge source="HTML_VERSION" target="PDF_FORMAT">
      <data key="d4">1.0</data>
      <data key="d5">The HTML version of the document contains interactive graphics, whereas the PDF format shows a static graphic</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376</data>
    </edge>
    <edge source="DATAPPOINTS" target="REGRESSION_PLANE">
      <data key="d4">2.0</data>
      <data key="d5">The entity "DATAPPOINTS" represents individual data observations that are plotted in a three-dimensional (3D) space. These datapoints play a crucial role in defining the "REGRESSION_PLANE", which is a statistical model used to understand the relationship between the variables represented on the x, y, and z axes. The regression plane is determined by the datapoints through a process that minimizes the sum of squared distances along the z-axis, effectively fitting the plane to the data in a way that best represents the linear relationship between the variables. This visualization of the datapoints in relation to the regression plane provides insights into the structure and trends within the dataset, facilitating a deeper understanding of the underlying statistical model and its predictive capabilities.</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376,b1690cb1a67892245c0665e5099e322d</data>
    </edge>
    <edge source="REGRESSION_PLANE" target="PLANE">
      <data key="d4">1.0</data>
      <data key="d5">The plane is the same as the regression plane, used to represent the relationship between sales, price, and advert</data>
      <data key="d6">b1690cb1a67892245c0665e5099e322d</data>
    </edge>
    <edge source="REGRESSION_PLANE" target="PDF_FORMAT">
      <data key="d4">1.0</data>
      <data key="d5">PDF format shows a static graphic of the regression plane, but does not support interactive graphics</data>
      <data key="d6">b1690cb1a67892245c0665e5099e322d</data>
    </edge>
    <edge source="REGRESSION_PLANE" target="Z_AXIS">
      <data key="d4">1.0</data>
      <data key="d5">The z-axis is used in least squares estimation to measure the distance between the observed datapoints and the regression plane</data>
      <data key="d6">b1690cb1a67892245c0665e5099e322d</data>
    </edge>
    <edge source="REGRESSION_PLANE" target="RETAIL_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Retail data is used to determine the regression plane by analyzing the relationship between sales, price, and advert</data>
      <data key="d6">b1690cb1a67892245c0665e5099e322d</data>
    </edge>
    <edge source="PDF_FORMAT" target="STATIC_GRAPHIC">
      <data key="d4">1.0</data>
      <data key="d5">Static graphic is used in the PDF format to represent the regression plane and datapoints</data>
      <data key="d6">8a9b984c146f59b2af83d1c2f373d376</data>
    </edge>
    <edge source="FIGURE_1_7" target="RETAIL_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Figure 1.7 is a graphical representation of the Retail data, including the plane of best fit</data>
      <data key="d6">b1690cb1a67892245c0665e5099e322d</data>
    </edge>
    <edge source="RETAIL_DATA" target="EXERCISE_4">
      <data key="d4">1.0</data>
      <data key="d5">Retail data is used in Exercise 4 to practice reparameterising a multiple regression model</data>
      <data key="d6">2ced3e26eaed2dfcd8e4caf49737cab4</data>
    </edge>
    <edge source="RETAIL_DATA" target="BRAND_C">
      <data key="d4">1.0</data>
      <data key="d5">Brand C is a category in the retail data whose observations are analyzed in the regression models</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="BRAND_B">
      <data key="d4">1.0</data>
      <data key="d5">Brand B is a category in the retail data whose observations are analyzed in the regression models</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="BRAND_A">
      <data key="d4">1.0</data>
      <data key="d5">Brand A is a category in the retail data whose observations are analyzed in the regression models</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="PARALLEL_LINES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The parallel lines model is applied to the retail data to analyze the relationship between sales, price, and brand</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="MU">
      <data key="d4">1.0</data>
      <data key="d5">Mu is a parameter in the regression models that represents the baseline sales level in the retail data</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="ALPHA_B">
      <data key="d4">1.0</data>
      <data key="d5">Alpha B is a parameter in the regression models that represents the effect of Brand B on sales in the retail data</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="ALPHA_C">
      <data key="d4">1.0</data>
      <data key="d5">Alpha C is a parameter in the regression models that represents the effect of Brand C on sales in the retail data</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="BETA">
      <data key="d4">1.0</data>
      <data key="d5">Beta is a parameter in the regression models that represents the effect of price on sales in the retail data</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="XJB">
      <data key="d4">1.0</data>
      <data key="d5">XjB is an indicator variable in the regression models that represents whether the jth observation in the retail data is of Brand B</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="XJC">
      <data key="d4">1.0</data>
      <data key="d5">XjC is an indicator variable in the regression models that represents whether the jth observation in the retail data is of Brand C</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="PRICEJ">
      <data key="d4">1.0</data>
      <data key="d5">Pricej is a variable in the regression models that represents the price of the jth observation in the retail data</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="FITTED_LINE">
      <data key="d4">1.0</data>
      <data key="d5">The fitted line is obtained from the regression models and is used to predict sales based on price and brand in the retail data</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="FIGURE_9_4">
      <data key="d4">1.0</data>
      <data key="d5">Figure 9.4 is a graphical representation showing separate scatterplots for each brand with the corresponding fitted regression line from the parallel lines model in the retail data</data>
      <data key="d6">ac15b639b0849006471dfe102376c2c0</data>
    </edge>
    <edge source="RETAIL_DATA" target="BRAND_MODEL4">
      <data key="d4">1.0</data>
      <data key="d5">Retail data is used in the regression model fitted by brand.model4</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLE" target="OBSERVED_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Explanatory variables are part of the observed data</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLE" target="ERROR_TERM">
      <data key="d4">1.0</data>
      <data key="d5">The values of the explanatory variable (x) for each unit of observation (xj) are used in the model equation, and the difference between the observed values of the response variable and the values predicted by the model is represented by the error term (&#1013;).</data>
      <data key="d6">b9af17718641389ba07f53be13f31f8c</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLE" target="MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The model uses the explanatory variable to predict or explain the response variable</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLE" target="TRANSFORMATION">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, the explanatory variable undergoes a transformation to ensure adherence to the model's assumptions, particularly the linearity assumption. This process is crucial when the original form of the explanatory variable does not exhibit a linear relationship with the response variable, necessitating a mathematical adjustment to meet the linearity requirement for accurate model estimation. The transformation, therefore, serves as a critical step in preprocessing the data, enabling the model to effectively capture the underlying relationship between the variables.</data>
      <data key="d6">7347b44ffb25a066e43321f4eaf5a806,b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLE" target="VARIANCE">
      <data key="d4">1.0</data>
      <data key="d5">The variance of the response variable may increase with the explanatory variable</data>
      <data key="d6">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </edge>
    <edge source="EXPLANATORY_VARIABLE" target="STATISTICAL_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Explanatory variables, such as brand, are included in statistical models to predict sales volume.</data>
      <data key="d6">e079b7c92d5c0b009ff02040eb652bc6</data>
    </edge>
    <edge source="MODEL" target="PARAMETERS">
      <data key="d4">1.0</data>
      <data key="d5">The model contains parameters that need to be estimated or optimized</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="MODEL" target="STOCHASTIC_NATURE">
      <data key="d4">1.0</data>
      <data key="d5">The model takes into account the stochastic nature of the response</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="MODEL" target="SYSTEMATIC_COMPONENT">
      <data key="d4">1.0</data>
      <data key="d5">The model includes a systematic component that follows a specific pattern or rule</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="MODEL" target="MODEL_ADEQUACY">
      <data key="d4">1.0</data>
      <data key="d5">Model adequacy is a criterion for evaluating the model</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="MODEL" target="GEORGE E. BOX">
      <data key="d4">1.0</data>
      <data key="d5">George E. Box's quote relates to the concept of models being approximations of reality</data>
      <data key="d6">22061b1108c7f9963497b7a320be22b8</data>
    </edge>
    <edge source="MODEL" target="DESCRIPTION">
      <data key="d4">1.0</data>
      <data key="d5">Models can be used for description, summarizing data and reducing dimensionality</data>
      <data key="d6">22061b1108c7f9963497b7a320be22b8</data>
    </edge>
    <edge source="MODEL" target="PREDICTION">
      <data key="d4">1.0</data>
      <data key="d5">Models can be used for prediction, focusing on forecasting and accuracy</data>
      <data key="d6">22061b1108c7f9963497b7a320be22b8</data>
    </edge>
    <edge source="MODEL" target="EXPLANATION">
      <data key="d4">1.0</data>
      <data key="d5">Models can be used for explanation, shedding light on relationships between predictor variables and response</data>
      <data key="d6">22061b1108c7f9963497b7a320be22b8</data>
    </edge>
    <edge source="MODEL" target="PARSIMONY">
      <data key="d4">1.0</data>
      <data key="d5">Parsimony is a criterion for evaluating the adequacy of a model</data>
      <data key="d6">22061b1108c7f9963497b7a320be22b8</data>
    </edge>
    <edge source="MODEL" target="PLAUSIBILITY">
      <data key="d4">1.0</data>
      <data key="d5">Plausibility is a criterion for evaluating the adequacy of a model</data>
      <data key="d6">22061b1108c7f9963497b7a320be22b8</data>
    </edge>
    <edge source="MODEL" target="RESIDUAL_PLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Residual plots are used to assess the fit of a statistical model</data>
      <data key="d6">7cd6069e88e81548a237fa937adfecc6</data>
    </edge>
    <edge source="MODEL" target="R_SOFTWARE">
      <data key="d4">1.0</data>
      <data key="d5">R software is used to fit statistical models and produce residual plots</data>
      <data key="d6">7cd6069e88e81548a237fa937adfecc6</data>
    </edge>
    <edge source="BETA0" target="BETA">
      <data key="d4">1.0</data>
      <data key="d5">Beta is the column vector containing all Beta parameters, including Beta0</data>
      <data key="d6">b5d0a103e1f34a00aef67fedd0e8c693</data>
    </edge>
    <edge source="BETA0" target="YI">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, the entities BETA0 and YI are intricately related. Specifically, YI is a dependent variable that is a function of BETA0 (\u03b20), which acts as the intercept in the linear relationship within the second and third models. This indicates that YI's value is influenced by BETA0 in a linear fashion in these models. Additionally, YI is also calculated using BETA0 as the intercept in an exponential model, suggesting a more complex relationship where the effect of BETA0 on YI is not only linear but also exponential. This dual representation of YI as a function of BETA0 in both linear and exponential models highlights the versatility of BETA0 in influencing YI across different types of statistical models.</data>
      <data key="d6">34fceaaf7d835828b5ee2327325c37f8,90ed6030c5a5a0764b7dcd4115b4d4d3</data>
    </edge>
    <edge source="BETA0" target="BETA1">
      <data key="d4">3.0</data>
      <data key="d5">In the context of a simple linear regression model, BETA0 and BETA1 serve as crucial parameters. BETA0, often denoted as b\u03b20, is determined through a calculation that involves BETA1, denoted as b\u03b21, following the formula ybar - (Sxy/Sxx)xbar. This highlights the interdependence of BETA0 and BETA1, where BETA1 is not only a parameter in its own right but also plays a pivotal role in the computation of BETA0. This relationship underscores the structure of the linear regression model, where BETA0 acts as the intercept and BETA1 as the slope, both essential for understanding the linear relationship between the independent and dependent variables.</data>
      <data key="d6">254a8a17b1be06702934341e3bf41e85,5b24b5382abe9d1898810b3e4b9b455a,86c401dda130c2d201c3339526062a24</data>
    </edge>
    <edge source="BETA0" target="VOLUMEI">
      <data key="d4">1.0</data>
      <data key="d5">&#946;0 is the intercept term in the model used to calculate Volumei</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="BETA0" target="X_BAR">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, particularly within the framework of simple linear regression models, X_BAR plays a pivotal role. X_BAR, often denoted as the mean of the independent variable X, is an essential component in the calculation of BETA0. BETA0, in turn, represents the y-intercept of the regression line, crucial for understanding the baseline prediction when X is zero. The utilization of X_BAR in the computation of BETA0 is a fundamental aspect of least squares estimation, a method employed to find the line of best fit for a set of data points. This process ensures that the sum of the squares of the vertical distances between the observed data points and the regression line is minimized, thereby providing the most accurate predictions possible.</data>
      <data key="d6">28cf5ff0c09fa5c0390267bb9aa3ce47,5b24b5382abe9d1898810b3e4b9b455a</data>
    </edge>
    <edge source="BETA0" target="Y_BAR">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, particularly within the framework of simple linear regression models, Y_BAR plays a pivotal role. Y_BAR, often denoted as y&#773;, is the mean of the dependent variable Y. This value is crucial in the calculation of BETA0, which represents the intercept of the regression line. BETA0 is determined through the least squares estimation method, a technique that minimizes the sum of the squared differences between the observed and predicted values. In this process, Y_BAR serves as a fundamental component, aiding in the accurate estimation of BETA0 and, by extension, the entire regression model. This highlights the interdependence between Y_BAR and BETA0 in statistical modeling, where Y_BAR's role is indispensable for the computation of BETA0, ensuring the model's reliability and predictive power.</data>
      <data key="d6">28cf5ff0c09fa5c0390267bb9aa3ce47,5b24b5382abe9d1898810b3e4b9b455a</data>
    </edge>
    <edge source="BETA0" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Beta0 is part of the Beta hat (b&#946;) vector, representing the intercept in the linear regression model</data>
      <data key="d6">28cf5ff0c09fa5c0390267bb9aa3ce47</data>
    </edge>
    <edge source="BETA0" target="SXY">
      <data key="d4">1.0</data>
      <data key="d5">Beta0 (b&#946;0) is calculated using Sxy as part of the formula ybar - (Sxy/Sxx)xbar</data>
      <data key="d6">254a8a17b1be06702934341e3bf41e85</data>
    </edge>
    <edge source="BETA0" target="SXX">
      <data key="d4">1.0</data>
      <data key="d5">Beta0 (b&#946;0) is calculated using Sxx as part of the formula ybar - (Sxy/Sxx)xbar</data>
      <data key="d6">254a8a17b1be06702934341e3bf41e85</data>
    </edge>
    <edge source="BETA0" target="LOG_LIKELIHOOD">
      <data key="d4">1.0</data>
      <data key="d5">The log-likelihood function depends on Beta0, the intercept parameter</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12</data>
    </edge>
    <edge source="BETA0" target="BETA_HAT0">
      <data key="d4">1.0</data>
      <data key="d5">Beta0 is the true parameter that Beta hat 0 (b&#946;0) estimates in the linear regression model</data>
      <data key="d6">09caa54ca1372d152e47051be4d44ede</data>
    </edge>
    <edge source="BETA1" target="BETA">
      <data key="d4">1.0</data>
      <data key="d5">Beta is the column vector containing all Beta parameters, including Beta1</data>
      <data key="d6">b5d0a103e1f34a00aef67fedd0e8c693</data>
    </edge>
    <edge source="BETA1" target="BETA2">
      <data key="d4">1.0</data>
      <data key="d5">Beta1 and Beta2 are coefficients in the quadratic regression model that together determine the expected change in the response due to a unit change in the explanatory variable</data>
      <data key="d6">11452a08471d93959558de2ece9a69af</data>
    </edge>
    <edge source="BETA1" target="EXPECTED_CHANGE">
      <data key="d4">1.0</data>
      <data key="d5">Beta1 contributes to the linear term of the expected change in the response due to a unit change in the explanatory variable</data>
      <data key="d6">c9a01b92d11585f6549f62e8bd78d652</data>
    </edge>
    <edge source="BETA1" target="YI">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, the entities BETA1 and YI are central to understanding the relationship between a response variable and an explanatory variable. Specifically, YI is described as a function of BETA1 (\u03b21) and xi,1 in the first model, which outlines a linear relationship between the two variables. This suggests that changes in xi,1 can be used to predict changes in YI through the linear regression model, where BETA1 serves as the coefficient that quantifies the strength and direction of the relationship.

Additionally, YI is also calculated using BETA1 as the coefficient of xi1 in an exponential model. This indicates that there is a second model being considered, where the relationship between YI and xi1 is not linear but exponential. In this model, BETA1 still plays a crucial role as the coefficient that determines the rate at which YI changes with respect to xi1.

It is important to note that while both models utilize BETA1 as a coefficient, the nature of the relationship between YI and xi1 differs significantly. The linear model assumes a constant rate of change, whereas the exponential model suggests a rate of change that increases or decreases over time, depending on the value of BETA1.

In summary, BETA1 is a key parameter in both the linear and exponential models that describe the relationship between YI and xi1. The linear model represents a straightforward, constant rate of change, while the exponential model indicates a more complex, varying rate of change. Both models are essential for a comprehensive understanding of how YI responds to changes in xi1.</data>
      <data key="d6">34fceaaf7d835828b5ee2327325c37f8,90ed6030c5a5a0764b7dcd4115b4d4d3</data>
    </edge>
    <edge source="BETA1" target="BRAINI">
      <data key="d4">1.0</data>
      <data key="d5">BRAINi is the response variable in the linear regression model, which is related to the slope coefficient Beta1</data>
      <data key="d6">86c401dda130c2d201c3339526062a24</data>
    </edge>
    <edge source="BETA1" target="BODYI">
      <data key="d4">1.0</data>
      <data key="d5">BODYi is the explanatory variable in the linear regression model, which is related to the slope coefficient Beta1</data>
      <data key="d6">86c401dda130c2d201c3339526062a24</data>
    </edge>
    <edge source="BETA1" target="EPSILONI">
      <data key="d4">1.0</data>
      <data key="d5">EPSILONi is the error term in the linear regression model, which is independent of the slope coefficient Beta1</data>
      <data key="d6">86c401dda130c2d201c3339526062a24</data>
    </edge>
    <edge source="BETA1" target="VOLUMEI">
      <data key="d4">1.0</data>
      <data key="d5">&#946;1 is the coefficient for Diameteri in the model used to calculate Volumei</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="BETA1" target="SXY">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis, particularly within the framework of simple linear regression models, BETA1, denoted as &#946;1, plays a pivotal role as a parameter that quantifies the relationship between the independent and dependent variables. This parameter is calculated through a formula that involves Sxy, a key statistical measure. Sxy, in essence, is a covariance-like statistic that reflects the linear relationship between the independent and dependent variables. It is integral to the computation of BETA1, serving as the numerator in the formula Sxy/Sxx, where Sxx represents the sum of squares of the independent variable. This method of calculation is foundational to the least squares estimation technique, which aims to minimize the sum of the squared residuals, thereby providing the best fit line for the data. Through this process, BETA1 and Sxy are inextricably linked, with Sxy's role being pivotal in the accurate determination of BETA1, facilitating a deeper understanding of the underlying data structure and the strength of the linear relationship between the variables of interest.</data>
      <data key="d6">254a8a17b1be06702934341e3bf41e85,28cf5ff0c09fa5c0390267bb9aa3ce47,5b24b5382abe9d1898810b3e4b9b455a</data>
    </edge>
    <edge source="BETA1" target="SXX">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis, particularly within the framework of simple linear regression models, BETA1, denoted as &#946;1, plays a pivotal role as a parameter that quantifies the relationship between the independent and dependent variables. This parameter is calculated through a formula that involves SXX, a key component in the computation of BETA1. SXX, representing the sum of squares of the differences between each x-value and the mean of x, is crucial in the least squares estimation method. It is used directly in the calculation of BETA1, serving as the denominator in the formula Sxy/Sxx, where Sxy is the covariance between the x and y variables. This highlights the interdependence between BETA1 and SXX, with SXX's role being indispensable in determining the slope of the regression line, thus providing insights into the strength and direction of the linear relationship between the variables under study.</data>
      <data key="d6">254a8a17b1be06702934341e3bf41e85,28cf5ff0c09fa5c0390267bb9aa3ce47,5b24b5382abe9d1898810b3e4b9b455a</data>
    </edge>
    <edge source="BETA1" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Beta1 is part of the Beta hat (b&#946;) vector, representing the slope in the linear regression model</data>
      <data key="d6">28cf5ff0c09fa5c0390267bb9aa3ce47</data>
    </edge>
    <edge source="BETA1" target="LOG_LIKELIHOOD">
      <data key="d4">1.0</data>
      <data key="d5">The log-likelihood function depends on Beta1, one of the slope parameters</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12</data>
    </edge>
    <edge source="BETA1" target="BETA_HAT1">
      <data key="d4">1.0</data>
      <data key="d5">Beta1 is the true parameter that Beta hat 1 (b&#946;1) estimates in the linear regression model</data>
      <data key="d6">09caa54ca1372d152e47051be4d44ede</data>
    </edge>
    <edge source="BETA1" target="E_Y3_GIVEN_X3">
      <data key="d4">1.0</data>
      <data key="d5">Beta1 is used in the calculation involving the expected value of Y3 given X3=x3 and Beta2</data>
      <data key="d6">45f31b040576e9f3b4def6d0466cc016</data>
    </edge>
    <edge source="BETA2" target="EXPECTED_CHANGE">
      <data key="d4">1.0</data>
      <data key="d5">Beta2 contributes to the quadratic term of the expected change in the response due to a unit change in the explanatory variable</data>
      <data key="d6">c9a01b92d11585f6549f62e8bd78d652</data>
    </edge>
    <edge source="BETA2" target="YI">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, BETA2 (\u03b22) plays a significant role as a coefficient in the relationship with YI. Specifically, YI is determined through two distinct models that highlight the versatility of BETA2 in different mathematical frameworks. In the fifth model, YI is a function of BETA2 and xi,2, presented in a logarithmic form. This indicates that the explanatory variable is transformed logarithmically, suggesting a non-linear relationship between YI and xi,2, where BETA2 quantifies the strength of this association. Additionally, YI is also calculated using BETA2 as the coefficient of xi2 in an exponential model. This implies that in another model, the relationship between YI and xi2 is exponential, with BETA2 serving as the scaling factor that determines the rate of change. Both models underscore the importance of BETA2 in understanding the dynamics between YI and the explanatory variables, xi,2 and xi2, respectively, albeit through different mathematical lenses.</data>
      <data key="d6">34fceaaf7d835828b5ee2327325c37f8,90ed6030c5a5a0764b7dcd4115b4d4d3</data>
    </edge>
    <edge source="BETA2" target="VOLUMEI">
      <data key="d4">1.0</data>
      <data key="d5">&#946;2 is the coefficient for Heighti in the model used to calculate Volumei</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="BETA2" target="E_Y3_GIVEN_X3">
      <data key="d4">1.0</data>
      <data key="d5">Beta2 is used in the calculation of the expected value of Y3 given X3=x3</data>
      <data key="d6">45f31b040576e9f3b4def6d0466cc016</data>
    </edge>
    <edge source="MLR_MODEL" target="AVERAGE_EFFECT">
      <data key="d4">1.0</data>
      <data key="d5">Mlr.model is used to determine the average effect of explanatory variables</data>
      <data key="d6">c48bd062d5f6cc20c5df5758d6285562</data>
    </edge>
    <edge source="X1" target="LINEAR_PREDICTOR">
      <data key="d4">1.0</data>
      <data key="d5">X1 is one of the explanatory variables whose value x1 is used in the linear predictor to determine the mean of the response</data>
      <data key="d6">6a47154bf457c25f22c3cf9f649c5db0</data>
    </edge>
    <edge source="X1" target="YI">
      <data key="d4">1.0</data>
      <data key="d5">X1 is one of the explanatory variables in the model that influences the log-transformed response variable Yi.</data>
      <data key="d6">0fbc9037ca9a440e79e9ac05664b9b3d</data>
    </edge>
    <edge source="X1" target="YB">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, X1 and YB are identified as key entities within a model. X1 serves as a predictor variable, demonstrating a direct influence on YB. Specifically, when X1 changes by one unit, while X2 remains constant, there is a corresponding effect on the predicted response variable, YB. This relationship highlights the sensitivity of YB to variations in X1, underlining the importance of X1 as a determinant of YB's behavior in the model. The analysis suggests that X1's unit increase is a critical factor in predicting the changes in YB, illustrating a clear and quantifiable relationship between these two entities.</data>
      <data key="d6">21e429490eeefe7d9c245058fd48ca68,995fb26a0261f824952fa7b2fac3382e</data>
    </edge>
    <edge source="X2" target="LINEAR_PREDICTOR">
      <data key="d4">1.0</data>
      <data key="d5">X2 is one of the explanatory variables whose value x2 is used in the linear predictor to determine the mean of the response</data>
      <data key="d6">6a47154bf457c25f22c3cf9f649c5db0</data>
    </edge>
    <edge source="X2" target="YI">
      <data key="d4">1.0</data>
      <data key="d5">X2 is another explanatory variable in the model that affects the log-transformed response variable Yi.</data>
      <data key="d6">0fbc9037ca9a440e79e9ac05664b9b3d</data>
    </edge>
    <edge source="X2" target="YB">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, X2 serves as a constant predictor variable when assessing the impact of changes in X1 on the predicted response YB. X2 remains fixed and does not influence YB directly, allowing for a clear evaluation of the relationship between X1 and YB. This setup is crucial for understanding the independent effects of X1 on YB, as it isolates X2 from contributing to the variance in YB, ensuring that any observed changes in YB can be attributed to alterations in X1 alone.</data>
      <data key="d6">21e429490eeefe7d9c245058fd48ca68,995fb26a0261f824952fa7b2fac3382e</data>
    </edge>
    <edge source="DATASET" target="EXERCISE_13">
      <data key="d4">1.0</data>
      <data key="d5">The dataset is used in Exercise 13 to compute parameter estimates, fitted values, residuals, and the deviance of the model</data>
      <data key="d6">5b24b5382abe9d1898810b3e4b9b455a</data>
    </edge>
    <edge source="DATASET" target="RHESUS_MONKEY">
      <data key="d4">1.0</data>
      <data key="d5">Rhesus monkey is included in the mammals dataset for analysis</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="EXERCISE_4" target="LINEAR_MODELS">
      <data key="d4">1.0</data>
      <data key="d5">Exercise 4 involves reparameterising a model within the class of linear models</data>
      <data key="d6">2ced3e26eaed2dfcd8e4caf49737cab4</data>
    </edge>
    <edge source="LINEAR_MODELS" target="TRANSFORMED_EXPLANATORY_VARIABLES">
      <data key="d4">1.0</data>
      <data key="d5">Linear models can use transformed explanatory variables to better fit the data or to satisfy model assumptions</data>
      <data key="d6">0328e428a30c44572676dd571dd1e9bd</data>
    </edge>
    <edge source="LINEAR_MODELS" target="MODEL_DEVELOPMENT_STRATEGY">
      <data key="d4">1.0</data>
      <data key="d5">A model development strategy guides the building and refining of linear models, ensuring they are both realistic and as simple as possible</data>
      <data key="d6">0328e428a30c44572676dd571dd1e9bd</data>
    </edge>
    <edge source="LINEAR_MODELS" target="CATEGORICAL_PREDICTORS">
      <data key="d4">1.0</data>
      <data key="d5">Categorical predictors can be incorporated into linear models to analyze their effect on the response variable</data>
      <data key="d6">77e76692753fdf53493182b09018e6bc</data>
    </edge>
    <edge source="LINEAR_MODELS" target="QUANTITATIVE_VARIABLES">
      <data key="d4">1.0</data>
      <data key="d5">Quantitative variables are used as predictors in linear models to describe their relationship with the response variable</data>
      <data key="d6">77e76692753fdf53493182b09018e6bc</data>
    </edge>
    <edge source="LINEAR_MODELS" target="PARALLEL_LINES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">A parallel lines model is a specific type of linear model that assumes the regression lines for different categories of a factor variable are parallel</data>
      <data key="d6">77e76692753fdf53493182b09018e6bc</data>
    </edge>
    <edge source="BETA" target="BETAK">
      <data key="d4">1.0</data>
      <data key="d5">Beta is the column vector containing all Beta parameters, including Betak</data>
      <data key="d6">b5d0a103e1f34a00aef67fedd0e8c693</data>
    </edge>
    <edge source="BETA" target="YI">
      <data key="d4">1.0</data>
      <data key="d5">Yi is a function of the parameters Beta (&#946;0, &#946;1, &#946;2) in the quadratic regression model equation</data>
      <data key="d6">6c1684ed2a4840576c6b0f4d1a3a482f</data>
    </edge>
    <edge source="BETA" target="DESIGN_MATRIX">
      <data key="d4">1.0</data>
      <data key="d5">The design matrix X is used in conjunction with the vector of parameters Beta to predict the response vector Y in the linear model. The relationship is given by the equation Y = X Beta + Epsilon.</data>
      <data key="d6">e7494d6cfc3e38a4d2f3f6b21ef6445d</data>
    </edge>
    <edge source="BETA" target="BETA_HAT">
      <data key="d4">24.0</data>
      <data key="d5">In the context of linear regression analysis, BETA, denoted by the Greek letter &#946;, represents the true parameter vector that characterizes the relationship between the dependent and independent variables. BETA_HAT, symbolized as &#946;&#770; or &#946;b, is the Maximum Likelihood Estimate (MLE) of BETA. This means that BETA_HAT is the best guess of the true parameter vector BETA, derived from the data using the MLE method. The estimation process aims to minimize the difference between the observed data and the predictions made by the linear regression model, making BETA_HAT a crucial component in statistical inference and predictive modeling.</data>
      <data key="d6">01d5ee79489582b4135fc96f676b24a0,248924760a2bfbc82501fd6b11cfa0aa,255685e281cc5a9edf073c700f425a6b,2673d078d29f2af78fab9b6eacd15e37,28cf5ff0c09fa5c0390267bb9aa3ce47,2b6d31b6bff4eae3a4809451c4fb9fa6,2de7a36b32bf79c8f32612c8aaa9daa8,3d357cfa3ef0d00f49cf4acaeac1c9d1,542f546c5a131196e4701fb33c9b1dee,6b55b41598d5264f8dc6b72769748722,7955aae3fd4ca51b9ef8843e13c1f517,82932abd152e0b84a1c26a2daa4c08df,9d300fc83afb3261af61b2ab9721cadc,9dddcd96af7b557e578b3f5f36efacd7,9fc2b1e8b2b61b557f88eb9e9c708597,a4a817bb79d6ae8812c808ca41d47f43,aa195e72eb5285a4bcae9c856af30a87,ad799500572246a07f983a3b92c0e61f,b70a75a6412b2e5c44af50734844f4be,d14413709de2897231aaa83be3aa346f,d94760a5f9f6ea115fcc18024035a627,e7edd8b2874a350779ae20f1ecdf4733,f9b615b879f72501f338f8983d4cac3d,fc5b725f3c662c5471af20efdcc2dbff</data>
    </edge>
    <edge source="BETA" target="BETA_BAR">
      <data key="d4">1.0</data>
      <data key="d5">Beta bar is the notation for the estimated parameter vector Beta</data>
      <data key="d6">01d5ee79489582b4135fc96f676b24a0</data>
    </edge>
    <edge source="BETA" target="S">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis and algorithmic modeling, BETA, denoted by the Greek letter \u03b2, is a pivotal parameter within the linear regression framework. This parameter is central to the calculation of the function S, which represents the sum of squared differences. BETA serves as the parameter vector with respect to which S is calculated, highlighting its critical role in the assessment of model fit and prediction accuracy. Specifically, BETA is utilized in the computation of predicted values, denoted as X\u03b2, which are essential components in the residual analysis and the overall evaluation of the linear regression model's performance. The function S(\u03b2) encapsulates the discrepancies between the observed and predicted values, providing a quantitative measure of the model's adequacy.</data>
      <data key="d6">2167274129d4cfa74a002c4cc39df8a8,255685e281cc5a9edf073c700f425a6b,eac62cdd5518e1269fed150639331c2c</data>
    </edge>
    <edge source="BETA" target="F_BETA">
      <data key="d4">1.0</data>
      <data key="d5">Beta (&#946;) is the variable in the function f(&#946;) = &#946;T A&#946;</data>
      <data key="d6">21ec28dfe2b2c18030d541d63e51f45e</data>
    </edge>
    <edge source="BETA" target="S_BETA">
      <data key="d4">1.0</data>
      <data key="d5">Beta (&#946;) is the variable in the function S(&#946;)</data>
      <data key="d6">21ec28dfe2b2c18030d541d63e51f45e</data>
    </edge>
    <edge source="BETA" target="ALPHA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Beta is the original parameter vector that Alpha hat (&#945;b) estimates in the reparameterized model</data>
      <data key="d6">f5716ce115458c0652124734ca344806</data>
    </edge>
    <edge source="BETA" target="GAMMA">
      <data key="d4">1.0</data>
      <data key="d5">Beta is transformed by A to obtain Gamma</data>
      <data key="d6">d94760a5f9f6ea115fcc18024035a627</data>
    </edge>
    <edge source="BETA" target="ALPHA">
      <data key="d4">3.0</data>
      <data key="d5">In the context of the given entities, BETA and ALPHA, a comprehensive and enriched summary can be constructed as follows:

BETA and ALPHA are intricately connected through a reparameterisation of the model, where ALPHA is not merely a linear transformation of BETA but is specifically derived using a transformation matrix A. This relationship is further elucidated in the reparameterised model (1), where ALPHA is expressed as a function of BETA and the transformation xj - x\u00af, highlighting the linear dependency and the role of matrix A in this transformation. The linear transformation and the reparameterisation process underscore the interdependence of BETA and ALPHA, with ALPHA being the resultant vector after applying the transformation matrix A to BETA, thus encapsulating the essence of their relationship within the model's framework.</data>
      <data key="d6">5609007c6229060ffc85d8056a7fefde,6c66e9414880964ee899ceb0f16d22e9,82932abd152e0b84a1c26a2daa4c08df</data>
    </edge>
    <edge source="BETA" target="MLE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, particularly within the framework of linear regression models, BETA (denoted as \u03b2) represents the parameter vector that encapsulates the coefficients of the independent variables. Maximum Likelihood Estimation (MLE), a robust statistical method, is employed to estimate these parameters, BETA, under the assumption that the errors in the model are independently and identically distributed (iid) as normal distributions with a mean of 0 and a variance of \u03c3^2. This method leverages the likelihood function to find the parameter values that maximize the probability of observing the given data, making it a powerful tool for inferring the structure and relationships within the community of interest.</data>
      <data key="d6">9fc2b1e8b2b61b557f88eb9e9c708597,d738df7d83784c8a41b3948271c537b6</data>
    </edge>
    <edge source="BETA" target="LSE">
      <data key="d4">1.0</data>
      <data key="d5">LSE is an estimate for the parameter vector &#946; in the linear regression model</data>
      <data key="d6">d738df7d83784c8a41b3948271c537b6</data>
    </edge>
    <edge source="BETA" target="L">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, BETA, denoted as \u03b2, is a significant parameter within the likelihood function, L(\u03b2, \u03c3^2|y). This function plays a crucial role in estimating the parameters of a statistical model, where BETA is one of the key variables that help in understanding the relationship between the model's predictors and the response variable. The likelihood function, L, is used to assess the goodness of fit for a given set of parameters, including BETA, and the variance, \u03c3^2, given the observed data, y. This function is pivotal in the process of maximum likelihood estimation, where the goal is to find the set of parameters that maximizes the likelihood of observing the given data.</data>
      <data key="d6">e7edd8b2874a350779ae20f1ecdf4733,f632f01188d2c6e3091a965580cb4600</data>
    </edge>
    <edge source="BETA" target="EB">
      <data key="d4">1.0</data>
      <data key="d5">The expectation of Eb is influenced by the parameter Beta through the calculation X&#946;</data>
      <data key="d6">5a0d392715f06d5e873f45ae06aa729a</data>
    </edge>
    <edge source="BETA" target="SALES_VOLUME">
      <data key="d4">2.0</data>
      <data key="d5">In the context of brand A, BETA, denoted as &#946;, is a crucial parameter in the linear regression model that quantifies the relationship between price and SALES_VOLUME. Specifically, BETA serves as the coefficient for the price variable, enabling the prediction of the change in sales volume that corresponds to a one-unit alteration in price. This statistical measure is pivotal for understanding how price fluctuations impact the sales volume of brand A, providing valuable insights into the price sensitivity of the market and aiding in strategic decision-making.</data>
      <data key="d6">7a605c3b689bec7ab2c46df9c123e3f3,b6870535f3975c49d45e62fbe475f198</data>
    </edge>
    <edge source="BETA" target="BRAND_MODEL1">
      <data key="d4">1.0</data>
      <data key="d5">Beta is a parameter in the model Brand.model1, representing the coefficient of the price variable</data>
      <data key="d6">ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </edge>
    <edge source="BETA" target="SALESJ">
      <data key="d4">1.0</data>
      <data key="d5">beta is a parameter in the model for Salesj, representing the effect of price</data>
      <data key="d6">228bdca7843406def245d755e8df49f6</data>
    </edge>
    <edge source="BETA" target="BRAND_MODEL2">
      <data key="d4">1.0</data>
      <data key="d5">Beta is the parameter for price in Brand.model2, with a specific estimate</data>
      <data key="d6">93da9813e10a119798de6982977f1239</data>
    </edge>
    <edge source="BETA" target="SALESI">
      <data key="d4">1.0</data>
      <data key="d5">beta is the slope parameter for price in the salesi vector</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="BETA" target="PARALLEL_LINES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Beta is a parameter in the parallel lines model</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="BETA" target="PRICE_J">
      <data key="d4">1.0</data>
      <data key="d5">Beta (&#946;) is the coefficient for price j in the regression model</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="NN" target="IN">
      <data key="d4">1.0</data>
      <data key="d5">In is part of the covariance matrix of the multivariate normal distribution Nn</data>
      <data key="d6">67e4c1866b0c6e162e6e3317949e8da9</data>
    </edge>
    <edge source="NN" target="BETA_HAT">
      <data key="d4">5.0</data>
      <data key="d5">In the context of statistical modeling, NN, which can be understood as the distribution of the error term &#981;, plays a pivotal role in shaping the distribution of BETA_HAT (&#946;&#770;). This relationship is further elucidated by NN's influence on the distribution of the dependent variable Y. The connection between NN and BETA_HAT is not merely coincidental; NN is integral to the likelihood function L, which is employed to estimate BETA_HAT. This estimation process is crucial for understanding the relationship between the independent variables and the dependent variable Y within the model. The distribution of NN, therefore, has a direct impact on the accuracy and reliability of BETA_HAT, making it a fundamental component in the statistical analysis of the model.</data>
      <data key="d6">3d357cfa3ef0d00f49cf4acaeac1c9d1,45f31b040576e9f3b4def6d0466cc016,542f546c5a131196e4701fb33c9b1dee,9dddcd96af7b557e578b3f5f36efacd7,b70a75a6412b2e5c44af50734844f4be</data>
    </edge>
    <edge source="IN" target="N">
      <data key="d4">1.0</data>
      <data key="d5">The dimension n is the size of the identity matrix In</data>
      <data key="d6">e4f14e6785c6d7b7469e695aaeb170d0</data>
    </edge>
    <edge source="IN" target="BETA_HAT">
      <data key="d4">6.0</data>
      <data key="d5">In the context of statistical analysis, particularly within the framework of linear regression models, the entities IN and BETA_HAT (\u03b2b) play significant roles. IN refers to a component that is intricately involved in the covariance matrix, which is central to understanding the distribution of BETA_HAT (\u03b2b). This covariance matrix not only reflects the variability of BETA_HAT but also its relationship with the distribution of Y, a variable that exerts influence on the distribution of BETA_HAT. Moreover, IN is a crucial element in the Maximum Likelihood Estimation (MLE) process for calculating BETA_HAT, highlighting its importance in estimating the parameters of the model. The covariance matrix, which includes IN, is pivotal in the calculation of BETA_HAT, serving as a fundamental tool for assessing the precision of the estimated regression coefficients.</data>
      <data key="d6">3d357cfa3ef0d00f49cf4acaeac1c9d1,45f31b040576e9f3b4def6d0466cc016,542f546c5a131196e4701fb33c9b1dee,9dddcd96af7b557e578b3f5f36efacd7,b70a75a6412b2e5c44af50734844f4be,e7edd8b2874a350779ae20f1ecdf4733</data>
    </edge>
    <edge source="IN" target="H">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis, the entities "IN" and "H" are central to understanding the properties and calculations within a specific model. Matrix H, often referred to as the hat matrix, plays a pivotal role in the computation of residuals and exhibits a significant relationship with the identity matrix In. Specifically, the operation (In - H) results in a matrix that is both symmetric and idempotent, indicating that it maintains its form when squared. This matrix, (In - H), is crucial for residual analysis in regression models, as it helps in identifying the unexplained variance in the dependent variable.

Moreover, the trace of In minus the trace of H, denoted as tr(In) - tr(H), is equal to n - p, where n represents the number of observations and p is the number of parameters in the model, including the intercept. This relationship underscores the connection between the dimensions of the data and the complexity of the model, providing insights into the degrees of freedom in the residual space.

In summary, the hat matrix H and the identity matrix In are integral components in the algorithmic analysis of regression models. Their interplay, particularly through the symmetric and idempotent matrix (In - H), and the trace relationship, tr(In) - tr(H) = n - p, are fundamental to understanding the structure and performance of the model within the context of statistical inference.</data>
      <data key="d6">46629f2efc6c82e81265a131b4bab2ee,bd98ac7b4b5df4f63e7ecc8f4a821f57,f470791d2d3fedede166f9bb11598c9c</data>
    </edge>
    <edge source="IN" target="YC">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entities IN and YC are intricately related through the covariance matrix. Specifically, IN is an integral component of the covariance matrix that characterizes the distribution of YC. This relationship underscores the dependency structure and variability within the community of interest represented by YC, where IN plays a crucial role in quantifying the interdependencies among the variables in YC's distribution. This insight is pivotal for understanding the underlying statistical model and conducting accurate inference on the data associated with YC.</data>
      <data key="d6">679722cf8ce5ce5aee4e379528470efe,74d190f10bf6e6936242ca3cdfc4a09f</data>
    </edge>
    <edge source="IN" target="EB">
      <data key="d4">3.0</data>
      <data key="d5">In the context of algorithmic analysis, particularly within the realm of statistical modeling, the entities IN and EB play significant roles. EB is derived from IN through a transformation that involves the application of a matrix, specifically (IN - H), highlighting the direct relationship between the two entities. This transformation is not merely an abstract mathematical operation but is integral to the calculation of EB, underscoring its dependency on IN. Moreover, IN is utilized in the computation of the residual vector EB, denoted as Eb in one of the descriptions, through the formula (IN - H)Y. This formula reveals the method by which the residual vector is calculated, further emphasizing the pivotal role of IN in the analytical process. The use of matrix operations and the calculation of residuals are indicative of techniques commonly employed in linear regression analysis, where IN likely represents an identity matrix, and H could be a hat matrix used in the least squares estimation. The transformation (IN - H) is a key component in the process of estimating residuals, which are crucial for assessing the fit of a model and understanding the structure and relations within the community of interest.</data>
      <data key="d6">1da117a2f92b2db00290d2a0bfc06beb,74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6</data>
    </edge>
    <edge source="XK" target="LINEAR_PREDICTOR">
      <data key="d4">1.0</data>
      <data key="d5">Xk is one of the explanatory variables whose value xk is used in the linear predictor to determine the mean of the response</data>
      <data key="d6">6a47154bf457c25f22c3cf9f649c5db0</data>
    </edge>
    <edge source="XN" target="BETA_0_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Xn is used in the calculation of Beta_0 hat (b&#946;0) as part of the least squares estimation</data>
      <data key="d6">3fdeeb7593174f5e8a9cff55a7cd92e3</data>
    </edge>
    <edge source="XN" target="BETA_1_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Xn is used in the calculation of Beta_1 hat (b&#946;1) as part of the least squares estimation</data>
      <data key="d6">3fdeeb7593174f5e8a9cff55a7cd92e3</data>
    </edge>
    <edge source="XN" target="S">
      <data key="d4">1.0</data>
      <data key="d5">Xn is a function used in the function S(&#946;) to sum over the squared residuals</data>
      <data key="d6">10ac76f99674a01ca0f4a55586dea07e</data>
    </edge>
    <edge source="BETAK" target="LOG_LIKELIHOOD">
      <data key="d4">1.0</data>
      <data key="d5">The log-likelihood function depends on Beta_k, one of the slope parameters</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12</data>
    </edge>
    <edge source="EPSILONJ" target="N">
      <data key="d4">1.0</data>
      <data key="d5">N is the distribution of Epsilonj</data>
      <data key="d6">b5d0a103e1f34a00aef67fedd0e8c693</data>
    </edge>
    <edge source="EPSILONJ" target="EPSILON_BJ">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon j (&#1013;j) is the error term that the residual &#1013;bj estimates, indicating the similarity between the error and the residual</data>
      <data key="d6">e6f79ceb0df54119a4dc71b2162ac50b</data>
    </edge>
    <edge source="EPSILONJ" target="SALESI">
      <data key="d4">1.0</data>
      <data key="d5">epsilonj is the error term for the jth store, affecting salesi</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="N" target="ERRORS">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entity "ERRORS" is characterized by following a normal distribution denoted as "N". This normal distribution is specifically defined as N(0, &#963;^2), indicating that the errors are centered around a mean of zero with a variance of &#963;^2. This assumption is fundamental in understanding the behavior and distribution of errors within the statistical model, providing a basis for further analysis and inference.</data>
      <data key="d6">25fce1af816975003128126b5cfea73b,d738df7d83784c8a41b3948271c537b6</data>
    </edge>
    <edge source="N" target="NORMAL_DISTRIBUTION">
      <data key="d4">1.0</data>
      <data key="d5">N data points are simulated from the normal distribution</data>
      <data key="d6">ee22e1f5947947f9bd3f7f8922745e48</data>
    </edge>
    <edge source="N" target="T2_DISTRIBUTION">
      <data key="d4">1.0</data>
      <data key="d5">N data points are also simulated from the t2 distribution</data>
      <data key="d6">ee22e1f5947947f9bd3f7f8922745e48</data>
    </edge>
    <edge source="N" target="BETA_HAT_1">
      <data key="d4">1.0</data>
      <data key="d5">Sample size N is used in the calculation of Beta_hat_1 (b&#946;1)</data>
      <data key="d6">8f7a05b6d231105a6194eebdb2df372e</data>
    </edge>
    <edge source="N" target="XTX">
      <data key="d4">1.0</data>
      <data key="d5">N is used in the calculation of the elements of the matrix XTX</data>
      <data key="d6">56ff186fc629e1e42f2759fc4b984199</data>
    </edge>
    <edge source="N" target="L">
      <data key="d4">1.0</data>
      <data key="d5">N is used in the calculation of the likelihood function L</data>
      <data key="d6">e7edd8b2874a350779ae20f1ecdf4733</data>
    </edge>
    <edge source="N" target="SIGMA_HAT_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">N is used in the calculation of the unbiased estimator &#963;b^2 for the variance</data>
      <data key="d6">6648f0d6deed51fb4fb25e6992a71ddf</data>
    </edge>
    <edge source="N" target="S2">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, N represents the total number of observations within the dataset. This figure is crucial for the computation of the unbiased estimate s^2, which is a key component in understanding the variance of the data. The term S2, likely a typographical error for s^2, refers to this unbiased estimator that provides insights into the spread of the observations around the mean. By leveraging N in the calculation of s^2, one can accurately assess the variability within the dataset, ensuring that the estimate is not biased by the sample size.</data>
      <data key="d6">9923e77ac6b3de95cb5026bc5e7fe8c0,fc5b725f3c662c5471af20efdcc2dbff</data>
    </edge>
    <edge source="N" target="HII">
      <data key="d4">1.0</data>
      <data key="d5">The leverage hii is bounded between 0 and 1 for all i from 1 to n</data>
      <data key="d6">0b650eb2f1dcd603b64fec3c4b5cd24b</data>
    </edge>
    <edge source="N" target="DI">
      <data key="d4">1.0</data>
      <data key="d5">Di is calculated using n, the number of observations in the regression model</data>
      <data key="d6">98d6982108f2d42fe0437bff8c666e17</data>
    </edge>
    <edge source="N" target="COOKS_DISTANCE">
      <data key="d4">1.0</data>
      <data key="d5">n is used in the calculation of Cook's distance (Di) and in the F-distribution threshold</data>
      <data key="d6">09391efd3b8c510205098b548bc8dc74</data>
    </edge>
    <edge source="N" target="BRAND_MODEL3">
      <data key="d4">1.0</data>
      <data key="d5">Number of observations (n=96) is used in the fitting of the linear model Brand_model3</data>
      <data key="d6">6a6f85d0a6e46196ab3a901fcc82a720</data>
    </edge>
    <edge source="YN" target="BETA_K">
      <data key="d4">1.0</data>
      <data key="d5">Yn is related to BETA_k as part of the linear regression model for the nth observation</data>
      <data key="d6">8f1d95acff56e1633dceb775fa713174</data>
    </edge>
    <edge source="YN" target="EPSILON_N">
      <data key="d4">1.0</data>
      <data key="d5">Yn is related to EPSILON_n as part of the linear regression model for the nth observation</data>
      <data key="d6">8f1d95acff56e1633dceb775fa713174</data>
    </edge>
    <edge source="YN" target="HOMOSCEDASTICITY">
      <data key="d4">1.0</data>
      <data key="d5">The response variables Y1, ..., Yn are assumed to have constant variance, which is a requirement for homoscedasticity</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6</data>
    </edge>
    <edge source="YN" target="NORMALITY">
      <data key="d4">1.0</data>
      <data key="d5">The response variables Y1, ..., Yn are assumed to have a normal distribution, which is a requirement for the normality assumption</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6</data>
    </edge>
    <edge source="YN" target="INDEPENDENCE">
      <data key="d4">1.0</data>
      <data key="d5">The response variables Y1, ..., Yn are assumed to be independent, although they are not identically distributed</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6</data>
    </edge>
    <edge source="BETA_K" target="LINEAR_PREDICTOR">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, BETA_K, denoted as \u03b2k, plays a pivotal role as the coefficient of the variable Xk within the linear predictor. This coefficient is integral to the linear predictor, as it significantly influences the slope of the mean function of the response variable. BETA_K's value determines the extent to which changes in Xk are associated with shifts in the expected value of the response, thereby shaping the linear relationship between the predictor and response variables. As a component of the LINEAR_PREDICTOR, BETA_K is crucial for understanding and predicting the behavior of the response variable based on the explanatory variable Xk.</data>
      <data key="d6">67e4c1866b0c6e162e6e3317949e8da9,6a47154bf457c25f22c3cf9f649c5db0</data>
    </edge>
    <edge source="LINEAR_PREDICTOR" target="X_1">
      <data key="d4">1.0</data>
      <data key="d5">X_1 is an explanatory variable that, along with its coefficient Beta_1, contributes to the linear predictor</data>
      <data key="d6">67e4c1866b0c6e162e6e3317949e8da9</data>
    </edge>
    <edge source="LINEAR_PREDICTOR" target="X_K">
      <data key="d4">1.0</data>
      <data key="d5">X_k is an explanatory variable that, along with its coefficient Beta_k, contributes to the linear predictor</data>
      <data key="d6">67e4c1866b0c6e162e6e3317949e8da9</data>
    </edge>
    <edge source="E_Y" target="LOG_Y">
      <data key="d4">1.0</data>
      <data key="d5">The expected value of Y can be calculated from log(Y) using the formula exp(&#181; + &#963;^2/2)</data>
      <data key="d6">21e429490eeefe7d9c245058fd48ca68</data>
    </edge>
    <edge source="X_BETA" target="ALPHA">
      <data key="d4">1.0</data>
      <data key="d5">X_beta is the design matrix of the original model parameterisation, which is transformed into X_alpha</data>
      <data key="d6">5609007c6229060ffc85d8056a7fefde</data>
    </edge>
    <edge source="EPSILON_J" target="BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">&#1013;j is the error term that contributes to the calculation of Beta hat (&#946;b) in the simple linear regression model</data>
      <data key="d6">d14413709de2897231aaa83be3aa346f</data>
    </edge>
    <edge source="EPSILON_J" target="STORE_J">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon j (&#1013;j) is the error term associated with store j in the regression model</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="P" target="S_HAT_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">P is used in the calculation of the unbiased estimator s^2(Y) for the error variance</data>
      <data key="d6">6648f0d6deed51fb4fb25e6992a71ddf</data>
    </edge>
    <edge source="P" target="S2">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entity P represents the number of parameters involved in the model. This parameter count, P, is crucial for the computation of the unbiased estimate s^2, which is a key statistic used to measure the variability in the data. The unbiased estimator s^2, denoted as S2 in the provided descriptions, is calculated taking into account the number of parameters (P) to ensure that the estimate is not systematically too high or too low, providing a more accurate representation of the data's dispersion.</data>
      <data key="d6">9923e77ac6b3de95cb5026bc5e7fe8c0,fc5b725f3c662c5471af20efdcc2dbff</data>
    </edge>
    <edge source="P" target="HII">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entities P and HII are central to understanding the structure and relations within a given dataset. Specifically, the average leverage (HII) across all observations is found to be equivalent to the ratio of the number of parameters (P) to the number of observations (n). This relationship is further elucidated by the observation that the sum of all leverages (HII) divided by the number of observations (n) precisely matches the ratio of the number of parameters (P) to the number of observations (n). This indicates a consistent and direct correlation between the leverage values and the parameters to observation ratio, providing a clear insight into the statistical model's fit and the influence of each observation on the regression estimates.</data>
      <data key="d6">0b650eb2f1dcd603b64fec3c4b5cd24b,bd98ac7b4b5df4f63e7ecc8f4a821f57</data>
    </edge>
    <edge source="P" target="LEVERAGE">
      <data key="d4">1.0</data>
      <data key="d5">Leverage is used to identify influential observations, with a threshold related to p/n</data>
      <data key="d6">bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </edge>
    <edge source="P" target="COOKS_DISTANCE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, particularly within the realm of regression models, the entities P and COOKS_DISTANCE are closely intertwined. P refers to the number of parameters in the regression model, a critical component that significantly impacts the model's complexity and its ability to fit the data. COOKS_DISTANCE, denoted as Di, is a measure of the influence of an observation on the regression model. It is calculated using the number of parameters, P, and is instrumental in identifying outliers or influential points that may disproportionately affect the model's results. The F-distribution threshold, another statistical measure, also involves P in its computation, highlighting the pivotal role of P in assessing the significance of model parameters and the overall model fit. Through these relationships, P and COOKS_DISTANCE provide insights into the robustness and reliability of regression analysis, enabling analysts to make informed decisions about model selection and data integrity.</data>
      <data key="d6">09391efd3b8c510205098b548bc8dc74,9e2ebbb113c00fa43f0af3c0696baf95</data>
    </edge>
    <edge source="P" target="DI">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, particularly within the realm of regression models, the entities P and DI play significant roles. DI, often referred to as Cook's distance, is a measure that quantifies the influence of a single data point on the overall regression model. It is calculated using P, which denotes the number of parameters in the regression model. P, in this scenario, is not merely a numerical value but a variable that represents the number of predictors contributing to the Cook's distance DI. This relationship underscores the importance of understanding how individual predictors affect the model's stability and reliability. By examining DI in relation to P, one can identify influential data points that may disproportionately affect the regression coefficients, thus aiding in the diagnosis of potential outliers and the robustness of the model.</data>
      <data key="d6">0443ab5e20a4f6b2f243c989ef6c723a,98d6982108f2d42fe0437bff8c666e17</data>
    </edge>
    <edge source="RESPONSE_RANDOM_VECTOR" target="MODEL_DESCRIPTION">
      <data key="d4">1.0</data>
      <data key="d5">The response random vector is a key component of the model described in vector/matrix notation</data>
      <data key="d6">6616e10c85e86291147e72776854b8a2</data>
    </edge>
    <edge source="MODEL_DESCRIPTION" target="ESTIMATED_COEFFICIENTS">
      <data key="d4">1.0</data>
      <data key="d5">The model description includes the interpretation of the estimated coefficients of the model</data>
      <data key="d6">6616e10c85e86291147e72776854b8a2</data>
    </edge>
    <edge source="MODEL_DESCRIPTION" target="TEXTBOOK_RECOMMENDATION">
      <data key="d4">1.0</data>
      <data key="d5">The textbook recommendation provides further reading on the topics covered in the model description</data>
      <data key="d6">6616e10c85e86291147e72776854b8a2</data>
    </edge>
    <edge source="GRAPHICAL_EXPLORATION" target="ANSCOMBE_S_QUARTET">
      <data key="d4">1.0</data>
      <data key="d5">Anscombe's Quartet illustrates the importance of graphical exploration in statistical analysis</data>
      <data key="d6">6616e10c85e86291147e72776854b8a2</data>
    </edge>
    <edge source="GRAPHICAL_EXPLORATION" target="ANSCOMBE_QUARTET">
      <data key="d4">1.0</data>
      <data key="d5">Anscombe's Quartet illustrates the importance of graphical exploration in statistical analysis</data>
      <data key="d6">9f335f1ecb85a1427df926df8bb1e89f</data>
    </edge>
    <edge source="DESIGN_MATRIX" target="QUADRATIC_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">The design matrix is used in the matrix notation of the model equations of the quadratic regression model</data>
      <data key="d6">e5131a1158e58f1b7b44b21ced7b6f60</data>
    </edge>
    <edge source="DESIGN_MATRIX" target="PARAMETER_VECTOR">
      <data key="d4">1.0</data>
      <data key="d5">The design matrix (X) is multiplied by the parameter vector (&#946;) to calculate the linear predictor for the given dataset, which is a key component of the linear model.</data>
      <data key="d6">b9af17718641389ba07f53be13f31f8c</data>
    </edge>
    <edge source="DESIGN_MATRIX" target="BRAND_A">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the DESIGN_MATRIX, BRAND_A is characterized by a dedicated column that consists of indicator variables. These variables serve to identify the presence of Brand A stores within the matrix. The use of indicator variables ensures that the binary nature of whether a store is of Brand A or not is accurately captured, facilitating a clear and concise representation of Brand A's presence in the DESIGN_MATRIX. This approach allows for effective statistical analysis and modeling, particularly when examining the relationships and impacts of Brand A stores within the broader dataset.</data>
      <data key="d6">06199787dd7f75f7338dd24d4f3dc26e,48971100deb5bb374a41c1f2b7b2a86a</data>
    </edge>
    <edge source="DESIGN_MATRIX" target="BRAND_B">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the DESIGN_MATRIX, Brand B is notably featured as a column of indicator variables. This specific column in the DESIGN_MATRIX serves to represent stores that belong to Brand B. The indicator variables, often binary (0 or 1), denote the presence or absence of Brand B stores in the dataset, providing a structured way to analyze and interpret the relationship between Brand B and other variables within the matrix. This representation facilitates a clearer understanding of the role and impact of Brand B stores within the statistical model being analyzed.</data>
      <data key="d6">06199787dd7f75f7338dd24d4f3dc26e,48971100deb5bb374a41c1f2b7b2a86a</data>
    </edge>
    <edge source="DESIGN_MATRIX" target="BRAND_C">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the DESIGN_MATRIX, Brand C is notably featured as a specific column. This column is comprised of indicator variables, which serve to identify stores that belong to Brand C. The use of indicator variables allows for a binary representation, typically coded as 1s and 0s, to denote the presence or absence of Brand C stores in the dataset. This method of representation is crucial for statistical analysis, as it enables the model to distinguish and quantify the effects associated with Brand C stores within the broader DESIGN_MATRIX.</data>
      <data key="d6">06199787dd7f75f7338dd24d4f3dc26e,48971100deb5bb374a41c1f2b7b2a86a</data>
    </edge>
    <edge source="DESIGN_MATRIX" target="ALPHA_B">
      <data key="d4">1.0</data>
      <data key="d5">Alpha B is represented in the design matrix for the re-parameterised model</data>
      <data key="d6">06199787dd7f75f7338dd24d4f3dc26e</data>
    </edge>
    <edge source="DESIGN_MATRIX" target="ALPHA_C">
      <data key="d4">1.0</data>
      <data key="d5">Alpha C is represented in the design matrix for the re-parameterised model</data>
      <data key="d6">06199787dd7f75f7338dd24d4f3dc26e</data>
    </edge>
    <edge source="DESIGN_MATRIX" target="MU">
      <data key="d4">1.0</data>
      <data key="d5">Mu is represented in the design matrix as the intercept for Brand A stores</data>
      <data key="d6">06199787dd7f75f7338dd24d4f3dc26e</data>
    </edge>
    <edge source="DESIGN_MATRIX" target="EXERCISE_26">
      <data key="d4">1.0</data>
      <data key="d5">Exercise 26 asks for the design matrix of a specific model, indicating the relevance of design matrices in exercises</data>
      <data key="d6">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </edge>
    <edge source="PARAMETER_VECTOR" target="DESIGN_MATRIX_X">
      <data key="d4">1.0</data>
      <data key="d5">The parameter vector is associated with the design matrix X, as it contains the coefficients for the intercept and the independent variables</data>
      <data key="d6">c103c6d096d52868eda26d991194b5f2</data>
    </edge>
    <edge source="ANSCOMBE_QUARTET" target="STATISTICAL_ANALYSIS">
      <data key="d4">1.0</data>
      <data key="d5">Anscombe's Quartet is an example used in statistical analysis to demonstrate the limitations of numerical methods</data>
      <data key="d6">9f335f1ecb85a1427df926df8bb1e89f</data>
    </edge>
    <edge source="ANSCOMBE_QUARTET" target="SUMMARY_STATISTICS">
      <data key="d4">1.0</data>
      <data key="d5">Anscombe's Quartet has the same summary statistics for all four datasets</data>
      <data key="d6">9f335f1ecb85a1427df926df8bb1e89f</data>
    </edge>
    <edge source="ANSCOMBE_QUARTET" target="DATASET_2">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 2 is one of the datasets in Anscombe&#8217;s quartet, which is a set of four datasets that illustrate the importance of graphical data analysis</data>
      <data key="d6">87ba4f416a28aabc3b396908f5913b54</data>
    </edge>
    <edge source="ANSCOMBE" target="AMERICAN_STATISTICIAN">
      <data key="d4">1.0</data>
      <data key="d5">Anscombe is the author of the paper published in The American Statistician</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="ANSCOMBE" target="DOI">
      <data key="d4">1.0</data>
      <data key="d5">Anscombe's paper has a DOI that is used for referencing</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="DATASET_1" target="SAMPLE_MEAN_X">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 1 has a specific sample mean of x</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="DATASET_1" target="SAMPLE_VARIANCE_X">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 1 has a specific sample variance of x</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="DATASET_1" target="SAMPLE_MEAN_Y">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 1 has a specific sample mean of y</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="DATASET_1" target="SAMPLE_VARIANCE_Y">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 1 has a specific sample variance of y</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="DATASET_1" target="CORRELATION_COEFFICIENT">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 1 has a specific correlation coefficient of x and y</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="DATASET_1" target="FITTED_REGRESSION_LINE">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 1 has a specific fitted regression line</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="DATASET_2" target="SAMPLE_MEAN_X">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 2 has a specific sample mean of x</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="DATASET_2" target="SAMPLE_VARIANCE_X">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 2 has a specific sample variance of x</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="DATASET_2" target="SAMPLE_MEAN_Y">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 2 has a specific sample mean of y</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="DATASET_2" target="SAMPLE_VARIANCE_Y">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 2 has a specific sample variance of y</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="DATASET_2" target="CORRELATION_COEFFICIENT">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 2 has a specific correlation coefficient of x and y</data>
      <data key="d6">2a5997c641e47fc6c32ebf81101c54e0</data>
    </edge>
    <edge source="DATASET_2" target="SIMPLE_LINEAR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 2 suggests that a simple linear model is not adequate due to the curved relationship visible in the data</data>
      <data key="d6">84ffe1b8496dc660c47248c9f7b5bdea</data>
    </edge>
    <edge source="DATASET_2" target="PARABOLA">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 2 exhibits a parabolic relationship between the response and the explanatory variable, which can be modeled using polynomial regression</data>
      <data key="d6">87ba4f416a28aabc3b396908f5913b54</data>
    </edge>
    <edge source="DATASET_3" target="OUTLIER">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 3 contains an outlier that has a strong influence on the fitted line</data>
      <data key="d6">84ffe1b8496dc660c47248c9f7b5bdea</data>
    </edge>
    <edge source="DATASET_3" target="SIMPLE_LINEAR_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 3 challenges the adequacy of a simple linear model due to its unique data structure</data>
      <data key="d6">b8ec334f8c87bf1d9cb6043fa1a64214</data>
    </edge>
    <edge source="DATASET_4" target="OUTLIER">
      <data key="d4">1.0</data>
      <data key="d5">Dataset 4 contains an outlier (a single unusual data point) that determines the slope of the line</data>
      <data key="d6">84ffe1b8496dc660c47248c9f7b5bdea</data>
    </edge>
    <edge source="DATASET_4" target="SIMPLE_LINEAR_MODEL">
      <data key="d4">2.0</data>
      <data key="d5">DATASET_4, a specific collection of data, challenges the reliability of employing a SIMPLE_LINEAR_MODEL for analysis. This skepticism arises from the observation that the model's slope is significantly swayed by a solitary data point, indicating a potential outlier. Moreover, the presence of a unique x-value within DATASET_4 further complicates the straightforward application of the SIMPLE_LINEAR_MODEL, suggesting that the model may not accurately represent the underlying relationship within the dataset due to these exceptional characteristics. These findings underscore the importance of conducting a thorough residual analysis and considering alternative models that can better accommodate the idiosyncrasies of DATASET_4.</data>
      <data key="d6">84ffe1b8496dc660c47248c9f7b5bdea,b8ec334f8c87bf1d9cb6043fa1a64214</data>
    </edge>
    <edge source="SIMPLE_LINEAR_MODEL" target="ANSCOME_QUARTET">
      <data key="d4">1.0</data>
      <data key="d5">Anscombe's quartet demonstrates the limitations of simple linear models in capturing the true nature of data relationships</data>
      <data key="d6">b8ec334f8c87bf1d9cb6043fa1a64214</data>
    </edge>
    <edge source="SIMPLE_LINEAR_MODEL" target="DATASAUROS_DOZEN">
      <data key="d4">1.0</data>
      <data key="d5">The Datasaurus Dozen further illustrates the importance of visualizing data and the limitations of simple linear models</data>
      <data key="d6">b8ec334f8c87bf1d9cb6043fa1a64214</data>
    </edge>
    <edge source="HISTOGRAMS" target="SCATTERPLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Histograms and scatterplots are both tools used for exploratory data analysis, helping to visualize the distribution and relationships in data</data>
      <data key="d6">87ba4f416a28aabc3b396908f5913b54</data>
    </edge>
    <edge source="HISTOGRAMS" target="VOLUMEI">
      <data key="d4">1.0</data>
      <data key="d5">Histograms include the distribution of Volumei for initial exploratory analysis</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="HISTOGRAMS" target="DIAMETERI">
      <data key="d4">1.0</data>
      <data key="d5">Histograms include the distribution of Diameteri for initial exploratory analysis</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="HISTOGRAMS" target="HEIGHTI">
      <data key="d4">1.0</data>
      <data key="d5">Histograms include the distribution of Heighti for initial exploratory analysis</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="QUADRATIC_REGRESSION" target="ERRORS">
      <data key="d4">1.0</data>
      <data key="d5">Errors are part of the assumptions of the quadratic regression model. They are assumed to be iid (independent and identically distributed) N (0, &#963;^2)</data>
      <data key="d6">e5131a1158e58f1b7b44b21ced7b6f60</data>
    </edge>
    <edge source="QUADRATIC_REGRESSION" target="QUADRATIC_TERM">
      <data key="d4">1.0</data>
      <data key="d5">Quadratic term is part of the systematic component of the quadratic regression model. It is used to model the curvature in the relationship between the predictor and the response variable</data>
      <data key="d6">e5131a1158e58f1b7b44b21ced7b6f60</data>
    </edge>
    <edge source="QUADRATIC_REGRESSION" target="SUM_OF_SQUARED_DISTANCES">
      <data key="d4">1.0</data>
      <data key="d5">The sum of squared vertical distances between the observations and the parabola given by &#946;0 + &#946;1x + &#946;2x^2 is the criterion used in least squares estimation to determine the parameters of the quadratic regression model</data>
      <data key="d6">e5131a1158e58f1b7b44b21ced7b6f60</data>
    </edge>
    <edge source="QUADRATIC_REGRESSION" target="MODEL_EQUATIONS">
      <data key="d4">1.0</data>
      <data key="d5">Model equations in (2.1) expressed in matrix notation are part of the quadratic regression model</data>
      <data key="d6">e5131a1158e58f1b7b44b21ced7b6f60</data>
    </edge>
    <edge source="QUADRATIC_REGRESSION" target="COMPLEX_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">A quadratic regression model is an example of a complex model that can be used when a simple linear regression does not adequately describe the relationship</data>
      <data key="d6">15c7b5750483a382ce59751008e86751</data>
    </edge>
    <edge source="PARAMETERS" target="SUM_OF_SQUARED_DISTANCES">
      <data key="d4">1.0</data>
      <data key="d5">The parameters in a model are chosen to minimize the sum of squared distances, a metric used in least squares estimation to assess the fit of the model to the data</data>
      <data key="d6">87ba4f416a28aabc3b396908f5913b54</data>
    </edge>
    <edge source="PARAMETERS" target="FITTING_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Fitting the model involves finding the best set of values for the parameters</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="ERRORS" target="SEQUENTIAL_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Sequential data can lead to time-correlated errors if the data points are not independent</data>
      <data key="d6">7cd6069e88e81548a237fa937adfecc6</data>
    </edge>
    <edge source="ERRORS" target="SPATIAL_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Spatial data can lead to correlated errors if the data points are not independent</data>
      <data key="d6">7cd6069e88e81548a237fa937adfecc6</data>
    </edge>
    <edge source="ERRORS" target="HOMOSCEDASTICITY">
      <data key="d4">1.0</data>
      <data key="d5">Homoscedasticity is a property of the errors in a regression model, indicating that they have constant variance</data>
      <data key="d6">e361ac139c268d5c3f3623f920e68af2</data>
    </edge>
    <edge source="ERRORS" target="INDEPENDENCE">
      <data key="d4">1.0</data>
      <data key="d5">Independence is a property of the errors in a regression model, indicating that they are independent of each other</data>
      <data key="d6">e361ac139c268d5c3f3623f920e68af2</data>
    </edge>
    <edge source="ERRORS" target="NORMALITY">
      <data key="d4">1.0</data>
      <data key="d5">Normality is a property of the errors in a regression model, indicating that they have a normal distribution</data>
      <data key="d6">e361ac139c268d5c3f3623f920e68af2</data>
    </edge>
    <edge source="ERRORS" target="REMEDIAL_ACTIONS">
      <data key="d4">1.0</data>
      <data key="d5">Remedial actions are taken to address violations of the modelling assumptions, which can be indicated by the errors</data>
      <data key="d6">e361ac139c268d5c3f3623f920e68af2</data>
    </edge>
    <edge source="PLR_MODEL" target="FIGURE_2_5">
      <data key="d4">1.0</data>
      <data key="d5">The quadratic regression model plr.model is visually represented by the parabola of best fit in Figure 2.5</data>
      <data key="d6">084dadebfca8bcb6377c205c45bee295</data>
    </edge>
    <edge source="PLOT" target="SMOOTHING_CURVE">
      <data key="d4">1.0</data>
      <data key="d5">A smoothing curve is often added to a plot to help identify trends or patterns in the data</data>
      <data key="d6">b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </edge>
    <edge source="PLOT" target="RESIDUAL_PLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Residual plots are derived from a plot of the data, used to check the assumptions of a regression model</data>
      <data key="d6">b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </edge>
    <edge source="ERROR_TERM" target="SALES_VOLUME">
      <data key="d4">1.0</data>
      <data key="d5">The error term represents the difference between the observed sales volume and the predicted sales volume, and is an important component of the linear regression model.</data>
      <data key="d6">7a605c3b689bec7ab2c46df9c123e3f3</data>
    </edge>
    <edge source="YI" target="XI">
      <data key="d4">1.0</data>
      <data key="d5">Yi is a function of the explanatory variable xi for the ith unit of observation in the quadratic regression model</data>
      <data key="d6">6c1684ed2a4840576c6b0f4d1a3a482f</data>
    </edge>
    <edge source="YI" target="EXERCISE_7">
      <data key="d4">1.0</data>
      <data key="d5">Exercise 7 involves the quadratic regression model equation for Yi</data>
      <data key="d6">6c1684ed2a4840576c6b0f4d1a3a482f</data>
    </edge>
    <edge source="YI" target="EPSILONI">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, YI is a pivotal variable that encapsulates the essence of the observed data. It is noteworthy that YI incorporates the error term, EPSILONI, which is denoted as \u03f5i. This error term, EPSILONI, is a critical component in all models involving YI, as it accounts for the unexplained variation that is not captured by the linear combination of predictors. The inclusion of EPSILONI in YI ensures that the models can accommodate the inherent randomness and unpredictability in the data, thereby providing a more comprehensive and realistic representation of the phenomena under study. The addition of EPSILONI to the linear combination before exponentiation is a methodological approach that allows for the transformation of the linear model into a more flexible form, capable of fitting a wider range of data distributions. This process is fundamental in statistical analysis, as it helps in understanding the structure and relationships within the community of interest, while also acknowledging the limitations of the model in fully explaining all variations in the data.</data>
      <data key="d6">34fceaaf7d835828b5ee2327325c37f8,90ed6030c5a5a0764b7dcd4115b4d4d3</data>
    </edge>
    <edge source="YI" target="XI,1">
      <data key="d4">1.0</data>
      <data key="d5">Yi is a function of xi,1 in various forms in the models, representing the relationship between the response and explanatory variable</data>
      <data key="d6">90ed6030c5a5a0764b7dcd4115b4d4d3</data>
    </edge>
    <edge source="YI" target="XI,2">
      <data key="d4">1.0</data>
      <data key="d5">Yi is a function of xi,2 in the logarithmic form in the fifth model, representing the relationship between the response and the logarithmic explanatory variable</data>
      <data key="d6">90ed6030c5a5a0764b7dcd4115b4d4d3</data>
    </edge>
    <edge source="YI" target="ZI">
      <data key="d4">1.0</data>
      <data key="d5">Zi and Yi are the explanatory and response variables, respectively, in the dataset for Exercise 9</data>
      <data key="d6">22093a562f5f05dc9891b45ab9bcbea8</data>
    </edge>
    <edge source="YI" target="EPSILON_I">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical modeling, Epsilon_i (&#949;_i) serves as the error term for the ith observation, encapsulating the discrepancy between the observed response variable, Yi, and its predicted counterpart. This error term is pivotal in quantifying the model's uncertainty, as it reflects the unexplained variance that is not accounted for by the model's predictors. Specifically, in the scenario where Yi is log-transformed, Epsilon_i delineates the deviation of the observed log-transformed Yi from its expected value, thereby providing a measure of the model's residual variability. This highlights the role of Epsilon_i in capturing the inherent randomness or unpredictability in the data, which is crucial for a comprehensive understanding of the model's performance and reliability.</data>
      <data key="d6">0fbc9037ca9a440e79e9ac05664b9b3d,90b7e0427699cc1bb461e37939935138</data>
    </edge>
    <edge source="YI" target="EXP">
      <data key="d4">1.0</data>
      <data key="d5">The exponential function Exp is used to transform the log-transformed response variable Yi back to its original scale, where the errors act multiplicatively.</data>
      <data key="d6">0fbc9037ca9a440e79e9ac05664b9b3d</data>
    </edge>
    <edge source="YI" target="MULTIPLICATIVE_ERRORS">
      <data key="d4">1.0</data>
      <data key="d5">In the original scale of the response variable Yi, the errors act multiplicatively rather than additively, reflecting the nature of the log-transformed model.</data>
      <data key="d6">0fbc9037ca9a440e79e9ac05664b9b3d</data>
    </edge>
    <edge source="YI" target="XI1">
      <data key="d4">1.0</data>
      <data key="d5">Yi is influenced by xi1, one of the predictor variables in the model</data>
      <data key="d6">34fceaaf7d835828b5ee2327325c37f8</data>
    </edge>
    <edge source="YI" target="XI2">
      <data key="d4">1.0</data>
      <data key="d5">Yi is influenced by xi2, one of the predictor variables in the model</data>
      <data key="d6">34fceaaf7d835828b5ee2327325c37f8</data>
    </edge>
    <edge source="YI" target="E">
      <data key="d4">1.0</data>
      <data key="d5">The expectation of Yi is greater than the exponential of the expectation of the log of Yi, due to Jensen's inequality</data>
      <data key="d6">34fceaaf7d835828b5ee2327325c37f8</data>
    </edge>
    <edge source="YI" target="LOG">
      <data key="d4">1.0</data>
      <data key="d5">The log of Yi is used to transform the response variable in the model</data>
      <data key="d6">34fceaaf7d835828b5ee2327325c37f8</data>
    </edge>
    <edge source="YI" target="ZB0">
      <data key="d4">1.0</data>
      <data key="d5">zb0 is the prediction for the logged response, which is back-transformed to predict Yi in the original scale</data>
      <data key="d6">34fceaaf7d835828b5ee2327325c37f8</data>
    </edge>
    <edge source="YI" target="YB_I">
      <data key="d4">1.0</data>
      <data key="d5">Yi is the observed value corresponding to the predicted value Yb_i</data>
      <data key="d6">1da117a2f92b2db00290d2a0bfc06beb</data>
    </edge>
    <edge source="YI" target="EB">
      <data key="d4">1.0</data>
      <data key="d5">As hii approaches 1, Eb_i tends to zero, implying y_hat_i will tend to yi</data>
      <data key="d6">5a0d392715f06d5e873f45ae06aa729a</data>
    </edge>
    <edge source="YI" target="Y_HAT_I">
      <data key="d4">1.0</data>
      <data key="d5">When hii approaches 1, the fitted value y_hat_i will tend to the observed value yi</data>
      <data key="d6">2685edb9e8031c8ea725c43a40af22a8</data>
    </edge>
    <edge source="YI" target="RI">
      <data key="d4">1.0</data>
      <data key="d5">Yi is the observed response value, used in the calculation of the raw residual Eb_i</data>
      <data key="d6">90b7e0427699cc1bb461e37939935138</data>
    </edge>
    <edge source="XI" target="HII">
      <data key="d4">1.0</data>
      <data key="d5">The distance of the ith observation (xi) from the mean (x_bar) influences the leverage (hii) of the data point</data>
      <data key="d6">6ee02b38ae842fd5eac9a11c4fd6659f</data>
    </edge>
    <edge source="DBH" target="UFCWC">
      <data key="d4">1.0</data>
      <data key="d5">Dbh is a variable in the ufcwc dataset</data>
      <data key="d6">2d5cdecc342ddacd2c090f1838430cee</data>
    </edge>
    <edge source="DBH" target="CREEK_STAND">
      <data key="d4">1.0</data>
      <data key="d5">The diameter of the trees (Dbh) is measured at the Creek stand</data>
      <data key="d6">656dce234514b9db38b5b5616557c1e9</data>
    </edge>
    <edge source="DBH" target="FIGURE_2_6">
      <data key="d4">1.0</data>
      <data key="d5">Dbh is plotted against height in Figure 2.6</data>
      <data key="d6">656dce234514b9db38b5b5616557c1e9</data>
    </edge>
    <edge source="HEIGHT" target="UFCWC">
      <data key="d4">1.0</data>
      <data key="d5">Height is a variable in the ufcwc dataset</data>
      <data key="d6">2d5cdecc342ddacd2c090f1838430cee</data>
    </edge>
    <edge source="HEIGHT" target="CREEK_STAND">
      <data key="d4">1.0</data>
      <data key="d5">The height of the trees is measured at the Creek stand</data>
      <data key="d6">656dce234514b9db38b5b5616557c1e9</data>
    </edge>
    <edge source="HEIGHT" target="FIGURE_2_6">
      <data key="d4">1.0</data>
      <data key="d5">Height is plotted against Dbh in Figure 2.6</data>
      <data key="d6">656dce234514b9db38b5b5616557c1e9</data>
    </edge>
    <edge source="HEIGHT" target="TREE_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Tree.model predicts the Height of Western red cedar trees based on the log2-transformed Dbh.</data>
      <data key="d6">c619949b08fc2b7edf3a7635b46dc147</data>
    </edge>
    <edge source="HEIGHT" target="VOLUME">
      <data key="d4">2.0</data>
      <data key="d5">The comprehensive summary of the data provided revolves around the entities "HEIGHT" and "VOLUME". It is stated that the volume of a tree is predicted to be proportional to the square of its diameter and its height. This relationship is not merely a prediction but is confirmed to be a consistent proportionality, indicating a strong correlation between the volume of the tree and the combined factors of its diameter squared and its height. This statistical relationship is crucial for understanding the structure and growth patterns of trees within the community of interest, as it allows for the estimation of tree volume based on measurable dimensions such as diameter and height. This information is particularly valuable in forestry and ecological studies where accurate volume estimations are essential for resource management and ecological assessments.</data>
      <data key="d6">9611ea31ff53888971694cdefe806f64,efeeb664622c1ee594e6a08a8322ffe3</data>
    </edge>
    <edge source="HEIGHT" target="TREE">
      <data key="d4">1.0</data>
      <data key="d5">The height is a measurement taken from the tree</data>
      <data key="d6">9611ea31ff53888971694cdefe806f64</data>
    </edge>
    <edge source="UFCWC" target="ALR4">
      <data key="d4">1.0</data>
      <data key="d5">ufcwc is a dataset contained in the alr4 package</data>
      <data key="d6">2d5cdecc342ddacd2c090f1838430cee</data>
    </edge>
    <edge source="UFCWC" target="UNIVERSITY IDAHO EXPERIMENTAL FOREST">
      <data key="d4">1.0</data>
      <data key="d5">University Idaho Experimental Forest is the location where the data for the ufcwc dataset were collected</data>
      <data key="d6">2d5cdecc342ddacd2c090f1838430cee</data>
    </edge>
    <edge source="UFCWC" target="UPPER FLAT CREEK STAND">
      <data key="d4">1.0</data>
      <data key="d5">Upper Flat Creek stand is the specific location within the University Idaho Experimental Forest where the data for the ufcwc dataset were collected</data>
      <data key="d6">2d5cdecc342ddacd2c090f1838430cee</data>
    </edge>
    <edge source="ALR4" target="WEISBERG">
      <data key="d4">1.0</data>
      <data key="d5">Weisberg is the author of the book that contains the alr4 package</data>
      <data key="d6">2d5cdecc342ddacd2c090f1838430cee</data>
    </edge>
    <edge source="DIAMETER" target="TREE">
      <data key="d4">1.0</data>
      <data key="d5">The diameter is a measurement taken from the tree</data>
      <data key="d6">9611ea31ff53888971694cdefe806f64</data>
    </edge>
    <edge source="DIAMETER" target="VOLUME">
      <data key="d4">1.0</data>
      <data key="d5">Volume is proportional to the square of the diameter and the height of the tree</data>
      <data key="d6">9611ea31ff53888971694cdefe806f64</data>
    </edge>
    <edge source="LOG2" target="PREDICTOR">
      <data key="d4">1.0</data>
      <data key="d5">Log2 is applied to the predictor variable (Dbh) to transform it for use in the linear regression model. This transformation makes the interpretation of the coefficients easier.</data>
      <data key="d6">c619949b08fc2b7edf3a7635b46dc147</data>
    </edge>
    <edge source="BASE_2" target="LOG2_DIAMETERS">
      <data key="d4">1.0</data>
      <data key="d5">Base 2 is used to transform the diameters into log2-diameters for the linear regression model</data>
      <data key="d6">25fce1af816975003128126b5cfea73b</data>
    </edge>
    <edge source="TREE_MODEL" target="LOG2_DBH">
      <data key="d4">1.0</data>
      <data key="d5">Log2(Dbh) is a parameter in the tree.model</data>
      <data key="d6">25fce1af816975003128126b5cfea73b</data>
    </edge>
    <edge source="TREE_MODEL" target="PREDICTOR">
      <data key="d4">1.0</data>
      <data key="d5">Tree.model uses the log2-transformed Dbh as the predictor variable to predict the Height of Western red cedar trees.</data>
      <data key="d6">c619949b08fc2b7edf3a7635b46dc147</data>
    </edge>
    <edge source="NATURAL_LOGARITHM" target="LOG2_X_PLUS_1">
      <data key="d4">1.0</data>
      <data key="d5">Natural logarithm is related to the base 2 logarithm through the expression log2(x) + 1</data>
      <data key="d6">25fce1af816975003128126b5cfea73b</data>
    </edge>
    <edge source="PREDICTOR" target="INTERACTION">
      <data key="d4">1.0</data>
      <data key="d5">The concept of interaction is relevant when considering the relationship between two or more predictor variables in a regression model</data>
      <data key="d6">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </edge>
    <edge source="VARIABLE" target="LOG_TRANSFORMED_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">The variable can be transformed using a logarithmic function to create a log-transformed variable, which may improve the fit of a statistical model</data>
      <data key="d6">ae1e66f5b64284090abc285c1d4389f5</data>
    </edge>
    <edge source="VARIABLE" target="FIT">
      <data key="d4">1.0</data>
      <data key="d5">The fit of a statistical model can be assessed by how well the model describes the relationship between variables</data>
      <data key="d6">ae1e66f5b64284090abc285c1d4389f5</data>
    </edge>
    <edge source="LOG_TRANSFORMED_VARIABLE" target="LOGARITHM_BASE_10">
      <data key="d4">1.0</data>
      <data key="d5">The logarithm with base 10 can be used to transform a variable, often for interpretation in statistical practice, especially when considering a ten-fold increase in the predictor variable</data>
      <data key="d6">ae1e66f5b64284090abc285c1d4389f5</data>
    </edge>
    <edge source="ARITHMETIC_OPERATORS" target="INHIBITOR_FUNCTION_I">
      <data key="d4">1.0</data>
      <data key="d5">Arithmetic operators can be treated as such in R by using the inhibitor function I(), allowing for proper mathematical operations in statistical models</data>
      <data key="d6">ae1e66f5b64284090abc285c1d4389f5</data>
    </edge>
    <edge source="WESTERN_RED_CEDAR_DATA" target="LOG_TRANSFORM">
      <data key="d4">2.0</data>
      <data key="d5">The Western red cedar data, a prime instance in statistical analysis, effectively showcases the application of the log-transform to the explanatory variable. This transformation is strategically employed to tackle issues of heteroscedasticity, a common problem in regression analysis where the variance of the error terms is not constant across all levels of the independent variables. By applying the log-transform, the data set demonstrates how to stabilize variance, thereby improving the reliability and accuracy of statistical models. This approach is particularly useful in contexts where the variance of the residuals increases with the value of the explanatory variable, ensuring that the assumptions of linear regression are more closely met.</data>
      <data key="d6">0fbc9037ca9a440e79e9ac05664b9b3d,c03eb12d07d48f9e94260f08dae10cdf</data>
    </edge>
    <edge source="EXERCISE_8" target="PARAMETER_VECTOR_BETA">
      <data key="d4">1.0</data>
      <data key="d5">Exercise 8 involves the use of a parameter vector &#946;, which contains the parameters of a statistical model, such as &#946;0, &#946;1, and &#946;2</data>
      <data key="d6">ae1e66f5b64284090abc285c1d4389f5</data>
    </edge>
    <edge source="MODEL_ADEQUACY" target="PARAMETER_ESTIMATION">
      <data key="d4">1.0</data>
      <data key="d5">Model adequacy is a criterion that affects the choice of parameter estimation methods and the interpretation of results</data>
      <data key="d6">768c516c8b27fb9800427e848f02fc33</data>
    </edge>
    <edge source="MODEL_ADEQUACY" target="PARSIMONY">
      <data key="d4">1.0</data>
      <data key="d5">Parsimony is a criterion for model adequacy, guiding the selection of simpler models that are easier to interpret and predict</data>
      <data key="d6">768c516c8b27fb9800427e848f02fc33</data>
    </edge>
    <edge source="MODEL_ADEQUACY" target="PLAUSIBILITY">
      <data key="d4">1.0</data>
      <data key="d5">Plausibility is a criterion for model adequacy, ensuring that the model makes practical sense in the context of the data</data>
      <data key="d6">768c516c8b27fb9800427e848f02fc33</data>
    </edge>
    <edge source="MODEL_ADEQUACY" target="WASSERSTEIN_R">
      <data key="d4">1.0</data>
      <data key="d5">R. Wasserstein's article discusses the importance of model adequacy in statistical practice</data>
      <data key="d6">768c516c8b27fb9800427e848f02fc33</data>
    </edge>
    <edge source="PLAUSIBLE_MODELS" target="CHOSEN_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Plausible models are compared to choose the most appropriate one</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="CHOSEN_MODEL" target="QUESTIONS_OF_INTEREST">
      <data key="d4">1.0</data>
      <data key="d5">The chosen model is used to answer the questions of interest</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="GEORGE_BOX" target="WASSERSTEIN_PAPER">
      <data key="d4">1.0</data>
      <data key="d5">The Wasserstein paper provides more information about George Box</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="WASSERSTEIN_PAPER" target="SIGNIFICANCE_JOURNAL">
      <data key="d4">1.0</data>
      <data key="d5">The Wasserstein paper was published in the Significance journal</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="SIGNIFICANCE_JOURNAL" target="ROYAL_STATISTICAL_SOCIETY">
      <data key="d4">1.0</data>
      <data key="d5">The Significance journal is published by the Royal Statistical Society</data>
      <data key="d6">bb31f1c77bbba73300f735a100086a67</data>
    </edge>
    <edge source="GEORGE E. BOX" target="WASSERSTEIN6">
      <data key="d4">1.0</data>
      <data key="d5">Wasserstein6 provides more information about George E. Box</data>
      <data key="d6">22061b1108c7f9963497b7a320be22b8</data>
    </edge>
    <edge source="WASSERSTEIN6" target="SIGNIFICANCE">
      <data key="d4">1.0</data>
      <data key="d5">Significance is the journal where Wasserstein6 was published</data>
      <data key="d6">22061b1108c7f9963497b7a320be22b8</data>
    </edge>
    <edge source="SIGNIFICANCE" target="ROYAL STATISTICAL SOCIETY">
      <data key="d4">1.0</data>
      <data key="d5">The Royal Statistical Society is the organization that publishes Significance</data>
      <data key="d6">22061b1108c7f9963497b7a320be22b8</data>
    </edge>
    <edge source="GAPMINDER" target="ECONOMIC INDICES">
      <data key="d4">1.0</data>
      <data key="d5">Gapminder provides data and visualizations on economic indices</data>
      <data key="d6">22061b1108c7f9963497b7a320be22b8</data>
    </edge>
    <edge source="PARSIMONY" target="OCKHAMS_RAZOR">
      <data key="d4">1.0</data>
      <data key="d5">Parsimony is often referred to as Ockham's razor, which is a principle that directs us to choose the simplest explanation</data>
      <data key="d6">188219b9e5b6b6368360840921877de9</data>
    </edge>
    <edge source="PARSIMONY" target="SIMPLE_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Parsimony directs us to choose the simplest model that still adequately fits the data</data>
      <data key="d6">188219b9e5b6b6368360840921877de9</data>
    </edge>
    <edge source="PARSIMONY" target="COMPLEX_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Parsimony advises against choosing unnecessarily complex models, as they can decrease the precision of estimation and prediction</data>
      <data key="d6">188219b9e5b6b6368360840921877de9</data>
    </edge>
    <edge source="PLAUSIBILITY" target="SIMPLER_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">A simpler model may be more plausible for prediction within a specific range of input values</data>
      <data key="d6">768c516c8b27fb9800427e848f02fc33</data>
    </edge>
    <edge source="PLAUSIBILITY" target="TRUE_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The true model is not always the most plausible for prediction, especially when considering a limited range of input values</data>
      <data key="d6">768c516c8b27fb9800427e848f02fc33</data>
    </edge>
    <edge source="PLAUSIBILITY" target="ACCELERATION_DUE_TO_GRAVITY">
      <data key="d4">1.0</data>
      <data key="d5">The assumption of constant acceleration due to gravity is plausible for modeling the path of a ball in a room but not in space</data>
      <data key="d6">768c516c8b27fb9800427e848f02fc33</data>
    </edge>
    <edge source="PREDICTOR_VARIABLES" target="PARAMETER_ESTIMATION">
      <data key="d4">1.0</data>
      <data key="d5">Predictor variables are used in parameter estimation to understand their relationship with the response variable</data>
      <data key="d6">768c516c8b27fb9800427e848f02fc33</data>
    </edge>
    <edge source="BALL" target="GRAVITY">
      <data key="d4">1.0</data>
      <data key="d5">The ball is influenced by gravity, which causes it to fall</data>
      <data key="d6">188219b9e5b6b6368360840921877de9</data>
    </edge>
    <edge source="BALL" target="ROOM">
      <data key="d4">1.0</data>
      <data key="d5">The path of the ball is modelled in the room, where the acceleration due to gravity is assumed to be constant</data>
      <data key="d6">188219b9e5b6b6368360840921877de9</data>
    </edge>
    <edge source="BALL" target="SPACE">
      <data key="d4">1.0</data>
      <data key="d5">The path of the ball in space is affected by gravity, but the relationship is only valid locally</data>
      <data key="d6">188219b9e5b6b6368360840921877de9</data>
    </edge>
    <edge source="COMPLEX_MODEL" target="NON-LINEARITY">
      <data key="d4">1.0</data>
      <data key="d5">A complex model can be used to address non-linearity when transformation of the explanatory variable is not sufficient</data>
      <data key="d6">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </edge>
    <edge source="COMPLEX_MODEL" target="TRANSFORMATION">
      <data key="d4">1.0</data>
      <data key="d5">If transformations do not resolve issues, a complex model may be considered</data>
      <data key="d6">7347b44ffb25a066e43321f4eaf5a806</data>
    </edge>
    <edge source="RESIDUAL_PLOTS" target="MODEL_ASSUMPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">Residual plots are used to visually assess the validity of model assumptions by plotting the residuals against the predicted values or other variables. They help identify patterns that may indicate violations of assumptions such as non-linearity, heteroscedasticity, or non-normality.</data>
      <data key="d6">e7494d6cfc3e38a4d2f3f6b21ef6445d</data>
    </edge>
    <edge source="RESIDUAL_PLOTS" target="HOMOSCEDASTICITY">
      <data key="d4">2.0</data>
      <data key="d5">Residual plots serve as a crucial tool in the statistical analysis of regression models, particularly in evaluating the assumption of homoscedasticity. Homoscedasticity refers to a condition where the variance of the error terms, or residuals, remains constant across all levels of the independent variables. By examining residual plots, one can visually inspect whether the residuals are evenly dispersed around the horizontal axis, indicating that the variance is consistent and the homoscedasticity assumption is met. This is essential for ensuring the reliability and validity of the regression analysis, as violations of homoscedasticity can lead to inefficient and biased estimates of the model parameters. Thus, residual plots are indispensable for detecting any potential breaches of the homoscedasticity assumption, allowing for appropriate corrective measures to be taken, such as using weighted least squares or applying transformations to the data.</data>
      <data key="d6">7cd6069e88e81548a237fa937adfecc6,a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="RESIDUAL_PLOTS" target="NULL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">A null plot is a specific type of residual plot that indicates no violations of model assumptions</data>
      <data key="d6">7cd6069e88e81548a237fa937adfecc6</data>
    </edge>
    <edge source="RESIDUAL_PLOTS" target="WESTERN_CEDAR_TREE_DATA">
      <data key="d4">1.0</data>
      <data key="d5">The Western cedar tree data is used as an example in the context of residual plots to illustrate issues with linearity</data>
      <data key="d6">b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </edge>
    <edge source="RESIDUAL_PLOTS" target="ACCEPTABLE_RESIDUAL_PLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Residual plots that are considered acceptable do not show any systematic patterns, indicating that the model assumptions are likely met. These plots are used to assess the fit of a regression model and identify any potential issues with the assumptions.</data>
      <data key="d6">23fc620f1238c6a1b5c5e3a08e149c53</data>
    </edge>
    <edge source="RESIDUAL_PLOTS" target="REMEDIAL_ACTIONS">
      <data key="d4">1.0</data>
      <data key="d5">When residual plots show patterns that indicate violations of model assumptions, remedial actions are necessary to address these issues. These actions can include transformations of the data, adding or removing variables, or using a different model.</data>
      <data key="d6">23fc620f1238c6a1b5c5e3a08e149c53</data>
    </edge>
    <edge source="RESIDUAL_PLOTS" target="SMOOTHING_CURVE">
      <data key="d4">1.0</data>
      <data key="d5">A smoothing curve can be added to residual plots to help identify any patterns or trends in the residuals. A relatively horizontal smoothing curve suggests that the residuals do not exhibit any systematic patterns, which is desirable for an acceptable residual plot.</data>
      <data key="d6">23fc620f1238c6a1b5c5e3a08e149c53</data>
    </edge>
    <edge source="RESIDUAL_PLOTS" target="CONSTANT_VARIANCE_ASSUMPTION">
      <data key="d4">1.0</data>
      <data key="d5">Residual plots are used to assess the constant variance assumption of linear regression models. A plot that shows no clear pattern in the spread of residuals across the range of fitted values supports the assumption of homoscedasticity.</data>
      <data key="d6">23fc620f1238c6a1b5c5e3a08e149c53</data>
    </edge>
    <edge source="RESIDUAL_PLOTS" target="FIGURE_4_5">
      <data key="d4">1.0</data>
      <data key="d5">Figure 4.5 is an example of a residual plot showing residuals versus fitted values for the log-transformed tree data</data>
      <data key="d6">a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="RESIDUAL_PLOTS" target="FIGURE_4_6">
      <data key="d4">1.0</data>
      <data key="d5">Figure 4.6 is another example of a residual plot showing residuals versus log-height and versus log-diameter</data>
      <data key="d6">a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="Q_Q_PLOTS" target="MODEL_ASSUMPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">Q-Q plots are used to assess the normality assumption of the model by comparing the distribution of the residuals to a theoretical normal distribution. If the points on the plot form a straight line, it suggests that the residuals follow a normal distribution, which is one of the assumptions of the linear model.</data>
      <data key="d6">e7494d6cfc3e38a4d2f3f6b21ef6445d</data>
    </edge>
    <edge source="MODEL_ASSUMPTIONS" target="VIOLATIONS_OF_ASSUMPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">Violations of assumptions can lead to biased or inefficient parameter estimates, incorrect standard errors, and invalid hypothesis tests. Identifying and addressing these violations is crucial for the reliability and validity of the model.</data>
      <data key="d6">e7494d6cfc3e38a4d2f3f6b21ef6445d</data>
    </edge>
    <edge source="MODEL_ASSUMPTIONS" target="NORMAL_LINEAR_MODEL">
      <data key="d4">2.0</data>
      <data key="d5">The MODEL_ASSUMPTIONS play a crucial role in ensuring the validity and reliability of the NORMAL_LINEAR_MODEL. These assumptions include the requirement that the response variable is measured on a continuous scale, which is fundamental for the model's applicability. The adherence to these MODEL_ASSUMPTIONS is essential for the normal linear model to accurately represent the relationships within the data, providing meaningful statistical inferences. Without these assumptions being met, the NORMAL_LINEAR_MODEL may yield misleading results, compromising the integrity of the statistical analysis.</data>
      <data key="d6">22093a562f5f05dc9891b45ab9bcbea8,c03eb12d07d48f9e94260f08dae10cdf</data>
    </edge>
    <edge source="MODEL_ASSUMPTIONS" target="NULL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">Model assumptions, when satisfied, result in a null plot with no distinct patterns</data>
      <data key="d6">aa13c33a7e61206e6021e2736002ca9a</data>
    </edge>
    <edge source="MODEL_ASSUMPTIONS" target="HORIZONTAL_LINE_AT_ZERO">
      <data key="d4">1.0</data>
      <data key="d5">If the model assumptions hold, the smoothing curve should resemble a horizontal line at zero</data>
      <data key="d6">674b8d5bb1f830d0fb944942514d1a16</data>
    </edge>
    <edge source="MODEL_ASSUMPTIONS" target="DIAGNOSIS">
      <data key="d4">1.0</data>
      <data key="d5">Diagnosis is the process of checking whether the model assumptions are met</data>
      <data key="d6">c03eb12d07d48f9e94260f08dae10cdf</data>
    </edge>
    <edge source="BETA_HAT" target="YB">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis, BETA_HAT, denoted as \u03b2b, plays a pivotal role as the least squares estimator of the parameter Beta. This estimator is crucial for computing the vector of fitted values, YB. Specifically, YB is determined through the formula YB = X\u03b2b, where X represents the matrix of explanatory variables. This calculation underscores the relationship between the explanatory variables and the predicted values, YB, providing insights into the structure and behavior of the community of interest. The use of BETA_HAT in this formula ensures that the predictions are based on the best linear unbiased estimates, facilitating a deeper understanding of the underlying statistical model.</data>
      <data key="d6">2b6d31b6bff4eae3a4809451c4fb9fa6,5cc49d301d9cd1f8e20b92ab9d8346b0,6ee02b38ae842fd5eac9a11c4fd6659f</data>
    </edge>
    <edge source="BETA_HAT" target="YBJ">
      <data key="d4">1.0</data>
      <data key="d5">Beta hat (&#946;b) is used to compute the fitted value ybj for the jth unit of observation</data>
      <data key="d6">2b6d31b6bff4eae3a4809451c4fb9fa6</data>
    </edge>
    <edge source="BETA_HAT" target="ZJ">
      <data key="d4">1.0</data>
      <data key="d5">Zj is used in the calculation of the jth fitted value ybj, which is part of YB</data>
      <data key="d6">5cc49d301d9cd1f8e20b92ab9d8346b0</data>
    </edge>
    <edge source="BETA_HAT" target="EPSILON_BJ">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, BETA_HAT (\u03b2b) and EPSILON_BJ (\u03f5bj) play significant roles. BETA_HAT, often referred to as the least squares estimator, is a crucial component in the calculation of the fitted values. These fitted values are then used in the computation of the residuals, which are represented by EPSILON_BJ. EPSILON_BJ quantifies the difference between the observed values and the values predicted by the model, providing insights into the accuracy of the model's predictions. The relationship between BETA_HAT and EPSILON_BJ highlights the iterative nature of statistical modeling, where the least squares estimator is used to refine the model until the residuals are minimized, indicating a better fit of the model to the data.</data>
      <data key="d6">255685e281cc5a9edf073c700f425a6b,5cc49d301d9cd1f8e20b92ab9d8346b0</data>
    </edge>
    <edge source="BETA_HAT" target="S">
      <data key="d4">8.0</data>
      <data key="d5">In the context of statistical analysis, the function S(&#946;) plays a pivotal role in estimating the optimal parameters for a given model. Specifically, S(&#946;) is the function that is minimized to find the least squares estimator, Beta hat (&#946;&#770;), which is synonymous with the global minimizer of S(&#946;). This indicates that Beta hat (&#946;&#770;) is the point at which S(&#946;) achieves its global minimum, making it the optimal parameter estimate. Moreover, Beta hat (&#946;&#770;) is the stationary point of S(&#946;), where the function's derivative equals zero. It is also noteworthy that Beta hat (&#946;&#770;) is the value that minimizes the sum of squared differences function S(&#946;) in the least squares estimation, which is a common method for estimating model parameters. Furthermore, S(&#946;) is the function that is minimized by &#946;b to obtain the Maximum Likelihood Estimator (MLE) for a given &#963;^2, highlighting the versatility of Beta hat (&#946;&#770;) in different estimation contexts.</data>
      <data key="d6">50a56c34050fb7f7709300a51399b150,6b55b41598d5264f8dc6b72769748722,7955aae3fd4ca51b9ef8843e13c1f517,9d300fc83afb3261af61b2ab9721cadc,9dddcd96af7b557e578b3f5f36efacd7,a4a817bb79d6ae8812c808ca41d47f43,aa195e72eb5285a4bcae9c856af30a87</data>
    </edge>
    <edge source="BETA_HAT" target="XT">
      <data key="d4">11.0</data>
      <data key="d5">In the context of statistical analysis, XT plays a crucial role in the computation of BETA_HAT, denoted as \u03b2b. This entity, XT, is integral to the formulation of the least squares estimate and the least squares estimator, both of which are pivotal in determining the value of BETA_HAT. Additionally, XT is involved in the normal equations that BETA_HAT satisfies, further emphasizing its significance in the estimation process. It is also noteworthy that XT contributes to the calculation of \u03b2b as part of the Maximum Likelihood Estimation (MLE) method, showcasing its versatility in different statistical estimation techniques. Overall, XT is a fundamental component in the algorithmic analysis, particularly in the estimation of BETA_HAT through various statistical models.</data>
      <data key="d6">255685e281cc5a9edf073c700f425a6b,2f2523c52c6d2869fb19f77b66ce8259,542f546c5a131196e4701fb33c9b1dee,6b55b41598d5264f8dc6b72769748722,7955aae3fd4ca51b9ef8843e13c1f517,82932abd152e0b84a1c26a2daa4c08df,9dddcd96af7b557e578b3f5f36efacd7,9fc2b1e8b2b61b557f88eb9e9c708597,a4a817bb79d6ae8812c808ca41d47f43,aa195e72eb5285a4bcae9c856af30a87,f9b615b879f72501f338f8983d4cac3d</data>
    </edge>
    <edge source="BETA_HAT" target="XTX">
      <data key="d4">7.0</data>
      <data key="d5">In the context of statistical analysis and algorithmic modeling, particularly within the framework of least squares estimation, two key entities emerge: BETA_HAT (&#946;&#770;) and XTX. BETA_HAT, often denoted as &#946;&#770;, represents the estimated coefficients in a linear regression model, which are calculated through the least squares method. XTX, on the other hand, is a crucial component in the formula used to derive BETA_HAT. It plays a pivotal role not only in the calculation of BETA_HAT but also in the formulation of the normal equations, which are a set of linear equations used to find the best-fitting line for a given set of data points. The consistency across the descriptions underscores the significance of XTX in the least squares estimation process, where it facilitates the computation of the optimal values for the model parameters, ensuring that the sum of the squares of the residuals is minimized.</data>
      <data key="d6">28cf5ff0c09fa5c0390267bb9aa3ce47,2f2523c52c6d2869fb19f77b66ce8259,56ff186fc629e1e42f2759fc4b984199,9d300fc83afb3261af61b2ab9721cadc,a4a817bb79d6ae8812c808ca41d47f43,d14413709de2897231aaa83be3aa346f,f9b615b879f72501f338f8983d4cac3d</data>
    </edge>
    <edge source="BETA_HAT" target="XTX_INVERSE">
      <data key="d4">1.0</data>
      <data key="d5">XTX^-1 is used in the calculation of Beta hat (&#946;b) as part of the least squares estimator</data>
      <data key="d6">f9b615b879f72501f338f8983d4cac3d</data>
    </edge>
    <edge source="BETA_HAT" target="RANK_X">
      <data key="d4">1.0</data>
      <data key="d5">Rank (X) must be equal to p for the calculation of Beta hat (&#946;b) to be valid</data>
      <data key="d6">f9b615b879f72501f338f8983d4cac3d</data>
    </edge>
    <edge source="BETA_HAT" target="NORMAL_EQUATIONS">
      <data key="d4">3.0</data>
      <data key="d5">In the context of statistical analysis and algorithmic understanding, the entities BETA_HAT (\u03b2b) and NORMAL_EQUATIONS are central to the process of least squares estimation. The normal equations, a set of linear equations derived from the method of least squares, are crucial for finding the least squares estimate, which is denoted as BETA_HAT (\u03b2b). This estimate, \u03b2b, is not only the solution to the normal equations but also represents the maximum likelihood estimate for the parameter \u03b2. It is the stationary point that satisfies the normal equations, indicating that at this point, the sum of the squared residuals is minimized, making it the optimal solution in the context of least squares regression.</data>
      <data key="d6">21ec28dfe2b2c18030d541d63e51f45e,ad799500572246a07f983a3b92c0e61f,f632f01188d2c6e3091a965580cb4600</data>
    </edge>
    <edge source="BETA_HAT" target="S_BETA_BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">&#946;b is the point at which S(&#946;) achieves its global minimum</data>
      <data key="d6">21ec28dfe2b2c18030d541d63e51f45e</data>
    </edge>
    <edge source="BETA_HAT" target="XTY">
      <data key="d4">1.0</data>
      <data key="d5">XTY is part of the formula used to calculate Beta hat (b&#946;) in the least squares estimation</data>
      <data key="d6">28cf5ff0c09fa5c0390267bb9aa3ce47</data>
    </edge>
    <edge source="BETA_HAT" target="SXX">
      <data key="d4">1.0</data>
      <data key="d5">Sxx is used in the calculation of Beta hat (&#946;b) as part of the least squares estimator</data>
      <data key="d6">2f2523c52c6d2869fb19f77b66ce8259</data>
    </edge>
    <edge source="BETA_HAT" target="X_BAR">
      <data key="d4">1.0</data>
      <data key="d5">x_bar is used in the calculation of Beta hat (&#946;b) as part of the least squares estimator</data>
      <data key="d6">2f2523c52c6d2869fb19f77b66ce8259</data>
    </edge>
    <edge source="BETA_HAT" target="Y_BAR">
      <data key="d4">1.0</data>
      <data key="d5">y_bar is used in the calculation of Beta hat (&#946;b) as part of the least squares estimator</data>
      <data key="d6">2f2523c52c6d2869fb19f77b66ce8259</data>
    </edge>
    <edge source="BETA_HAT" target="GAMMA_HAT">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, BETA_HAT (\u03b2b) and GAMMA_HAT (\u03b3b) are two significant entities that are intricately connected. BETA_HAT is a crucial component in the computation of GAMMA_HAT, as it undergoes a transformation through an invertible matrix A to derive GAMMA_HAT. This algebraic relationship signifies that the values of GAMMA_HAT are directly influenced by the values of BETA_HAT, highlighting the dependency and interrelation between these two statistical measures. The transformation process, facilitated by the invertible matrix A, ensures that the transition from BETA_HAT to GAMMA_HAT is both mathematically sound and reversible, providing a robust framework for statistical inference and analysis.</data>
      <data key="d6">82932abd152e0b84a1c26a2daa4c08df,d94760a5f9f6ea115fcc18024035a627</data>
    </edge>
    <edge source="BETA_HAT" target="LSE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, BETA_HAT, denoted as \u03b2b, is the least squares estimate for the parameter vector \u03b2. This estimation is derived through the method of Least Squares Estimation (LSE), which is a fundamental technique in regression analysis. BETA_HAT is obtained by minimizing the sum of the squared differences between the observed and predicted values. Notably, under the assumption that the errors are independently and identically distributed (iid) as a Normal distribution with mean 0 and variance \u03c3^2, BETA_HAT is identical to the Maximum Likelihood Estimator (MLE) for \u03b2. This equivalence highlights the robustness and efficiency of the least squares method in estimating parameters under certain conditions.</data>
      <data key="d6">9fc2b1e8b2b61b557f88eb9e9c708597,d738df7d83784c8a41b3948271c537b6</data>
    </edge>
    <edge source="BETA_HAT" target="MLE">
      <data key="d4">1.0</data>
      <data key="d5">MLE is the method used to calculate the estimate &#946;b</data>
      <data key="d6">9dddcd96af7b557e578b3f5f36efacd7</data>
    </edge>
    <edge source="BETA_HAT" target="L">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, BETA_HAT, denoted as \u03b2b, represents the least squares estimate, which is a fundamental component in the estimation process of linear regression models. This estimate is derived by optimizing the likelihood function, L(\u03b2, \u03c3^2|y), where L signifies the likelihood function that plays a pivotal role in determining the maximum likelihood estimate (MLE). The likelihood function, L, is maximized by BETA_HAT, ensuring that the parameters of the model are estimated in a way that maximizes the probability of observing the given data, y. This process underscores the statistical inference approach, where the goal is to make inferences about the population parameters based on the sample data. The use of the likelihood function and the least squares method in conjunction provides a robust framework for estimating model parameters and conducting residual analysis, which is crucial for assessing the goodness of fit and the reliability of the model predictions.</data>
      <data key="d6">9dddcd96af7b557e578b3f5f36efacd7,f632f01188d2c6e3091a965580cb4600</data>
    </edge>
    <edge source="BETA_HAT" target="LOG_LIKELIHOOD">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entities BETA_HAT and LOG_LIKELIHOOD are central to understanding the estimation process. BETA_HAT, denoted as \u03b2b, is the least squares estimate obtained by minimizing the residual sum of squares function S(\u03b2). This process ensures that the vector \u03b2b best fits the data by reducing the squared differences between the observed and predicted values. Simultaneously, LOG_LIKELIHOOD, represented by the symbol \u2113, plays a pivotal role in the Maximum Likelihood Estimation (MLE) method. The log-likelihood function is maximized by \u03b2b for a given variance \u03c3^2, leading to the identification of the Maximum Likelihood Estimate. This dual approach, combining least squares and maximum likelihood methods, provides a robust framework for estimating model parameters and assessing model fit in statistical analysis.</data>
      <data key="d6">9dddcd96af7b557e578b3f5f36efacd7,ad799500572246a07f983a3b92c0e61f</data>
    </edge>
    <edge source="BETA_HAT" target="RESIDUAL_SUM_SQUARES">
      <data key="d4">1.0</data>
      <data key="d5">The residual sum of squares function S(&#946;) is minimised by the least squares estimate &#946;b, which is the maximum likelihood estimate for &#946;.</data>
      <data key="d6">ad799500572246a07f983a3b92c0e61f</data>
    </edge>
    <edge source="BETA_HAT" target="BETA_HAT_Y">
      <data key="d4">1.0</data>
      <data key="d5">Beta hat (&#946;b) is a realization of the estimator Beta hat (&#946;b(Y)) when applied to the observed data</data>
      <data key="d6">2de7a36b32bf79c8f32612c8aaa9daa8</data>
    </edge>
    <edge source="BETA_HAT" target="NP">
      <data key="d4">4.0</data>
      <data key="d5">Np is the distribution of Beta hat (&#946;b) itself</data>
      <data key="d6">3d357cfa3ef0d00f49cf4acaeac1c9d1,45f31b040576e9f3b4def6d0466cc016,542f546c5a131196e4701fb33c9b1dee,d14413709de2897231aaa83be3aa346f</data>
    </edge>
    <edge source="BETA_HAT" target="A">
      <data key="d4">1.0</data>
      <data key="d5">A is the matrix transformation applied to Y to obtain Beta hat (&#946;b)</data>
      <data key="d6">542f546c5a131196e4701fb33c9b1dee</data>
    </edge>
    <edge source="BETA_HAT" target="S_HAT_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">&#946;b is the estimator for &#946; used in the calculation of the unbiased estimator s^2(Y) for the error variance</data>
      <data key="d6">6648f0d6deed51fb4fb25e6992a71ddf</data>
    </edge>
    <edge source="BETA_HAT" target="S2">
      <data key="d4">1.0</data>
      <data key="d5">Beta hat (&#946;b) is used in the calculation of the unbiased estimator s^2</data>
      <data key="d6">9923e77ac6b3de95cb5026bc5e7fe8c0</data>
    </edge>
    <edge source="BETA_HAT" target="EXERCISE_18">
      <data key="d4">1.0</data>
      <data key="d5">Exercise 18 involves calculating the estimator Beta hat (&#946;b) for a given linear model</data>
      <data key="d6">fc5b725f3c662c5471af20efdcc2dbff</data>
    </edge>
    <edge source="BETA_HAT" target="YC">
      <data key="d4">1.0</data>
      <data key="d5">Beta hat (&#946;b) is used in the computation of the expected value of Yc, which is X&#946;</data>
      <data key="d6">679722cf8ce5ce5aee4e379528470efe</data>
    </edge>
    <edge source="BETA_HAT" target="PARALLEL_LINES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The parallel lines model uses the estimated regression coefficient Beta hat (&#946;&#710;) for the price variable</data>
      <data key="d6">b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </edge>
    <edge source="BETA_HAT" target="STORE_BRAND">
      <data key="d4">1.0</data>
      <data key="d5">Store brand is a categorical variable whose effect on sales is quantified by the estimated coefficient for Beta (&#946;)</data>
      <data key="d6">248924760a2bfbc82501fd6b11cfa0aa</data>
    </edge>
    <edge source="BETA_HAT" target="BRAND_MODEL3">
      <data key="d4">1.0</data>
      <data key="d5">Brand_model3 is the model that provides the estimate for the price parameter Beta_hat</data>
      <data key="d6">6a6f85d0a6e46196ab3a901fcc82a720</data>
    </edge>
    <edge source="BETA_HAT" target="Z">
      <data key="d4">1.0</data>
      <data key="d5">Beta hat (b&#946;) is the maximum likelihood estimator for the parameter Beta, as mentioned in the context of distributional properties</data>
      <data key="d6">aac5b4f040b9c773bd1aa696dec469f6</data>
    </edge>
    <edge source="BETA_HAT" target="CHI_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">The chi-squared distribution is related to the distribution of the least squares estimator Beta hat (&#946;b) in certain conditions</data>
      <data key="d6">0ac60299320c55d642b3e38440c25f90</data>
    </edge>
    <edge source="BETA_HAT" target="S_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">The distributional properties of Beta hat (&#946;b) and S squared (s^2) are related in the context of linear regression models</data>
      <data key="d6">0ac60299320c55d642b3e38440c25f90</data>
    </edge>
    <edge source="BETA_BAR" target="COEF_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The function coef(model) returns the estimated parameter vector Beta bar</data>
      <data key="d6">01d5ee79489582b4135fc96f676b24a0</data>
    </edge>
    <edge source="YB" target="YA">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entities YB and YA play significant roles in understanding the impact of changes in a variable, denoted as X1. Specifically, YA represents the back-transformed prediction for the response variable Y prior to any alterations in X1. This pre-change prediction serves as a baseline measurement, crucial for calculating the subsequent shifts in response. Following the increase in X1, YB emerges as the updated prediction, reflecting the new state of Y after the change has been implemented. The comparison between YA and YB allows for a detailed examination of how modifications in X1 influence the response variable Y, providing insights into the nature and magnitude of the relationship between these variables.</data>
      <data key="d6">21e429490eeefe7d9c245058fd48ca68,995fb26a0261f824952fa7b2fac3382e</data>
    </edge>
    <edge source="YB" target="Z">
      <data key="d4">1.0</data>
      <data key="d5">Z is used in the calculation of the fitted values Yb in the reparameterised model</data>
      <data key="d6">82932abd152e0b84a1c26a2daa4c08df</data>
    </edge>
    <edge source="YB" target="H">
      <data key="d4">1.0</data>
      <data key="d5">Yb is calculated as Hy, where H is the hat matrix</data>
      <data key="d6">6ee02b38ae842fd5eac9a11c4fd6659f</data>
    </edge>
    <edge source="YB" target="HII">
      <data key="d4">1.0</data>
      <data key="d5">The leverage (hii) of the ith data point influences the weight that the observed value yi has when computing the fitted value ybi</data>
      <data key="d6">6ee02b38ae842fd5eac9a11c4fd6659f</data>
    </edge>
    <edge source="YB" target="COOKS_DISTANCE">
      <data key="d4">1.0</data>
      <data key="d5">yb is used in the calculation of Cook's distance (Di) as the vector of fitted values</data>
      <data key="d6">09391efd3b8c510205098b548bc8dc74</data>
    </edge>
    <edge source="ZJ" target="YBJ">
      <data key="d4">1.0</data>
      <data key="d5">zj is the jth observation of the explanatory variable used to compute the fitted value ybj in simple linear regression</data>
      <data key="d6">2b6d31b6bff4eae3a4809451c4fb9fa6</data>
    </edge>
    <edge source="YBJ" target="EPSILON_BJ">
      <data key="d4">1.0</data>
      <data key="d5">Epsilon bj (&#1013;bj) is calculated as the difference between the observed value Yj and the fitted value Ybj, indicating the residual for the jth observation</data>
      <data key="d6">e6f79ceb0df54119a4dc71b2162ac50b</data>
    </edge>
    <edge source="YBJ" target="XTJ">
      <data key="d4">1.0</data>
      <data key="d5">XTj is used in the calculation of the fitted value Ybj, which is the predicted response for the jth observation</data>
      <data key="d6">e6f79ceb0df54119a4dc71b2162ac50b</data>
    </edge>
    <edge source="YBJ" target="BETA_B">
      <data key="d4">1.0</data>
      <data key="d5">Beta b (&#946;b) is used in the calculation of the fitted value Ybj, which is the predicted response for the jth observation</data>
      <data key="d6">e6f79ceb0df54119a4dc71b2162ac50b</data>
    </edge>
    <edge source="EPSILON_BJ" target="DEVIANCE">
      <data key="d4">1.0</data>
      <data key="d5">Deviance or Residual Sum of Squares (ResidSS) is calculated using the squared residuals &#1013;bj, indicating the sum of squared residuals</data>
      <data key="d6">e6f79ceb0df54119a4dc71b2162ac50b</data>
    </edge>
    <edge source="EPSILON_BJ" target="RESIDSS">
      <data key="d4">1.0</data>
      <data key="d5">The residuals (&#1013;bj) are squared and summed to calculate the residual sum of squares (ResidSS)</data>
      <data key="d6">255685e281cc5a9edf073c700f425a6b</data>
    </edge>
    <edge source="BETA_B" target="BRAND_A">
      <data key="d4">1.0</data>
      <data key="d5">Brand A's price effect is represented by Beta hat (b&#946;), indicating the average reduction in sales with every additional pound charged</data>
      <data key="d6">adbc52b340a69a8633c919c4fd2cd3f6</data>
    </edge>
    <edge source="PARAMETER_ESTIMATES" target="RESIDUAL_SUM_OF_SQUARES">
      <data key="d4">1.0</data>
      <data key="d5">Parameter estimates are chosen to minimize the residual sum of squares, optimizing the model's fit to the data</data>
      <data key="d6">22093a562f5f05dc9891b45ab9bcbea8</data>
    </edge>
    <edge source="RESIDUAL_SUM_OF_SQUARES" target="MODEL_DEVIANCE">
      <data key="d4">1.0</data>
      <data key="d5">Residual sum of squares and model deviance are synonymous terms, both measuring the discrepancy between the observed data and the model's predictions</data>
      <data key="d6">22093a562f5f05dc9891b45ab9bcbea8</data>
    </edge>
    <edge source="SIMPLE_REGRESSION_LINE" target="EXERCISE_9">
      <data key="d4">1.0</data>
      <data key="d5">The simple regression line is provided for Exercise 9, which requires calculating the fitted values, residuals, and deviance</data>
      <data key="d6">22093a562f5f05dc9891b45ab9bcbea8</data>
    </edge>
    <edge source="NORMAL_LINEAR_MODEL" target="TEXTBOOKS">
      <data key="d4">1.0</data>
      <data key="d5">The recommended textbooks cover material related to the normal linear model</data>
      <data key="d6">c03eb12d07d48f9e94260f08dae10cdf</data>
    </edge>
    <edge source="NORMAL_LINEAR_MODEL" target="MLE">
      <data key="d4">1.0</data>
      <data key="d5">The MLE for &#946; is derived within the context of the normal linear model</data>
      <data key="d6">d738df7d83784c8a41b3948271c537b6</data>
    </edge>
    <edge source="NORMALITY" target="NORMAL_Q_Q_PLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Normal Q-Q plots are used to assess the normality assumption in a statistical model</data>
      <data key="d6">7cd6069e88e81548a237fa937adfecc6</data>
    </edge>
    <edge source="NORMALITY" target="STATISTICAL_INFERENCE">
      <data key="d4">1.0</data>
      <data key="d5">Normality is an assumption that is assessed in statistical inference to ensure the validity of the results</data>
      <data key="d6">ef24ca5edd06893b737e6a1c8a9825f6</data>
    </edge>
    <edge source="HETEROSCEDASTICITY" target="VARIANCE">
      <data key="d4">1.0</data>
      <data key="d5">Heteroscedasticity is characterized by non-constant variance of the error terms</data>
      <data key="d6">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </edge>
    <edge source="HETEROSCEDASTICITY" target="NON-LINEARITY">
      <data key="d4">1.0</data>
      <data key="d5">Heteroscedasticity and non-linearity are issues that may occur together in a statistical model</data>
      <data key="d6">7347b44ffb25a066e43321f4eaf5a806</data>
    </edge>
    <edge source="HETEROSCEDASTICITY" target="LINEARITY_ASSUMPTION">
      <data key="d4">1.0</data>
      <data key="d5">Linearity is prioritized over addressing heteroscedasticity if both issues cannot be resolved</data>
      <data key="d6">7347b44ffb25a066e43321f4eaf5a806</data>
    </edge>
    <edge source="HETEROSCEDASTICITY" target="TRANSFORMATIONS">
      <data key="d4">1.0</data>
      <data key="d5">Transformations, particularly log transformations, are used to address heteroscedasticity in linear models</data>
      <data key="d6">07951ffe6787af44aa60c90c69e62f83</data>
    </edge>
    <edge source="DATA_COLLECTION" target="SEQUENTIAL_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Sequential data collection can lead to violations of the independence assumption in the data collection process</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6</data>
    </edge>
    <edge source="DATA_COLLECTION" target="SPATIAL_DATA">
      <data key="d4">1.0</data>
      <data key="d5">Spatial data collection can lead to violations of the independence assumption in the data collection process</data>
      <data key="d6">0da640a09a395a50b6e16e047fa8d0d6</data>
    </edge>
    <edge source="NORMAL_Q_Q_PLOTS" target="STANDARDISED_RESIDUALS">
      <data key="d4">1.0</data>
      <data key="d5">Normal Q-Q plots are used to assess the normality of the standardised residuals</data>
      <data key="d6">e361ac139c268d5c3f3623f920e68af2</data>
    </edge>
    <edge source="NULL_PLOT" target="SMOOTHER">
      <data key="d4">1.0</data>
      <data key="d5">A smoother is added to a null plot to help evaluate the pattern of residuals</data>
      <data key="d6">aa13c33a7e61206e6021e2736002ca9a</data>
    </edge>
    <edge source="NULL_PLOT" target="HORIZONTAL_LINE">
      <data key="d4">1.0</data>
      <data key="d5">A horizontal line at zero is expected in a null plot if the model assumptions hold</data>
      <data key="d6">aa13c33a7e61206e6021e2736002ca9a</data>
    </edge>
    <edge source="NULL_PLOT" target="VARIABILITY">
      <data key="d4">1.0</data>
      <data key="d5">Constant variability in the residuals is a characteristic of a null plot</data>
      <data key="d6">aa13c33a7e61206e6021e2736002ca9a</data>
    </edge>
    <edge source="SMOOTHER" target="RESIDUAL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The smoother is used in the residual plot to identify patterns in the residuals</data>
      <data key="d6">82cfcd5865cffe55e965a50745656e60</data>
    </edge>
    <edge source="SMOOTHED_RESIDUAL_PLOT" target="SMOOTHING_CURVE">
      <data key="d4">1.0</data>
      <data key="d5">The smoothing curve is added to the smoothed residual plot to estimate the mean of the residuals</data>
      <data key="d6">674b8d5bb1f830d0fb944942514d1a16</data>
    </edge>
    <edge source="SMOOTHED_RESIDUAL_PLOT" target="WELL_FITTING_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">A well-fitting model is illustrated by a smoothed residual plot where the smoothing curve resembles a horizontal line and the variation of the residuals appears relatively stable</data>
      <data key="d6">674b8d5bb1f830d0fb944942514d1a16</data>
    </edge>
    <edge source="SMOOTHING_CURVE" target="RESIDUAL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The smoothing curve is fitted to the data in the residual plot</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="UNACCEPTABLE_RESIDUAL_PLOTS" target="WESTERN_CEDAR_TREE_DATA">
      <data key="d4">1.0</data>
      <data key="d5">The Western cedar tree data is an example of an unacceptable residual plot</data>
      <data key="d6">674b8d5bb1f830d0fb944942514d1a16</data>
    </edge>
    <edge source="UNACCEPTABLE_RESIDUAL_PLOTS" target="CURVED_RELATIONSHIP">
      <data key="d4">1.0</data>
      <data key="d5">Unacceptable residual plots may indicate a curved relationship between the explanatory variable and the response variable</data>
      <data key="d6">674b8d5bb1f830d0fb944942514d1a16</data>
    </edge>
    <edge source="WESTERN_CEDAR_TREE_DATA" target="TRANSFORM">
      <data key="d4">1.0</data>
      <data key="d5">A transform was applied to the Western cedar tree data to address non-linearity</data>
      <data key="d6">15c7b5750483a382ce59751008e86751</data>
    </edge>
    <edge source="LINEARITY_ASSUMPTION" target="NON-LINEARITY">
      <data key="d4">1.0</data>
      <data key="d5">Non-linearity violates the linearity assumption in the relationship between the response and explanatory variables</data>
      <data key="d6">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </edge>
    <edge source="LINEARITY_ASSUMPTION" target="TRANSFORMATION">
      <data key="d4">1.0</data>
      <data key="d5">The assumption of linearity is often addressed by using transformations, such as the log-transform, to ensure that the relationship between the explanatory variables and the response variable is linear.</data>
      <data key="d6">0fbc9037ca9a440e79e9ac05664b9b3d</data>
    </edge>
    <edge source="TRANSFORMATION" target="SQUARE_ROOT_TRANSFORM">
      <data key="d4">1.0</data>
      <data key="d5">The square root transform is a specific type of transformation that can be applied to the explanatory variable</data>
      <data key="d6">b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </edge>
    <edge source="TRANSFORMATION" target="LOG_TRANSFORM">
      <data key="d4">2.0</data>
      <data key="d5">The TRANSFORMATION methods encompass a variety of techniques used to modify the distribution of a variable, with the LOG_TRANSFORM being a specific and widely utilized type within this category. The LOG_TRANSFORM is particularly noted for its ability to stabilize variance and linearize relationships, making it a preferred choice when dealing with explanatory variables. By applying the LOG_TRANSFORM to an explanatory variable, one can effectively address issues of non-linearity and heteroscedasticity, thus enhancing the model's performance and interpretability. This transformation is commonly employed in statistical analysis to ensure that the assumptions of linear regression models are met, thereby improving the accuracy of predictions and the reliability of statistical inferences.</data>
      <data key="d6">0fbc9037ca9a440e79e9ac05664b9b3d,b9ec8a6c7960cc6196ec94fd976f05b0</data>
    </edge>
    <edge source="TRANSFORMATION" target="VARIATION">
      <data key="d4">1.0</data>
      <data key="d5">Transformation of the response variable is used to stabilize the variance</data>
      <data key="d6">7347b44ffb25a066e43321f4eaf5a806</data>
    </edge>
    <edge source="TRANSFORMATION" target="RESIDUAL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">Residual plots are used to assess the effectiveness of transformations</data>
      <data key="d6">7347b44ffb25a066e43321f4eaf5a806</data>
    </edge>
    <edge source="SQUARE_ROOT_TRANSFORM" target="LOG_TRANSFORM">
      <data key="d4">1.0</data>
      <data key="d5">Square root and log transforms are both used to address non-linearity in the explanatory variable</data>
      <data key="d6">15c7b5750483a382ce59751008e86751</data>
    </edge>
    <edge source="LOG_TRANSFORM" target="POSITIVE_MEASUREMENTS">
      <data key="d4">1.0</data>
      <data key="d5">Log-transform is often applied to positive physical measurements to stabilize variance and make the distribution more symmetrical</data>
      <data key="d6">995fb26a0261f824952fa7b2fac3382e</data>
    </edge>
    <edge source="MONOTONIC_NON_LINEAR" target="TRANSFORM">
      <data key="d4">1.0</data>
      <data key="d5">A transform is applied to the explanatory variable to address monotonic but non-linear relationships</data>
      <data key="d6">15c7b5750483a382ce59751008e86751</data>
    </edge>
    <edge source="TRANSFORM" target="NON_MONOTONIC_NON_LINEAR">
      <data key="d4">1.0</data>
      <data key="d5">A transform will not address a non-monotonic non-linearity</data>
      <data key="d6">15c7b5750483a382ce59751008e86751</data>
    </edge>
    <edge source="NON_MONOTONIC_NON_LINEAR" target="FIGURE_3_3">
      <data key="d4">1.0</data>
      <data key="d5">Figure 3.3 shows an example of a non-monotonic and non-linear relationship between the response variable and the explanatory variable</data>
      <data key="d6">15c7b5750483a382ce59751008e86751</data>
    </edge>
    <edge source="RESIDUAL_PLOT" target="VARIANCE_INCREASING_WITH_FITTED_VALUES">
      <data key="d4">1.0</data>
      <data key="d5">The pattern of increasing variance with fitted values is observed in the residual plot</data>
      <data key="d6">82cfcd5865cffe55e965a50745656e60</data>
    </edge>
    <edge source="RESIDUAL_PLOT" target="REMEDIAL_ACTION">
      <data key="d4">1.0</data>
      <data key="d5">Residual plots guide the selection of remedial actions to address model issues</data>
      <data key="d6">7347b44ffb25a066e43321f4eaf5a806</data>
    </edge>
    <edge source="RESIDUAL_PLOT" target="RANDOM_VARIATION">
      <data key="d4">1.0</data>
      <data key="d5">Residual plots help distinguish between systematic features and random variation</data>
      <data key="d6">7347b44ffb25a066e43321f4eaf5a806</data>
    </edge>
    <edge source="RESIDUAL_PLOT" target="COURSEWORK">
      <data key="d4">1.0</data>
      <data key="d5">In coursework, students are expected to analyze and interpret residual plots</data>
      <data key="d6">7347b44ffb25a066e43321f4eaf5a806</data>
    </edge>
    <edge source="RESIDUAL_PLOT" target="ACCEPTABILITY">
      <data key="d4">1.0</data>
      <data key="d5">Acceptability of a model is determined by the features identified in the residual plot</data>
      <data key="d6">7347b44ffb25a066e43321f4eaf5a806</data>
    </edge>
    <edge source="VARIATION" target="HORIZONTAL_AXIS">
      <data key="d4">1.0</data>
      <data key="d5">Variation of observations can be observed as we move along the horizontal axis in a scatterplot or residual plot</data>
      <data key="d6">15c7b5750483a382ce59751008e86751</data>
    </edge>
    <edge source="VARIATION" target="CURVE">
      <data key="d4">1.0</data>
      <data key="d5">Curve is associated with an increase in variation as we move along the horizontal axis</data>
      <data key="d6">7347b44ffb25a066e43321f4eaf5a806</data>
    </edge>
    <edge source="NON_MONOTONIC_NON_LINEAR_RELATIONSHIP" target="QUADRATIC_REGRESSION_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">A quadratic regression model is a suitable choice to model the non-monotonic non-linear relationship</data>
      <data key="d6">82cfcd5865cffe55e965a50745656e60</data>
    </edge>
    <edge source="VARIANCE_INCREASING_WITH_FITTED_VALUES" target="TRANSFORM_RESPONSE_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">Transforming the response variable is a method to address the issue of increasing variance</data>
      <data key="d6">82cfcd5865cffe55e965a50745656e60</data>
    </edge>
    <edge source="VARIANCE" target="SQUARE_ROOT_TRANSFORMATION">
      <data key="d4">1.0</data>
      <data key="d5">Square root transformation is used to stabilize the variance of the response variable</data>
      <data key="d6">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </edge>
    <edge source="VARIANCE" target="LOGARITHM_TRANSFORMATION">
      <data key="d4">1.0</data>
      <data key="d5">Logarithm transformation is used to stabilize the variance of the response variable, particularly when the response variable is positive</data>
      <data key="d6">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </edge>
    <edge source="VARIANCE" target="MEGAPHONE_SHAPE_RESIDUAL_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">A megaphone shape in a residual plot indicates increasing variance with the explanatory variable</data>
      <data key="d6">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </edge>
    <edge source="VARIANCE" target="PRECISION_OF_MEASUREMENT">
      <data key="d4">1.0</data>
      <data key="d5">The precision of measurement is higher for smaller observations, leading to a variance that is increasing with the mean</data>
      <data key="d6">66f7fae9d896ff2b3fd40186cc833503</data>
    </edge>
    <edge source="NON-LINEARITY" target="TRANSFORMATION_OF_EXPLANATORY_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">Transformation of the explanatory variable can address non-linearity in the relationship between the response and explanatory variables</data>
      <data key="d6">b9eb75001a4f68f7240b2ca9e0d79eb8</data>
    </edge>
    <edge source="REMEDIAL_ACTION" target="DIAGNOSIS">
      <data key="d4">1.0</data>
      <data key="d5">Remedial action is taken when the diagnosis reveals that the model assumptions are violated</data>
      <data key="d6">c03eb12d07d48f9e94260f08dae10cdf</data>
    </edge>
    <edge source="REMEDIAL_ACTIONS" target="MORE_COMPLEX_MODELS">
      <data key="d4">1.0</data>
      <data key="d5">Using more complex models is another remedial action that can be taken to address violations of the modelling assumptions</data>
      <data key="d6">e361ac139c268d5c3f3623f920e68af2</data>
    </edge>
    <edge source="SIMULATED_DATA" target="LINEAR_REGRESSION_MODELS">
      <data key="d4">1.0</data>
      <data key="d5">Simulated data can be used to test the performance of linear regression models under known conditions. This helps in verifying that the model works as expected and can provide insights into the model's behavior under various scenarios.</data>
      <data key="d6">23fc620f1238c6a1b5c5e3a08e149c53</data>
    </edge>
    <edge source="SIMULATED_DATA" target="LINEAR_MODEL_ASSUMPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">Simulated data can be designed to satisfy the linear model assumptions, allowing for the testing of statistical methods or models under controlled conditions where the assumptions are known to be met.</data>
      <data key="d6">23fc620f1238c6a1b5c5e3a08e149c53</data>
    </edge>
    <edge source="SIMULATED_DATA" target="VARIANCE_ASSUMPTION">
      <data key="d4">1.0</data>
      <data key="d5">The constant variance assumption is tested using simulated data</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="DISTRIBUTION" target="VARIANCE_ASSUMPTION">
      <data key="d4">1.0</data>
      <data key="d5">The constant variance assumption is based on the distribution of the data</data>
      <data key="d6">312309b45c59e1c84695ac3c7e202742</data>
    </edge>
    <edge source="DISTRIBUTION" target="DI">
      <data key="d4">1.0</data>
      <data key="d5">The distribution on p and n - p degrees of freedom is used to determine if Di is large enough to flag the ith observation as influential</data>
      <data key="d6">323899f01972255cd3278bccee20d5d8</data>
    </edge>
    <edge source="STATISTICAL_INFERENCE" target="ROBUSTNESS">
      <data key="d4">1.0</data>
      <data key="d5">Robustness is a property of statistical inference that ensures its validity even when the normality assumption is not met</data>
      <data key="d6">ef24ca5edd06893b737e6a1c8a9825f6</data>
    </edge>
    <edge source="STATISTICAL_INFERENCE" target="NORMAL_Q_Q_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The results of a normal Q-Q plot can influence the validity of statistical inferences</data>
      <data key="d6">521acf88540d5897188c9ec65b17e6a6</data>
    </edge>
    <edge source="ROBUSTNESS" target="SAMPLE_SIZE">
      <data key="d4">1.0</data>
      <data key="d5">Sample size (n) affects the robustness of statistical inference, with larger sample sizes leading to more robust results</data>
      <data key="d6">ef24ca5edd06893b737e6a1c8a9825f6</data>
    </edge>
    <edge source="ASSESSING_NORMALITY" target="NORMAL_Q_Q_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">A normal Q-Q plot is a tool used in assessing the normality of data or errors</data>
      <data key="d6">521acf88540d5897188c9ec65b17e6a6</data>
    </edge>
    <edge source="ASSESSING_NORMALITY" target="NORMALITY_ASSUMPTION">
      <data key="d4">1.0</data>
      <data key="d5">The normality assumption is a hypothesis that is tested using methods such as assessing normality</data>
      <data key="d6">521acf88540d5897188c9ec65b17e6a6</data>
    </edge>
    <edge source="ASSESSING_NORMALITY" target="LARGE_SAMPLE_SIZES">
      <data key="d4">1.0</data>
      <data key="d5">In large sample sizes, the assumption of normality is less critical for the validity of statistical inferences</data>
      <data key="d6">521acf88540d5897188c9ec65b17e6a6</data>
    </edge>
    <edge source="ASSESSING_NORMALITY" target="DEPARTURES_FROM_NORMALITY">
      <data key="d4">1.0</data>
      <data key="d5">Departures from normality can be identified and assessed using methods such as the normal Q-Q plot</data>
      <data key="d6">521acf88540d5897188c9ec65b17e6a6</data>
    </edge>
    <edge source="NORMAL_Q_Q_PLOT" target="PERCENTAGE_SCALE">
      <data key="d4">1.0</data>
      <data key="d5">Percentage scale can be used in the context of a normal Q-Q plot, although it is not directly related to the creation of the plot itself. It is mentioned in the context of different scales that can be used for data representation.</data>
      <data key="d6">eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </edge>
    <edge source="NORMAL_Q_Q_PLOT" target="NORMAL_SCORE_SCALE">
      <data key="d4">1.0</data>
      <data key="d5">Normal score scale is often used in the vertical axis of a normal Q-Q plot, which is a graphical technique for assessing the normality of a dataset.</data>
      <data key="d6">eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </edge>
    <edge source="NORMAL_Q_Q_PLOT" target="NORMAL_DISTRIBUTION">
      <data key="d4">1.0</data>
      <data key="d5">Normal distribution is the reference distribution against which the data are compared in a normal Q-Q plot. The plot is used to assess whether the data could have come from a normal distribution.</data>
      <data key="d6">eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </edge>
    <edge source="NORMAL_Q_Q_PLOT" target="STANDARDISED_RESIDUALS">
      <data key="d4">1.0</data>
      <data key="d5">Standardised residuals are plotted on the normal Q-Q plot to assess the normality of the residuals, which is an assumption of many statistical models.</data>
      <data key="d6">eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </edge>
    <edge source="NORMAL_Q_Q_PLOT" target="FITTED_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The fitted model's residuals are used to create the normal Q-Q plot, which helps in assessing the model's assumption of normality.</data>
      <data key="d6">eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </edge>
    <edge source="NORMAL_Q_Q_PLOT" target="FIGURE_3_9">
      <data key="d4">1.0</data>
      <data key="d5">Figure 3.9 shows two examples of normal Q-Q plots, one with data from a standard normal distribution and the other with data from a t-distribution on 2 degrees of freedom, demonstrating how the plots can be used to assess normality.</data>
      <data key="d6">eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </edge>
    <edge source="NORMAL_Q_Q_PLOT" target="T_DISTRIBUTION">
      <data key="d4">1.0</data>
      <data key="d5">T-distribution is used to simulate data for the right plot in Figure 3.9, which is a normal Q-Q plot. The plot shows how data from a t-distribution can deviate from the normal distribution, particularly in the tails.</data>
      <data key="d6">eafa2cc6cc64d8bca1c080bdd2ad7654</data>
    </edge>
    <edge source="NORMAL_DISTRIBUTION" target="Q_Q_PLOT">
      <data key="d4">2.0</data>
      <data key="d5">The NORMAL_DISTRIBUTION, a fundamental concept in statistical analysis, serves as a benchmark in Q_Q_PLOT assessments. The Q_Q_PLOT is a graphical tool that compares observed data against the normal distribution, specifically to evaluate if a dataset adheres to a normal distribution. In this context, the normal distribution acts as a reference line on the plot. When data points closely align with this line, it suggests that the dataset is normally distributed. Conversely, significant deviations from the straight line indicate that the data do not conform to a normal distribution, providing valuable insights into the nature and structure of the dataset under analysis.</data>
      <data key="d6">3bfc9b92571973e54c8095302acc1aaa,ee22e1f5947947f9bd3f7f8922745e48</data>
    </edge>
    <edge source="NORMAL_DISTRIBUTION" target="T_STATISTIC">
      <data key="d4">1.0</data>
      <data key="d5">The T-statistic is based on the assumption of a normal distribution for the errors in a linear model. This allows for the calculation of the distributional properties of the T-statistic, which are used in hypothesis testing.</data>
      <data key="d6">d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </edge>
    <edge source="STANDARDISED_RESIDUALS" target="MODEL_FIT">
      <data key="d4">1.0</data>
      <data key="d5">Standardised residuals are used to assess the model fit</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="STANDARDISED_RESIDUALS" target="INDEX_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">Index plot displays the absolute standardised residuals</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="STANDARDISED_RESIDUALS" target="REGRESSION_OUTLIER">
      <data key="d4">1.0</data>
      <data key="d5">Standardised residuals are used to identify regression outliers</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="STANDARDISED_RESIDUALS" target="PLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Plots can be used to visualize standardized residuals</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="STANDARDISED_RESIDUALS" target="RULES_OF_THUMB">
      <data key="d4">1.0</data>
      <data key="d5">Rules of thumb can be used to interpret standardized residuals</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="FITTED_MODEL" target="STATISTICS">
      <data key="d4">1.0</data>
      <data key="d5">The fitted model is a product of statistical analysis, which is part of the field of statistics</data>
      <data key="d6">83bb91cf725e5116ca2f5748fddccfae</data>
    </edge>
    <edge source="T_DISTRIBUTION" target="Q_Q_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The t-distribution is used in Q-Q plots when the sample size is small or the population variance is unknown. It is compared to the normal distribution to assess the tail behavior of the data.</data>
      <data key="d6">3bfc9b92571973e54c8095302acc1aaa</data>
    </edge>
    <edge source="T2_DISTRIBUTION" target="Q_Q_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The Q-Q plot is used to compare the observed data against the t2 distribution</data>
      <data key="d6">ee22e1f5947947f9bd3f7f8922745e48</data>
    </edge>
    <edge source="Q_Q_PLOT" target="REFERENCE_LINE">
      <data key="d4">1.0</data>
      <data key="d5">The reference line is plotted on the Q-Q plot to compare the sample quantiles against the theoretical quantiles</data>
      <data key="d6">ee22e1f5947947f9bd3f7f8922745e48</data>
    </edge>
    <edge source="Q_Q_PLOT" target="SAMPLE_QUANTILES">
      <data key="d4">1.0</data>
      <data key="d5">Sample quantiles are plotted on the Q-Q plot to compare against the reference line</data>
      <data key="d6">ee22e1f5947947f9bd3f7f8922745e48</data>
    </edge>
    <edge source="Q_Q_PLOT" target="LEFT_TAIL">
      <data key="d4">1.0</data>
      <data key="d5">The left tail of the distribution is observed to have more observations than expected under a normal distribution</data>
      <data key="d6">ee22e1f5947947f9bd3f7f8922745e48</data>
    </edge>
    <edge source="Q_Q_PLOT" target="RIGHT_TAIL">
      <data key="d4">1.0</data>
      <data key="d5">The right tail of the distribution is observed to have more observations than expected under a normal distribution</data>
      <data key="d6">ee22e1f5947947f9bd3f7f8922745e48</data>
    </edge>
    <edge source="Q_Q_PLOT" target="STANDARD_NORMAL_DENSITY">
      <data key="d4">1.0</data>
      <data key="d5">The standard normal density is compared with the t2 density to illustrate the tail behavior</data>
      <data key="d6">ee22e1f5947947f9bd3f7f8922745e48</data>
    </edge>
    <edge source="Q_Q_PLOT" target="T2_DENSITY">
      <data key="d4">1.0</data>
      <data key="d5">The t2 density is compared with the standard normal density to illustrate the tail behavior</data>
      <data key="d6">ee22e1f5947947f9bd3f7f8922745e48</data>
    </edge>
    <edge source="E" target="M">
      <data key="d4">1.0</data>
      <data key="d5">&#181; is the mean of the normal distribution of log(Y), which is used to calculate the expected value of Y</data>
      <data key="d6">34fceaaf7d835828b5ee2327325c37f8</data>
    </edge>
    <edge source="E" target="YC">
      <data key="d4">1.0</data>
      <data key="d5">E is used to calculate the expected value of Yc</data>
      <data key="d6">1da117a2f92b2db00290d2a0bfc06beb</data>
    </edge>
    <edge source="E" target="EB">
      <data key="d4">1.0</data>
      <data key="d5">E is used to calculate the expected value of the residual vector Eb</data>
      <data key="d6">1da117a2f92b2db00290d2a0bfc06beb</data>
    </edge>
    <edge source="LOG" target="BRAIN_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Logarithmic function is applied to the average body weight in the model to predict brain weight</data>
      <data key="d6">efeeb664622c1ee594e6a08a8322ffe3</data>
    </edge>
    <edge source="LOG" target="L">
      <data key="d4">1.0</data>
      <data key="d5">The logarithmic function is used in the calculation of the likelihood function L</data>
      <data key="d6">e7edd8b2874a350779ae20f1ecdf4733</data>
    </edge>
    <edge source="LOG_Y" target="LOGNORMAL_DISTRIBUTION">
      <data key="d4">1.0</data>
      <data key="d5">log(Y) follows a lognormal distribution when Y is lognormally distributed</data>
      <data key="d6">21e429490eeefe7d9c245058fd48ca68</data>
    </edge>
    <edge source="LOG_Y" target="GEOMETRIC_MEAN_Y">
      <data key="d4">1.0</data>
      <data key="d5">The geometric mean of Y is equal to exp[E(log(Y))] when Y is lognormally distributed</data>
      <data key="d6">21e429490eeefe7d9c245058fd48ca68</data>
    </edge>
    <edge source="BODY_WEIGHT" target="LOG_BODY_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Body weight is transformed into log(body weight) to improve the linearity of the relationship between body weight and brain weight</data>
      <data key="d6">e2422d8b80004aab4ea74d5209587861</data>
    </edge>
    <edge source="BRAIN_WEIGHT" target="LOG_BRAIN_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Brain weight is transformed into log(brain weight) to improve the linearity of the relationship between body weight and brain weight</data>
      <data key="d6">e2422d8b80004aab4ea74d5209587861</data>
    </edge>
    <edge source="BRAIN_WEIGHT" target="AVERAGE_BODY_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Brain weight is predicted based on the average body weight, with a 7.4% increase for a 10% larger average body weight</data>
      <data key="d6">efeeb664622c1ee594e6a08a8322ffe3</data>
    </edge>
    <edge source="HUMAN" target="INFLUENTIAL_OBSERVATION">
      <data key="d4">1.0</data>
      <data key="d5">Human is flagged as an influential observation in the mammals dataset</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="BRAIN" target="BODY">
      <data key="d4">1.0</data>
      <data key="d5">Brain weight and body weight are variables in the mammals dataset that are plotted against each other in a scatterplot</data>
      <data key="d6">f9d6c3504b8f8b5c25550076e45f8270</data>
    </edge>
    <edge source="LOG_BODY_WEIGHT" target="LOG_BRAIN_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">The relationship between log(body weight) and log(brain weight) is likely to be linear, unlike the non-linear relationship between the original body weight and brain weight variables</data>
      <data key="d6">e2422d8b80004aab4ea74d5209587861</data>
    </edge>
    <edge source="LOG_BODY_WEIGHT" target="RESIDUALS_VS_LEVERAGE_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The residuals vs leverage plot uses the residuals from the regression of log brain weight on log body weight to identify influential data points</data>
      <data key="d6">9e2ebbb113c00fa43f0af3c0696baf95</data>
    </edge>
    <edge source="LOG_BRAIN_WEIGHT" target="RESIDUALS_VS_LEVERAGE_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The residuals vs leverage plot uses the residuals from the regression of log brain weight on log body weight to identify influential data points</data>
      <data key="d6">9e2ebbb113c00fa43f0af3c0696baf95</data>
    </edge>
    <edge source="LOG_BRAINI" target="BETA_HAT1">
      <data key="d4">1.0</data>
      <data key="d5">LOG_BRAINi is the log-transformed response variable in the model, which is related to the estimated slope coefficient BETA_HAT1</data>
      <data key="d6">86c401dda130c2d201c3339526062a24</data>
    </edge>
    <edge source="LOG_BODYI" target="BETA_HAT1">
      <data key="d4">1.0</data>
      <data key="d5">LOG_BODYi is the log-transformed explanatory variable in the model, which is related to the estimated slope coefficient BETA_HAT1</data>
      <data key="d6">86c401dda130c2d201c3339526062a24</data>
    </edge>
    <edge source="BETA_HAT1" target="YBA">
      <data key="d4">1.0</data>
      <data key="d5">BETA_HAT1 is used to predict the average brain weight YbA for species A</data>
      <data key="d6">86c401dda130c2d201c3339526062a24</data>
    </edge>
    <edge source="BETA_HAT1" target="YBB">
      <data key="d4">1.0</data>
      <data key="d5">BETA_HAT1 is used to predict the average brain weight YbB for species B</data>
      <data key="d6">86c401dda130c2d201c3339526062a24</data>
    </edge>
    <edge source="BETA_HAT1" target="SIGMA_SQUARED_MLE">
      <data key="d4">1.0</data>
      <data key="d5">Beta hat 1 (b&#946;1) is used in the calculation of Sigma squared MLE (&#963;b^2_MLE)</data>
      <data key="d6">09caa54ca1372d152e47051be4d44ede</data>
    </edge>
    <edge source="GIRTH" target="VOLUME">
      <data key="d4">1.0</data>
      <data key="d5">Volume is predicted to be proportional to the square of the diameter (girth) and the height of the tree</data>
      <data key="d6">efeeb664622c1ee594e6a08a8322ffe3</data>
    </edge>
    <edge source="VOLUME" target="TREE">
      <data key="d4">1.0</data>
      <data key="d5">The volume of timber is a property of the tree</data>
      <data key="d6">9611ea31ff53888971694cdefe806f64</data>
    </edge>
    <edge source="VOLUMEI" target="DIAMETERI">
      <data key="d4">1.0</data>
      <data key="d5">Volumei is calculated using Diameteri as one of the independent variables in the model</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="VOLUMEI" target="HEIGHTI">
      <data key="d4">1.0</data>
      <data key="d5">Volumei is calculated using Heighti as one of the independent variables in the model</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="VOLUMEI" target="&#917;I">
      <data key="d4">1.0</data>
      <data key="d5">&#1013;i is the error term in the model used to calculate Volumei</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="VOLUMEI" target="SCATTERPLOT_MATRIX">
      <data key="d4">1.0</data>
      <data key="d5">Scatterplot matrix includes scatterplots of Volumei against other variables for initial exploratory analysis</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="DIAMETERI" target="SCATTERPLOT_MATRIX">
      <data key="d4">1.0</data>
      <data key="d5">Scatterplot matrix includes scatterplots of Diameteri against other variables for initial exploratory analysis</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="HEIGHTI" target="SCATTERPLOT_MATRIX">
      <data key="d4">1.0</data>
      <data key="d5">Scatterplot matrix includes scatterplots of Heighti against other variables for initial exploratory analysis</data>
      <data key="d6">60cc94e681863c9fcc6f9be1e500f840</data>
    </edge>
    <edge source="LOG_TRANSFORMED_TREE_DATA" target="TREES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The log-transformed tree data is used as the input data for the trees.model, which is a linear model</data>
      <data key="d6">a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="LOGTREES" target="TREES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The logTrees data frame is used as the data input for the trees.model</data>
      <data key="d6">a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="LOGTREES" target="TREES.MODEL">
      <data key="d4">1.0</data>
      <data key="d5">logTrees is the data frame used in the linear model trees.model</data>
      <data key="d6">9a28a6420fca4405488ca35762f9dc28</data>
    </edge>
    <edge source="TREES_MODEL" target="LOG_DIAMETER">
      <data key="d4">1.0</data>
      <data key="d5">Log-diameter is one of the parameters estimated by the trees.model</data>
      <data key="d6">a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="TREES_MODEL" target="LOG_HEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Log-height is one of the parameters estimated by the trees.model</data>
      <data key="d6">a60af43e42c72a41fa90da06beb29d1b</data>
    </edge>
    <edge source="LOG_DIAMETER" target="LOG_VOLUME">
      <data key="d4">1.0</data>
      <data key="d5">Log-volume is regressed on log-diameter, indicating a relationship where changes in log-diameter are associated with changes in log-volume</data>
      <data key="d6">d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="LOG_DIAMETER" target="COEFFICIENT_LOG_DIAMETER">
      <data key="d4">1.0</data>
      <data key="d5">The coefficient for log-diameter is associated with the independent variable log-diameter in the regression model</data>
      <data key="d6">d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="LOG_HEIGHT" target="LOG_VOLUME">
      <data key="d4">1.0</data>
      <data key="d5">Log-volume is regressed on log-height, indicating a relationship where changes in log-height are associated with changes in log-volume</data>
      <data key="d6">d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="TREES.MODEL" target="(INTERCEPT)">
      <data key="d4">1.0</data>
      <data key="d5">(Intercept) is the intercept coefficient in the linear model trees.model</data>
      <data key="d6">9a28a6420fca4405488ca35762f9dc28</data>
    </edge>
    <edge source="TREES.MODEL" target="LOGDIAMETER">
      <data key="d4">1.0</data>
      <data key="d5">logDiameter is a predictor variable in the linear model trees.model</data>
      <data key="d6">9a28a6420fca4405488ca35762f9dc28</data>
    </edge>
    <edge source="TREES.MODEL" target="LOGHEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">logHeight is a predictor variable in the linear model trees.model</data>
      <data key="d6">9a28a6420fca4405488ca35762f9dc28</data>
    </edge>
    <edge source="TREES.MODEL" target="LOGVOLUME">
      <data key="d4">1.0</data>
      <data key="d5">logVolume is the response variable in the linear model trees.model</data>
      <data key="d6">9a28a6420fca4405488ca35762f9dc28</data>
    </edge>
    <edge source="COEFFICIENT_LOG_DIAMETER" target="INTERCEPT_CYLINDER">
      <data key="d4">1.0</data>
      <data key="d5">The coefficient for log-diameter is used to calculate the intercept when the shape of a tree is approximated by a cylinder</data>
      <data key="d6">d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="COEFFICIENT_LOG_DIAMETER" target="INTERCEPT_CONE">
      <data key="d4">1.0</data>
      <data key="d5">The coefficient for log-diameter is used to calculate the intercept when the shape of a tree is approximated by a cone</data>
      <data key="d6">d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="INTERCEPT_CYLINDER" target="ESTIMATED_INTERCEPT">
      <data key="d4">1.0</data>
      <data key="d5">The intercept for the cylinder approximation is compared with the estimated intercept from the regression model</data>
      <data key="d6">d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="INTERCEPT_CONE" target="ESTIMATED_INTERCEPT">
      <data key="d4">1.0</data>
      <data key="d5">The intercept for the cone approximation is compared with the estimated intercept from the regression model</data>
      <data key="d6">d71b402ab9edbb4347e09c7af3257cf5</data>
    </edge>
    <edge source="TRANSFORMATIONS" target="COMPLETE_DATASET">
      <data key="d4">1.0</data>
      <data key="d5">Transformations can be applied to the complete dataset to reduce the influence of data points</data>
      <data key="d6">83bb91cf725e5116ca2f5748fddccfae</data>
    </edge>
    <edge source="TRANSFORMATIONS" target="ROBUST_REGRESSION">
      <data key="d4">1.0</data>
      <data key="d5">Transformations, such as log transformation, can be seen as a less systematic approach compared to robust regression</data>
      <data key="d6">83bb91cf725e5116ca2f5748fddccfae</data>
    </edge>
    <edge source="S" target="A">
      <data key="d4">2.0</data>
      <data key="d5">Vector a is used in the calculation of S(&#946;) through various vector operationsMatrix A is used in the calculation of S(&#946;) through matrix multiplication</data>
      <data key="d6">2167274129d4cfa74a002c4cc39df8a8</data>
    </edge>
    <edge source="S" target="B">
      <data key="d4">2.0</data>
      <data key="d5">Vector b is used in the calculation of S(&#946;) through various vector operationsMatrix B is used in the calculation of S(&#946;) through matrix multiplication</data>
      <data key="d6">2167274129d4cfa74a002c4cc39df8a8</data>
    </edge>
    <edge source="S" target="J">
      <data key="d4">1.0</data>
      <data key="d5">J is used in the calculation of the function S, which represents the sum of squared errors</data>
      <data key="d6">eac62cdd5518e1269fed150639331c2c</data>
    </edge>
    <edge source="S" target="XTX">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, XTX is recognized as a symmetric matrix, playing a pivotal role in the computation of the function S. Specifically, XTX is integral to the second term of S(&#946;), a function that involves a sum of squares. Given the nature of a sum of squares, it is inherently non-negative, a characteristic that underscores the importance of XTX in ensuring the mathematical integrity of S(&#946;). The symmetric property of XTX further facilitates the calculation, contributing to the robustness and reliability of the statistical model that S represents.</data>
      <data key="d6">a4a817bb79d6ae8812c808ca41d47f43,eac62cdd5518e1269fed150639331c2c</data>
    </edge>
    <edge source="S" target="XTY">
      <data key="d4">1.0</data>
      <data key="d5">XTY is a vector that is used in the calculation of the function S</data>
      <data key="d6">eac62cdd5518e1269fed150639331c2c</data>
    </edge>
    <edge source="S" target="PARTIAL_DERIVATIVE">
      <data key="d4">1.0</data>
      <data key="d5">The partial derivative of S(&#946;) with respect to Beta_0 is used to find the least squares estimate of Beta_0</data>
      <data key="d6">10ac76f99674a01ca0f4a55586dea07e</data>
    </edge>
    <edge source="S" target="PARTIAL_DERIVATIVES">
      <data key="d4">1.0</data>
      <data key="d5">The function S(&#946;) is differentiated to obtain the partial derivatives with respect to Beta_0 and Beta_1</data>
      <data key="d6">416494d940a9f505da9853caca26fe63</data>
    </edge>
    <edge source="S" target="L">
      <data key="d4">1.0</data>
      <data key="d5">S(&#946;) is derived from the likelihood function L(&#946;, &#963;^2|y) and is used in the calculation of the least squares estimate Beta hat (&#946;b)</data>
      <data key="d6">f632f01188d2c6e3091a965580cb4600</data>
    </edge>
    <edge source="S" target="RI">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entities S and RI are integral components of residual analysis. S, denoted as the estimated standard deviation, plays a crucial role in the calculation of the standardised residual RI. This standardised residual is a measure that helps in understanding the magnitude of a residual in relation to the estimated variability of the residuals. The calculation of RI involves S, which is derived from the unbiased estimate of the error variance, ensuring that the residual analysis is robust and reflective of the true variability within the data. This unbiased estimate, often referred to as s, is critical for obtaining accurate and reliable standardised residuals, thereby facilitating a deeper understanding of the model's fit and the underlying data structure.</data>
      <data key="d6">90b7e0427699cc1bb461e37939935138,c47968226557bc2eb5aec5bb7994fd0e</data>
    </edge>
    <edge source="S_BETA" target="BETA_HAT_0">
      <data key="d4">1.0</data>
      <data key="d5">S(&#946;) is minimized at the value of Beta_hat_0 (b&#946;0), which is the least squares estimate of Beta_0</data>
      <data key="d6">8f7a05b6d231105a6194eebdb2df372e</data>
    </edge>
    <edge source="S_BETA" target="BETA_HAT_1">
      <data key="d4">1.0</data>
      <data key="d5">S(&#946;) is minimized at the value of Beta_hat_1 (b&#946;1), which is the least squares estimate of Beta_1</data>
      <data key="d6">8f7a05b6d231105a6194eebdb2df372e</data>
    </edge>
    <edge source="BETA_HAT_0" target="Y_BAR">
      <data key="d4">1.0</data>
      <data key="d5">Y_bar is used in the calculation of Beta_hat_0 (b&#946;0)</data>
      <data key="d6">8f7a05b6d231105a6194eebdb2df372e</data>
    </edge>
    <edge source="BETA_HAT_0" target="X_BAR">
      <data key="d4">1.0</data>
      <data key="d5">X_bar is used in the calculation of Beta_hat_0 (b&#946;0)</data>
      <data key="d6">8f7a05b6d231105a6194eebdb2df372e</data>
    </edge>
    <edge source="BETA_HAT_0" target="BETA_HAT_1">
      <data key="d4">1.0</data>
      <data key="d5">Beta_hat_0 and Beta_hat_1 are the least squares estimates for the original model (2)</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="BETA_HAT_0" target="Y_HAT_J">
      <data key="d4">1.0</data>
      <data key="d5">Y_hat_j is calculated using Beta_hat_0 as part of the fitted value</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="BETA_HAT_1" target="X_BAR">
      <data key="d4">1.0</data>
      <data key="d5">X_bar is used in the calculation of Beta_hat_1 (b&#946;1)</data>
      <data key="d6">8f7a05b6d231105a6194eebdb2df372e</data>
    </edge>
    <edge source="BETA_HAT_1" target="Y_BAR">
      <data key="d4">1.0</data>
      <data key="d5">Y_bar is used in the calculation of Beta_hat_1 (b&#946;1)</data>
      <data key="d6">8f7a05b6d231105a6194eebdb2df372e</data>
    </edge>
    <edge source="BETA_HAT_1" target="Y_HAT_J">
      <data key="d4">1.0</data>
      <data key="d5">Y_hat_j is calculated using Beta_hat_1 as part of the fitted value</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="Y_BAR" target="BETA_0_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Y_bar (y&#175;) is used in the calculation of Beta_0 hat (b&#946;0) as part of the least squares estimation</data>
      <data key="d6">3fdeeb7593174f5e8a9cff55a7cd92e3</data>
    </edge>
    <edge source="Y_BAR" target="BETA_1_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Y_bar (y&#175;) is used in the calculation of Beta_1 hat (b&#946;1) as part of the least squares estimation</data>
      <data key="d6">3fdeeb7593174f5e8a9cff55a7cd92e3</data>
    </edge>
    <edge source="Y_BAR" target="BETA1_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Y_bar is used in the calculation of Beta1_hat, the least squares estimator for &#946;1</data>
      <data key="d6">f9e7b2eac9f82681301da3d1e2f23328</data>
    </edge>
    <edge source="Y_BAR" target="BETA0_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Y_bar is used in the calculation of Beta0_hat, the least squares estimator for &#946;0</data>
      <data key="d6">f9e7b2eac9f82681301da3d1e2f23328</data>
    </edge>
    <edge source="Y_BAR" target="SXY">
      <data key="d4">1.0</data>
      <data key="d5">Y_bar is used in the calculation of Sxy as part of the covariance between x and y</data>
      <data key="d6">f5716ce115458c0652124734ca344806</data>
    </edge>
    <edge source="X_BAR" target="BETA_0_HAT">
      <data key="d4">1.0</data>
      <data key="d5">X_bar (x&#175;) is used in the calculation of Beta_0 hat (b&#946;0) as part of the least squares estimation</data>
      <data key="d6">3fdeeb7593174f5e8a9cff55a7cd92e3</data>
    </edge>
    <edge source="X_BAR" target="BETA_1_HAT">
      <data key="d4">1.0</data>
      <data key="d5">X_bar (x&#175;) is used in the calculation of Beta_1 hat (b&#946;1) as part of the least squares estimation</data>
      <data key="d6">3fdeeb7593174f5e8a9cff55a7cd92e3</data>
    </edge>
    <edge source="X_BAR" target="BETA1_HAT">
      <data key="d4">1.0</data>
      <data key="d5">X_bar is used in the calculation of Beta1_hat, the least squares estimator for &#946;1</data>
      <data key="d6">f9e7b2eac9f82681301da3d1e2f23328</data>
    </edge>
    <edge source="X_BAR" target="BETA0_HAT">
      <data key="d4">1.0</data>
      <data key="d5">X_bar is used in the calculation of Beta0_hat, the least squares estimator for &#946;0</data>
      <data key="d6">f9e7b2eac9f82681301da3d1e2f23328</data>
    </edge>
    <edge source="X_BAR" target="XTX">
      <data key="d4">1.0</data>
      <data key="d5">X_bar is used in the calculation of the elements of the matrix XTX</data>
      <data key="d6">56ff186fc629e1e42f2759fc4b984199</data>
    </edge>
    <edge source="X_BAR" target="SXY">
      <data key="d4">1.0</data>
      <data key="d5">X_bar is used in the calculation of Sxy as part of the covariance between x and y</data>
      <data key="d6">f5716ce115458c0652124734ca344806</data>
    </edge>
    <edge source="X_BAR" target="ALPHA">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the reparameterised model (1), Bar X (X_BAR), a transformation of the explanatory variable, plays a crucial role. This transformed variable is not only pivotal in the model's structure but also significantly contributes to the calculation of Alpha (ALPHA). The utilization of X_BAR in the computation of ALPHA highlights its importance in understanding the underlying relationships and structure within the model. This interrelation between X_BAR and ALPHA underscores the complex dynamics of the reparameterised model, where the transformed explanatory variable is essential for estimating key parameters.</data>
      <data key="d6">5609007c6229060ffc85d8056a7fefde,82932abd152e0b84a1c26a2daa4c08df</data>
    </edge>
    <edge source="X_BAR" target="X_ALPHA">
      <data key="d4">1.0</data>
      <data key="d5">Mean X_bar is subtracted from each explanatory variable observation in X to obtain X_alpha</data>
      <data key="d6">6c66e9414880964ee899ceb0f16d22e9</data>
    </edge>
    <edge source="X_BAR" target="HII">
      <data key="d4">1.0</data>
      <data key="d5">The sample mean (x_bar) of the explanatory variable is used in calculating the leverage (hii) of the data points</data>
      <data key="d6">6ee02b38ae842fd5eac9a11c4fd6659f</data>
    </edge>
    <edge source="X_BAR" target="X_I">
      <data key="d4">1.0</data>
      <data key="d5">X_i is compared to X_bar to determine the leverage of the observation</data>
      <data key="d6">bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </edge>
    <edge source="BETA_0_HAT" target="VARIANCE_BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">VARIANCE_BETA_HAT includes the variance of BETA_0_HAT (b&#946;0)</data>
      <data key="d6">69ffba28a61d98d8d18f91c24b74dd4a</data>
    </edge>
    <edge source="BETA_0_HAT" target="COVARIANCE_BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">COVARIANCE_BETA_HAT includes the covariance between BETA_0_HAT (b&#946;0) and BETA_1_HAT (b&#946;1)</data>
      <data key="d6">69ffba28a61d98d8d18f91c24b74dd4a</data>
    </edge>
    <edge source="BETA_1_HAT" target="SXX">
      <data key="d4">1.0</data>
      <data key="d5">Sxx is used in the calculation of Beta_1 hat (b&#946;1) as part of the least squares estimation</data>
      <data key="d6">3fdeeb7593174f5e8a9cff55a7cd92e3</data>
    </edge>
    <edge source="BETA_1_HAT" target="SXY">
      <data key="d4">1.0</data>
      <data key="d5">Sxy is used in the calculation of Beta_1 hat (b&#946;1) as part of the least squares estimation</data>
      <data key="d6">3fdeeb7593174f5e8a9cff55a7cd92e3</data>
    </edge>
    <edge source="BETA_1_HAT" target="VARIANCE_BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">VARIANCE_BETA_HAT includes the variance of BETA_1_HAT (b&#946;1)</data>
      <data key="d6">69ffba28a61d98d8d18f91c24b74dd4a</data>
    </edge>
    <edge source="BETA_1_HAT" target="COVARIANCE_BETA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">COVARIANCE_BETA_HAT includes the covariance between BETA_0_HAT (b&#946;0) and BETA_1_HAT (b&#946;1)</data>
      <data key="d6">69ffba28a61d98d8d18f91c24b74dd4a</data>
    </edge>
    <edge source="SXX" target="BETA1_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Sxx is directly used in the calculation of Beta1_hat, the least squares estimator for &#946;1</data>
      <data key="d6">f9e7b2eac9f82681301da3d1e2f23328</data>
    </edge>
    <edge source="SXX" target="XTX">
      <data key="d4">1.0</data>
      <data key="d5">Sxx is used in the calculation of the inverse of the matrix XTX</data>
      <data key="d6">56ff186fc629e1e42f2759fc4b984199</data>
    </edge>
    <edge source="SXX" target="ALPHA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Sxx is used in the calculation of Alpha hat (&#945;b) as part of the least squares estimator in the reparameterized model</data>
      <data key="d6">f5716ce115458c0652124734ca344806</data>
    </edge>
    <edge source="SXY" target="BETA1_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Sxy is directly used in the calculation of Beta1_hat, the least squares estimator for &#946;1</data>
      <data key="d6">f9e7b2eac9f82681301da3d1e2f23328</data>
    </edge>
    <edge source="BETA1_HAT" target="BETA0_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Beta1_hat is used in the calculation of Beta0_hat, the least squares estimator for &#946;0</data>
      <data key="d6">f9e7b2eac9f82681301da3d1e2f23328</data>
    </edge>
    <edge source="BETA1_HAT" target="LOG_LIKELIHOOD">
      <data key="d4">1.0</data>
      <data key="d5">Beta1 hat is the value of Beta1 that maximizes the log-likelihood function</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12</data>
    </edge>
    <edge source="BETA0_HAT" target="LOG_LIKELIHOOD">
      <data key="d4">1.0</data>
      <data key="d5">Beta0 hat is the value of Beta0 that maximizes the log-likelihood function</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12</data>
    </edge>
    <edge source="XT" target="XTX">
      <data key="d4">1.0</data>
      <data key="d5">Matrix XT is multiplied by matrix X to form matrix XTX</data>
      <data key="d6">254a8a17b1be06702934341e3bf41e85</data>
    </edge>
    <edge source="XT" target="XTY">
      <data key="d4">1.0</data>
      <data key="d5">Matrix XT is multiplied by vector Y to form vector XTY</data>
      <data key="d6">254a8a17b1be06702934341e3bf41e85</data>
    </edge>
    <edge source="XT" target="ALPHA_HAT">
      <data key="d4">1.0</data>
      <data key="d5">XT is used in the calculation of Alpha hat (&#945;b) as part of the least squares estimator</data>
      <data key="d6">f5716ce115458c0652124734ca344806</data>
    </edge>
    <edge source="XT" target="H">
      <data key="d4">1.0</data>
      <data key="d5">Matrix H is calculated using the transpose of matrix X, making it dependent on XT</data>
      <data key="d6">46629f2efc6c82e81265a131b4bab2ee</data>
    </edge>
    <edge source="XTX" target="RANK">
      <data key="d4">1.0</data>
      <data key="d5">The rank of XTX is equal to p, indicating that XTX is non-singular and its inverse exists</data>
      <data key="d6">9d300fc83afb3261af61b2ab9721cadc</data>
    </edge>
    <edge source="XTX" target="H">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entities XTX and H are central to understanding the structure and relations within a given dataset. XTX represents the product of matrix X transposed and multiplied by itself, serving as a fundamental component in various statistical models, particularly in linear regression analysis. This matrix plays a crucial role in calculating the least squares estimators, which are essential for estimating the parameters of a linear regression model.

Matrix H, on the other hand, is derived from XTX through the formula H = X(XTX)^-1XT. This formula indicates that H is directly dependent on XTX, highlighting the interrelation between these two entities. The calculation of H is pivotal in residual analysis, where it is used to understand the influence of each data point on the fitted values in a regression model.

Furthermore, the trace of matrix H, denoted as tr(H), is also computed using XTX. Specifically, tr(H) = tr(XTX(XTX)^-1XT), which simplifies to the trace of XTX due to the properties of matrix multiplication and the trace function. This calculation provides insights into the degree of influence that the independent variables have on the dependent variable in a regression context.

In summary, XTX and H are intricately linked, with H being a function of XTX and both playing critical roles in statistical inference, particularly in the context of linear regression and residual analysis. The trace of H, which is derived from XTX, offers additional information about the model's structure and the influence of the data points.</data>
      <data key="d6">46629f2efc6c82e81265a131b4bab2ee,bd98ac7b4b5df4f63e7ecc8f4a821f57</data>
    </edge>
    <edge source="A" target="F_BETA">
      <data key="d4">1.0</data>
      <data key="d5">A is the matrix in the function f(&#946;) = &#946;T A&#946;</data>
      <data key="d6">21ec28dfe2b2c18030d541d63e51f45e</data>
    </edge>
    <edge source="A" target="GAMMA">
      <data key="d4">3.0</data>
      <data key="d5">In the context of the reparameterized model, A serves as the pivotal matrix transformation that facilitates the conversion of Beta into Gamma. This relationship underscores the invertibility of matrix A, highlighting its critical role in establishing the connection between Beta and Gamma. Through A's application, the model undergoes a reparameterization that enables the derivation of Gamma from Beta, emphasizing the interdependence and structural relationship between these entities within the model's framework.</data>
      <data key="d6">82932abd152e0b84a1c26a2daa4c08df,d94760a5f9f6ea115fcc18024035a627,f5716ce115458c0652124734ca344806</data>
    </edge>
    <edge source="A" target="ALPHA">
      <data key="d4">1.0</data>
      <data key="d5">A is the transformation matrix used to convert Beta into Alpha</data>
      <data key="d6">5609007c6229060ffc85d8056a7fefde</data>
    </edge>
    <edge source="A" target="X_ALPHA">
      <data key="d4">1.0</data>
      <data key="d5">Matrix A is used to transform X into X_alpha for the reparameterised model</data>
      <data key="d6">6c66e9414880964ee899ceb0f16d22e9</data>
    </edge>
    <edge source="A" target="AZ">
      <data key="d4">1.0</data>
      <data key="d5">A transforms Z into AZ, changing its distribution</data>
      <data key="d6">542f546c5a131196e4701fb33c9b1dee</data>
    </edge>
    <edge source="A" target="U">
      <data key="d4">2.0</data>
      <data key="d5">In the context of matrix transformations, Matrix A plays a pivotal role in converting the vector Z into another vector, U. This conversion is achieved through the operation defined as AZ, where A is the transformation matrix. Consequently, U emerges as the resultant vector post the application of Matrix A on Z, encapsulating the linear transformation's effect on the original vector Z. This highlights the interplay between the entities A, Z, and U, where A's properties and Z's initial state determine the characteristics of the transformed vector U.</data>
      <data key="d6">0ac60299320c55d642b3e38440c25f90,aac5b4f040b9c773bd1aa696dec469f6</data>
    </edge>
    <edge source="B" target="V">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the given entities and descriptions, Matrix B plays a pivotal role in the transformation process of a variable denoted as Z. Specifically, Matrix B is applied to Z through a matrix multiplication operation, resulting in the generation of a new variable, V. This transformation, represented as V = BZ, highlights the direct relationship between the input Z and the output V, where V is the resultant vector or matrix after undergoing the linear transformation defined by Matrix B. This process underscores the fundamental application of linear algebra in data manipulation and analysis, where matrices like B are utilized to alter the structure and properties of data represented by Z, yielding V as the transformed output.</data>
      <data key="d6">0ac60299320c55d642b3e38440c25f90,aac5b4f040b9c773bd1aa696dec469f6</data>
    </edge>
    <edge source="ALPHA_HAT" target="GAMMA">
      <data key="d4">1.0</data>
      <data key="d5">Gamma is the new parameter vector that Alpha hat (&#945;b) estimates in the reparameterized model</data>
      <data key="d6">f5716ce115458c0652124734ca344806</data>
    </edge>
    <edge source="ALPHA_HAT" target="Z">
      <data key="d4">1.0</data>
      <data key="d5">Z is the new design matrix used in the calculation of Alpha hat (&#945;b) as part of the least squares estimator in the reparameterized model</data>
      <data key="d6">f5716ce115458c0652124734ca344806</data>
    </edge>
    <edge source="GAMMA" target="GAMMA_HAT">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis and algorithmic modeling, GAMMA represents the true parameter that is being estimated by GAMMA_HAT (\u03b3b) within both the linear regression model and the reparameterised model. GAMMA serves as the foundational truth against which the performance of GAMMA_HAT is evaluated, ensuring the accuracy and reliability of predictions made by these models. The relationship between GAMMA and GAMMA_HAT is central to understanding the precision of estimations in the models under consideration.</data>
      <data key="d6">82932abd152e0b84a1c26a2daa4c08df,d94760a5f9f6ea115fcc18024035a627</data>
    </edge>
    <edge source="Z" target="AZ">
      <data key="d4">1.0</data>
      <data key="d5">Z is transformed by A to produce AZ, changing its distribution</data>
      <data key="d6">542f546c5a131196e4701fb33c9b1dee</data>
    </edge>
    <edge source="Z" target="SIGMA_HAT_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">z1, ..., zn are the observed values used in the calculation of the unbiased estimator &#963;b^2 for the variance</data>
      <data key="d6">6648f0d6deed51fb4fb25e6992a71ddf</data>
    </edge>
    <edge source="Z" target="MULTIVARIATE_NORMAL_DISTRIBUTION">
      <data key="d4">1.0</data>
      <data key="d5">Z follows the multivariate normal distribution with mean &#181; and covariance matrix &#931;</data>
      <data key="d6">aac5b4f040b9c773bd1aa696dec469f6</data>
    </edge>
    <edge source="Z" target="U">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the given data, the entities Z and U are related through a transformation process. Specifically, the vector Z is transformed by the application of matrix A, resulting in the vector U. This transformation indicates a linear relationship between Z and U, where U can be mathematically represented as U = A * Z, assuming A is a matrix capable of transforming the vector Z into U. This process is fundamental in various fields such as linear algebra, signal processing, and data science, where transformations are used to manipulate data for analysis or to fit it into a model. The transformation from Z to U using matrix A could imply a change in the basis, a linear combination of the original components, or a projection onto a new space, depending on the properties of matrix A.</data>
      <data key="d6">0ac60299320c55d642b3e38440c25f90,aac5b4f040b9c773bd1aa696dec469f6</data>
    </edge>
    <edge source="Z" target="V">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the given entities and descriptions, Vector Z undergoes a transformation through the application of matrix B, resulting in the generation of Vector V. This process signifies that V is a direct outcome of the linear transformation of Z by matrix B. Both descriptions are consistent in conveying the relationship between Z, V, and the transformation matrix B, highlighting the pivotal role of matrix B in the conversion of Z into V.</data>
      <data key="d6">0ac60299320c55d642b3e38440c25f90,aac5b4f040b9c773bd1aa696dec469f6</data>
    </edge>
    <edge source="Z" target="CHI_SQUARED_DISTRIBUTION">
      <data key="d4">1.0</data>
      <data key="d5">The quadratic form (Z - &#181;)T &#931;-1 (Z - &#181;) follows a chi-squared distribution with n degrees of freedom, as stated in Lemma 10.2</data>
      <data key="d6">aac5b4f040b9c773bd1aa696dec469f6</data>
    </edge>
    <edge source="Z" target="S2">
      <data key="d4">1.0</data>
      <data key="d5">S2 is the unbiased estimator for the error variance, as mentioned in the context of distributional properties</data>
      <data key="d6">aac5b4f040b9c773bd1aa696dec469f6</data>
    </edge>
    <edge source="GAMMA_HAT" target="AT">
      <data key="d4">1.0</data>
      <data key="d5">AT is used in the calculation of Gamma hat (&#947;b) as part of the least squares estimator</data>
      <data key="d6">82932abd152e0b84a1c26a2daa4c08df</data>
    </edge>
    <edge source="ALPHA" target="A_INVERSE">
      <data key="d4">1.0</data>
      <data key="d5">A_inverse is the inverse transformation matrix used to convert Alpha back into Beta</data>
      <data key="d6">5609007c6229060ffc85d8056a7fefde</data>
    </edge>
    <edge source="ALPHA" target="X_ALPHA">
      <data key="d4">1.0</data>
      <data key="d5">X_alpha is the design matrix of the reparameterised model, obtained by transforming X_beta using A_inverse</data>
      <data key="d6">5609007c6229060ffc85d8056a7fefde</data>
    </edge>
    <edge source="ALPHA" target="C">
      <data key="d4">1.0</data>
      <data key="d5">Constant c is used to multiply all observations of the explanatory variable in the reparameterised model, affecting the relationship between Beta and Alpha</data>
      <data key="d6">6c66e9414880964ee899ceb0f16d22e9</data>
    </edge>
    <edge source="ALPHA_HAT_0" target="ALPHA_HAT_1">
      <data key="d4">1.0</data>
      <data key="d5">Alpha_hat_0 and Alpha_hat_1 are the least squares estimates for the reparameterised model (1)</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="MAYA" target="AMBIENT_TEMPERATURE">
      <data key="d4">1.0</data>
      <data key="d5">Maya has fitted a simple linear regression model using ambient temperature as the explanatory variable</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="AMBIENT_TEMPERATURE" target="FAHRENHEIT">
      <data key="d4">1.0</data>
      <data key="d5">Ambient temperature can be measured in Fahrenheit instead of Celsius</data>
      <data key="d6">d1b6fcd55d937c5fe2d6add69e0bcf05</data>
    </edge>
    <edge source="RESIDSS" target="S2">
      <data key="d4">1.0</data>
      <data key="d5">ResidSS is the residual sum of squares used in the calculation of the unbiased estimate s^2</data>
      <data key="d6">fc5b725f3c662c5471af20efdcc2dbff</data>
    </edge>
    <edge source="LSE" target="SHEATHER_BOOK">
      <data key="d4">1.0</data>
      <data key="d5">A Modern Approach to Regression with R (2009) by Sheather provides further reading on least squares estimation</data>
      <data key="d6">9fc2b1e8b2b61b557f88eb9e9c708597</data>
    </edge>
    <edge source="LOG_LIKELIHOOD" target="BETAK_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Beta_k hat is the value of Beta_k that maximizes the log-likelihood function</data>
      <data key="d6">87b717ba065d6d7c7431af284137eb12</data>
    </edge>
    <edge source="PI" target="L">
      <data key="d4">1.0</data>
      <data key="d5">Pi (&#960;) is used in the calculation of the likelihood function L</data>
      <data key="d6">e7edd8b2874a350779ae20f1ecdf4733</data>
    </edge>
    <edge source="D" target="L">
      <data key="d4">1.0</data>
      <data key="d5">D is the deviance or residual sum of squares, which is part of the likelihood function L</data>
      <data key="d6">e7edd8b2874a350779ae20f1ecdf4733</data>
    </edge>
    <edge source="L" target="SIGMA_HAT_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">Sigma hat squared (&#963;b^2MLE) is the maximum likelihood estimate of Sigma squared derived from the likelihood function L(&#946;, &#963;^2|y)</data>
      <data key="d6">f632f01188d2c6e3091a965580cb4600</data>
    </edge>
    <edge source="SIGMA_HAT_SQUARED" target="BIAS">
      <data key="d4">1.0</data>
      <data key="d5">The estimator for the error variance Sigma squared (&#963;b^2MLE) is biased</data>
      <data key="d6">f632f01188d2c6e3091a965580cb4600</data>
    </edge>
    <edge source="SIGMA_HAT_SQUARED" target="ERROR_VARIANCE">
      <data key="d4">1.0</data>
      <data key="d5">Error variance (&#963;^2) is estimated by Sigma hat squared (&#963;b^2MLE)</data>
      <data key="d6">f632f01188d2c6e3091a965580cb4600</data>
    </edge>
    <edge source="SIGMA_HAT_SQUARED" target="Z_BAR">
      <data key="d4">1.0</data>
      <data key="d5">Z_bar is the sample mean used in the calculation of the unbiased estimator &#963;b^2 for the variance</data>
      <data key="d6">6648f0d6deed51fb4fb25e6992a71ddf</data>
    </edge>
    <edge source="SIGMA_HAT_SQUARED" target="SIGMA_HAT_SQUARED_BIAS">
      <data key="d4">1.0</data>
      <data key="d5">&#963;b^2 (biased) is a related but biased version of the unbiased estimator &#963;b^2 for the variance</data>
      <data key="d6">6648f0d6deed51fb4fb25e6992a71ddf</data>
    </edge>
    <edge source="ERROR_VARIANCE" target="STANDARDISED_RESIDUAL">
      <data key="d4">1.0</data>
      <data key="d5">The standardised residual (ri) is calculated using an unbiased estimate of the error variance (&#963;^2)</data>
      <data key="d6">7e05f1b457a496c8b3630e7044fc5981</data>
    </edge>
    <edge source="ERROR_VARIANCE" target="S_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">s^2 is an unbiased estimate of &#963;^2, the error variance in the regression model</data>
      <data key="d6">09391efd3b8c510205098b548bc8dc74</data>
    </edge>
    <edge source="ERROR_VARIANCE" target="T_STATISTIC">
      <data key="d4">1.0</data>
      <data key="d5">The unbiased estimator for the error variance in a linear model is used in the calculation of the T-statistic. The T-statistic is calculated by dividing the estimated parameter by its standard error, which is based on the error variance.</data>
      <data key="d6">d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </edge>
    <edge source="AZ" target="MU">
      <data key="d4">1.0</data>
      <data key="d5">&#181; is the mean of Z, which is transformed by A to become the mean of AZ</data>
      <data key="d6">542f546c5a131196e4701fb33c9b1dee</data>
    </edge>
    <edge source="AZ" target="SIGMA">
      <data key="d4">1.0</data>
      <data key="d5">&#931; is the covariance matrix of Z, which is transformed by A to become the covariance matrix of AZ</data>
      <data key="d6">542f546c5a131196e4701fb33c9b1dee</data>
    </edge>
    <edge source="MU" target="BRAND_A">
      <data key="d4">1.0</data>
      <data key="d5">Mu (&#181;) is the intercept for the regression line of Brand A</data>
      <data key="d6">1d141ab04db553f78a313e430e54abb5</data>
    </edge>
    <edge source="MU" target="SALES_VOLUME">
      <data key="d4">1.0</data>
      <data key="d5">Mu is the intercept for brand A in the regression model</data>
      <data key="d6">b6870535f3975c49d45e62fbe475f198</data>
    </edge>
    <edge source="MU" target="BRAND_B">
      <data key="d4">1.0</data>
      <data key="d5">Mu (&#181;) is the intercept parameter in the regression model for brand B</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="MU" target="BRAND_C">
      <data key="d4">1.0</data>
      <data key="d5">Mu (&#181;) is the intercept parameter in the regression model for brand C</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="SIGMA_SQUARED_MLE" target="BETA_HAT0">
      <data key="d4">1.0</data>
      <data key="d5">Beta hat 0 (b&#946;0) is used in the calculation of Sigma squared MLE (&#963;b^2_MLE)</data>
      <data key="d6">09caa54ca1372d152e47051be4d44ede</data>
    </edge>
    <edge source="S_HAT_SQUARED" target="THEOREM_6_4">
      <data key="d4">1.0</data>
      <data key="d5">Theorem 6.4 provides the formula for s^2(Y), the unbiased estimator for the error variance</data>
      <data key="d6">6648f0d6deed51fb4fb25e6992a71ddf</data>
    </edge>
    <edge source="THEOREM_6_4" target="S2">
      <data key="d4">1.0</data>
      <data key="d5">Theorem 6.4 states that s^2 is an unbiased estimator for &#963;^2</data>
      <data key="d6">9923e77ac6b3de95cb5026bc5e7fe8c0</data>
    </edge>
    <edge source="S2" target="COOKS_DISTANCE">
      <data key="d4">1.0</data>
      <data key="d5">Cook's distance uses the estimated variance s^2 of the error term in the regression model to calculate the influence of an observation</data>
      <data key="d6">9e2ebbb113c00fa43f0af3c0696baf95</data>
    </edge>
    <edge source="S_SQUARED" target="DI">
      <data key="d4">1.0</data>
      <data key="d5">Di is indirectly related to S^2, the unbiased estimate of the error variance</data>
      <data key="d6">0443ab5e20a4f6b2f243c989ef6c723a</data>
    </edge>
    <edge source="H" target="Y_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Y hat (yb) is obtained by applying the hat matrix H to the observed values Y</data>
      <data key="d6">f470791d2d3fedede166f9bb11598c9c</data>
    </edge>
    <edge source="H" target="EPSILON_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Residuals (b&#1013;) are calculated as the difference between the observed values Y and the fitted values Y hat (yb), which are obtained by applying the hat matrix H to Y</data>
      <data key="d6">f470791d2d3fedede166f9bb11598c9c</data>
    </edge>
    <edge source="H" target="LEMMA_7_1">
      <data key="d4">1.0</data>
      <data key="d5">Lemma 7.1 describes the properties of the hat matrix H, including its symmetry, idempotence, and the properties of (In - H)</data>
      <data key="d6">f470791d2d3fedede166f9bb11598c9c</data>
    </edge>
    <edge source="H" target="IP">
      <data key="d4">2.0</data>
      <data key="d5">The entities in focus are H and IP, where H refers to the hat matrix and IP refers to the p x p identity matrix. A significant relationship exists between these two entities, as evidenced by the fact that the trace of matrix H is equal to the trace of IP. This equality in traces highlights a direct connection between the hat matrix H and the identity matrix IP, suggesting that the structure and properties of H are closely tied to those of the identity matrix of size p x p. This observation is crucial in understanding the behavior and impact of the hat matrix within the context of statistical modeling and algorithmic analysis.</data>
      <data key="d6">46629f2efc6c82e81265a131b4bab2ee,bd98ac7b4b5df4f63e7ecc8f4a821f57</data>
    </edge>
    <edge source="H" target="HII">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entities H and HII are intricately related. H, often referred to as the hat matrix, plays a pivotal role in the calculation of leverage values for data points within a dataset. Specifically, the diagonal elements of the hat matrix, denoted as hii, are crucial as they quantify the leverage of the ith data point. This leverage value, hii, is a measure of how much the ith data point influences the fitted values of the regression model. A high hii value indicates that the corresponding data point has a significant impact on the regression line, potentially making it an influential point in the context of the model's predictions. Thus, the hat matrix H and its diagonal elements hii are fundamental components in assessing the influence and leverage of data points in regression analysis.</data>
      <data key="d6">0b650eb2f1dcd603b64fec3c4b5cd24b,bd98ac7b4b5df4f63e7ecc8f4a821f57</data>
    </edge>
    <edge source="H" target="YC">
      <data key="d4">4.0</data>
      <data key="d5">In the context of algorithmic analysis, particularly focusing on the entities H and YC, a comprehensive and enriched description can be formulated as follows: H plays a pivotal role in the computation of YC, serving as a fundamental component in its calculation. Specifically, H is utilized to compute YC, often denoted as HY, highlighting a direct and significant relationship between the two entities. YC is directly derived from H, emphasizing the dependency of YC on H for its determination. This relationship underscores the importance of H in statistical modeling, where it acts as a crucial input for the calculation of YC, thereby influencing the structure and relations within the community of interest.</data>
      <data key="d6">1da117a2f92b2db00290d2a0bfc06beb,679722cf8ce5ce5aee4e379528470efe,74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6</data>
    </edge>
    <edge source="H" target="EB">
      <data key="d4">2.0</data>
      <data key="d5">In the context of algorithmic analysis, particularly focusing on the entities H and EB, a comprehensive and enriched description can be formulated as follows:

The entity H plays a pivotal role in the calculation of EB, serving as a foundational component in the transformation process. Specifically, EB is indirectly derived from H through the application of the matrix (In - H), where In represents the identity matrix. This transformation, (In - H), is a critical operation that facilitates the indirect derivation of EB from H, highlighting the intricate relationship between these two entities within the statistical framework being analyzed. The use of H in this transformation underscores its significance in the structure and relations of the community of interest, as it directly influences the computation of EB, thereby shaping the overall algorithmic analysis.</data>
      <data key="d6">74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6</data>
    </edge>
    <edge source="HII" target="H2">
      <data key="d4">1.0</data>
      <data key="d5">The diagonal elements of H2 are related to the squared leverage values hii</data>
      <data key="d6">679722cf8ce5ce5aee4e379528470efe</data>
    </edge>
    <edge source="HII" target="HIK">
      <data key="d4">1.0</data>
      <data key="d5">hii is the sum of its squared value and the sum of all other elements in the same row, excluding the diagonal element itself</data>
      <data key="d6">679722cf8ce5ce5aee4e379528470efe</data>
    </edge>
    <edge source="HII" target="YB_I">
      <data key="d4">1.0</data>
      <data key="d5">hii is the leverage value corresponding to the predicted value Yb_i</data>
      <data key="d6">1da117a2f92b2db00290d2a0bfc06beb</data>
    </edge>
    <edge source="HII" target="EB">
      <data key="d4">1.0</data>
      <data key="d5">The variance of the ith residual Eb_i is influenced by the leverage value hii</data>
      <data key="d6">5a0d392715f06d5e873f45ae06aa729a</data>
    </edge>
    <edge source="HII" target="EI">
      <data key="d4">1.0</data>
      <data key="d5">As hii approaches 1, the ith residual Ei tends to zero</data>
      <data key="d6">2685edb9e8031c8ea725c43a40af22a8</data>
    </edge>
    <edge source="HII" target="VAR_EI">
      <data key="d4">1.0</data>
      <data key="d5">The variance of the ith residual Var(Ei) is influenced by the leverage value hii</data>
      <data key="d6">2685edb9e8031c8ea725c43a40af22a8</data>
    </edge>
    <edge source="HII" target="RI">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, HII, denoted as hii, plays a pivotal role in the calculation of the standardized residual, RI. Specifically, hii, which represents the leverage value for the ith observation, is a critical component in the adjustment of RI. This leverage value is utilized in the denominator of the formula for calculating the standardized residual, thereby influencing its magnitude. The relationship between HII and RI underscores the importance of leverage values in residual analysis, highlighting how individual observations impact the overall model fit and the reliability of the residuals.</data>
      <data key="d6">90b7e0427699cc1bb461e37939935138,c47968226557bc2eb5aec5bb7994fd0e</data>
    </edge>
    <edge source="HII" target="DI">
      <data key="d4">1.0</data>
      <data key="d5">Di is calculated using Hii, the leverage value for the ith observation</data>
      <data key="d6">0443ab5e20a4f6b2f243c989ef6c723a</data>
    </edge>
    <edge source="YC" target="VAR">
      <data key="d4">2.0</data>
      <data key="d5">Var is used to calculate the variance of Yc</data>
      <data key="d6">1da117a2f92b2db00290d2a0bfc06beb,74d190f10bf6e6936242ca3cdfc4a09f</data>
    </edge>
    <edge source="YC" target="EB">
      <data key="d4">1.0</data>
      <data key="d5">The covariance between EB and Yc is equal to the zero matrix</data>
      <data key="d6">74d190f10bf6e6936242ca3cdfc4a09f</data>
    </edge>
    <edge source="YC" target="COV">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entities YC and COV are involved in a specific calculation. COV, which stands for covariance, is utilized to measure the degree to which two random variables, in this case, YC and EB, linearly vary together. The covariance between YC and EB is described as 0n&#215;n, which could imply that there is no linear relationship between the two variables, or it could be a typographical error suggesting the covariance is an n&#215;n matrix, indicating that the data is multivariate with n dimensions. Further clarification would be needed to determine if "0n&#215;n" is meant to represent a zero matrix or a covariance matrix of size n&#215;n. Assuming it is a covariance matrix, it would provide insights into the pairwise covariances between n variables, including the covariance between YC and EB.</data>
      <data key="d6">74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6</data>
    </edge>
    <edge source="VAR" target="EB">
      <data key="d4">1.0</data>
      <data key="d5">Var is used to calculate the variance of EB</data>
      <data key="d6">74d190f10bf6e6936242ca3cdfc4a09f</data>
    </edge>
    <edge source="YB_I" target="COOKS_DISTANCE">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, YB_I, denoted as yb(i), plays a pivotal role in the calculation of COOKS_DISTANCE, also known as Di. YB_I represents the fitted values obtained from a dataset where the ith observation has been deliberately excluded. This exclusion is a methodological approach designed to assess the impact of the ith observation on the overall model. COOKS_DISTANCE, therefore, serves as a measure of influence, quantifying how much the fitted values change when the ith observation is removed. By leveraging YB_I in its computation, COOKS_DISTANCE provides insights into the sensitivity of the regression model to individual data points, highlighting those that exert a disproportionate influence on the model's predictions.</data>
      <data key="d6">09391efd3b8c510205098b548bc8dc74,9e2ebbb113c00fa43f0af3c0696baf95</data>
    </edge>
    <edge source="YB_I" target="DI">
      <data key="d4">1.0</data>
      <data key="d5">Di is calculated using Yb(i), the fitted value from the dataset with the ith observation removed</data>
      <data key="d6">0443ab5e20a4f6b2f243c989ef6c723a</data>
    </edge>
    <edge source="EB" target="Y_HAT_I">
      <data key="d4">1.0</data>
      <data key="d5">As hii approaches 1, Eb_i tends to zero, implying y_hat_i will tend to yi</data>
      <data key="d6">5a0d392715f06d5e873f45ae06aa729a</data>
    </edge>
    <edge source="EB" target="COV">
      <data key="d4">2.0</data>
      <data key="d5">In the context of statistical analysis, the entities EB and COV, often referred to as Eb and Yc respectively, are central. The covariance, denoted as COV, serves as a measure of the linear relationship between EB and Y. It quantifies the direction and degree to which these two variables vary together. Specifically, the covariance between Yc and Eb is described as 0n&#215;n, which might suggest a zero matrix of dimensions n&#215;n, indicating no linear relationship between the variables when considering their covariance structure. This implies that changes in one variable are not associated with changes in the other, at least not in a linear fashion. This insight is crucial for understanding the structure and relationships within the data, particularly when employing statistical models such as linear regression, where the covariance between predictors and the response variable is a fundamental consideration.</data>
      <data key="d6">74d190f10bf6e6936242ca3cdfc4a09f,7d074208b1259e7d84f9f870d3828bb6</data>
    </edge>
    <edge source="VAR_EI" target="&#931;_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">The variance of the ith residual Var(Ei) is given by &#963;^2(1 - hii)</data>
      <data key="d6">2685edb9e8031c8ea725c43a40af22a8</data>
    </edge>
    <edge source="COOKS_DISTANCE" target="INFLUENTIAL_OBSERVATION">
      <data key="d4">1.0</data>
      <data key="d5">Cook's distance is used to measure the influence of observations</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="COOKS_DISTANCE" target="BETA_HAT_I">
      <data key="d4">1.0</data>
      <data key="d5">Cook's distance uses the parameter estimates &#946;b(i) calculated from the dataset with the ith observation removed to measure the influence of the ith observation</data>
      <data key="d6">9e2ebbb113c00fa43f0af3c0696baf95</data>
    </edge>
    <edge source="COOKS_DISTANCE" target="DI">
      <data key="d4">1.0</data>
      <data key="d5">Di is a specific instance of Cook's distance for the ith observation</data>
      <data key="d6">323899f01972255cd3278bccee20d5d8</data>
    </edge>
    <edge source="COOKS_DISTANCE" target="FIGURE_8_4">
      <data key="d4">1.0</data>
      <data key="d5">Cook's distances are plotted in Figure 8.4 for the regression analysis</data>
      <data key="d6">323899f01972255cd3278bccee20d5d8</data>
    </edge>
    <edge source="COOKS_DISTANCE" target="STANDARDISED_RESIDUAL">
      <data key="d4">1.0</data>
      <data key="d5">The standardised residual (ri) is related to Cook's distance as both are measures of influence in regression analysis</data>
      <data key="d6">7e05f1b457a496c8b3630e7044fc5981</data>
    </edge>
    <edge source="COOKS_DISTANCE" target="INFLUENTIAL_DATA_POINT">
      <data key="d4">2.0</data>
      <data key="d5">Cook's distance, denoted as Di, is a pivotal statistical measure utilized in the realm of regression analysis. It serves as a diagnostic tool to identify influential data points within a dataset. By quantifying the impact of each observation on the regression model, Cook's distance helps in detecting those data points that significantly affect the model's parameters. An influential data point, as indicated by a high Cook's distance value, can potentially skew the results of the regression analysis, making it crucial for data scientists to carefully consider such points when interpreting the model's outcomes. This measure is particularly useful in assessing the robustness of the model and ensuring that the regression analysis accurately reflects the relationships within the data.</data>
      <data key="d6">09391efd3b8c510205098b548bc8dc74,629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="COOKS_DISTANCE" target="F_DISTRIBUTION">
      <data key="d4">1.0</data>
      <data key="d5">The F-distribution (Fp,n-p(0.5)) is used as a threshold for Cook's distance (Di) to identify influential data points</data>
      <data key="d6">09391efd3b8c510205098b548bc8dc74</data>
    </edge>
    <edge source="COOKS_DISTANCE" target="PLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Plots can be used to visualize Cook's distance</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="COOKS_DISTANCE" target="RULES_OF_THUMB">
      <data key="d4">1.0</data>
      <data key="d5">Rules of thumb can be used to interpret Cook's distance</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="X_I" target="LEVERAGE">
      <data key="d4">1.0</data>
      <data key="d5">Leverage is calculated based on the distance of X_i from X_bar</data>
      <data key="d6">bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </edge>
    <edge source="LEVERAGE" target="INFLUENCE_INDEX_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">InfluenceIndexPlot is used to visualize the leverages of data points</data>
      <data key="d6">bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </edge>
    <edge source="LEVERAGE" target="INFLUENTIAL_OBSERVATION">
      <data key="d4">1.0</data>
      <data key="d5">Influential observations may have high leverage</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="LEVERAGE" target="STANDARDISED_RESIDUAL">
      <data key="d4">1.0</data>
      <data key="d5">Leverage is used in the calculation of the standardised residual (ri) to assess the influence of each observation</data>
      <data key="d6">7e05f1b457a496c8b3630e7044fc5981</data>
    </edge>
    <edge source="LEVERAGE" target="INFLUENTIAL_DATA_POINT">
      <data key="d4">1.0</data>
      <data key="d5">Leverage is a measure used to identify influential data points by assessing their distance from the center of the predictor space</data>
      <data key="d6">09391efd3b8c510205098b548bc8dc74</data>
    </edge>
    <edge source="LEVERAGE" target="HIGH_LEVERAGE_DATA_POINT">
      <data key="d4">1.0</data>
      <data key="d5">Leverage is a measure used to identify high leverage data points</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="LEVERAGE" target="PLOTS">
      <data key="d4">1.0</data>
      <data key="d5">Plots can be used to visualize leverage</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="LEVERAGE" target="RULES_OF_THUMB">
      <data key="d4">1.0</data>
      <data key="d5">Rules of thumb can be used to interpret leverage values</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="INFLUENCE_INDEX_PLOT" target="CAR_PACKAGE">
      <data key="d4">1.0</data>
      <data key="d5">InfluenceIndexPlot is a function provided by the car package</data>
      <data key="d6">bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </edge>
    <edge source="INFLUENCE_INDEX_PLOT" target="FIGURE_8_1">
      <data key="d4">1.0</data>
      <data key="d5">Figure 8.1 is the output of the InfluenceIndexPlot function for the mammals dataset</data>
      <data key="d6">bd05fe6a05f9a13d33c4f1b5a771ada5</data>
    </edge>
    <edge source="REGRESSION" target="LOGARITHM">
      <data key="d4">1.0</data>
      <data key="d5">The logarithm transformation is applied to both the average brain weight and average body weight in the regression model.</data>
      <data key="d6">f0b1289a0623b82a686b3b7dfb3a6ee8</data>
    </edge>
    <edge source="REGRESSION" target="LOG_AVERAGE_BRAIN_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Log average brain weight is the dependent variable in the regression analysis</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="REGRESSION" target="LOG_AVERAGE_BODY_WEIGHT">
      <data key="d4">1.0</data>
      <data key="d5">Log average body weight is the independent variable in the regression analysis</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="RI" target="EBI">
      <data key="d4">1.0</data>
      <data key="d5">Eb_i is the raw residual, used in the calculation of the standardised residual Ri</data>
      <data key="d6">90b7e0427699cc1bb461e37939935138</data>
    </edge>
    <edge source="RI" target="EPSILON_BI">
      <data key="d4">1.0</data>
      <data key="d5">RI is calculated using &#1013;bi as part of the standardised residual formula</data>
      <data key="d6">c47968226557bc2eb5aec5bb7994fd0e</data>
    </edge>
    <edge source="RI" target="DI">
      <data key="d4">1.0</data>
      <data key="d5">Di is calculated using ri, the standardised residual for the ith observation</data>
      <data key="d6">98d6982108f2d42fe0437bff8c666e17</data>
    </edge>
    <edge source="EPSILON_BI" target="DI">
      <data key="d4">1.0</data>
      <data key="d5">Di is calculated using Epsilon bi (&#1013;bi), the residual for the ith observation</data>
      <data key="d6">0443ab5e20a4f6b2f243c989ef6c723a</data>
    </edge>
    <edge source="DATASET_SIZE" target="THRESHOLD">
      <data key="d4">1.0</data>
      <data key="d5">The dataset size determines the threshold for identifying outliers based on the standardised residuals</data>
      <data key="d6">c47968226557bc2eb5aec5bb7994fd0e</data>
    </edge>
    <edge source="DATASET_SIZE" target="MODEL_FIT">
      <data key="d4">1.0</data>
      <data key="d5">The size of the dataset affects the model fit</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="HUMAN_OBSERVATION" target="RESIDUALS_VS_LEVERAGE_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The residuals vs leverage plot flags the human observation as potentially influential</data>
      <data key="d6">9e2ebbb113c00fa43f0af3c0696baf95</data>
    </edge>
    <edge source="HUMAN_OBSERVATION" target="FIGURE_8_4">
      <data key="d4">1.0</data>
      <data key="d5">The observation for Human is highlighted in Figure 8.4 as having the largest Cook's distance</data>
      <data key="d6">323899f01972255cd3278bccee20d5d8</data>
    </edge>
    <edge source="WATER_OPOSSUM_OBSERVATION" target="RESIDUALS_VS_LEVERAGE_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">The residuals vs leverage plot flags the water opossum observation as potentially influential</data>
      <data key="d6">9e2ebbb113c00fa43f0af3c0696baf95</data>
    </edge>
    <edge source="LOG_AVERAGE_BRAIN_WEIGHT" target="FIGURE_8_4">
      <data key="d4">1.0</data>
      <data key="d5">Logarithm of average brain weight is the dependent variable in the regression analysis whose results are plotted in Figure 8.4</data>
      <data key="d6">323899f01972255cd3278bccee20d5d8</data>
    </edge>
    <edge source="LOG_AVERAGE_BODY_WEIGHT" target="FIGURE_8_4">
      <data key="d4">1.0</data>
      <data key="d5">Logarithm of average body weight is the independent variable in the regression analysis whose results are plotted in Figure 8.4</data>
      <data key="d6">323899f01972255cd3278bccee20d5d8</data>
    </edge>
    <edge source="INFLUENTIAL_OBSERVATION" target="RESIDUALS_VS_LEVERAGE_PLOT">
      <data key="d4">1.0</data>
      <data key="d5">Residuals vs leverage plot is used to identify influential observations</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="INFLUENTIAL_OBSERVATION" target="WATER_OPOSSUM">
      <data key="d4">1.0</data>
      <data key="d5">Water opossum is flagged as an influential observation in the mammals dataset</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="INFLUENTIAL_OBSERVATION" target="MUSK_SHREW">
      <data key="d4">1.0</data>
      <data key="d5">Musk shrew is flagged as an influential observation in the mammals dataset</data>
      <data key="d6">428db872e71a17a2cf7868b03a52def0</data>
    </edge>
    <edge source="RESIDUALS_VS_LEVERAGE_PLOT" target="MUSK_SHREW_OBSERVATION">
      <data key="d4">1.0</data>
      <data key="d5">The residuals vs leverage plot flags the musk shrew observation as potentially influential</data>
      <data key="d6">9e2ebbb113c00fa43f0af3c0696baf95</data>
    </edge>
    <edge source="BETA_HAT_I" target="DI">
      <data key="d4">1.0</data>
      <data key="d5">Di is calculated using Beta hat (&#946;b(i)), the parameter estimate from the dataset with the ith observation removed</data>
      <data key="d6">0443ab5e20a4f6b2f243c989ef6c723a</data>
    </edge>
    <edge source="DI" target="TXTX">
      <data key="d4">1.0</data>
      <data key="d5">Di is calculated using the matrix product TXTX</data>
      <data key="d6">0443ab5e20a4f6b2f243c989ef6c723a</data>
    </edge>
    <edge source="DI" target="PS_SQUARED">
      <data key="d4">1.0</data>
      <data key="d5">Di is calculated using Ps^2, the estimated variance</data>
      <data key="d6">0443ab5e20a4f6b2f243c989ef6c723a</data>
    </edge>
    <edge source="DI" target="R_SQUARED_I">
      <data key="d4">1.0</data>
      <data key="d5">Di is calculated using Ri^2, the squared standardised residual for the ith observation</data>
      <data key="d6">0443ab5e20a4f6b2f243c989ef6c723a</data>
    </edge>
    <edge source="DI" target="BI">
      <data key="d4">1.0</data>
      <data key="d5">bi is part of the subscript used in the calculation of the squared error term that contributes to the Cook's distance Di</data>
      <data key="d6">98d6982108f2d42fe0437bff8c666e17</data>
    </edge>
    <edge source="DI" target="PS2">
      <data key="d4">1.0</data>
      <data key="d5">ps2 is part of the variable used in the calculation of the squared error term that contributes to the Cook's distance Di</data>
      <data key="d6">98d6982108f2d42fe0437bff8c666e17</data>
    </edge>
    <edge source="DI" target="HI">
      <data key="d4">1.0</data>
      <data key="d5">hi is part of the subscript used in the calculation of the leverage and influence terms that contribute to the Cook's distance Di</data>
      <data key="d6">98d6982108f2d42fe0437bff8c666e17</data>
    </edge>
    <edge source="DI" target="F_DISTRIBUTION">
      <data key="d4">1.0</data>
      <data key="d5">Di is compared to the F-distribution to determine if the ith observation is influential</data>
      <data key="d6">98d6982108f2d42fe0437bff8c666e17</data>
    </edge>
    <edge source="DI" target="FIGURE_8_4">
      <data key="d4">1.0</data>
      <data key="d5">Di is plotted in Figure 8.4, an index plot of the Cook's distances for the regression</data>
      <data key="d6">98d6982108f2d42fe0437bff8c666e17</data>
    </edge>
    <edge source="DI" target="FP_N_P">
      <data key="d4">1.0</data>
      <data key="d5">Di is compared to Fp,n-p(0.5) to determine if the ith observation is influential</data>
      <data key="d6">323899f01972255cd3278bccee20d5d8</data>
    </edge>
    <edge source="DATA_ENTRY_ERROR" target="ANOTHER_POPULATION">
      <data key="d4">1.0</data>
      <data key="d5">A data entry error can result in a data point that appears to be from another population</data>
      <data key="d6">83bb91cf725e5116ca2f5748fddccfae</data>
    </edge>
    <edge source="ROBUST_REGRESSION" target="HIGH_LEVERAGE_DATA_POINT">
      <data key="d4">1.0</data>
      <data key="d5">Robust regression is designed to handle high leverage data points by being less sensitive to their influence</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="ROBUST_REGRESSION" target="REGRESSION_OUTLIER">
      <data key="d4">1.0</data>
      <data key="d5">Robust regression is designed to handle regression outliers by being less sensitive to their influence</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="ROBUST_REGRESSION" target="INFLUENTIAL_DATA_POINT">
      <data key="d4">1.0</data>
      <data key="d5">Robust regression is designed to handle influential data points by being less sensitive to their influence</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="ROBUST_REGRESSION" target="GENERALIZED_LINEAR_MODELS_BOOK">
      <data key="d4">1.0</data>
      <data key="d5">The book covers material related to robust regression</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="ROBUST_REGRESSION" target="DATA_ANALYSIS_AND_GRAPHICS_BOOK">
      <data key="d4">1.0</data>
      <data key="d5">The book covers material related to robust regression</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="STANDARDISED_RESIDUAL" target="INFLUENTIAL_DATA_POINT">
      <data key="d4">1.0</data>
      <data key="d5">Standardised residuals are used to identify influential data points by assessing the size of the residuals relative to their standard deviation</data>
      <data key="d6">09391efd3b8c510205098b548bc8dc74</data>
    </edge>
    <edge source="INFLUENTIAL_DATA_POINT" target="REGRESSION_OUTLIER">
      <data key="d4">1.0</data>
      <data key="d5">A regression outlier can be an influential data point if it has a large residual</data>
      <data key="d6">09391efd3b8c510205098b548bc8dc74</data>
    </edge>
    <edge source="INFLUENTIAL_DATA_POINT" target="HANDLING_INFLUENTIAL_DATA_POINTS">
      <data key="d4">1.0</data>
      <data key="d5">Options on how to handle influential data points are relevant for dealing with influential data points</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="CATEGORICAL_PREDICTOR_VARIABLES" target="GENDER">
      <data key="d4">1.0</data>
      <data key="d5">Gender is an example of a categorical predictor variable</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="CATEGORICAL_PREDICTOR_VARIABLES" target="DEGREE_COURSE">
      <data key="d4">1.0</data>
      <data key="d5">Degree course is an example of a categorical predictor variable</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="GENDER" target="QUALITATIVE_PREDICTOR">
      <data key="d4">1.0</data>
      <data key="d5">Gender is a specific example of a qualitative predictor variable</data>
      <data key="d6">39aef0392258a09378ce45d8b03a268a</data>
    </edge>
    <edge source="DEGREE_COURSE" target="LEVELS">
      <data key="d4">1.0</data>
      <data key="d5">Degree course has levels such as Data Science, MathStat, and MORSE</data>
      <data key="d6">629ce6550294d332948e19171a4acd2d</data>
    </edge>
    <edge source="DEGREE_COURSE" target="QUALITATIVE_PREDICTOR">
      <data key="d4">1.0</data>
      <data key="d5">Degree course is a specific example of a qualitative predictor variable</data>
      <data key="d6">39aef0392258a09378ce45d8b03a268a</data>
    </edge>
    <edge source="QUANTITATIVE_PREDICTOR" target="QUALITATIVE_PREDICTOR">
      <data key="d4">1.0</data>
      <data key="d5">Quantitative and qualitative predictor variables are both used in statistical models to predict outcomes, but they differ in their nature and the way they are analyzed</data>
      <data key="d6">39aef0392258a09378ce45d8b03a268a</data>
    </edge>
    <edge source="RETAIL_DATASET" target="COMPARATIVE_BOXPLOTS">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the RETAIL_DATASET, comparative boxplots serve as a valuable visualization tool to explore the relationship dynamics between sales volumes and brands. This graphical representation effectively highlights the distribution of sales volumes across different brands, providing insights into central tendencies, variability, and potential outliers within each brand's sales data. Additionally, comparative boxplots are utilized to analyze the relationship between a categorical variable and a quantitative variable, offering a comprehensive view of how various categories influence sales volumes. This approach facilitates a deeper understanding of the structure and patterns within the retail market, enabling data-driven decisions and strategies for optimizing sales and brand performance.</data>
      <data key="d6">39aef0392258a09378ce45d8b03a268a,ab898d123f48e380384aa01e035a83ca</data>
    </edge>
    <edge source="RETAIL_DATASET" target="SALES_VOLUMES">
      <data key="d4">1.0</data>
      <data key="d5">The Retail dataset contains sales volumes data</data>
      <data key="d6">ab898d123f48e380384aa01e035a83ca</data>
    </edge>
    <edge source="RETAIL_DATASET" target="BRAND_C">
      <data key="d4">1.0</data>
      <data key="d5">The Retail dataset contains information about Brand C</data>
      <data key="d6">ab898d123f48e380384aa01e035a83ca</data>
    </edge>
    <edge source="RETAIL_DATASET" target="BRAND_A">
      <data key="d4">1.0</data>
      <data key="d5">The Retail dataset contains information about Brand A</data>
      <data key="d6">ab898d123f48e380384aa01e035a83ca</data>
    </edge>
    <edge source="RETAIL_DATASET" target="BRAND_B">
      <data key="d4">1.0</data>
      <data key="d5">The Retail dataset contains information about Brand B</data>
      <data key="d6">ab898d123f48e380384aa01e035a83ca</data>
    </edge>
    <edge source="RETAIL_DATASET" target="REGRESSION_MODELS">
      <data key="d4">1.0</data>
      <data key="d5">Regression models can be used to analyze the relationship between sales volumes and brands in the Retail dataset</data>
      <data key="d6">ab898d123f48e380384aa01e035a83ca</data>
    </edge>
    <edge source="COMPARATIVE_BOXPLOTS" target="BOXPLOT_COMMAND">
      <data key="d4">1.0</data>
      <data key="d5">The boxplot command is used to produce comparative boxplots</data>
      <data key="d6">39aef0392258a09378ce45d8b03a268a</data>
    </edge>
    <edge source="SALES_VOLUMES" target="BRAND_C">
      <data key="d4">1.0</data>
      <data key="d5">Sales volumes of Brand C stores are higher, on average, than those of other brands</data>
      <data key="d6">ab898d123f48e380384aa01e035a83ca</data>
    </edge>
    <edge source="SALES_VOLUMES" target="BRAND_A">
      <data key="d4">1.0</data>
      <data key="d5">Sales volumes of Brand A stores are lower, on average, than those of Brand C</data>
      <data key="d6">ab898d123f48e380384aa01e035a83ca</data>
    </edge>
    <edge source="SALES_VOLUMES" target="BRAND_B">
      <data key="d4">1.0</data>
      <data key="d5">Sales volumes of Brand B stores are lower, on average, than those of Brand C</data>
      <data key="d6">ab898d123f48e380384aa01e035a83ca</data>
    </edge>
    <edge source="BRAND_C" target="SALES_VOLUME">
      <data key="d4">3.0</data>
      <data key="d5">Brand C, denoted as BRAND_C, is characterized by a notably high sales volume, averaging 126,680 units, significantly outperforming its competitors in terms of sales. The sales volume for Brand C, as indicated by SALES_VOLUME, is not solely determined by price, suggesting a complex interplay of factors that contribute to its market success. This is evidenced by the data points that consistently lie above the fitted regression line, indicating that Brand C's sales volume is robust even when prices are not the primary driver. The statistical model predicts the sales volume for Brand C stores, providing insights into the underlying dynamics that influence its sales performance beyond just pricing strategies.</data>
      <data key="d6">096afa471635bc59c3bfa9af4d04d625,7a605c3b689bec7ab2c46df9c123e3f3,e079b7c92d5c0b009ff02040eb652bc6</data>
    </edge>
    <edge source="BRAND_C" target="SALESJ">
      <data key="d4">1.0</data>
      <data key="d5">Brand C's indicator variable xjC is used in the model for Salesj</data>
      <data key="d6">228bdca7843406def245d755e8df49f6</data>
    </edge>
    <edge source="BRAND_C" target="AVERAGE_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">At the average price, the expected number of units sold by Brand C stores can be calculated</data>
      <data key="d6">9854704301b8df256ca1013b8d53dfac</data>
    </edge>
    <edge source="BRAND_C" target="CEO">
      <data key="d4">1.0</data>
      <data key="d5">The CEO is responsible for overseeing the stores of Brand C, and is interested in the performance of these stores</data>
      <data key="d6">9854704301b8df256ca1013b8d53dfac</data>
    </edge>
    <edge source="BRAND_C" target="FACTOR_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">Factor variable defines the group of Brand C stores</data>
      <data key="d6">906eb7d6b49fa360e7e5b65c56cd4d76</data>
    </edge>
    <edge source="BRAND_C" target="ALPHA_C">
      <data key="d4">2.0</data>
      <data key="d5">In the context of the regression analysis, Alpha C (&#945;C) serves as a crucial parameter for Brand C. This parameter quantifies the unique characteristic of Brand C by indicating the difference in intercept between the regression line for Brand C and that for Brand A. This statistical measure is pivotal in understanding how Brand C's performance or characteristics diverge from those of Brand A within the model, providing insights into the comparative positioning of Brand C in the market or study.</data>
      <data key="d6">1d141ab04db553f78a313e430e54abb5,825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="BRAND_C" target="X_C">
      <data key="d4">1.0</data>
      <data key="d5">X_C (xjC) is an indicator variable for Brand C</data>
      <data key="d6">1d141ab04db553f78a313e430e54abb5</data>
    </edge>
    <edge source="BRAND_C" target="XJC">
      <data key="d4">1.0</data>
      <data key="d5">Brand C is represented by the indicator variable xjC in the design matrix</data>
      <data key="d6">c103c6d096d52868eda26d991194b5f2</data>
    </edge>
    <edge source="BRAND_C" target="BRAND_A">
      <data key="d4">1.0</data>
      <data key="d5">Brand A is the baseline category against which Brand C is compared in terms of sales volume</data>
      <data key="d6">baa0dc3d4ec0e51c0a321e5579caf8aa</data>
    </edge>
    <edge source="BRAND_C" target="PARALLEL_LINES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Brand C is analyzed using the parallel lines model to assess the fit of the regression line</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="BRAND_C" target="STORE_J">
      <data key="d4">1.0</data>
      <data key="d5">Store j can be of brand C, which has its own regression line</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="BRAND_C" target="BRAND_B">
      <data key="d4">1.0</data>
      <data key="d5">Brand B and Brand C are different brands in the dataset, each with its own regression line</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="BRAND_C" target="GAMMA_C">
      <data key="d4">1.0</data>
      <data key="d5">Gamma C (&#947;C) describes the difference in slope between the regression line for Brand C and Brand A</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="BRAND_C" target="ALPHA_B_C">
      <data key="d4">1.0</data>
      <data key="d5">Brand C's intercept difference from Brand A is represented by Alpha hat for Brand C (&#945;bC)</data>
      <data key="d6">adbc52b340a69a8633c919c4fd2cd3f6</data>
    </edge>
    <edge source="BRAND_C" target="GAMMA_B_C">
      <data key="d4">1.0</data>
      <data key="d5">Brand C's price effect difference from Brand A is represented by Gamma hat for Brand C (&#947;bC)</data>
      <data key="d6">adbc52b340a69a8633c919c4fd2cd3f6</data>
    </edge>
    <edge source="BRAND_C" target="FIGURE_9_5">
      <data key="d4">1.0</data>
      <data key="d5">Figure 9.5 visualizes the sales model for Brand C stores, showing the relationship between sales and price</data>
      <data key="d6">3dd24a54028976ba54304ec7169bb74b</data>
    </edge>
    <edge source="BRAND_C" target="INTERACTION_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">In the interaction model, the effect of price on sales volume for Brand C stores is compared to that of Brand A and Brand B</data>
      <data key="d6">1820d10ee0f23f34b3ea88ba475bc52d</data>
    </edge>
    <edge source="BRAND_A" target="SALES_VOLUME">
      <data key="d4">2.0</data>
      <data key="d5">BRAND_A, a specific brand in the market, has been analyzed in terms of its SALES_VOLUME. The average sales volume for BRAND_A is recorded at 109,679 units, which is notably lower than the average sales volume of Brand C, indicating a potential competitive disadvantage in terms of market share. Despite this, the analysis reveals that BRAND_A's sales volume is not solely determined by price. This conclusion is drawn from the observation that the data points for BRAND_A's sales volume fall below the fitted regression line in a statistical model. This suggests that there are other influential factors affecting the sales volume of BRAND_A, beyond just the pricing strategy. These factors could include marketing efforts, product quality, brand reputation, or consumer preferences, all of which play a significant role in shaping the sales performance of the brand.</data>
      <data key="d6">7a605c3b689bec7ab2c46df9c123e3f3,e079b7c92d5c0b009ff02040eb652bc6</data>
    </edge>
    <edge source="BRAND_A" target="SALESJ">
      <data key="d4">1.0</data>
      <data key="d5">Brand A's indicator variable xjA is used in the model for Salesj, calculated as 1 - xjB - xjC</data>
      <data key="d6">228bdca7843406def245d755e8df49f6</data>
    </edge>
    <edge source="BRAND_A" target="AVERAGE_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">At the average price, the expected number of units sold by Brand A stores can be calculated</data>
      <data key="d6">9854704301b8df256ca1013b8d53dfac</data>
    </edge>
    <edge source="BRAND_A" target="CEO">
      <data key="d4">1.0</data>
      <data key="d5">The CEO is responsible for overseeing the stores of Brand A, and is interested in the performance of these stores</data>
      <data key="d6">9854704301b8df256ca1013b8d53dfac</data>
    </edge>
    <edge source="BRAND_A" target="FACTOR_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">Factor variable defines the group of Brand A stores</data>
      <data key="d6">906eb7d6b49fa360e7e5b65c56cd4d76</data>
    </edge>
    <edge source="BRAND_A" target="ALPHA_B">
      <data key="d4">1.0</data>
      <data key="d5">Alpha_B (&#945;B) represents the expected difference in sales volume of Brand B stores compared to Brand A stores for a fixed product price</data>
      <data key="d6">baa0dc3d4ec0e51c0a321e5579caf8aa</data>
    </edge>
    <edge source="BRAND_A" target="ALPHA_C">
      <data key="d4">1.0</data>
      <data key="d5">Alpha_C (&#945;C) represents the expected difference in sales volume of Brand C stores compared to Brand A stores for a fixed product price</data>
      <data key="d6">baa0dc3d4ec0e51c0a321e5579caf8aa</data>
    </edge>
    <edge source="BRAND_A" target="BRAND_B">
      <data key="d4">1.0</data>
      <data key="d5">Brand A is the baseline category against which Brand B is compared in terms of sales volume</data>
      <data key="d6">baa0dc3d4ec0e51c0a321e5579caf8aa</data>
    </edge>
    <edge source="BRAND_A" target="TREATMENT_CODING">
      <data key="d4">1.0</data>
      <data key="d5">Treatment coding uses Brand A as the reference category for comparing the sales volumes of other brands</data>
      <data key="d6">baa0dc3d4ec0e51c0a321e5579caf8aa</data>
    </edge>
    <edge source="BRAND_A" target="PARALLEL_LINES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Brand A is analyzed using the parallel lines model to assess the fit of the regression line</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="BRAND_A" target="MU_B">
      <data key="d4">1.0</data>
      <data key="d5">Brand A's intercept in the model is represented by Mu hat (&#181;b)</data>
      <data key="d6">adbc52b340a69a8633c919c4fd2cd3f6</data>
    </edge>
    <edge source="BRAND_A" target="FIGURE_9_5">
      <data key="d4">1.0</data>
      <data key="d5">Figure 9.5 visualizes the sales model for Brand A stores, showing the relationship between sales and price</data>
      <data key="d6">3dd24a54028976ba54304ec7169bb74b</data>
    </edge>
    <edge source="BRAND_A" target="INTERACTION_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">In the interaction model, the effect of price on sales volume for Brand A stores differs from that of Brand C</data>
      <data key="d6">1820d10ee0f23f34b3ea88ba475bc52d</data>
    </edge>
    <edge source="BRAND_B" target="SALES_VOLUME">
      <data key="d4">2.0</data>
      <data key="d5">Brand B, identified as one of the entities in focus, exhibits an average sales volume of 105,728 units, a figure noted to be less than that of Brand C. An in-depth statistical analysis reveals that the sales volume of Brand B is not solely dictated by price fluctuations. This insight is derived from the observation that data points, representing sales volume, consistently fall below the fitted regression line. This suggests that there are additional, non-price-related factors that significantly impact Brand B's sales volume. These factors could encompass marketing strategies, product quality, customer preferences, or economic conditions, all of which contribute to the observed sales performance of Brand B in the market.</data>
      <data key="d6">7a605c3b689bec7ab2c46df9c123e3f3,e079b7c92d5c0b009ff02040eb652bc6</data>
    </edge>
    <edge source="BRAND_B" target="SALESJ">
      <data key="d4">1.0</data>
      <data key="d5">Brand B's indicator variable xjB is used in the model for Salesj</data>
      <data key="d6">228bdca7843406def245d755e8df49f6</data>
    </edge>
    <edge source="BRAND_B" target="AVERAGE_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">At the average price, the expected number of units sold by Brand B stores can be calculated</data>
      <data key="d6">9854704301b8df256ca1013b8d53dfac</data>
    </edge>
    <edge source="BRAND_B" target="CEO">
      <data key="d4">1.0</data>
      <data key="d5">The CEO is responsible for overseeing the stores of Brand B, and is interested in the performance of these stores</data>
      <data key="d6">9854704301b8df256ca1013b8d53dfac</data>
    </edge>
    <edge source="BRAND_B" target="FACTOR_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">Factor variable defines the group of Brand B stores</data>
      <data key="d6">906eb7d6b49fa360e7e5b65c56cd4d76</data>
    </edge>
    <edge source="BRAND_B" target="ALPHA_B">
      <data key="d4">1.0</data>
      <data key="d5">Alpha B (&#945;B) represents the difference in intercept between the regression line for Brand B and that for Brand A</data>
      <data key="d6">1d141ab04db553f78a313e430e54abb5</data>
    </edge>
    <edge source="BRAND_B" target="X_B">
      <data key="d4">1.0</data>
      <data key="d5">X_B (xjB) is an indicator variable for Brand B</data>
      <data key="d6">1d141ab04db553f78a313e430e54abb5</data>
    </edge>
    <edge source="BRAND_B" target="XJB">
      <data key="d4">1.0</data>
      <data key="d5">Brand B is represented by the indicator variable xjB in the design matrix</data>
      <data key="d6">c103c6d096d52868eda26d991194b5f2</data>
    </edge>
    <edge source="BRAND_B" target="PARALLEL_LINES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Brand B is analyzed using the parallel lines model to assess the fit of the regression line</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="BRAND_B" target="STORE_J">
      <data key="d4">1.0</data>
      <data key="d5">Store j can be of brand B, which has its own regression line</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="BRAND_B" target="ALPHA_B_B">
      <data key="d4">1.0</data>
      <data key="d5">Brand B's intercept difference from Brand A is represented by Alpha hat for Brand B (&#945;bB)</data>
      <data key="d6">adbc52b340a69a8633c919c4fd2cd3f6</data>
    </edge>
    <edge source="BRAND_B" target="GAMMA_B_B">
      <data key="d4">1.0</data>
      <data key="d5">Brand B's price effect difference from Brand A is represented by Gamma hat for Brand B (&#947;bB)</data>
      <data key="d6">adbc52b340a69a8633c919c4fd2cd3f6</data>
    </edge>
    <edge source="BRAND_B" target="FIGURE_9_5">
      <data key="d4">1.0</data>
      <data key="d5">Figure 9.5 visualizes the sales model for Brand B stores, showing the relationship between sales and price</data>
      <data key="d6">3dd24a54028976ba54304ec7169bb74b</data>
    </edge>
    <edge source="BRAND_B" target="INTERACTION_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">In the interaction model, the effect of price on sales volume for Brand B stores differs from that of Brand C</data>
      <data key="d6">1820d10ee0f23f34b3ea88ba475bc52d</data>
    </edge>
    <edge source="REGRESSION_MODELS" target="MEDIAN">
      <data key="d4">1.0</data>
      <data key="d5">The median is a statistical measure, but regression models focus on predicting the mean, not the median.</data>
      <data key="d6">e079b7c92d5c0b009ff02040eb652bc6</data>
    </edge>
    <edge source="REGRESSION_MODELS" target="SALES_VOLUME">
      <data key="d4">1.0</data>
      <data key="d5">Regression models are used to predict sales volume based on independent variables like price and brand.</data>
      <data key="d6">e079b7c92d5c0b009ff02040eb652bc6</data>
    </edge>
    <edge source="SALES_VOLUME" target="FIGURE_9.2">
      <data key="d4">1.0</data>
      <data key="d5">Figure 9.2 visualizes the relationship between sales volume and price, with color-coded data points according to brand.</data>
      <data key="d6">e079b7c92d5c0b009ff02040eb652bc6</data>
    </edge>
    <edge source="SALES_VOLUME" target="MU_A">
      <data key="d4">1.0</data>
      <data key="d5">&#181;A is the intercept parameter for Brand A in the linear regression model, and is used to predict sales volume for Brand A when price is zero.</data>
      <data key="d6">7a605c3b689bec7ab2c46df9c123e3f3</data>
    </edge>
    <edge source="SALES_VOLUME" target="MU_B">
      <data key="d4">1.0</data>
      <data key="d5">&#181;B is the intercept parameter for Brand B in the linear regression model, and is used to predict sales volume for Brand B when price is zero.</data>
      <data key="d6">7a605c3b689bec7ab2c46df9c123e3f3</data>
    </edge>
    <edge source="SALES_VOLUME" target="MU_C">
      <data key="d4">1.0</data>
      <data key="d5">&#181;C is the intercept parameter for Brand C in the linear regression model, and is used to predict sales volume for Brand C when price is zero.</data>
      <data key="d6">7a605c3b689bec7ab2c46df9c123e3f3</data>
    </edge>
    <edge source="SALES_VOLUME" target="PRODUCT_PRICE">
      <data key="d4">1.0</data>
      <data key="d5">Sales volume is influenced by the product price, which is an independent variable in the regression model</data>
      <data key="d6">baa0dc3d4ec0e51c0a321e5579caf8aa</data>
    </edge>
    <edge source="SALES_VOLUME" target="MODEL_FITTED_IN_H">
      <data key="d4">1.0</data>
      <data key="d5">The model fitted in h. predicts the sales volume for Brand C stores</data>
      <data key="d6">096afa471635bc59c3bfa9af4d04d625</data>
    </edge>
    <edge source="SALES_VOLUME" target="MODEL_FITTED_IN_C">
      <data key="d4">1.0</data>
      <data key="d5">The model fitted in c. predicts the sales volume for Brand C stores, which is compared to the prediction from the model fitted in h.</data>
      <data key="d6">096afa471635bc59c3bfa9af4d04d625</data>
    </edge>
    <edge source="SALES_VOLUME" target="STORE">
      <data key="d4">1.0</data>
      <data key="d5">The sales volume is observed at the store level</data>
      <data key="d6">b6870535f3975c49d45e62fbe475f198</data>
    </edge>
    <edge source="SALES_VOLUME" target="ALPHA_B">
      <data key="d4">1.0</data>
      <data key="d5">Alpha B represents the difference in intercept for brand B compared to brand A in the regression model</data>
      <data key="d6">b6870535f3975c49d45e62fbe475f198</data>
    </edge>
    <edge source="SALES_VOLUME" target="ALPHA_C">
      <data key="d4">1.0</data>
      <data key="d5">Alpha C represents the difference in intercept for brand C compared to brand A in the regression model</data>
      <data key="d6">b6870535f3975c49d45e62fbe475f198</data>
    </edge>
    <edge source="SALES_VOLUME" target="GAMMA_B">
      <data key="d4">1.0</data>
      <data key="d5">Gamma B represents the difference in slope for brand B compared to brand A in the regression model</data>
      <data key="d6">b6870535f3975c49d45e62fbe475f198</data>
    </edge>
    <edge source="SALES_VOLUME" target="GAMMA_C">
      <data key="d4">1.0</data>
      <data key="d5">Gamma C represents the difference in slope for brand C compared to brand A in the regression model</data>
      <data key="d6">b6870535f3975c49d45e62fbe475f198</data>
    </edge>
    <edge source="SALES_VOLUME" target="INTERACTION">
      <data key="d4">1.0</data>
      <data key="d5">The expected sales volume can be influenced by the interaction between price and brand, affecting the response variable</data>
      <data key="d6">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </edge>
    <edge source="FIGURE_9.2" target="DATAPOINTS">
      <data key="d4">1.0</data>
      <data key="d5">Datapoints in Figure 9.2 are color-coded and have different plotting symbols according to brand, showing the relationship between sales volume and price.</data>
      <data key="d6">e079b7c92d5c0b009ff02040eb652bc6</data>
    </edge>
    <edge source="BRAND_MODEL1" target="MUA">
      <data key="d4">1.0</data>
      <data key="d5">MuA is a parameter in the model Brand.model1, representing the intercept for Brand A</data>
      <data key="d6">ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </edge>
    <edge source="BRAND_MODEL1" target="MUB">
      <data key="d4">1.0</data>
      <data key="d5">MuB is a parameter in the model Brand.model1, representing the intercept for Brand B</data>
      <data key="d6">ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </edge>
    <edge source="BRAND_MODEL1" target="MUC">
      <data key="d4">1.0</data>
      <data key="d5">MuC is a parameter in the model Brand.model1, representing the intercept for Brand C</data>
      <data key="d6">ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </edge>
    <edge source="BRAND_MODEL1" target="FIGURE_9_3">
      <data key="d4">1.0</data>
      <data key="d5">Brand.model1 is the model whose results are visualized in Figure 9.3</data>
      <data key="d6">ee295ebc4c2d6a2a5c738796fcc2ab71</data>
    </edge>
    <edge source="MUA" target="SALESJ">
      <data key="d4">1.0</data>
      <data key="d5">muA is a parameter in the model for Salesj, representing the effect of Brand A</data>
      <data key="d6">228bdca7843406def245d755e8df49f6</data>
    </edge>
    <edge source="MUA" target="SALESI">
      <data key="d4">1.0</data>
      <data key="d5">muA is the intercept parameter for Brand A in the salesi vector</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="MUB" target="SALESJ">
      <data key="d4">1.0</data>
      <data key="d5">muB is a parameter in the model for Salesj, representing the effect of Brand B</data>
      <data key="d6">228bdca7843406def245d755e8df49f6</data>
    </edge>
    <edge source="MUB" target="SALESI">
      <data key="d4">1.0</data>
      <data key="d5">muB is the intercept parameter for Brand B in the salesi vector</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="MUB" target="ALPHAB">
      <data key="d4">1.0</data>
      <data key="d5">alphaB is the difference in intercept for Brand B compared to Brand A, affecting muB</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="MUB" target="BRAND_MODEL4">
      <data key="d4">1.0</data>
      <data key="d5">Mu hat (&#181;b) is the estimated intercept parameter from brand.model4</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="MUC" target="SALESJ">
      <data key="d4">1.0</data>
      <data key="d5">muC is a parameter in the model for Salesj, representing the effect of Brand C</data>
      <data key="d6">228bdca7843406def245d755e8df49f6</data>
    </edge>
    <edge source="MUC" target="SALESI">
      <data key="d4">1.0</data>
      <data key="d5">muC is the intercept parameter for Brand C in the salesi vector</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="MUC" target="ALPHAC">
      <data key="d4">1.0</data>
      <data key="d5">alphaC is the difference in intercept for Brand C compared to Brand A, affecting muC</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="FIGURE_9_3" target="PARALLEL_LINES_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Figure 9.3 displays the fitted regression lines from the parallel lines model</data>
      <data key="d6">b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </edge>
    <edge source="PARALLEL_LINES_MODEL" target="MU_HAT_A">
      <data key="d4">1.0</data>
      <data key="d5">The parallel lines model uses the estimated intercept Mu hat A (&#181;&#710;A) for Brand A</data>
      <data key="d6">b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </edge>
    <edge source="PARALLEL_LINES_MODEL" target="MU_HAT_B">
      <data key="d4">1.0</data>
      <data key="d5">The parallel lines model uses the estimated intercept Mu hat B (&#181;&#710;B) for Brand B</data>
      <data key="d6">b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </edge>
    <edge source="PARALLEL_LINES_MODEL" target="MU_HAT_C">
      <data key="d4">1.0</data>
      <data key="d5">The parallel lines model uses the estimated intercept Mu hat C (&#181;&#710;C) for Brand C</data>
      <data key="d6">b2c33cb151a8e7724ebfb7b2d88bc45f</data>
    </edge>
    <edge source="PARALLEL_LINES_MODEL" target="ALPHA_B">
      <data key="d4">1.0</data>
      <data key="d5">Alpha B is a parameter in the parallel lines model</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="PARALLEL_LINES_MODEL" target="ALPHA_C">
      <data key="d4">1.0</data>
      <data key="d5">Alpha C is a parameter in the parallel lines model</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="PARALLEL_LINES_MODEL" target="GAMMA_A">
      <data key="d4">1.0</data>
      <data key="d5">Gamma A is a parameter in the parallel lines model</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="PARALLEL_LINES_MODEL" target="GAMMA_B">
      <data key="d4">1.0</data>
      <data key="d5">Gamma B is a parameter in the parallel lines model</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="PARALLEL_LINES_MODEL" target="INDICATOR_VARIABLES">
      <data key="d4">1.0</data>
      <data key="d5">Indicator variables are used in the parallel lines model to represent the brand of the store</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="PARALLEL_LINES_MODEL" target="INTERACTION">
      <data key="d4">1.0</data>
      <data key="d5">Interaction is a concept used in the parallel lines model to assess the effect of one explanatory variable on the response depending on the value of another explanatory variable</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="PARALLEL_LINES_MODEL" target="FIGURE_9_4">
      <data key="d4">1.0</data>
      <data key="d5">Figure 9.4 is a graphical representation of the parallel lines model</data>
      <data key="d6">0cb40986e6c2bb439e1ffcaae2df96ac</data>
    </edge>
    <edge source="STORE_BRAND" target="XJA">
      <data key="d4">1.0</data>
      <data key="d5">XjA is an indicator variable that encodes the store brand information for brand A</data>
      <data key="d6">248924760a2bfbc82501fd6b11cfa0aa</data>
    </edge>
    <edge source="STORE_BRAND" target="XJB">
      <data key="d4">1.0</data>
      <data key="d5">XjB is an indicator variable that encodes the store brand information for brand B</data>
      <data key="d6">248924760a2bfbc82501fd6b11cfa0aa</data>
    </edge>
    <edge source="STORE_BRAND" target="XJC">
      <data key="d4">1.0</data>
      <data key="d5">XjC is an indicator variable that encodes the store brand information for brand C</data>
      <data key="d6">248924760a2bfbc82501fd6b11cfa0aa</data>
    </edge>
    <edge source="XJB" target="DESIGN_MATRIX_X">
      <data key="d4">1.0</data>
      <data key="d5">xjB is a column in the design matrix X, representing the presence of Brand B in each store</data>
      <data key="d6">c103c6d096d52868eda26d991194b5f2</data>
    </edge>
    <edge source="XJC" target="DESIGN_MATRIX_X">
      <data key="d4">1.0</data>
      <data key="d5">xjC is a column in the design matrix X, representing the presence of Brand C in each store</data>
      <data key="d6">c103c6d096d52868eda26d991194b5f2</data>
    </edge>
    <edge source="SALESJ" target="PRICEJ">
      <data key="d4">1.0</data>
      <data key="d5">Pricej is a variable in the model for Salesj</data>
      <data key="d6">228bdca7843406def245d755e8df49f6</data>
    </edge>
    <edge source="PRICEJ" target="SALESI">
      <data key="d4">1.0</data>
      <data key="d5">pricej is the price for the jth store, influencing salesi</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="BRANDA" target="BRAND_MODEL2">
      <data key="d4">1.0</data>
      <data key="d5">BrandA is one of the brands analyzed in Brand.model2, with a specific coefficient</data>
      <data key="d6">93da9813e10a119798de6982977f1239</data>
    </edge>
    <edge source="BRANDB" target="BRAND_MODEL2">
      <data key="d4">1.0</data>
      <data key="d5">BrandB is one of the brands analyzed in Brand.model2, with a specific coefficient</data>
      <data key="d6">93da9813e10a119798de6982977f1239</data>
    </edge>
    <edge source="BRANDC" target="BRAND_MODEL2">
      <data key="d4">1.0</data>
      <data key="d5">BrandC is one of the brands analyzed in Brand.model2, with a specific coefficient</data>
      <data key="d6">93da9813e10a119798de6982977f1239</data>
    </edge>
    <edge source="C_PRICE" target="BRAND_MODEL2">
      <data key="d4">1.0</data>
      <data key="d5">c_price is a variable in Brand.model2, used to adjust for price differences</data>
      <data key="d6">93da9813e10a119798de6982977f1239</data>
    </edge>
    <edge source="BRAND_MODEL3" target="MU_HAT">
      <data key="d4">1.0</data>
      <data key="d5">Brand_model3 is the model that provides the estimate for the intercept parameter Mu_hat</data>
      <data key="d6">6a6f85d0a6e46196ab3a901fcc82a720</data>
    </edge>
    <edge source="BRAND_MODEL3" target="ALPHA_HAT_B">
      <data key="d4">1.0</data>
      <data key="d5">Brand_model3 is the model that provides the estimate for the brand B parameter Alpha_hat_B</data>
      <data key="d6">6a6f85d0a6e46196ab3a901fcc82a720</data>
    </edge>
    <edge source="BRAND_MODEL3" target="ALPHA_HAT_C">
      <data key="d4">1.0</data>
      <data key="d5">Brand_model3 is the model that provides the estimate for the brand C parameter Alpha_hat_C</data>
      <data key="d6">6a6f85d0a6e46196ab3a901fcc82a720</data>
    </edge>
    <edge source="SA" target="SALESI">
      <data key="d4">1.0</data>
      <data key="d5">The salesi vector contains sales data for observations in the set SA</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="SA" target="NA">
      <data key="d4">1.0</data>
      <data key="d5">nA is the number of elements in the set SA</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="SB" target="SALESI">
      <data key="d4">1.0</data>
      <data key="d5">The salesi vector contains sales data for observations in the set SB</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="SB" target="NB">
      <data key="d4">1.0</data>
      <data key="d5">nB is the number of elements in the set SB</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="SC" target="SALESI">
      <data key="d4">1.0</data>
      <data key="d5">The salesi vector contains sales data for observations in the set SC</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="SC" target="NC">
      <data key="d4">1.0</data>
      <data key="d5">nC is the number of elements in the set SC</data>
      <data key="d6">925e17c26fb7d979f52538f4632333e7</data>
    </edge>
    <edge source="REFERENCE_CATEGORY" target="INDICATOR_VARIABLES">
      <data key="d4">1.0</data>
      <data key="d5">The reference category is determined by the categorical predictor and is the level not represented by an indicator variable in the model</data>
      <data key="d6">77e76692753fdf53493182b09018e6bc</data>
    </edge>
    <edge source="INTERACTION" target="FACTOR">
      <data key="d4">1.0</data>
      <data key="d5">Factors, such as brand, can interact with quantitative variables, like price, in regression models</data>
      <data key="d6">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </edge>
    <edge source="INTERACTION" target="PRODUCT_TERMS">
      <data key="d4">1.0</data>
      <data key="d5">Product terms are introduced into linear models to represent interaction, where the effect of one explanatory variable on the response variable depends on the value of another explanatory variable</data>
      <data key="d6">77e76692753fdf53493182b09018e6bc</data>
    </edge>
    <edge source="INTERACTION" target="PARALLEL_LINES_REGRESSION_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">An interaction between the categorical predictor and the quantitative predictor can be introduced into a parallel lines regression model to allow for the relationship between the response and the quantitative predictor to vary across levels of the categorical predictor.</data>
      <data key="d6">d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </edge>
    <edge source="INDICATOR_VARIABLES" target="CATEGORICAL_PREDICTORS">
      <data key="d4">1.0</data>
      <data key="d5">Categorical predictors are encoded using indicator variables in linear models to accommodate their discrete nature within the model framework</data>
      <data key="d6">77e76692753fdf53493182b09018e6bc</data>
    </edge>
    <edge source="BRAND_MODEL4" target="BBETA">
      <data key="d4">1.0</data>
      <data key="d5">Beta hat (b&#946;) is the estimated coefficient for the price predictor from brand.model4</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="BRAND_MODEL4" target="ALPHAB_B">
      <data key="d4">1.0</data>
      <data key="d5">Alpha hat for brand B (&#945;bB) is the estimated parameter for brand B from brand.model4</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="BRAND_MODEL4" target="ALPHAB_C">
      <data key="d4">1.0</data>
      <data key="d5">Alpha hat for brand C (&#945;bC) is the estimated parameter for brand C from brand.model4</data>
      <data key="d6">825b600cbab3535ce67e9f561ddcb84b</data>
    </edge>
    <edge source="FACTOR" target="INDICATOR_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">Indicator variables are used to represent factors in regression models, allowing for the accommodation of categorical predictors</data>
      <data key="d6">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </edge>
    <edge source="FACTOR" target="DUMMY_VARIABLE">
      <data key="d4">1.0</data>
      <data key="d5">Dummy variables are a specific type of indicator variable used to encode factors in regression models</data>
      <data key="d6">b0ca3e6c22c4cf884d03b1f6f82be5df</data>
    </edge>
    <edge source="CATEGORICAL_PREDICTORS" target="PARALLEL_LINES_REGRESSION_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">A parallel lines regression model can be extended by introducing an interaction between the categorical predictor and the quantitative predictor. This allows for the relationship between the response and the quantitative predictor to vary across levels of the categorical predictor.</data>
      <data key="d6">d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </edge>
    <edge source="T_STATISTIC" target="MAXIMUM_LIKELIHOOD_ESTIMATOR">
      <data key="d4">1.0</data>
      <data key="d5">The maximum likelihood estimator for the parameters of a linear model is used in the calculation of the T-statistic. The T-statistic is calculated by dividing the estimated parameter by its standard error, which is based on the maximum likelihood estimator.</data>
      <data key="d6">d7f3a28534ffe830fe6f4cef8c41a9b4</data>
    </edge>
    <edge source="U" target="V">
      <data key="d4">2.0</data>
      <data key="d5">U and V, as independent random variables, exhibit a significant statistical property as highlighted in Lemma 10.3. This lemma asserts that the independence of U and V is confirmed when their covariance is zero. This zero covariance indicates that there is no linear relationship between the two variables, meaning that the variation in one variable does not influence the variation in the other. This property is pivotal in understanding the structure and relations within the context of algorithmic analysis, particularly in statistical inference, where the independence of variables is crucial for accurate modeling and prediction. The confirmation of independence through zero covariance is a robust criterion that ensures the variables U and V can be analyzed separately without considering their mutual influence, providing a clear and concise summary of their relationship within the statistical framework.</data>
      <data key="d6">0ac60299320c55d642b3e38440c25f90,aac5b4f040b9c773bd1aa696dec469f6</data>
    </edge>
    <edge source="U" target="COV_U_V">
      <data key="d4">1.0</data>
      <data key="d5">The covariance between U and V being zero indicates that U and V are uncorrelated</data>
      <data key="d6">0ac60299320c55d642b3e38440c25f90</data>
    </edge>
    <edge source="V" target="COV_U_V">
      <data key="d4">1.0</data>
      <data key="d5">The covariance between U and V being zero indicates that U and V are uncorrelated</data>
      <data key="d6">0ac60299320c55d642b3e38440c25f90</data>
    </edge>
  </graph>
</graphml>