<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d6" for="edge" attr.name="source_id" attr.type="string" />
  <key id="d5" for="edge" attr.name="description" attr.type="string" />
  <key id="d4" for="edge" attr.name="weight" attr.type="double" />
  <key id="d3" for="node" attr.name="entity_type" attr.type="string" />
  <key id="d2" for="node" attr.name="source_id" attr.type="string" />
  <key id="d1" for="node" attr.name="description" attr.type="string" />
  <key id="d0" for="node" attr.name="type" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="PYTHON 3.10-3.12">
      <data key="d0">SOFTWARE, VERSION</data>
      <data key="d1">Python 3.10-3.12 is the version of the Python programming language required for the development of GraphRAG. It is the foundation for running the library and its associated scripts.
Python 3.10 to 3.12 are the supported versions of the Python programming language for running the GraphRAG system. The system is designed to be compatible with these versions.</data>
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0,84d24b5db902baca7217b5e3bb6ec462</data>
    </node>
    <node id="POETRY">
      <data key="d0">SOFTWARE, TOOL</data>
      <data key="d1">Poetry is a tool used for managing dependencies and virtual environments in Python projects. It is essential for installing and managing the dependencies of GraphRAG.
Poetry is a package manager that can be used to run the CLI. It allows users to execute the pipeline commands by providing the necessary environment and dependencies.&gt;</data>
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0,b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="AZURITE">
      <data key="d0">SOFTWARE, EMULATOR</data>
      <data key="d1">Azurite is an emulator for Azure resources, used in unit and smoke tests to simulate Azure environments for testing purposes.</data>
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </node>
    <node id="INDEXING ENGINE">
      <data key="d0">SOFTWARE, ENGINE</data>
      <data key="d1">The Indexing Engine is a component of GraphRAG that can be executed using Poetry to process and index data for the graph database.</data>
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </node>
    <node id="QUERY ENGINE">
      <data key="d0">SOFTWARE, ENGINE</data>
      <data key="d1">The Query Engine is a component of GraphRAG that can be executed using Poetry to query the graph database.
The Query Engine is a retrieval module of the Graph RAG Library, responsible for tasks such as local search, global search, and question generation. It is one of the two main components of the Graph RAG library, the other being the Indexing Pipeline. The Query Engine is designed to generate answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents and searching over all AI-generated community reports in a map-reduce fashion. It also generates the next candidate questions based on a list of user queries. The local search method is suitable for questions that require an understanding of specific entities mentioned in the documents, while the global search method is resource-intensive but gives good responses for questions that require an understanding of the dataset as a whole. The question generation functionality is useful for generating follow-up questions in a conversation or for generating a list of questions for the investigator to dive deeper into the dataset. The local search method combines structured data from the knowledge graph.&gt;
The Query Engine is a component of GraphRAG that allows users to ask questions and retrieve information from the indexed data. It supports different query methods, such as Global search for high-level questions and Local search for more specific inquiries.&gt;</data>
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0,7c1bad237a1ef86cb41b6c5dbad4ffc3,f8cf53ce98a8bc52581f7907ad98ef70</data>
    </node>
    <node id="LIFECYCLE SCRIPTS">
      <data key="d0">SOFTWARE, SCRIPTS</data>
      <data key="d1">Lifecycle Scripts are a set of scripts managed by Poetry and poethepoet, used for various tasks such as building, testing, and executing the GraphRAG package.</data>
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </node>
    <node id="UNIT AND SMOKE TESTS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </node>
    <node id="CLI">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">CLI stands for Command Line Interface, a tool used to interact with software through text commands. In this context, it refers to the interface for executing various tasks using the poetry tool.
CLI (Command Line Interface) is a tool that allows users to run the pipeline by providing commands and arguments. It can be used with the Config File in default or custom config mode. The CLI can be accessed via Poetry or Node, depending on the user's environment.&gt;</data>
      <data key="d2">563caa38fe33c495449888d62950b959,b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="POETRY RUN POE QUERY">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe query is a command used to run the Query CLI, which allows for executing queries through the command line interface.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY BUILD">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry build is a command that invokes the build process, creating a wheel file and other distributable artifacts for the package.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE TEST">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe test is a command that executes all tests for the package, ensuring its functionality and integrity.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE TEST_UNIT">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe test_unit is a command that specifically executes unit tests, which are tests that verify the correctness of individual units of code.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE TEST_INTEGRATION">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe test_integration is a command that executes integration tests, which are tests that verify the interaction between different parts of the system.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE TEST_SMOKE">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe test_smoke is a command that executes smoke tests, which are a preliminary set of tests to ensure that the system is in a stable state before more extensive testing.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE CHECK">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poe check is a command that performs a suite of static checks across the package, including formatting, documentation formatting, linting, security patterns, and type-checking.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE FIX">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe fix is a command that applies any available auto-fixes to the package, typically limited to formatting fixes.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE FIX_UNSAFE">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe fix_unsafe is a command that applies any available auto-fixes to the package, including those that may be unsafe, potentially altering the code in ways that could affect its functionality.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE FORMAT">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe format is a command that explicitly runs the formatter across the package, ensuring consistent code style and formatting.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="TROUBLESHOOTING">
      <data key="d0">SECTION, DOCUMENT</data>
      <data key="d1">Troubleshooting is a section that provides solutions to common problems encountered when using the software or executing commands. It includes specific steps to resolve issues such as missing dependencies or configuration errors.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="RUNTIMEERROR">
      <data key="d0">ERROR, SOFTWARE_ISSUE</data>
      <data key="d1">A RuntimeError occurred when executing the command 'poetry install', indicating that 'llvm-config' failed to execute. The error suggests that the user should point the LLVM_CONFIG environment variable to the path for llvm-config. This issue is related to the LLVM configuration and is a common software error encountered during the installation process.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="LLVM_CONFIG">
      <data key="d0">ENVIRONMENT_VARIABLE, CONFIGURATION</data>
      <data key="d1">LLVM_CONFIG is an environment variable that needs to be set to the path of the llvm-config executable. This variable is crucial for the proper functioning of LLVM-related tools and components in the software development environment.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="LLVM-9">
      <data key="d0">SOFTWARE, DEVELOPMENT_TOOL</data>
      <data key="d1">llvm-9 is a version of the LLVM compiler infrastructure, which is a collection of modular and reusable compiler and toolchain technologies. It is required for the installation process to proceed without errors related to the 'llvm-config' executable.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="LLVM-9-DEV">
      <data key="d0">SOFTWARE, DEVELOPMENT_TOOL</data>
      <data key="d1">llvm-9-dev is a development package for the LLVM compiler infrastructure version 9. It contains headers and libraries necessary for building software that uses LLVM, and is required for the installation process to proceed without errors related to the 'llvm-config' executable.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="POETRY INSTALL">
      <data key="d0">COMMAND, INSTALLATION_PROCESS</data>
      <data key="d1">poetry install is a command used in the poetry package manager to install all dependencies for a project as specified in the pyproject.toml file. This command is essential for setting up the development environment for a Python project.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="PYTHON.H">
      <data key="d0">FILE, HEADER_FILE</data>
      <data key="d1">Python.h is a header file that provides the definitions and declarations for the Python C API. It is required for building C extensions for Python and is missing when the error "numba/_pymodule.h:6:10: fatal error: Python.h: No such file or directory" occurs during the 'poetry install' process.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="PYTHON3.10-DEV">
      <data key="d0">SOFTWARE, DEVELOPMENT_TOOL</data>
      <data key="d1">python3.10-dev is a development package for Python version 3.10. It contains headers, libraries, and other resources necessary for building Python extensions and applications. It is required to resolve the error related to the missing Python.h file during the 'poetry install' process.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="GRAPHRAG_LLM_THREAD_COUNT">
      <data key="d0">ENVIRONMENT_VARIABLE, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_THREAD_COUNT is an environment variable that determines the number of threads used by the LLM (Language Model) component of the GraphRAG system. By default, it is set to 50, which can lead to high concurrency and potential performance issues. Modifying this value can help reduce the concurrency and improve system stability.
GRAPHRAG_LLM_THREAD_COUNT is a configuration property that specifies the number of threads to be used for processing requests to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 50, indicating that 50 threads will be used for processing requests.&gt;</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205,9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_THREAD_COUNT">
      <data key="d0">ENVIRONMENT_VARIABLE, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_EMBEDDING_THREAD_COUNT is an environment variable that determines the number of threads used by the embedding component of the GraphRAG system. By default, it is set to 50, which can lead to high concurrency and potential performance issues. Modifying this value can help reduce the concurrency and improve system stability.
</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205,2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG">
      <data key="d0">PRODUCT, TECHNOLOGY</data>
      <data key="d1">GraphRAG is a structured, hierarchical approach to Retrieval Augmented Generation (RAG) that involves extracting a knowledge graph from raw text, building a community hierarchy, generating summaries for these communities, and leveraging these structures for RAG-based tasks. It is designed to enhance the ability of Large Language Models (LLMs) to reason about private data.
GraphRAG is a software product that enhances the capabilities of Language Models (LMs) by using knowledge graphs to improve question-and-answer performance, particularly in reasoning about complex information. It is a Retrieval-Augmented Generation (RAG) technique that aims to provide better insights and understanding compared to Baseline RAG.
GraphRAG is a new approach developed by Microsoft Research to extend and enhance RAG. It uses Large Language Models (LLMs) to create a knowledge graph based on an input corpus, which is then augmented with community summaries and graph machine learning outputs to improve query responses. GraphRAG demonstrates substantial improvement in answering complex questions and outperforms other approaches applied to private datasets.
GraphRAG is a process that involves using graph machine learning to analyze text data. It includes indexing, slicing the input corpus into TextUnits, entity and relationship extraction, hierarchical clustering using the Leiden technique, and generating summaries of each community for holistic understanding of the dataset.
GraphRAG is a software tool or framework designed for reasoning over graphs, which can be used to analyze and process data represented as graphs, such as the relationships between entities in a corpus.
GraphRAG (Graph Retrieval-Augmented Generation) is a technology that enhances the ability to answer queries that require understanding of the entire dataset. It generates a knowledge graph from the dataset, which is used to identify themes and semantic clusters, enabling more effective global search.&gt;
GraphRAG is a tool that enables the analysis of large language model (LLM)-generated knowledge graphs, providing insights into the structure and themes of datasets. It organizes private datasets into meaningful semantic clusters that are pre-summarized, facilitating the summarization of themes in response to user queries.
GraphRAG is an application that uses the natural modularity of graphs to partition data for global summarization, distinguishing it from other graph-based RAG systems.
GraphRAG is a software tool that offers the capability to create domain adaptive templates for the generation of knowledge graphs. It is designed to improve the results of an Index Run by adapting to specific domains through the use of templates. The tool operates by loading inputs, dividing them into text units, and then executing a series of LLM (Language Model) invocations and template substitutions to generate the final prompts. The default values provided by the script are recommended, but users can customize the template generation algorithm by tweaking various parameters. The tool requires an initialized workspace with the graphrag.index --init command to function properly. The initialization process creates necessary configuration files and default prompts. For more information, refer to the Init Documentation.
GraphRAG is a software tool used for indexing and processing data. It requires configuration settings to be properly initialized and can generate default prompts for its operations.
GraphRAG is a system for managing and processing graph-based data, offering features such as indexing, querying, and adapting prompts for better data handling.</data>
      <data key="d2">12294feb07a1d202b27241eaaf64718b,32603b739bed06b4695b0cc3915b2c4b,369b39fdfd649d6df32a5d7b4cc559b7,3e143a60e2aeb57eb418a68d1484bbb3,812b3414c467da0b62f7932d2adcbad4,849698743b07680402ff8572b1c6c469,9b364093aeecfc789c70fc5bd9503487,d0f7c236538005bc3056b7daed2401d8,d441b136505c273cf3577b6867e872e4,e015335cdcae20e6546fe7cbdef56c1a,e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="GPT-4 TURBO">
      <data key="d0">PRODUCT, TECHNOLOGY</data>
      <data key="d1">GPT-4 Turbo is a version of the GPT-4 model that is used to generate knowledge graphs from text data. It is capable of creating detailed and structured representations of information, which can be used in various applications including GraphRAG.</data>
      <data key="d2">e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="MICROSOFT RESEARCH BLOG POST">
      <data key="d0">ONLINE_RESOURCE</data>
      <data key="d1">The Microsoft Research Blog Post is an online resource that provides information about GraphRAG and how it can be used to enhance the ability of Large Language Models (LLMs) to reason about private data. It offers insights into the GraphRAG system and its applications.</data>
      <data key="d2">e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="GITHUB REPOSITORY">
      <data key="d0">ONLINE_RESOURCE</data>
      <data key="d1">The GitHub Repository is an online resource where the source code and documentation for GraphRAG can be found. It is a platform for developers to access, contribute to, and collaborate on the GraphRAG project.</data>
      <data key="d2">e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="GRAPHRAG ARXIV">
      <data key="d0">ONLINE_RESOURCE</data>
      <data key="d1">GraphRAG Arxiv is an online resource that contains research papers and documentation related to GraphRAG. It is a repository for academic and technical papers that discuss the theory, implementation, and applications of GraphRAG.</data>
      <data key="d2">e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="SOLUTION ACCELERATOR">
      <data key="d0">PRODUCT, TECHNOLOGY</data>
      <data key="d1">The Solution Accelerator is a package that provides a user-friendly end-to-end experience with Azure resources for quickstarting the GraphRAG system. It is recommended for users who want to start using GraphRAG with ease.</data>
      <data key="d2">e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="GET STARTED GUIDE">
      <data key="d0">ONLINE_RESOURCE</data>
      <data key="d1">The Get Started guide is an online resource that provides instructions on how to start using GraphRAG. It offers a step-by-step guide for users who are new to the system.
The Get Started guide is a document that provides instructions and information for users to begin using GraphRAG effectively. It is a resource for those who are new to the software and want to understand its basic functionalities and setup process.</data>
      <data key="d2">32603b739bed06b4695b0cc3915b2c4b,e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="INDEXER">
      <data key="d0">PRODUCT, TECHNOLOGY</data>
      <data key="d1">The Indexer is a sub-system of GraphRAG that is responsible for extracting information from raw text and building a knowledge graph. It plays a crucial role in the GraphRAG process by creating a structured representation of information.
The Indexer is a subsystem of GraphRAG that is responsible for indexing and organizing data for efficient retrieval. It plays a crucial role in the software's ability to handle and process large datasets effectively.
The Indexer is a software tool that generates .parquet output files, which contain structured data for further analysis and processing.
The Indexer is a tool used to process and prepare data for search and analysis. It plays a crucial role in organizing the data in a way that makes it accessible for both Local and Global search methods, ensuring that the data is ready for querying and analysis.</data>
      <data key="d2">32603b739bed06b4695b0cc3915b2c4b,8c70a7321fb0e945054d226a8c69abee,ae6e91a8cc5773dbd4789773c9ef5a30,e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="QUERY PACKAGE">
      <data key="d0">PRODUCT, TECHNOLOGY</data>
      <data key="d1">The Query package is a sub-system of GraphRAG that is used for querying the knowledge graph. It allows users to search for information within the graph and retrieve relevant results.
The Query package is a subsystem of GraphRAG that enables users to search and retrieve information from the indexed data. It provides the functionality for users to interact with the knowledge graph and obtain relevant results.</data>
      <data key="d2">32603b739bed06b4695b0cc3915b2c4b,e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="SOLUTION ACCELERATOR PACKAGE">
      <data key="d0">PRODUCT, SOFTWARE</data>
      <data key="d1">The Solution Accelerator package is a software product designed to offer a user-friendly experience in managing and utilizing Azure resources, streamlining the process for end-users.</data>
      <data key="d2">32603b739bed06b4695b0cc3915b2c4b</data>
    </node>
    <node id="BASELINE RAG">
      <data key="d0">PRODUCT, SOFTWARE</data>
      <data key="d1">Baseline RAG is a Retrieval-Augmented Generation technique that uses vector similarity as the primary search method to enhance the outputs of Language Models (LMs) by incorporating real-world information. It is a foundational approach in LLM-based tools but has limitations in handling complex information and reasoning about private datasets.
Baseline RAG is a technology designed to help solve problems in communications by connecting disparate pieces of information. However, it struggles to connect the dots when answering questions that require traversing through shared attributes for new insights and performs poorly in understanding summarized semantic concepts over large data collections or singular large documents.
Baseline RAG (Retrieval-Augmented Generation) is a technology that performs a vector search of semantically similar text content within a dataset. It struggles with queries that require aggregation of information across the dataset, as it relies on finding text that is semantically similar to the query.&gt;</data>
      <data key="d2">32603b739bed06b4695b0cc3915b2c4b,3e143a60e2aeb57eb418a68d1484bbb3,d441b136505c273cf3577b6867e872e4</data>
    </node>
    <node id="LLMS">
      <data key="d0">TECHNOLOGY, ARTIFICIAL INTELLIGENCE</data>
      <data key="d1">LLMs, or Large Language Models, are AI models used in GraphRAG to create a knowledge graph from an input corpus. They are capable of understanding and generating human language, which makes them suitable for tasks such as entity and relationship extraction, and hierarchical clustering.


LLMs (Large Language Models) are advanced AI models that can process and generate human-like text, often used in various research studies and applications.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96,53455f8552b0787cb13c5a03eb550842,6dace8e490674ac8e031aed987a63789,d441b136505c273cf3577b6867e872e4</data>
    </node>
    <node id="LEIDEN TECHNIQUE">
      <data key="d0">METHOD, ALGORITHM</data>
      <data key="d1">The Leiden technique is a method used in GraphRAG for hierarchical clustering of the graph created from the input corpus. It is a refinement of the Louvain method and is used to visually represent the clustering of entities and relationships in the graph.
The Leiden technique is an algorithm used in the GraphRAG process for hierarchical clustering of the graph. It visually represents entities as circles, with the size indicating the degree of the entity and the color representing its community.</data>
      <data key="d2">369b39fdfd649d6df32a5d7b4cc559b7,d441b136505c273cf3577b6867e872e4</data>
    </node>
    <node id="TEXTUNITS">
      <data key="d0">DATA UNIT</data>
      <data key="d1">TextUnits are the analyzable units created by slicing up the input corpus in the GraphRAG process. They serve as the basis for entity, relationship, and key claim extraction and provide fine-grained references into the outputs.</data>
      <data key="d2">369b39fdfd649d6df32a5d7b4cc559b7</data>
    </node>
    <node id="LLM">
      <data key="d0">TECHNOLOGY, TOOL</data>
      <data key="d1">LLM (Language Model) is a technology used in the GraphRAG process for extracting entities, relationships, and key claims from the TextUnits.


llm refers to the top-level configuration for the language model, which includes settings that define how the model operates and is used in the system.
LLM (Language Model) is a configuration property used in entity extraction, specifying the language model to use for text analysis.
llm (language model) is a configuration setting that specifies the language model to be used in various processes such as entity extraction, summarization, and claim extraction. This setting is crucial for determining the AI or machine learning model that will process the text.&gt;
llm is a configuration field within community_reports that refers to the LLM (Language Model) top-level configuration. It is used to specify settings related to the language model used in the system.
LLM (Language Learning Model) is a technology used for summarizing text, entity resolution, and claim extraction. It helps in capturing distinct information from descriptions, resolving entities that represent the same real-world entity but have different names, and extracting claims from source TextUnits.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc,369b39fdfd649d6df32a5d7b4cc559b7,3e143a60e2aeb57eb418a68d1484bbb3,53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,9cbd4e21339eeed5e22a638e52a094cb,abac77a5673e907cf8d65161c2612784,d27237468a1b9e89110eeeca8080f63c,d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </node>
    <node id="COMMUNITY SUMMARIES">
      <data key="d0">DATA, OUTPUT</data>
      <data key="d1">Community Summaries are generated from the bottom-up in the GraphRAG process. They provide a holistic understanding of each community and its constituents within the dataset.
Community summaries are condensed versions of information or data, often created by or for a community, which can be used to provide an overview or key insights from a larger set of data or documents.
Community Summaries are pre-generated summaries for groups of closely-related entities identified in the entity knowledge graph. These summaries are used to generate partial responses to a given question, which are then combined into a final response.
Community summaries are pre-generated summaries for groups of closely-related entities. They are used to generate partial responses to questions, which are then summarized into a final response to the user.
Community Summaries are report-like summaries created for each community in the Leiden hierarchy. These summaries are designed to scale to very large datasets and are independently useful for understanding the global structure and semantics of the dataset. They can be used to make sense of a corpus in the absence of a specific question and can serve as part of a graph-based index for answering global queries.
Community summaries are generated for leaf-level and higher-level communities to provide an overview of the elements (nodes, edges, covariates) within a corpus. They are prioritized based on the prominence of the elements and are used to make sense of a corpus or to answer global queries. For leaf-level communities, element summaries are added to the LLM context window until the token limit is reached. For higher-level communities, if all element summaries fit within the token limit, they are summarized as is; otherwise, sub-community summaries are substituted for their associated element summaries to fit within the context window.
Community Summaries are summaries of information within a community, which can be at different hierarchical levels. They are generated to provide a condensed view of the community's content, suitable for answering questions and providing insights. The summaries are ranked based on the number of tokens they contain, and shorter summaries may replace longer ones to fit within the context window. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process. The community summaries are a key component in the multi-stage process of generating a final answer to a user query. The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. The summaries can be used to generate community answers and ultimately a global answer to a user query. The hierarchical nature of the community structure allows for answering questions at different levels, which may offer varying balances of detail and scope for general sensemaking questions. The effectiveness of different levels is evaluated in section 3 of the text. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out
Community Summaries are summaries at different levels of each graph community hierarchy, as shown in Table 3.
Community Summaries refer to condensed versions of source texts, categorized into different levels (C0-C3). These summaries are created to provide a more concise representation of the original content, with C0 being the root-level summary and C3 the low-level summary.&gt;
Community summaries are a kind of self-memory for generation-augmented retrieval that facilitates future generation cycles. These summaries are created from community data and are used to enhance the retrieval process by providing additional context and information.&gt;
Community summaries are summaries generated for specific communities or groups, which can provide generic overviews of the content and can be a valuable aspect of the graph index approach.</data>
      <data key="d2">369b39fdfd649d6df32a5d7b4cc559b7,7040ba36a7c09899a355d14a30d65375,71f14506a6b15dfabd93fd1606a67b73,7da3d8d244b67f09425a4a7783e4bb55,849698743b07680402ff8572b1c6c469,93d4d4effbf989e6ef1c4c3b4f42494e,9b52298451f8936974ab08a129b0b92e,a660289d2bf43f25d3524d35cd2d9a96,b149708d0b4ac3ff417565739ea6b03b,d08fc91bbfe9749abab38a99a1a88dc6,f76c18c7582167c3626f8741c2c9374f</data>
    </node>
    <node id="GLOBAL SEARCH">
      <data key="d0">FUNCTION, QUERY MODE</data>
      <data key="d1">Global Search is a query mode in the GraphRAG process that allows reasoning about holistic questions concerning the corpus by leveraging the community summaries.
Global search is a method used by the Query Engine to generate answers by searching over all AI-generated community reports in a map-reduce fashion. This is a resource-intensive method, but often gives good responses for questions that require an understanding of the dataset as a whole.&gt;
Global Search is a method that allows for the aggregation of information across an entire dataset to answer complex queries. It utilizes the structure of an LLM-generated knowledge graph to identify themes and semantic clusters within the dataset, enabling the LLM to summarize these themes in response to user queries.&gt;
Global Search is a method used to ask high-level questions about a dataset, enabling the extraction of broad themes or insights from the data. It is a technique that can be applied after the Indexer has processed the data, allowing for the exploration of the dataset's overall content and structure.</data>
      <data key="d2">369b39fdfd649d6df32a5d7b4cc559b7,3e143a60e2aeb57eb418a68d1484bbb3,ae6e91a8cc5773dbd4789773c9ef5a30,f8cf53ce98a8bc52581f7907ad98ef70</data>
    </node>
    <node id="LOCAL SEARCH">
      <data key="d0">FUNCTION, QUERY MODE</data>
      <data key="d1">Local Search is a query mode in the GraphRAG process that enables reasoning about specific entities by fanning out to their neighbors and associated concepts.
Local Search is an algorithm or technique used for reasoning about specific entities by exploring their immediate neighbors and associated concepts in a graph or network.
Local search is a method used by the Query Engine to generate answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents.&gt;
Local Search is a method that combines structured data from the knowledge graph with unstructured data from input documents to augment the context of a language model with relevant entity information at query time. It is particularly suited for answering questions that require an understanding of specific entities mentioned in the input documents.
Local Search is a method used to find relevant information within a specific context or dataset. It can be used to extract and prioritize relevant structured and unstructured data, including entities, relationships, and other data records, which are then used to generate context for question generation.
Local Search refers to a method of searching within a dataset for information that is semantically similar to a query. It uses a context builder class to generate context for the search and can be enhanced with various parameters such as llm_params and context_builder_params. It is limited in its ability to aggregate information across the entire dataset.&gt;
Local Search is a method used to ask specific questions about particular aspects or entities within a dataset. It is designed to provide detailed information about specific characters, themes, or relationships within the data, offering a more targeted approach to data exploration.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4,364624242a84e1859e758069d914d8c8,369b39fdfd649d6df32a5d7b4cc559b7,3e143a60e2aeb57eb418a68d1484bbb3,849698743b07680402ff8572b1c6c469,ae6e91a8cc5773dbd4789773c9ef5a30,f8cf53ce98a8bc52581f7907ad98ef70</data>
    </node>
    <node id="PROMPT TUNING">
      <data key="d0">FUNCTION, TECHNIQUE</data>
      <data key="d1">Prompt Tuning is a recommended technique for optimizing the use of GraphRAG with specific data. It involves fine-tuning prompts according to the Prompt Tuning Guide in the documentation to achieve the best possible results.
Prompt Tuning is a technique or process used to fine-tune prompts, which are input sequences or questions, to improve the performance of models, especially in the context of natural language processing and graph-based reasoning.
Prompt Tuning is a feature of the GraphRAG indexing engine that allows for the customization of prompts used in the generation of the knowledge graph. It is an optional step but highly encouraged as it can improve the results of an Index Run by creating domain adaptive templates. The process involves loading inputs, splitting them into text units, and running a series of LLM invocations and template substitutions to generate the final prompts. Default values are provided by the system for ease of use.
Prompt Tuning is the process of customizing the prompts used by the GraphRAG indexer to better suit specific use cases. It involves specifying a custom prompt file, which is then used internally with a series of token replacements to enhance the effectiveness of knowledge discovery.
Prompt Tuning refers to the process of customizing the default prompts used by the GraphRAG indexer to better suit specific use cases in knowledge discovery. This involves writing a custom prompt file in plaintext and utilizing token-replacements for flexibility and customization.</data>
      <data key="d2">369b39fdfd649d6df32a5d7b4cc559b7,4f37c0e9c3c9bac4e5c1c6821eea442e,6a7157695d90d434b2625c3f05420916,849698743b07680402ff8572b1c6c469,bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="CORPUS">
      <data key="d0">DOCUMENT, DATA</data>
      <data key="d1">The corpus refers to a collection of documents or data that is being analyzed or processed, often used in natural language processing tasks to train models or extract information.</data>
      <data key="d2">849698743b07680402ff8572b1c6c469</data>
    </node>
    <node id="INDEXING PIPELINE">
      <data key="d0">SOFTWARE, MODULE</data>
      <data key="d1">The Indexing Pipeline is one of the two main components of the Graph RAG library, the other being the Query Engine. It is responsible for indexing and preparing data for the Query Engine to perform searches and generate answers.&gt;
The Indexing pipeline is a process for indexing data using GraphRAG. It involves running a Python script with specific parameters, such as the root directory and model configuration, to create an index of the data for later querying.&gt;
The Indexing Pipeline is a system component that indexes your data, making it searchable and accessible for various operations and analyses.
The Indexing Pipeline is a process that indexes data for use with GraphRAG. It can be run after initializing the workspace and adapting prompts.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,7c1bad237a1ef86cb41b6c5dbad4ffc3,d0f7c236538005bc3056b7daed2401d8,f8cf53ce98a8bc52581f7907ad98ef70</data>
    </node>
    <node id="QUESTION GENERATION">
      <data key="d0">FUNCTION, PROCESS</data>
      <data key="d1">Question generation is a functionality of the Query Engine that takes a list of user queries and generates the next candidate questions. This is useful for generating follow-up questions in a conversation or for generating a list of questions for the investigator to dive deeper into the dataset.&gt;
Question Generation is a feature that takes a list of user queries and generates the next candidate questions. It is useful for creating follow-up questions in a conversation or for generating a list of questions for an investigator to explore a dataset more thoroughly. Information about how this functionality works can be found at the Question Generation documentation page.
Question Generation is a method that combines structured data from a knowledge graph with unstructured data from input documents to generate candidate questions related to specific entities. This process is used to create follow-up questions that represent important or urgent information content or themes in the data.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4,364624242a84e1859e758069d914d8c8,f8cf53ce98a8bc52581f7907ad98ef70</data>
    </node>
    <node id="GLOBAL SEARCH DOCUMENTATION">
      <data key="d0">DOCUMENT, REFERENCE</data>
      <data key="d1">The Global Search documentation provides detailed information about the Global Search functionality, which is a tool for searching and retrieving information across various sources. It is referenced as a source for more information about the topic discussed in the text.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="ENTITY-BASED REASONING">
      <data key="d0">CONCEPT, APPROACH</data>
      <data key="d1">Entity-based Reasoning is an approach used in the local search method. It involves the use of entities and their relationships to reason about information in the context of a user query. This approach is effective for answering questions that require knowledge about specific entities and their properties.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="EMBEDDING">
      <data key="d0">PROPERTY, TECHNIQUE</data>
      <data key="d1">Embedding is a technique used in the local search method to represent entities and text in a numerical format that can be processed by machine learning models. It is a key component in the Entity-Text Unit Mapping process.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="ENTITY-TEXT UNIT MAPPING">
      <data key="d0">PROPERTY, TECHNIQUE</data>
      <data key="d1">Entity-Text Unit Mapping is a process in the local search method that associates entities with relevant text units. This mapping is used to identify and prioritize text units that are relevant to a user query.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="RANKING + FILTERING">
      <data key="d0">PROPERTY, TECHNIQUE</data>
      <data key="d1">Ranking + Filtering is a technique used in the local search method to prioritize and select the most relevant entities, text units, community reports, relationships, and covariates based on their relevance to a user query.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="ENTITY-REPORT MAPPING">
      <data key="d0">PROPERTY, TECHNIQUE</data>
      <data key="d1">Entity-Report Mapping is a process in the local search method that associates entities with relevant community reports. This mapping is used to identify and prioritize community reports that are relevant to a user query.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="ENTITY-ENTITY RELATIONSHIPS">
      <data key="d0">PROPERTY, CONCEPT</data>
      <data key="d1">Entity-Entity Relationships are connections between entities that are identified and prioritized in the local search method. These relationships are used to understand the connections between entities and to answer questions that require knowledge about these connections.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="ENTITY-COVARIATE MAPPINGS">
      <data key="d0">PROPERTY, TECHNIQUE</data>
      <data key="d1">Entity-Covariate Mappings is a process in the local search method that associates entities with relevant covariates. This mapping is used to identify and prioritize covariates that are relevant to a user query.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="USER QUERY">
      <data key="d0">INPUT, QUERY</data>
      <data key="d1">The User Query is the input provided by the user, which can be a question or a command, and is used to initiate the local search process in the knowledge graph. It serves as the starting point for entity and relationship extraction.
A user query is an input or request made by a user to GraphRAG, seeking information or answers based on the dataset and its themes. It can be accompanied by conversation history to provide additional context.
A user query is a specific question or request for information posed by a user. It can be used to navigate through community summaries or to seek answers to questions about the corpus.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea,812b3414c467da0b62f7932d2adcbad4,93d4d4effbf989e6ef1c4c3b4f42494e</data>
    </node>
    <node id="CONVERSATION HISTORY">
      <data key="d0">INPUT, CONTEXT</data>
      <data key="d1">Conversation History is the record of previous interactions between the user and the system. It provides context for the current user query and can influence the search and response generation process.
Conversation history is the record of previous interactions or queries made by the user. It serves as additional context for GraphRAG to generate more informed and relevant responses.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea,812b3414c467da0b62f7932d2adcbad4</data>
    </node>
    <node id="LOCAL SEARCH DATAFLOW">
      <data key="d0">PROCESS, ALGORITHM</data>
      <data key="d1">Local Search Dataflow is the method used to identify and prioritize entities, relationships, and covariates from the knowledge graph based on the user query and conversation history. It extracts relevant details and text chunks from the input documents and prioritizes them for fitting within a single context window.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="KNOWLEDGE GRAPH">
      <data key="d0">DATABASE, INFORMATION</data>
      <data key="d1">The Knowledge Graph is a structured database that contains entities, relationships, and covariates. It serves as the source of information for the local search method, providing the context and details needed to generate a response to the user query.
A Knowledge Graph is a database that uses graph theory to organize data into entities and their relationships. It is used to store and represent structured data in a way that can be easily queried and analyzed, making it a valuable resource for question generation and other AI applications.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4,3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="PRIORITIZED TEXT UNITS">
      <data key="d0">OUTPUT, INFORMATION</data>
      <data key="d1">Prioritized Text Units are the relevant text chunks extracted from the raw input documents and prioritized by the local search method. They are associated with the identified entities and are used to generate a response to the user query.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="PRIORITIZED COMMUNITY REPORTS">
      <data key="d0">OUTPUT, INFORMATION</data>
      <data key="d1">Prioritized Community Reports are the relevant reports extracted from the knowledge graph and prioritized by the local search method. They are associated with the identified entities and are used to generate a response to the user query.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="PRIORITIZED ENTITIES">
      <data key="d0">OUTPUT, INFORMATION</data>
      <data key="d1">Prioritized Entities are the entities identified from the knowledge graph and prioritized by the local search method. They serve as access points into the knowledge graph and are used to extract further relevant details.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="PRIORITIZED RELATIONSHIPS">
      <data key="d0">OUTPUT, INFORMATION</data>
      <data key="d1">Prioritized Relationships are the relationships identified from the knowledge graph and prioritized by the local search method. They provide context and details about the connections between entities.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="PRIORITIZED COVARIATES">
      <data key="d0">OUTPUT, INFORMATION</data>
      <data key="d1">Prioritized Covariates are the covariates identified from the knowledge graph and prioritized by the local search method. They provide additional details about the entities and their attributes.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="LOCALSEARCH CLASS">
      <data key="d0">CLASS, INFORMATION_RETRIEVAL</data>
      <data key="d1">The LocalSearch class is a component designed for searching and retrieving information from a collection of knowledge model objects. It prioritizes and filters candidate data sources to fit within a single context window of a pre-defined size, which is then used to generate a response to a user query. Key parameters include the OpenAI model object (llm), context builder object (context_builder), system prompt, response type, llm_params, context_builder_params, and callbacks for handling LLM's completion streaming events.</data>
      <data key="d2">25797740f434cc2bf16365fc498791f6</data>
    </node>
    <node id="QUESTION GENERATION METHOD">
      <data key="d0">METHOD, INFORMATION_EXTRACTION</data>
      <data key="d1">The Question Generation method is a process that combines structured data from the knowledge graph with unstructured data from input documents to generate candidate questions related to specific entities. This method uses the same context-building approach as the LocalSearch class to extract and prioritize relevant structured and unstructured information.</data>
      <data key="d2">25797740f434cc2bf16365fc498791f6</data>
    </node>
    <node id="CONFIGURATION PARAMETERS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Configuration parameters for the LocalSearch class include llm (the OpenAI model object), context_builder (the context builder object), system_prompt (the prompt template for generating search responses), response_type (the desired format of the response), llm_params (additional parameters for the LLM call), context_builder_params (additional parameters for the context_builder), and callbacks (optional functions for handling LLM's completion streaming events).</data>
      <data key="d2">25797740f434cc2bf16365fc498791f6</data>
    </node>
    <node id="LLM COMPLETION STREAMING EVENTS">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">LLM Completion Streaming Events refer to the real-time updates or responses generated by Large Language Models (LLM) during the completion of tasks or queries. These events can be handled by custom event handlers to process or react to the information as it is being generated.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="ENTITY-BASED QUESTION GENERATION">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Entity-based Question Generation is a specific approach within the question generation method that focuses on generating questions related to specific entities. It utilizes both structured and unstructured data to create context and generate relevant questions.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="STRUCTURED DATA">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">Structured Data refers to the organized and formatted data that is typically stored in databases or knowledge graphs. This data is well-organized and can be easily searched and analyzed, making it useful for various applications including question generation.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="UNSTRUCTURED DATA">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">Unstructured Data refers to the data that does not have a predefined format or organization. It includes text documents, images, and other forms of data that are not easily searchable or analyzed without prior processing. Unstructured data is often used in conjunction with structured data to provide a more comprehensive context for question generation.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="OPENAI MODEL">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">An OpenAI Model is a type of artificial intelligence model developed by OpenAI. It can be used for various tasks, including response generation in the context of question generation.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="CONTEXT BUILDER">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">A Context Builder is a component used in question generation to prepare context data from collections of knowledge model objects. It uses the same context-building approach as in local search to extract and prioritize relevant data for the question generation process.
The Context Builder is a component of the GlobalSearch class that prepares context data from community reports for use in the map-reduce process.&gt;</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4,1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="SYSTEM PROMPT">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">A System Prompt is a template used to generate candidate questions in the question generation process. It can be customized to fit specific needs and is used as a starting point for the LLM to generate follow-up questions.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="LLM PARAMETERS">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">LLM Parameters are additional parameters that can be passed to the LLM call during the question generation process. These parameters can include settings such as temperature and max_tokens, which affect the behavior and output of the LLM.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="CONTEXT BUILDER PARAMETERS">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Context Builder Parameters are additional parameters that can be passed to the Context Builder object when building context for the question generation prompt. These parameters can be used to customize the context-building process to better fit specific needs.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="CALLBACKS">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Callbacks are optional functions that can be used to provide custom event handlers for LLM completion streaming events. They allow for custom processing or reactions to the information generated by the LLM during the completion of tasks or queries.
The callbacks are optional callback functions that can be used to provide custom event handlers for the LLM, allowing for monitoring and intervention during the processing stages.
callbacks are optional functions that can be registered to handle custom events, particularly completion streaming events from the LLM, allowing for real-time processing or logging.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4,e0cc1cf05b92456e09100790815186fe,e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="LLM (LANGUAGE MODEL)">
      <data key="d0">TOOL, TECHNOLOGY</data>
      <data key="d1">LLM (Language Model) is a type of artificial intelligence model that can generate text based on a given prompt. It is used in various search and question generation processes, such as in Local Search and Global Search, to generate context and responses.&gt;
LLM, or Language Model, is a type of artificial intelligence model used for generating human-like text. In the context of the GlobalSearch class, it is used for response generation.&gt;
LLM, or Language Model, is a sophisticated AI tool capable of understanding and generating human-like text. It is used for various tasks, including entity extraction, relationship detection, and abstractive summarization. The LLM can create meaningful summaries of concepts that may be implied but not explicitly stated in the source texts.
LLM, or Large Language Model, is a model capable of generating text and answering questions. In the context of the text, it is used to automate the generation of summarization queries for evaluating RAG systems in data sensemaking tasks.
LLM, or Language Model, is an AI system capable of generating text and questions based on given prompts. In this context, it is used to identify users, tasks, and generate questions for evaluation.
LLM, or Language Model, is an algorithm used to generate summaries of each community, enabling the understanding of the information contained within each community from either a high-level or a low-level perspective.</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc,3e143a60e2aeb57eb418a68d1484bbb3,5b2968b8f1c891d47ecbe641c3391663,805a07a8f9c2ed5da2d9a61356aafa77,8e69f04648f5fc24c299591365f1aa68,a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="QUESTION GENERATION FUNCTION">
      <data key="d0">FUNCTION, ACTIVITY</data>
      <data key="d1">The Question Generation Function is a process that generates candidate questions based on a given context. It can be customized with parameters such as system_prompt, llm_params, and context_builder_params, and can use callback functions for custom event handling.&gt;</data>
      <data key="d2">3e143a60e2aeb57eb418a68d1484bbb3</data>
    </node>
    <node id="LLM-GENERATED KNOWLEDGE GRAPH">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">The LLM-generated knowledge graph is a data structure that represents the relationships and themes within a dataset. It is used by GraphRAG to organize data into semantic clusters and summarize information.</data>
      <data key="d2">812b3414c467da0b62f7932d2adcbad4</data>
    </node>
    <node id="SEMANTIC CLUSTERS">
      <data key="d0">DATA ORGANIZATION</data>
      <data key="d1">Semantic clusters are groups of related data within the LLM-generated knowledge graph. They are organized by GraphRAG to provide a structured representation of the dataset, allowing for the summarization of themes and meaningful responses to user queries.</data>
      <data key="d2">812b3414c467da0b62f7932d2adcbad4</data>
    </node>
    <node id="GLOBAL SEARCH METHOD">
      <data key="d0">METHOD, PROCEDURE</data>
      <data key="d1">The global search method is a process used by GraphRAG to generate responses to user queries. It involves using a collection of LLM-generated community reports as context data, segmenting them into text chunks, and producing intermediate responses. The most important points from these intermediate responses are then aggregated to generate the final response.</data>
      <data key="d2">812b3414c467da0b62f7932d2adcbad4</data>
    </node>
    <node id="AGGREGATED INTERMEDIATE RESPONSES">
      <data key="d0">OUTPUT, RESULT</data>
      <data key="d1">Aggregated intermediate responses are the combined results of the intermediate responses generated during the global search method. They are formed by filtering and selecting the most important points from the intermediate responses, which are then used to generate the final response.</data>
      <data key="d2">812b3414c467da0b62f7932d2adcbad4</data>
    </node>
    <node id="GRAPH'S COMMUNITY HIERARCHY">
      <data key="d0">CONCEPT, HIERARCHY</data>
      <data key="d1">The Graph's Community Hierarchy is a structured organization of communities within a graph, which can be used as context data for generating responses in a map-reduce manner. Different levels of the hierarchy provide varying degrees of detail in community reports.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="MAP-REDUCE PROCESS">
      <data key="d0">CONCEPT, PROCESS</data>
      <data key="d1">The Map-Reduce Process is a computational paradigm used to process large data sets. In the context of generating responses, it involves segmenting community reports into text chunks, producing intermediate responses with points rated for importance, and aggregating the most important points to generate the final response.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="COMMUNITY REPORTS">
      <data key="d0">DATA, DOCUMENT</data>
      <data key="d1">Community Reports are documents that contain information about communities at various levels of the graph's community hierarchy. These reports are segmented into text chunks during the map step of the map-reduce process.&gt;
Community Reports are a type of output from the GraphRAG system that provides insights and summaries based on the input data. These reports can be tailored to the needs of specific communities or user groups.
Community Reports are generated documents that summarize findings, insights, and data from a community or a specific context. They can include tables of entities and relationships extracted from the input text, providing a structured overview of the information.&gt;
Community Reports are summaries generated for each community, providing insights into the distinct information contained within each community and offering a scoped understanding of the graph from high-level to low-level perspectives.
Community Reports are summaries generated for each community using the LLM, providing an overview of the distinct information within each community, including key entities, relationships, and claims. They are created from a high-level or low-level perspective depending on the community's level in the hierarchy.</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc,21cdf11c58927ae505d3d375d1b75c82,3e292d936b7efa377ba9530456cfd888,5b2968b8f1c891d47ecbe641c3391663,bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="INTERMEDIATE RESPONSE">
      <data key="d0">DATA, DOCUMENT</data>
      <data key="d1">An Intermediate Response is a document containing a list of points, each accompanied by a numerical rating indicating the importance of the point. These responses are produced during the map step of the map-reduce process and are used as input for the reduce step.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="FINAL RESPONSE">
      <data key="d0">DATA, DOCUMENT</data>
      <data key="d1">The Final Response is the aggregated output of the map-reduce process, containing a filtered set of the most important points from the intermediate responses. It is generated using the context provided by the community reports and the map-reduce process.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="GLOBALSEARCH CLASS">
      <data key="d0">CONCEPT, CLASS</data>
      <data key="d1">The GlobalSearch Class is a software class that implements the map-reduce process for generating responses. It includes key parameters such as the LLM model, context builder, map and reduce system prompts, response type, and settings for including general knowledge.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="MAP SYSTEM PROMPT">
      <data key="d0">CONCEPT, TEMPLATE</data>
      <data key="d1">The Map System Prompt is a template used in the map stage of the map-reduce process. It provides instructions for processing community reports and generating intermediate responses.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="REDUCE SYSTEM PROMPT">
      <data key="d0">CONCEPT, TEMPLATE</data>
      <data key="d1">The Reduce System Prompt is a template used in the reduce stage of the map-reduce process. It provides instructions for aggregating intermediate responses and generating the final response.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="RESPONSE TYPE">
      <data key="d0">CONCEPT, FORMAT</data>
      <data key="d1">The Response Type is a parameter of the GlobalSearch class that describes the desired format of the final response, such as multiple paragraphs or a multi-page report.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="ALLOW GENERAL KNOWLEDGE">
      <data key="d0">CONCEPT, SETTING</data>
      <data key="d1">Allow General Knowledge is a setting in the GlobalSearch class that, when enabled, includes additional instructions in the reduce_system_prompt to prompt the LLM to include general knowledge in the final response.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="MAP_SYSTEM_PROMPT">
      <data key="d0">PROPERTY, TEMPLATE</data>
      <data key="d1">The map_system_prompt is a template used in the map stage of data processing. It serves as a guideline for the initial processing of data, with a default template available for use.</data>
      <data key="d2">e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="REDUCE_SYSTEM_PROMPT">
      <data key="d0">PROPERTY, TEMPLATE</data>
      <data key="d1">The reduce_system_prompt is a template used in the reduce stage of data processing. It guides the consolidation and analysis of data, with a default template available for use.</data>
      <data key="d2">e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="RESPONSE_TYPE">
      <data key="d0">PROPERTY, FORMAT</data>
      <data key="d1">The response_type is a specification of the desired format and structure of the final response or output, such as Multiple Paragraphs or Multi-Page Report.
response_type is a free-form text field that describes the format and type of response expected from the query, such as multiple paragraphs, a single paragraph, a list of points, or a multi-page report.
response_type is a parameter that describes the format and type of response expected. It can be any free-form text, such as Multiple Paragraphs, Single Paragraph, Single Sentence, List of 3-7 Points, Single Page, or Multi-Page Report. The default is Multiple Paragraphs.</data>
      <data key="d2">8c70a7321fb0e945054d226a8c69abee,e0cc1cf05b92456e09100790815186fe,e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="ALLOW_GENERAL_KNOWLEDGE">
      <data key="d0">PROPERTY, SETTING</data>
      <data key="d1">The allow_general_knowledge is a setting that, when enabled, allows the reduce stage to incorporate real-world knowledge outside of the dataset. This can enhance the response but may increase the risk of hallucinations.</data>
      <data key="d2">e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="GENERAL_KNOWLEDGE_INCLUSION_PROMPT">
      <data key="d0">PROPERTY, INSTRUCTION</data>
      <data key="d1">The general_knowledge_inclusion_prompt is an instruction added to the reduce_system_prompt when allow_general_knowledge is set to True. It guides the LLM to include relevant real-world knowledge in the response.</data>
      <data key="d2">e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="MAX_DATA_TOKENS">
      <data key="d0">PROPERTY, LIMIT</data>
      <data key="d1">The max_data_tokens is a token budget that limits the amount of context data that can be used in the processing stages.</data>
      <data key="d2">e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="MAP_LLM_PARAMS">
      <data key="d0">PROPERTY, PARAMETERS</data>
      <data key="d1">The map_llm_params is a set of additional parameters for the LLM call during the map stage, such as temperature and max_tokens, to customize the behavior of the LLM.</data>
      <data key="d2">e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="REDUCE_LLM_PARAMS">
      <data key="d0">PROPERTY, PARAMETERS</data>
      <data key="d1">The reduce_llm_params is a set of additional parameters for the LLM call during the reduce stage, such as temperature and max_tokens, to customize the behavior of the LLM.
reduce_llm_params is a dictionary containing additional parameters, such as temperature and max_tokens, that are passed to the LLM call during the reduce stage to fine-tune the model's behavior.</data>
      <data key="d2">e0cc1cf05b92456e09100790815186fe,e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="CONTEXT_BUILDER_PARAMS">
      <data key="d0">PROPERTY, PARAMETERS</data>
      <data key="d1">The context_builder_params is a set of additional parameters for the context_builder object, used to customize the building of the context window for the map stage.
context_builder_params is a dictionary of parameters that are passed to the context_builder object to customize the context window for the map stage, affecting how the model understands the context of the input data.</data>
      <data key="d2">e0cc1cf05b92456e09100790815186fe,e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="CONCURRENT_COROUTINES">
      <data key="d0">PROPERTY, SETTING</data>
      <data key="d1">The concurrent_coroutines is a setting that controls the degree of parallelism in the map stage, determining how many tasks can be processed concurrently.
concurrent_coroutines is a parameter that controls the degree of parallelism in the map stage, determining how many tasks can be executed concurrently.</data>
      <data key="d2">e0cc1cf05b92456e09100790815186fe,e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="LLM CALL">
      <data key="d0">PROCESS, ACTION</data>
      <data key="d1">LLM call refers to the invocation of a Large Language Model, typically at the map stage of a data processing pipeline, to perform tasks such as text generation or analysis.</data>
      <data key="d2">e0cc1cf05b92456e09100790815186fe</data>
    </node>
    <node id="GRAPHRAG QUERY CLI">
      <data key="d0">TOOL, INTERFACE</data>
      <data key="d1">The GraphRAG query CLI is a command-line interface that enables no-code usage of the GraphRAG Query engine, allowing users to perform searches and generate responses based on the data indexed by the system.</data>
      <data key="d2">e0cc1cf05b92456e09100790815186fe</data>
    </node>
    <node id="DATA">
      <data key="d0">DATA, INPUT</data>
      <data key="d1">data refers to the input data, typically stored in .parquet files, that are used as the basis for queries and responses in the GraphRAG system.</data>
      <data key="d2">e0cc1cf05b92456e09100790815186fe</data>
    </node>
    <node id="COMMUNITY_LEVEL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">community_level is a parameter that specifies the level in the Leiden community hierarchy from which community reports are loaded, with higher values indicating smaller communities.
community_level is a parameter that specifies the level in the Leiden community hierarchy from which community reports are loaded. Higher values indicate smaller communities. The default value is 2.</data>
      <data key="d2">8c70a7321fb0e945054d226a8c69abee,e0cc1cf05b92456e09100790815186fe</data>
    </node>
    <node id="METHOD">
      <data key="d0">PROPERTY, PARAMETER</data>
      <data key="d1">method is a parameter that determines the approach used to answer a query, either "local" or "global". For detailed information, refer to the Overview section.
METHOD is a command-line option that determines the method to select documents. Options are all, random, or top. The default is random.
method is an optional property that specifies the document selection method for the auto-templating feature. The options are "random", "top", and "all". The default is "random". It is used in the configuration of the graphrag.prompt_tune command to control how text units are selected for template generation.</data>
      <data key="d2">8c70a7321fb0e945054d226a8c69abee,9243633f55cccd0885ba553e14fa5e3f,ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="GRAPHRAG_API_KEY">
      <data key="d0">PROPERTY, ENV_VARIABLE</data>
      <data key="d1">GRAPHRAG_API_KEY is a required environment variable that provides the API Key for executing the model. If not provided, it falls back to OPENAI_API_KEY.
GRAPHRAG_API_KEY is an environment variable defined in the .env file that holds the API key for the OpenAI API or Azure OpenAI endpoint. This key is essential for authenticating requests made by the GraphRAG pipeline to the respective API services.
GRAPHRAG_API_KEY is a required configuration setting that holds the API key for accessing the GraphRAG service. It is essential for authentication and authorization purposes. The value should be set to a valid API key provided by the service provider. This setting is crucial for any interaction with the GraphRAG API.
GRAPHRAG_API_KEY is a configuration property that specifies the API key for accessing the LLM (Language Model) service. It is a security measure that allows the system to authenticate and access the LLM service. The value "your_api_key" is a placeholder for the actual API key. This property is crucial for authenticating the system to access the LLM service.&gt;</data>
      <data key="d2">3da10b454f926a257b9fdf5d2487c0a5,5aaa26fbe97dc7573cd1a56d6fb11213,8ac79ce92be1254dfda9a10eb54ab703,8c70a7321fb0e945054d226a8c69abee</data>
    </node>
    <node id="GRAPHRAG_LLM_MODEL">
      <data key="d0">PROPERTY, ENV_VARIABLE</data>
      <data key="d1">GRAPHRAG_LLM_MODEL is an environment variable that specifies the model to use for Chat Completions.
GRAPHRAG_LLM_MODEL is a configuration property that specifies the model to be used for the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to gpt-4-turbo-preview, indicating that the GPT-4 Turbo Preview model is selected.&gt;</data>
      <data key="d2">8c70a7321fb0e945054d226a8c69abee,9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_MODEL">
      <data key="d0">PROPERTY, ENV_VARIABLE</data>
      <data key="d1">GRAPHRAG_EMBEDDING_MODEL is an environment variable that determines the model to use for Embeddings.</data>
      <data key="d2">8c70a7321fb0e945054d226a8c69abee</data>
    </node>
    <node id="GRAPHRAG_LLM_API_BASE">
      <data key="d0">PROPERTY, ENV_VARIABLE</data>
      <data key="d1">GRAPHRAG_LLM_API_BASE is an optional environment variable that sets the API Base URL for the LLM operation. The default is None.
GRAPHRAG_LLM_API_BASE is an environment variable that specifies the API Base URL for LLM (Language Model) operations. It has a default value of None, indicating that no specific base URL is set by default.
GRAPHRAG_LLM_API_BASE is a configuration property that specifies the base URL for the Azure OpenAI service API. It is set to "http://&lt;domain&gt;.openai.azure.com" by default, where "&lt;domain&gt;" should be replaced with the actual domain name provided by Azure.&gt;</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,7b45dafa74553d3899e2291a3c9fb86e,8c70a7321fb0e945054d226a8c69abee</data>
    </node>
    <node id="GRAPHRAG_LLM_TYPE">
      <data key="d0">PROPERTY, ENV_VARIABLE</data>
      <data key="d1">GRAPHRAG_LLM_TYPE is an environment variable that defines the LLM operation type, either openai_chat or azure_openai_chat. The default is openai_chat.
GRAPHRAG_LLM_TYPE is an environment variable that determines the type of LLM operation to be used, either openai_chat or azure_openai_chat. The default value is openai_chat, indicating that OpenAI's chat model is the default choice for LLM operations.
GRAPHRAG_LLM_TYPE is a configuration setting that determines the type of language model to be used for text generation. It can be set to 'azure_openai_chat' or 'openai_chat', depending on the desired model. This setting is crucial for specifying the model that will be used for generating text based on the input data.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,3da10b454f926a257b9fdf5d2487c0a5,8c70a7321fb0e945054d226a8c69abee</data>
    </node>
    <node id="GRAPHRAG_LLM_MAX_RETRIES">
      <data key="d0">PROPERTY, ENV_VARIABLE</data>
      <data key="d1">GRAPHRAG_LLM_MAX_RETRIES is an environment variable that sets the maximum number of retries to attempt when a request fails. The default is 20.
GRAPHRAG_LLM_MAX_RETRIES is an environment variable that sets the maximum number of retries to attempt when a request to the LLM API fails. The default value is 20, suggesting that the system will retry up to 20 times before giving up on a failed request.
GRAPHRAG_LLM_MAX_RETRIES is a configuration property that specifies the maximum number of retries allowed for a request to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 10, indicating that a request can be retried up to 10 times.&gt;</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,8c70a7321fb0e945054d226a8c69abee,9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_API_BASE">
      <data key="d0">ENVIRONMENT_VARIABLE, API_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_EMBEDDING_API_BASE is an environment variable that specifies the API Base URL for embedding operations. It has a default value of None, indicating that no specific base URL is set by default for embedding operations.
GRAPHRAG_EMBEDDING_API_BASE is a configuration property that specifies the base URL for the embedding service in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to "http://&lt;domain&gt;.openai.azure.com", indicating that the base URL for the embedding service is provided for Azure OpenAI users.&gt;
GRAPHRAG_EMBEDDING_API_BASE is a configuration variable set to "http://&lt;domain&gt;.openai.azure.com" for Azure OpenAI users. It serves as the base URL for embedding API requests when GRAPHRAG_API_BASE is not set.&gt;</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,485c17007ccb3102887eaa47d6a6100f,9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_TYPE">
      <data key="d0">ENVIRONMENT_VARIABLE, API_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_EMBEDDING_TYPE is an environment variable that determines the type of embedding client to use, either openai_embedding or azure_openai_embedding. The default value is openai_embedding, indicating that OpenAI's embedding model is the default choice for embedding operations.
GRAPHRAG_EMBEDDING_TYPE is a configuration setting that specifies the type of embedding model to be used for text embedding. It can be set to 'azure_openai_embedding' or 'openai_embedding', depending on the desired model. This setting is crucial for specifying the model that will be used for generating embeddings based on the input data.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,3da10b454f926a257b9fdf5d2487c0a5</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_MAX_RETRIES">
      <data key="d0">ENVIRONMENT_VARIABLE, API_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_EMBEDDING_MAX_RETRIES is an environment variable that sets the maximum number of retries to attempt when a request to the embedding API fails. The default value is 20, suggesting that the system will retry up to 20 times before giving up on a failed request for embedding operations.
GRAPHRAG_EMBEDDING_MAX_RETRIES is a configuration setting that specifies the maximum number of retries for embedding operations. It is set to 10, indicating that up to 10 retries will be attempted.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_TEXT_UNIT_PROP">
      <data key="d0">ENVIRONMENT_VARIABLE, SEARCH_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_TEXT_UNIT_PROP is an environment variable that sets the proportion of the context window dedicated to related text units. The default value is 0.5, indicating that half of the context window is reserved for related text units.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_COMMUNITY_PROP">
      <data key="d0">ENVIRONMENT_VARIABLE, SEARCH_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_COMMUNITY_PROP is an environment variable that sets the proportion of the context window dedicated to community reports. The default value is 0.1, indicating that 10% of the context window is reserved for community reports.
GRAPHRAG_LOCAL_SEARCH_COMMUNITY_PROP is a configuration property that determines the proportion of the context window dedicated to community reports. It has a default value of 0.1.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_CONVERSATION_HISTORY_MAX_TURNS">
      <data key="d0">ENVIRONMENT_VARIABLE, SEARCH_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_CONVERSATION_HISTORY_MAX_TURNS is an environment variable that sets the maximum number of turns to include in the conversation history. The default value is 5, indicating that the system will consider up to 5 turns of conversation history.
GRAPHRAG_LOCAL_SEARCH_CONVERSATION_HISTORY_MAX_TURNS is a configuration property that specifies the maximum number of turns to include in the conversation history. It has a default value of 5.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_TOP_K_ENTITIES">
      <data key="d0">ENVIRONMENT_VARIABLE, SEARCH_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_TOP_K_ENTITIES is an environment variable that determines the number of related entities to retrieve from the entity description embedding store. The default value is 10, indicating that the system will retrieve the top 10 related entities.
GRAPHRAG_LOCAL_SEARCH_TOP_K_ENTITIES is a configuration property that sets the number of related entities to retrieve from the entity description embedding store. It has a default value of 10.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_TOP_K_RELATIONSHIPS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_TOP_K_RELATIONSHIPS is a configuration property that controls the number of out-of-network relationships to pull into the context window. It has a default value of 10.</data>
      <data key="d2">2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS is a configuration property that can be changed based on the token limit of the model being used. It has a default value of 12000.</data>
      <data key="d2">2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_LLM_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_LLM_MAX_TOKENS is a configuration property that can be changed based on the token limit of the model being used. It has a default value of 2000.</data>
      <data key="d2">2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS is a configuration property that can be changed based on the token limit of the model being used. It has a default value of 12000.
GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS is a configuration property that determines the maximum number of tokens for global search. It should be adjusted based on the token limit of the model being used. For an 8k limit model, a good setting could be 5000. The default value is 12000.</data>
      <data key="d2">2049798d3000849f8bec3e88c0006807,2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS is a configuration property that can be changed based on the token limit of the model being used.
GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS is a configuration property that determines the maximum number of tokens for data in global search. It should be adjusted based on the token limit of the model being used. For an 8k limit model, a good setting could be 5000. The default value is 12000.</data>
      <data key="d2">2049798d3000849f8bec3e88c0006807,2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_GLOBAL_SEARCH_MAP_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_GLOBAL_SEARCH_MAP_MAX_TOKENS is a configuration property that determines the maximum number of tokens for mapping in global search. The default value is 500.</data>
      <data key="d2">2049798d3000849f8bec3e88c0006807</data>
    </node>
    <node id="GRAPHRAG_GLOBAL_SEARCH_REDUCE_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_GLOBAL_SEARCH_REDUCE_MAX_TOKENS is a configuration property that determines the maximum number of tokens for reduction in global search. It should be adjusted based on the token limit of the model being used. For an 8k limit model, a good setting could be 1000-1500. The default value is 2000.</data>
      <data key="d2">2049798d3000849f8bec3e88c0006807</data>
    </node>
    <node id="GRAPHRAG_GLOBAL_SEARCH_CONCURRENCY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_GLOBAL_SEARCH_CONCURRENCY is a configuration property that determines the level of concurrency for global search. The default value is 32.
GRAPHRAG_GLOBAL_SEARCH_CONCURRENCY is a configuration property that determines the level of concurrency for global search operations in the GraphRag system. The default value is set to 32. This property can be adjusted to optimize the performance of search operations based on system resources and requirements.)&lt;|COMPLETE|&gt;</data>
      <data key="d2">2049798d3000849f8bec3e88c0006807,60df16c009594c15c4ead6125e1453ce</data>
    </node>
    <node id="GRAPHRAG KNOWLEDGE MODEL">
      <data key="d0">CONCEPT, DATA MODEL</data>
      <data key="d1">The GraphRAG Knowledge Model is a conceptual model designed to abstract over the underlying data storage technology, providing a common interface for the GraphRAG system to interact with. It is aligned with the outputs of the indexing engine in the Default Configuration Mode and is used to load data into a database system for the GraphRAG's Query Engine to interact with.</data>
      <data key="d2">25e04f0e9a961dcdc3f6eae6df7807b2</data>
    </node>
    <node id="GRAPHRAG INDEXER">
      <data key="d0">SOFTWARE, DATA PROCESSING</data>
      <data key="d1">The GraphRAG Indexer is a component of the GraphRAG system responsible for indexing data. Its outputs, in the Default Configuration Mode, are aligned with the GraphRAG Knowledge Model and are typically loaded into a database system for further processing by the Query Engine.
The GraphRAG indexer is a tool designed for knowledge discovery that operates with a set of default prompts to process text data. It allows for the customization of these prompts through a custom prompt file, enhancing its adaptability to various contexts.</data>
      <data key="d2">25e04f0e9a961dcdc3f6eae6df7807b2,6a7157695d90d434b2625c3f05420916</data>
    </node>
    <node id="GRAPHRAG QUERY ENGINE">
      <data key="d0">SOFTWARE, QUERY PROCESSING</data>
      <data key="d1">The GraphRAG Query Engine is a component of the GraphRAG system that interacts with the database system using the knowledge model data-store types. It processes queries based on the data indexed by the GraphRAG Indexer and stored in the database.</data>
      <data key="d2">25e04f0e9a961dcdc3f6eae6df7807b2</data>
    </node>
    <node id="DATASHAPER">
      <data key="d0">SOFTWARE, DATA PROCESSING</data>
      <data key="d1">DataShaper is an open-source library used for data processing. It allows users to declaratively express data pipelines, schemas, and related assets using well-defined schemas. DataShaper has implementations in JavaScript and Python and is designed to be extensible to other languages.
DataShaper is a software tool that allows for the transformation and manipulation of data through workflows, which are sequences of steps called verbs. These verbs model relational concepts and can transform input data tables, passing them down a pipeline for further processing.</data>
      <data key="d2">25e04f0e9a961dcdc3f6eae6df7807b2,81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="WORKFLOW">
      <data key="d0">CONCEPT, DATA PROCESSING</data>
      <data key="d1">Workflow is a core resource type within DataShaper, expressed as sequences of steps called verbs. Each step has a verb name and a configuration object, and verbs model relational concepts such as SELECT, DROP, JOIN, etc. Each verb transforms an input data table, which is then passed down the pipeline.
A Workflow in DataShaper is a sequence of steps, known as verbs, that transform data. Each step has a verb name and a configuration object, and verbs can model relational concepts such as SELECT, DROP, JOIN, etc. Workflows are used to process data tables and pass them through a pipeline for further transformations.</data>
      <data key="d2">25e04f0e9a961dcdc3f6eae6df7807b2,81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="GRAPHRAG INDEXING PIPELINE">
      <data key="d0">SOFTWARE, DATA PROCESSING</data>
      <data key="d1">The GraphRAG Indexing Pipeline is built on top of DataShaper and is responsible for processing data through a series of steps or verbs. It is a part of the GraphRAG system and is used to index data for the GraphRAG Knowledge Model.
The GraphRAG Indexing Pipeline is a system that allows workflows to define dependencies on each other, creating a directed acyclic graph (DAG) of workflows. This DAG is used to schedule the processing of data indexing tasks.</data>
      <data key="d2">25e04f0e9a961dcdc3f6eae6df7807b2,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="VERB">
      <data key="d0">CONCEPT, PROCESS</data>
      <data key="d1">A Verb in DataShaper is a step within a workflow that models a specific data transformation action. Verbs can represent relational concepts like SELECT, JOIN, and BINARIZE, and each verb has a name and a configuration object that defines how it operates on an input data table.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="INPUT TABLE">
      <data key="d0">DATA, STRUCTURE</data>
      <data key="d1">An Input Table is the data structure that is fed into a verb within a DataShaper workflow. It is the starting point for data transformations and is passed through a series of verbs for processing.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="OUTPUT TABLE">
      <data key="d0">DATA, STRUCTURE</data>
      <data key="d1">An Output Table is the result of a verb's processing in a DataShaper workflow. It is the transformed data structure that is passed down the pipeline for further processing by subsequent verbs.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="GRAPHRAG'S INDEXING PIPELINE">
      <data key="d0">SOFTWARE, TOOL</data>
      <data key="d1">GraphRAG's Indexing Pipeline is a custom extension of DataShaper that implements additional verbs on top of the standard relational verbs. It is designed to augment text documents with rich, structured data using the power of LLMs like GPT-4, and it can be used to extract entities, relationships, claims, community structures, and community reports and summaries.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="LLM-BASED WORKFLOW STEPS">
      <data key="d0">CONCEPT, PROCESS</data>
      <data key="d1">LLM-based Workflow Steps are custom verbs in GraphRAG's Indexing Pipeline that utilize Large Language Models (LLMs) to perform data enrichment and extraction tasks. These steps can be customized and extended to support various AI-based data processing tasks.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="WORKFLOW GRAPHS">
      <data key="d0">CONCEPT, STRUCTURE</data>
      <data key="d1">Workflow Graphs in GraphRAG's Indexing Pipeline represent the complex interdependencies between multiple workflows. They form a directed acyclic graph (DAG) of workflows, which is used to schedule processing and manage the dependencies between different workflow steps.
Workflow Graphs represent the complexity of data indexing tasks by expressing data pipelines as a series of multiple, interdependent workflows. These workflows can define dependencies on each other, forming a directed acyclic graph (DAG) that is used for scheduling processing.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="DAG (DIRECTED ACYCLIC GRAPH)">
      <data key="d0">CONCEPT, STRUCTURE</data>
      <data key="d1">A Directed Acyclic Graph (DAG) is a structure used in GraphRAG's Indexing Pipeline to represent the dependencies between workflows. It is a graph that is directed (edges have a direction) and acyclic (no cycles exist), which allows for the scheduling and processing of workflows in a defined order.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="PREPARE">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">Prepare is a verb in GraphRAG's Indexing Pipeline that represents a step in the workflow for preparing data for further processing. It could involve tasks such as data cleaning, formatting, or setting up the data for subsequent verbs.
Prepare is a workflow step in the GraphRAG Indexing Pipeline that involves preparing data for processing. This could include initial data cleaning, formatting, or setting up the environment for subsequent tasks.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="CHUNK">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">Chunk is a verb in GraphRAG's Indexing Pipeline that represents a step in the workflow for dividing data into smaller, manageable pieces. This could be useful for processing large datasets in smaller, more efficient chunks.
Chunk is a workflow step in the GraphRAG Indexing Pipeline that involves breaking down data into smaller, manageable pieces. This is often done to facilitate more efficient processing or to enable parallel processing of data.
Chunks are segments of text that are used for analysis and processing in the workflow.
Chunk refers to a segment of text or data that is processed as a unit, often used in text analysis or information extraction workflows.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08,81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="EXTRACTGRAPH">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">ExtractGraph is a verb in GraphRAG's Indexing Pipeline that represents a step in the workflow for extracting graph structures from data. This could involve identifying relationships and connections within the data to create a graph representation.
ExtractGraph is a workflow step in the GraphRAG Indexing Pipeline that involves extracting graph structures from data. This could include identifying relationships, entities, or patterns within the data to create a graph representation.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="EMBEDDOCUMENTS">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">EmbedDocuments is a verb in GraphRAG's Indexing Pipeline that represents a step in the workflow for embedding documents into a vector space. This could be useful for tasks such as document similarity analysis or information retrieval.
EmbedDocuments is a workflow step in the GraphRAG Indexing Pipeline that involves converting documents into numerical representations, often for the purpose of machine learning or information retrieval.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="GENERATEREPORTS">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">GenerateReports is a verb in GraphRAG's Indexing Pipeline that represents a step in the workflow for generating reports based on the processed data. This could involve summarizing data, identifying trends, or providing insights based on the data transformations.
GenerateReports is a workflow step in the GraphRAG Indexing Pipeline that involves creating reports based on the processed data. These reports can summarize findings, provide insights, or document the results of data processing tasks.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="EMBEDGRAPH">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">EmbedGraph is a verb in GraphRAG's Indexing Pipeline that represents a step in the workflow for embedding graph structures into a vector space. This could be useful for tasks such as graph similarity analysis or graph-based information retrieval.
EmbedGraph is a workflow step in the GraphRAG Indexing Pipeline that involves converting graph structures into numerical representations. This is often done to enable machine learning algorithms to process graph data.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="ENTITYRESOLUTION">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">EntityResolution is a verb in GraphRAG's Indexing Pipeline that represents a step in the workflow for resolving entities within the data. This could involve identifying and merging duplicate entities or linking entities across different datasets.
EntityResolution is a workflow step in the GraphRAG Indexing Pipeline that involves resolving ambiguities in data, such as identifying and merging duplicate entities or disambiguating entities with similar names.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="SAMPLE WORKFLOW DAG">
      <data key="d0">CONCEPT, DIAGRAM</data>
      <data key="d1">Sample Workflow DAG is a visual representation of a directed acyclic graph (DAG) that shows the dependencies between different workflows in the GraphRAG Indexing Pipeline.</data>
      <data key="d2">d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="DATAFRAME MESSAGE FORMAT">
      <data key="d0">CONCEPT, FORMAT</data>
      <data key="d1">Dataframe Message Format is the primary unit of communication between workflows and workflow steps in the GraphRAG Indexing Pipeline. It is an instance of pandas.DataFrame, which facilitates data-centric and table-centric data processing.</data>
      <data key="d2">d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="LLM CACHING">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">LLM Caching is a technique used in the GraphRAG library to improve the resilience of the indexer to network issues. It involves caching the results of Large Language Model (LLM) interactions to avoid reprocessing the same input set, thus saving resources and improving efficiency.</data>
      <data key="d2">d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="GRAPHRAG LIBRARY">
      <data key="d0">SOFTWARE, TOOL</data>
      <data key="d1">The GraphRAG library is a software tool designed specifically for Large Language Model (LLM) interactions. It is equipped with features to handle common issues encountered when working with LLM APIs, such as network latency and throttling errors. The library includes a caching mechanism to improve the efficiency and reliability of LLM interactions.</data>
      <data key="d2">6335601c6ec22bd6f15c8b69c26f854b</data>
    </node>
    <node id="LLM INTERACTIONS">
      <data key="d0">PROCESS, COMMUNICATION</data>
      <data key="d1">LLM interactions refer to the communication and data exchange between a system and Large Language Models (LLMs). These interactions can be affected by network latency, throttling, and other errors, which can lead to decreased performance and reliability. Caching is used to mitigate these issues and improve the efficiency of the interactions.</data>
      <data key="d2">6335601c6ec22bd6f15c8b69c26f854b</data>
    </node>
    <node id="CACHING">
      <data key="d0">TECHNIQUE, STRATEGY</data>
      <data key="d1">Caching is a technique used in the GraphRAG library to store and reuse results from LLM interactions. When a completion request is made with the same input set (prompt and tuning parameters), the library checks if a cached result exists and returns it if available. This strategy enhances the system's resilience to network issues, ensures idempotency, and provides a more efficient end-user experience.</data>
      <data key="d2">6335601c6ec22bd6f15c8b69c26f854b</data>
    </node>
    <node id="LOCAL TO GLOBAL GRAPH RAG APPROACH">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">The Local to Global Graph RAG (Retrieval-Augmented Generation) Approach is a method designed for query-focused summarization tasks. It enables large language models to answer questions over private and/or previously unseen document collections by retrieving relevant information. The approach is particularly suited for global questions that require summarization of an entire text corpus, such as identifying main themes in a dataset. It combines the strengths of retrieval tasks and query-focused summarization methods, scaling to handle both the generality of user questions and the quantity of source text to be indexed.</data>
      <data key="d2">f76c18c7582167c3626f8741c2c9374f</data>
    </node>
    <node id="RETRIEVAL-AUGMENTED GENERATION (RAG)">
      <data key="d0">TECHNIQUE, TOOL</data>
      <data key="d1">Retrieval-Augmented Generation (RAG) is a technique that allows large language models to answer questions by retrieving relevant information from an external knowledge source. It is effective for answering questions over private and/or previously unseen document collections. However, RAG has limitations when dealing with global questions that require summarization of an entire text corpus, as these are inherently query-focused summarization tasks.
Retrieval-Augmented Generation (RAG) is an established approach to answering user questions over entire datasets by retrieving local regions of text that provide sufficient grounding for the generation task.</data>
      <data key="d2">c7669e6a1add9a2829b09196256b1492,f76c18c7582167c3626f8741c2c9374f</data>
    </node>
    <node id="QUERY-FOCUSED SUMMARIZATION (QFS)">
      <data key="d0">TECHNIQUE, TOOL</data>
      <data key="d1">Query-Focused Summarization (QFS) is a technique used to generate summaries in response to specific queries. It is designed to handle summarization tasks that require understanding of an entire text corpus, such as identifying main themes. However, QFS methods often fail to scale to the quantities of text indexed by typical RAG systems.
Query-Focused Summarization (QFS) is a task framing that generates natural language summaries in response to user queries, particularly focusing on abstractive summarization that goes beyond concatenated excerpts.
Query-Focused Summarization (QFS) is a type of summarization task that generates summaries tailored to specific queries, focusing on relevant information rather than providing generic summaries. It is distinct from other summarization methods in that it aims to produce natural language summaries rather than simple concatenations of excerpts. This concept was introduced by Dang in 2006.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9,c7669e6a1add9a2829b09196256b1492,f76c18c7582167c3626f8741c2c9374f</data>
    </node>
    <node id="ENTITY KNOWLEDGE GRAPH">
      <data key="d0">DATA STRUCTURE, CONCEPT</data>
      <data key="d1">An Entity Knowledge Graph is a data structure that represents the entities and their relationships found in source documents. It is derived from the source documents in the first stage of the Graph RAG approach. The graph is used to identify closely-related entities and generate community summaries for groups of these entities.
An entity knowledge graph is a data structure that represents entities and their relationships from source documents. It is derived in the first stage of the graph-based text index process and is used to generate community summaries for groups of closely-related entities.</data>
      <data key="d2">b149708d0b4ac3ff417565739ea6b03b,f76c18c7582167c3626f8741c2c9374f</data>
    </node>
    <node id="GRAPH-BASED TEXT INDEX">
      <data key="d0">METHOD, TECHNOLOGY</data>
      <data key="d1">A graph-based text index is a method for organizing and querying text data that involves creating an entity knowledge graph from source documents and generating community summaries for groups of closely-related entities to improve the comprehensiveness and diversity of generated answers to questions. This method is particularly effective for global sensemaking questions over datasets in the 1 million token range.</data>
      <data key="d2">b149708d0b4ac3ff417565739ea6b03b</data>
    </node>
    <node id="GLOBAL SENSEMAKING QUESTIONS">
      <data key="d0">QUERY, INFORMATION NEED</data>
      <data key="d1">Global sensemaking questions are a class of questions that require understanding connections among people, places, and events in order to anticipate their trajectories and act effectively. They are typically asked over datasets in the 1 million token range and are used to test the effectiveness of the graph-based text index method.</data>
      <data key="d2">b149708d0b4ac3ff417565739ea6b03b</data>
    </node>
    <node id="GRAPH RAG">
      <data key="d0">METHOD, TECHNOLOGY</data>
      <data key="d1">Graph RAG (Retrieval-Augmented Generation) is a method that uses a graph-based text index to improve the comprehensiveness and diversity of generated answers to global sensemaking questions. It leads to substantial improvements over a naive RAG baseline and is implemented in both global and local approaches.
Graph RAG is a variant of the RAG method that uses graph structures to enhance the summarization and question answering process. It is known for providing improvements in answer comprehensiveness and diversity when compared to source texts, especially with community summaries.&gt;
Graph RAG is a method for summarization that offers scalability advantages over source text summarization. It requires fewer context tokens for low-level community summaries (C3) and root-level community summaries (C0). It is efficient for iterative question answering, with high win rates in comprehensiveness and diversity. It can be tuned to retain more details in the index.
Graph RAG is an implementation of retrieval-augmented generation that incorporates multiple concepts related to other systems. It uses a graph-based approach to retrieval and generation, allowing for more complex and interconnected data handling.&gt;
Graph RAG is a research study that uses a self-generated graph index to enable advanced retrieval and analysis of information.
Graph RAG (Retrieval-Augmented Generation) is an AI model that uses a graph index to augment the generation of responses or summaries, and it has been observed to achieve the best head-to-head results against other methods in many cases.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96,7040ba36a7c09899a355d14a30d65375,71f14506a6b15dfabd93fd1606a67b73,7da3d8d244b67f09425a4a7783e4bb55,b149708d0b4ac3ff417565739ea6b03b,ed433e2f5d5387b47376eb0e45ca1c99</data>
    </node>
    <node id="LARGE LANGUAGE MODELS (LLMS)">
      <data key="d0">TECHNOLOGY, ARTIFICIAL INTELLIGENCE</data>
      <data key="d1">Large language models (LLMs) are artificial intelligence systems that are capable of processing and generating human-like language. They are used to automate human-like sensemaking in complex domains like scientific discovery and intelligence analysis.</data>
      <data key="d2">b149708d0b4ac3ff417565739ea6b03b</data>
    </node>
    <node id="SCIENTIFIC DISCOVERY">
      <data key="d0">DOMAIN, ACTIVITY</data>
      <data key="d1">Scientific discovery is a domain of human endeavor that involves the process of making new and significant contributions to scientific knowledge. It is an area where large language models are being used to automate human-like sensemaking.</data>
      <data key="d2">b149708d0b4ac3ff417565739ea6b03b</data>
    </node>
    <node id="INTELLIGENCE ANALYSIS">
      <data key="d0">DOMAIN, ACTIVITY</data>
      <data key="d1">Intelligence analysis is a domain of human endeavor that involves the process of gathering, processing, and analyzing information to support decision-making. It is an area where large language models are being used to automate human-like sensemaking.</data>
      <data key="d2">b149708d0b4ac3ff417565739ea6b03b</data>
    </node>
    <node id="AUTOMATED SENSEMAKING">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Automated sensemaking refers to the application of artificial intelligence and machine learning techniques to understand complex domains, such as scientific discovery and intelligence analysis, by identifying connections among people, places, and events to anticipate their trajectories and act effectively.</data>
      <data key="d2">c7669e6a1add9a2829b09196256b1492</data>
    </node>
    <node id="HUMAN-LED SENSEMAKING">
      <data key="d0">CONCEPT, PROCESS</data>
      <data key="d1">Human-led sensemaking is a process where humans apply and refine their mental model of data by asking questions of a global nature to understand connections among people, places, and events in order to anticipate their trajectories and act effectively.</data>
      <data key="d2">c7669e6a1add9a2829b09196256b1492</data>
    </node>
    <node id="ABSTRACTIVE SUMMARIZATION">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Abstractive summarization is a type of summarization that generates new sentences to convey the meaning of the original text, as opposed to extractive summarization which selects and concatenates existing sentences.
Abstractive Summarization is a type of summarization that generates new sentences to convey the meaning of the original text, rather than simply extracting and reordering existing sentences. It involves creating a summary that is not a direct copy of the source material but rather a new representation of its content.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9,c7669e6a1add9a2829b09196256b1492</data>
    </node>
    <node id="EXTRACTIVE SUMMARIZATION">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Extractive summarization is a type of summarization that selects and concatenates existing sentences from the original text to create a summary, without generating new sentences.
Extractive Summarization is a type of summarization that involves selecting and reordering existing sentences or phrases from the source text to create a summary. It does not generate new sentences but rather extracts the most important parts of the original text.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9,c7669e6a1add9a2829b09196256b1492</data>
    </node>
    <node id="TRANSFORMER ARCHITECTURE">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">The Transformer Architecture is a deep learning model introduced by Vaswani et al. in 2017, which has revolutionized natural language processing tasks, including summarization. It is based on the mechanism of self-attention, allowing the model to weigh the importance of different parts of the input data.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9</data>
    </node>
    <node id="LLMS (LARGE LANGUAGE MODELS)">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">LLMs (Large Language Models) are sophisticated AI models that can process and generate human-like text. They are capable of understanding and responding to complex queries, performing various language tasks, and have been shown to trivialize many summarization tasks due to their ability to use in-context learning to summarize content within their context window.
LLMs are advanced AI models capable of understanding the common entity behind multiple name variations. They are used in the context of processing the entity graph and are suited for handling rich descriptive text in potentially noisy graph structures.
LLMs, or Large Language Models, are advanced AI models capable of evaluating natural language generation, achieving state-of-the-art or competitive results compared to human judgments. They can generate reference-based metrics when gold standard answers are known and measure the qualities of generated texts in a reference-free style. LLMs can also perform head-to-head comparisons of competing outputs and evaluate the performance of conventional RAG (Retrieval-Augmented Generation) systems, assessing qualities like context relevance, faithfulness, and answer relevance.</data>
      <data key="d2">53455f8552b0787cb13c5a03eb550842,6dace8e490674ac8e031aed987a63789,85eff07c379a9dc24db0edb983acf3c9</data>
    </node>
    <node id="GPT (GENERATIVE PRE-TRAINED TRANSFORMER)">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">GPT (Generative Pre-trained Transformer) is a series of large language models developed by OpenAI. It is based on the transformer architecture and is known for its ability to generate coherent and contextually relevant text, making it highly effective for various natural language processing tasks, including summarization.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9</data>
    </node>
    <node id="LLAMA">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Llama is a large language model developed by Facebook AI Research (FAIR). It is designed to handle complex language tasks and can generate human-like text, making it suitable for summarization and other NLP tasks.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9</data>
    </node>
    <node id="GEMINI">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Gemini is a large language model developed by Alibaba Cloud. It is capable of understanding and generating text, making it useful for various NLP tasks, including summarization.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9</data>
    </node>
    <node id="BROWN ET AL., 2020">
      <data key="d0">REFERENCE, ACADEMIC PAPER</data>
      <data key="d1">Brown et al., 2020, is an academic paper that discusses the use of in-context learning for summarizing content within the context window of large language models (LLMs).</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="LLAMA (TOUVRON ET AL., 2023)">
      <data key="d0">REFERENCE, ACADEMIC PAPER</data>
      <data key="d1">Llama (Touvron et al., 2023) is an academic paper that explores the application of in-context learning for summarization tasks, focusing on the Llama series which can summarize content within the context window of LLMs.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="GEMINI (ANIL ET AL., 2023)">
      <data key="d0">REFERENCE, ACADEMIC PAPER</data>
      <data key="d1">Gemini (Anil et al., 2023) is an academic paper that discusses the Gemini series, which utilizes in-context learning to summarize content within the context window of LLMs.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="IN-CONTEXT LEARNING">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">In-context learning is a technique used by LLMs to adapt their responses based on the context provided in their input, enabling them to summarize content within their context window.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Query-focused abstractive summarization is a technique that aims to generate a summary of a corpus in response to a specific query, focusing on the most relevant information.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="LLM CONTEXT WINDOW LIMITS">
      <data key="d0">PROPERTY, CONSTRAINT</data>
      <data key="d1">LLM context window limits refer to the maximum amount of context that a large language model can process at once, which can be a constraint for summarizing entire corpora.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="INFORMATION LOSS IN LONGER CONTEXTS">
      <data key="d0">PROPERTY, CONSTRAINT</data>
      <data key="d1">Information loss in longer contexts is a phenomenon where information can be overlooked or lost when processing very long texts, as the middle part of the context may not receive adequate attention.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="NAIVE RAG (RETRIEVAL-AUGMENTED GENERATION)">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Naive RAG (Retrieval-Augmented Generation) is a technique that directly retrieves text chunks for summarization, which may not be sufficient for query-focused summarization tasks over large corpora.
Naive RAG (Retrieval-Augmented Generation) is a simple or basic method for generating text or responses by retrieving and augmenting information from a dataset. It is often used as a baseline for comparing the performance of more sophisticated methods.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912,e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="GRAPH RAG APPROACH">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Graph RAG approach is a technique based on global summarization of an LLM-derived knowledge graph, focusing on the modularity of graphs and the ability of community detection algorithms to partition graphs into closely-related nodes.
The Graph RAG (Retrieval-Augmented Generation) approach is a method for generating text by utilizing a graph index to retrieve and augment information. It involves processing text chunks extracted from source documents and is designed to improve comprehensiveness, diversity, and token efficiency in summarization tasks.
The Graph RAG (Retrieval-Augmented Generation) approach is a method for supporting human sensemaking over entire text corpora by combining knowledge graph generation, retrieval-augmented generation, and query-focused summarization. It aims to refine and adapt the current approach by operating in a more local manner, via embedding-based matching of user queries and graph annotations, and by implementing hybrid RAG schemes that combine embedding-based matching against community reports before employing map-reduce summarization mechanisms.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912,5e2933c9646c751e6a60c9de12a255f2,d2399fd0aae5bd200639806ca87184f8</data>
    </node>
    <node id="STRUCTURED RETRIEVAL AND TRAVERSAL">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Structured retrieval and traversal is a technique that exploits the structured nature of graph indexes for efficient information retrieval and navigation.
Structured retrieval and traversal refers to the process of accessing and navigating through structured data, such as graph indexes, to find and retrieve specific information. This process leverages the structured nature of the data to efficiently locate and extract relevant information.&gt;</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912,d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="GRAPH MODULARITY">
      <data key="d0">PROPERTY, CONCEPT</data>
      <data key="d1">Graph modularity is a property of graphs that refers to their inherent structure, allowing them to be partitioned into modular communities of closely-related nodes.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="COMMUNITY DETECTION ALGORITHMS">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Community detection algorithms are techniques used to identify groups of closely-related nodes in a graph, which can be useful for summarization tasks.
Community detection algorithms are computational methods used to identify groups of closely-related nodes within a graph. These algorithms aim to partition the graph into communities based on the connectivity patterns of the nodes, often optimizing for modularity or other criteria that define the quality of the partitioning.&gt;
Community Detection Algorithms are used to partition the entity graph into communities of nodes with stronger connections to one another than to other nodes in the graph. They are essential for summarizing the entity graph into communities.
Community Detection Algorithms are methods used to partition a graph into communities of nodes that have stronger connections to one another than to the other nodes in the graph. These algorithms help in identifying the structure and semantics of large datasets by grouping similar nodes together.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912,6dace8e490674ac8e031aed987a63789,a660289d2bf43f25d3524d35cd2d9a96,d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="LLM-DERIVED KNOWLEDGE GRAPH">
      <data key="d0">CONCEPT, INFORMATION STRUCTURE</data>
      <data key="d1">An LLM-derived knowledge graph is a structured representation of information, typically in the form of a graph, where nodes represent entities and edges represent relationships between those entities. This graph is used to store and retrieve knowledge in a more human-like manner, facilitating the understanding and analysis of complex information. The graph is characterized by its inherent modularity and the ability to be partitioned into modular communities of closely-related nodes using community detection algorithms.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="MODULARITY">
      <data key="d0">PROPERTY, CONCEPT</data>
      <data key="d1">Modularity is a property of graphs that describes the degree to which the graph can be partitioned into distinct subgraphs or communities. It is a measure of the inherent structure of the graph and its ability to be divided into meaningful, self-contained parts.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="LOUVAIN">
      <data key="d0">CONCEPT, ALGORITHM</data>
      <data key="d1">Louvain is a community detection algorithm that aims to optimize the modularity of a graph partition. It is an iterative process that starts by assigning each node to its own community and then iteratively merges the most similar communities until no further improvement in modularity can be achieved.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="LEIDEN">
      <data key="d0">CONCEPT, ALGORITHM</data>
      <data key="d1">Leiden is an extension of the Louvain algorithm for community detection. It aims to improve the resolution of the Louvain algorithm by adding a refinement step that allows for the detection of smaller communities. Leiden is designed to be more efficient and to produce more stable and reproducible results.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="QUERY-FOCUSED SUMMARIZATION">
      <data key="d0">CONCEPT, INFORMATION PROCESSING</data>
      <data key="d1">Query-focused summarization is a technique used to generate summaries that are specifically tailored to answer a particular query. It involves analyzing a corpus of documents or a graph index to extract information relevant to the query and then summarizing that information in a concise and coherent manner.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="MAP-REDUCE APPROACH">
      <data key="d0">CONCEPT, INFORMATION PROCESSING</data>
      <data key="d1">The map-reduce approach is a programming model for processing large data sets. It involves two main steps: the map step, where the data is divided into smaller chunks and processed independently, and the reduce step, where the results of the map step are combined to produce the final output. This approach is particularly useful for parallel and distributed processing of large datasets.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="ACTIVITY-CENTERED SENSEMAKING QUESTIONS">
      <data key="d0">CONCEPT, INFORMATION QUERY</data>
      <data key="d1">Activity-centered sensemaking questions are questions that are designed to help users understand complex information by focusing on specific activities or processes. These questions are often derived from short descriptions of datasets and are intended to guide the user in making sense of the underlying data and its implications.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="PODCAST TRANSCRIPTS">
      <data key="d0">CONCEPT, INFORMATION TYPE</data>
      <data key="d1">Podcast transcripts are written records of spoken content from podcasts. They are often used for accessibility, searchability, and analysis of podcast content.&gt;
Podcast transcripts are a type of dataset that can be analyzed to understand the perspectives of tech leaders on policy and regulation. They are used by tech journalists looking for insights and trends in the tech industry.
Podcast Transcripts is a dataset consisting of compiled transcripts of podcast conversations between Kevin Scott, Microsoft CTO, and other technology leaders. The dataset is part of the Behind the Tech series by Scott (2024). It contains 1669 &#215; 600-token text chunks with 100-token overlaps between chunks, totaling approximately 1 million tokens.</data>
      <data key="d2">5d04129d46662571f635a4e63cb4d6b7,aed2ea39de8a027cc818c7f4557f0514,d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="NEWS ARTICLES">
      <data key="d0">CONCEPT, INFORMATION TYPE</data>
      <data key="d1">News articles are written pieces that report on current events or issues. They are typically published in newspapers, magazines, or online news platforms and are intended to inform the public about important events and developments.&gt;
News articles are a type of dataset that can be analyzed to teach about health and wellness. They are used by educators incorporating current affairs into curricula.
News articles are written pieces that report on current events, issues, and trends. They provide information and insights on various topics, including health, technology, and politics.
News Articles is a benchmark dataset comprising news articles published from September 2013 to December 2023 across various categories such as entertainment, business, sports, technology, health, and science. The dataset, part of MultiHop-RAG (Tang and Yang, 2024), includes 3197 &#215; 600-token text chunks with 100-token overlaps between chunks, totaling approximately 1.7 million tokens.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc,5d04129d46662571f635a4e63cb4d6b7,aed2ea39de8a027cc818c7f4557f0514,d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="DIVERSE ACTIVITY-CENTERED SENSE-MAKING QUESTIONS">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">Diverse activity-centered sense-making questions are generated from short descriptions of real-world datasets, specifically podcast transcripts and news articles, to foster understanding of broad issues and themes. These questions aim to empower and provide comprehensive and diverse insights. The development of these questions involves exploring the impact of varying hierarchical levels of community summaries used to answer queries and comparing them to naive RAG and global map-reduce summarization of source texts.</data>
      <data key="d2">d2399fd0aae5bd200639806ca87184f8</data>
    </node>
    <node id="REAL-WORLD DATASETS">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">Real-world datasets, including podcast transcripts and news articles, serve as the source material for generating diverse activity-centered sense-making questions. These datasets are used to develop understanding of broad issues and themes through the creation of questions that aim to be comprehensive, diverse, and empowering.</data>
      <data key="d2">d2399fd0aae5bd200639806ca87184f8</data>
    </node>
    <node id="COMPREHENSIVENESS, DIVERSITY, EMPOWERMENT">
      <data key="d0">CONCEPT, CRITERIA</data>
      <data key="d1">Comprehensiveness, diversity, and empowerment are the target qualities that guide the development of sense-making questions. These qualities are designed to foster a deeper understanding of broad issues and themes by ensuring that the questions cover a wide range of topics, offer varied perspectives, and empower the user to engage with the material in a meaningful way.</data>
      <data key="d2">d2399fd0aae5bd200639806ca87184f8</data>
    </node>
    <node id="HIERARCHICAL LEVEL OF COMMUNITY SUMMARIES">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">The hierarchical level of community summaries refers to the varying degrees of detail and abstraction in summarizing information from source texts. These summaries are used to answer queries and are compared to naive RAG and global map-reduce summarization techniques to assess their effectiveness in terms of comprehensiveness and diversity.</data>
      <data key="d2">d2399fd0aae5bd200639806ca87184f8</data>
    </node>
    <node id="NAIVE RAG">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Naive RAG (Retrieval-Augmented Generation) is a technique for generating text by retrieving relevant information from a large corpus and augmenting it with generative models. It is compared to global approaches and hierarchical community summaries to evaluate its performance in terms of comprehensiveness and diversity.
Naive RAG (RAG stands for Retriever-Augmented Generation) is a method that produces the most direct responses across all comparisons in the context of question answering and summarization. It is characterized by its simplicity and directness in generating responses.&gt;
Naive RAG is a method for summarization that is compared to Graph RAG and other global approaches in terms of empowerment. It is less effective in providing specific examples, quotes, and citations that help users reach an informed understanding.
Naive RAG is a basic approach to Retrieval-Augmented Generation (RAG) that involves converting documents to text, splitting the text into chunks, and embedding these chunks into a vector space where similar positions represent similar semantics, to be used as context for queries.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97,71f14506a6b15dfabd93fd1606a67b73,d2399fd0aae5bd200639806ca87184f8,ed433e2f5d5387b47376eb0e45ca1c99</data>
    </node>
    <node id="GLOBAL MAP-REDUCE SUMMARIZATION">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Global map-reduce summarization is a technique that aggregates information from source texts to create summaries. It is compared to naive RAG and hierarchical community summaries to assess its effectiveness in providing comprehensive and diverse insights.</data>
      <data key="d2">d2399fd0aae5bd200639806ca87184f8</data>
    </node>
    <node id="TEXT CHUNKS">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">Text chunks are segments of text extracted from source documents for processing. They are the fundamental units of information that are passed to LLM (Large Language Model) prompts to extract various elements for a graph index. The granularity of these chunks affects the number of LLM calls required for extraction and the quality of the resulting information.
Text Chunks are segments of text derived from the Source Documents. They are created by splitting the source texts into smaller, manageable pieces for processing by LLM prompts. The size of these chunks affects the efficiency and effectiveness of information extraction.
Text Chunks refer to segments of source text that are processed for the identification and extraction of graph nodes and edges. These chunks are essential for the baseline requirement of identifying entities and their relationships within the text.</data>
      <data key="d2">d2399fd0aae5bd200639806ca87184f8,e50740c4332fdedb8739773592e2a402,e7caf4256ddea71533af1c4c50444146</data>
    </node>
    <node id="SOURCE DOCUMENTS">
      <data key="d0">DOCUMENT, INFORMATION</data>
      <data key="d1">Source Documents are the original texts or files from which information is extracted for processing. They serve as the primary input for the design and implementation of a graph index system.</data>
      <data key="d2">e7caf4256ddea71533af1c4c50444146</data>
    </node>
    <node id="LLM PROMPTS">
      <data key="d0">PROCEDURE, TOOL</data>
      <data key="d1">LLM (Language Model) Prompts are designed to extract various elements of a graph index from the Text Chunks. They are used to identify and extract instances of graph nodes and edges, as well as entities and their relationships, from each chunk of source text.</data>
      <data key="d2">e7caf4256ddea71533af1c4c50444146</data>
    </node>
    <node id="GRAPH INDEX">
      <data key="d0">DATA_STRUCTURE, INDEX</data>
      <data key="d1">Graph Index is a data structure that organizes information extracted from the source texts. It consists of nodes and edges that represent entities and their relationships, respectively. The design and implementation of the graph index are influenced by the granularity of text chunks and the effectiveness of LLM prompts.
The Graph Index is a data structure used for information retrieval, specifically designed to support conditions C0-C3. It was created using generic prompts for entity and relationship extraction, with entity types and few-shot examples tailored to the domain of the data. The indexing process utilized a context window size of 600 tokens with 1 gleaning for the Podcast dataset and 0 gleanings for the News dataset.
A graph index is a data structure that organizes information in a graph format, which can be used for efficient querying and analysis of relationships between entities, and it is a key component in the Graph RAG approach.</data>
      <data key="d2">53455f8552b0787cb13c5a03eb550842,7040ba36a7c09899a355d14a30d65375,e7caf4256ddea71533af1c4c50444146</data>
    </node>
    <node id="ENTITY REFERENCES">
      <data key="d0">REFERENCE, INFORMATION</data>
      <data key="d1">Entity References are mentions or instances of entities found within the Text Chunks. The number of entity references extracted is influenced by the size of the text chunks, with smaller chunks generally leading to higher recall but potentially lower precision.</data>
      <data key="d2">e7caf4256ddea71533af1c4c50444146</data>
    </node>
    <node id="GRAPH NODES">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">Graph Nodes represent entities or concepts that are identified within the text chunks. These nodes are the building blocks for constructing a graph representation of the information contained in the text.</data>
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="GRAPH EDGES">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">Graph Edges represent the relationships between graph nodes. They are identified and extracted from the text chunks to connect nodes in a graph, illustrating how entities are related to each other.</data>
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="LLM PROMPT">
      <data key="d0">CONCEPT, TOOL</data>
      <data key="d1">LLM (Language Model) Prompt is a multipart instruction used to guide the language model in identifying entities and relationships within the text chunks. It includes few-shot examples for in-context learning, tailored to the domain of the document corpus.</data>
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="FEW-SHOT EXAMPLES">
      <data key="d0">CONCEPT, TOOL</data>
      <data key="d1">Few-Shot Examples are specific instances provided to the LLM for in-context learning. These examples help the model understand the domain-specific entities and relationships, enhancing its ability to accurately identify and extract information.
Few-shot examples are specialized sets of data used to train the LLM in specific domains such as science, medicine, and law. These examples help the LLM to understand and perform tasks in these specialized fields with limited data.</data>
      <data key="d2">805a07a8f9c2ed5da2d9a61356aafa77,e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="NAMED ENTITIES">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">Named Entities are specific types of entities like people, places, and organizations that are generally applicable across various domains. They are identified by the default LLM prompt.</data>
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="SPECIALIZED KNOWLEDGE">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">Specialized Knowledge refers to domain-specific information that is relevant in fields such as science, medicine, and law. Few-shot examples specialized to these domains can improve the accuracy of entity and relationship extraction.</data>
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="COVARIATE PROMPT&lt;||COVARIATE PROMPT">
      <data key="d0">CONCEPT, TOOL</data>
      <data key="d1">Covariate Prompt is a secondary extraction prompt used to associate additional attributes or covariates with the detected node instances. It aims to extract claims linked to entities, including details such as the subject, object, type, description, source text span, and start and end dates.</data>
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="COVARIATE PROMPT">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="SECONDARY EXTRACTION PROMPT">
      <data key="d0">METHOD, PROCESS</data>
      <data key="d1">The secondary extraction prompt is a technique used to gather additional information or covariates associated with the extracted node instances. It aims to extract claims linked to detected entities, including details such as the subject, object, type, description, source text span, and start and end dates.</data>
      <data key="d2">805a07a8f9c2ed5da2d9a61356aafa77</data>
    </node>
    <node id="GLEANING PROCESS">
      <data key="d0">METHOD, PROCESS</data>
      <data key="d1">The gleaning process is a multi-stage method used to ensure that all entities are detected by the LLM. It involves multiple rounds of extraction, up to a specified maximum, to encourage the LLM to find any additional entities it may have missed in prior rounds. This process balances efficiency and quality by using larger chunk sizes without a drop in quality or the introduction of noise.</data>
      <data key="d2">805a07a8f9c2ed5da2d9a61356aafa77</data>
    </node>
    <node id="ELEMENT SUMMARIES">
      <data key="d0">OUTPUT, DATA</data>
      <data key="d1">Element summaries are the descriptions of entities, relationships, and claims that are generated by the LLM. These summaries are a form of abstractive summarization, where the LLM creates independently meaningful summaries of concepts that may be implied but not explicitly stated in the source texts.
Element Summaries are the condensed representations of information extracted from source texts by an LLM. These summaries are created for each graph element, including entity nodes, relationship edges, and claim covariates, and are used to convert instance-level summaries into single blocks of descriptive text.</data>
      <data key="d2">805a07a8f9c2ed5da2d9a61356aafa77,a73d3e7b661743b7583d8a0fd412b6a7</data>
    </node>
    <node id="LLM (LARGE LANGUAGE MODEL)">
      <data key="d0">TECHNOLOGY, TOOL</data>
      <data key="d1">LLM, or Large Language Model, is a sophisticated tool used for generating summaries and extracting information from source texts. It is capable of creating meaningful summaries of concepts that may be implied but not explicitly stated in the text, including entities, relationships, and claims. The LLM can handle variations in entity references and maintain consistency across multiple instances. It is also used to summarize groups of instances into single blocks of descriptive text for each graph element, such as entity nodes, relationship edges, and claim covariates.</data>
      <data key="d2">a73d3e7b661743b7583d8a0fd412b6a7</data>
    </node>
    <node id="ELEMENT INSTANCES">
      <data key="d0">PROPERTY, INPUT</data>
      <data key="d1">Element Instances refer to the specific occurrences or representations of entities, relationships, and claims within source texts. These instances are the raw data that an LLM processes to generate element summaries.</data>
      <data key="d2">a73d3e7b661743b7583d8a0fd412b6a7</data>
    </node>
    <node id="GRAPH ELEMENTS">
      <data key="d0">CONCEPT, STRUCTURE</data>
      <data key="d1">Graph Elements are the components of a graph structure used to represent information extracted from texts. These elements include entity nodes, relationship edges, and claim covariates, which are summarized and described using rich descriptive text generated by an LLM.</data>
      <data key="d2">a73d3e7b661743b7583d8a0fd412b6a7</data>
    </node>
    <node id="ENTITY GRAPH">
      <data key="d0">CONCEPT, DATA STRUCTURE</data>
      <data key="d1">The Entity Graph is a representation of entities and their relationships, where entities are nodes and relationships are edges. It is used to detect closely-related communities of entities and summarize their connections. The graph is potentially noisy and includes homogeneous nodes with rich descriptive text, which is beneficial for LLMs and global, query-focused summarization.</data>
      <data key="d2">6dace8e490674ac8e031aed987a63789</data>
    </node>
    <node id="CONNECTIVITY">
      <data key="d0">PROPERTY, CONCEPT</data>
      <data key="d1">Connectivity refers to the links between entities in the entity graph. It is crucial for the resilience of the overall approach to variations in entity names, as long as there is sufficient connectivity from all variations to a shared set of closely-related entities.</data>
      <data key="d2">6dace8e490674ac8e031aed987a63789</data>
    </node>
    <node id="RICH DESCRIPTIVE TEXT">
      <data key="d0">PROPERTY, CONCEPT</data>
      <data key="d1">Rich Descriptive Text is detailed information associated with homogeneous nodes in the entity graph. It aligns with the capabilities of LLMs and is essential for global, query-focused summarization.</data>
      <data key="d2">6dace8e490674ac8e031aed987a63789</data>
    </node>
    <node id="KNOWLEDGE GRAPHS">
      <data key="d0">CONCEPT, DATA STRUCTURE</data>
      <data key="d1">Knowledge Graphs are structured representations of information, typically relying on concise and consistent knowledge triples (subject, predicate, object) for downstream reasoning tasks. They are contrasted with the entity graph, which uses rich descriptive text.</data>
      <data key="d2">6dace8e490674ac8e031aed987a63789</data>
    </node>
    <node id="LEIDEN ALGORITHM">
      <data key="d0">CONCEPT, ALGORITHM</data>
      <data key="d1">The Leiden Algorithm is a specific community detection algorithm used in the pipeline for its ability to efficiently recover hierarchical community structure in large-scale graphs.
The Leiden Algorithm is a specific community detection algorithm that is efficient in recovering the hierarchical community structure of large-scale graphs. It is chosen for its ability to handle large datasets and provide a hierarchy of community partitions that cover the nodes of the graph in a mutually-exclusive, collective-exhaustive way.</data>
      <data key="d2">6dace8e490674ac8e031aed987a63789,a660289d2bf43f25d3524d35cd2d9a96</data>
    </node>
    <node id="GRAPHCOMMUNITIES">
      <data key="d0">DATA STRUCTURE, CONCEPT</data>
      <data key="d1">GraphCommunities refers to the communities of nodes identified within a graph by community detection algorithms. These communities are groups of nodes that have stronger connections to each other than to the rest of the nodes in the graph.</data>
      <data key="d2">a660289d2bf43f25d3524d35cd2d9a96</data>
    </node>
    <node id="GLOBAL QUERIES">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">Global queries are questions that seek to understand the overall themes or information within a corpus. They can be answered using community summaries, which provide a structured way to access and interpret the information contained in the corpus.</data>
      <data key="d2">93d4d4effbf989e6ef1c4c3b4f42494e</data>
    </node>
    <node id="CHUNKS OF PRE-SPECIFIED TOKEN SIZE">
      <data key="d0">PROPERTY, PROCESS</data>
      <data key="d1">Chunks of pre-specified token size are created to ensure that relevant information is distributed across multiple chunks, preventing concentration and potential loss of information in a single context window. This process is essential for handling large datasets or documents effectively.</data>
      <data key="d2">aed2ea39de8a027cc818c7f4557f0514</data>
    </node>
    <node id="INTERMEDIATE ANSWERS">
      <data key="d0">PROPERTY, PROCESS</data>
      <data key="d1">Intermediate answers are generated in parallel, one for each chunk, by mapping community answers. The Large Language Model (LLM) also generates a score between 0-100 for each answer, indicating its helpfulness in answering the target question. Answers with a score of 0 are filtered out.</data>
      <data key="d2">aed2ea39de8a027cc818c7f4557f0514</data>
    </node>
    <node id="GLOBAL ANSWER">
      <data key="d0">PROPERTY, PROCESS</data>
      <data key="d1">Global answers are created by sorting intermediate community answers in descending order of helpfulness score and iteratively adding them into a new context window until the token limit is reached. This final context is used to generate the global answer that is returned to the user.</data>
      <data key="d2">aed2ea39de8a027cc818c7f4557f0514</data>
    </node>
    <node id="DATASET">
      <data key="d0">PROPERTY, DATA</data>
      <data key="d1">Dataset refers to the collection of data used for various tasks, such as understanding tech leaders' views on policy and regulation or teaching about health and wellness. It can include podcast transcripts and news articles.
A Dataset is a collection of data that can be processed by the pipeline. It is typically represented as a DataFrame in the Python API. The Dataset can contain multiple columns, each with its own data type and values.&gt;
dataset is a pandas DataFrame containing the data to be processed. It has columns col1 and col2 with numerical values.</data>
      <data key="d2">aed2ea39de8a027cc818c7f4557f0514,b0505e11596cadd9890fef049c29473c,f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="TECH JOURNALIST">
      <data key="d0">PERSON, ROLE</data>
      <data key="d1">A tech journalist is a person who looks for insights and trends in the tech industry. They use podcast transcripts to understand how tech leaders view the role of policy and regulation.</data>
      <data key="d2">aed2ea39de8a027cc818c7f4557f0514</data>
    </node>
    <node id="EDUCATOR">
      <data key="d0">PERSON, ROLE</data>
      <data key="d1">An educator is a person who incorporates current affairs into curricula. They use news articles to teach about health and wellness.
An educator is a professional who teaches and facilitates learning. In the context of health education, educators incorporate current affairs and news into curricula to enhance students' understanding of health and wellness.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc,aed2ea39de8a027cc818c7f4557f0514</data>
    </node>
    <node id="PRIVACY LAWS">
      <data key="d0">LEGAL, REGULATION</data>
      <data key="d1">Privacy laws are government regulations that govern the collection, use, and protection of personal data. They impact technology development by setting boundaries for data usage and influencing how tech companies design and implement their products and services.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="TECHNOLOGY DEVELOPMENT">
      <data key="d0">INDUSTRY, INNOVATION</data>
      <data key="d1">Technology development refers to the process of creating new technologies or improving existing ones. It is influenced by various factors, including legal frameworks, market demands, and ethical considerations.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="ETHICAL CONSIDERATIONS">
      <data key="d0">PHILOSOPHY, ETHICS</data>
      <data key="d1">Ethical considerations involve the principles and values that guide behavior and decision-making. In the context of technology, they ensure that innovations are developed and used responsibly, considering the impact on individuals and society.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="CURRENT POLICIES">
      <data key="d0">LEGAL, REGULATION</data>
      <data key="d1">Current policies refer to the existing laws, regulations, and guidelines that govern various aspects of society, including technology, health, and education. They are subject to change based on new insights, societal needs, and technological advancements.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="COLLABORATIONS">
      <data key="d0">RELATIONSHIP, PARTNERSHIP</data>
      <data key="d1">Collaborations refer to cooperative efforts between different entities, such as tech companies and governments. They can lead to the development of new technologies, policies, or solutions that benefit society.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="HEALTH EDUCATION CURRICULA">
      <data key="d0">EDUCATION, CURRICULUM</data>
      <data key="d1">Health education curricula are structured programs of study that focus on teaching students about health and wellness. They can be enriched by integrating current topics and news articles to provide real-world relevance.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="PREVENTIVE MEDICINE">
      <data key="d0">MEDICINE, HEALTH</data>
      <data key="d1">Preventive medicine is a branch of medicine that focuses on preventing diseases and promoting health. It includes practices such as vaccinations, screenings, and lifestyle modifications.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="WELLNESS">
      <data key="d0">HEALTH, LIFESTYLE</data>
      <data key="d1">Wellness refers to a state of being in good health, both physically and mentally. It encompasses a holistic approach to health, including nutrition, exercise, and mental well-being.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="HEALTH ARTICLES">
      <data key="d0">MEDIA, INFORMATION</data>
      <data key="d1">Health articles are written pieces that focus on health-related topics. They can provide insights into public health priorities, contradicting views on health issues, and the importance of health literacy.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="PUBLIC HEALTH PRIORITIES">
      <data key="d0">HEALTH, POLICY</data>
      <data key="d1">Public health priorities are the areas of focus for public health agencies and policymakers. They are determined by the prevalence of health issues, available resources, and societal needs.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="HEALTH LITERACY">
      <data key="d0">EDUCATION, HEALTH</data>
      <data key="d1">Health literacy is the ability to understand and use health information to make informed decisions. It is crucial for individuals to navigate the healthcare system and maintain their health.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="TECH COMPANIES">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="HOTPOTQA">
      <data key="d0">DATASET</data>
      <data key="d1">HotPotQA is a benchmark dataset for open-domain question answering, focusing on explicit fact retrieval rather than summarization for data sensemaking. It was introduced by Yang et al. (2018).
HotPotQA is a benchmark dataset for open-domain question answering, focusing on explicit fact retrieval rather than summarization for data sensemaking. It was introduced by Yang et al. in 2018.</data>
      <data key="d2">5d04129d46662571f635a4e63cb4d6b7,8e69f04648f5fc24c299591365f1aa68</data>
    </node>
    <node id="MULTIHOP-RAG">
      <data key="d0">DATASET</data>
      <data key="d1">MultiHop-RAG is a benchmark dataset for open-domain question answering, focusing on explicit fact retrieval rather than summarization for data sensemaking. It was introduced by Tang and Yang (2024).
MultiHop-RAG is a benchmark dataset for open-domain question answering, introduced by Tang and Yang in 2024. It is designed for explicit fact retrieval rather than summarization for data sensemaking.</data>
      <data key="d2">5d04129d46662571f635a4e63cb4d6b7,8e69f04648f5fc24c299591365f1aa68</data>
    </node>
    <node id="MT-BENCH">
      <data key="d0">DATASET</data>
      <data key="d1">MT-Bench is a benchmark dataset for open-domain question answering, focusing on explicit fact retrieval rather than summarization for data sensemaking. It was introduced by Zheng et al. (2024).
MT-Bench is a benchmark dataset for open-domain question answering, introduced by Zheng et al. in 2024. It targets explicit fact retrieval rather than summarization for data sensemaking.</data>
      <data key="d2">5d04129d46662571f635a4e63cb4d6b7,8e69f04648f5fc24c299591365f1aa68</data>
    </node>
    <node id="DATA SENSEMAKING">
      <data key="d0">CONCEPT, PROCESS</data>
      <data key="d1">Data sensemaking is the process through which people inspect, engage with, and contextualize data within the broader scope of real-world activities. It involves summarization for understanding data rather than explicit fact retrieval. Koesten et al. (2021) discuss this concept.</data>
      <data key="d2">8e69f04648f5fc24c299591365f1aa68</data>
    </node>
    <node id="SUMMARIZATION QUERIES">
      <data key="d0">PROPERTY, QUERY</data>
      <data key="d1">Summarization queries are questions or queries that aim to extract a high-level understanding of dataset contents, rather than details of specific texts. They are used for data sensemaking and global understanding of the corpus.</data>
      <data key="d2">8e69f04648f5fc24c299591365f1aa68</data>
    </node>
    <node id="RAG (RETRIEVAL-AUGMENTED GENERATION) SYSTEMS">
      <data key="d0">SYSTEM, TECHNOLOGY</data>
      <data key="d1">RAG systems are retrieval-augmented generation systems that combine information retrieval with text generation to answer questions. They are evaluated for their effectiveness in global sensemaking tasks using summarization queries.</data>
      <data key="d2">8e69f04648f5fc24c299591365f1aa68</data>
    </node>
    <node id="ACTIVITY-CENTERED APPROACH">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">The Activity-Centered Approach is a methodology used for automating the generation of questions. It involves identifying potential users and tasks per user, and then generating questions that require an understanding of the entire corpus for each (user, task) combination.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="EVALUATION DATASETS">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">Evaluation Datasets are collections of data used for testing and evaluating the effectiveness of the LLM in generating questions. Each dataset results in 125 test questions when N = 5.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="GRAPH RAG (RETRIEVAL-AUGMENTED GENERATION)">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">Graph RAG is a method that uses graph communities at different levels (C0, C1, C2, C3) to answer user queries. It leverages summaries from these communities to provide responses.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="TEXT SUMMARIZATION METHOD">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">The Text Summarization Method is an approach that applies a map-reduce technique directly to source texts (TS) to generate summaries. It is used as a condition for comparison in the evaluation.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="NAIVE SEMANTIC SEARCH RAG APPROACH">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">The Naive Semantic Search RAG Approach (SS) is a simple method for answering queries by searching for semantic matches in the text. It is used as a baseline for comparison in the evaluation.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="C0 (ROOT-LEVEL COMMUNITY SUMMARIES)">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">C0 refers to the root-level community summaries, which are the fewest in number. They are used in the Graph RAG approach to answer user queries.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="C1 (HIGH-LEVEL COMMUNITY SUMMARIES)">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">C1 represents high-level community summaries, which are sub-communities of C0 if present, or C0 communities projected down. They are used in the Graph RAG approach to answer queries.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="C2 (INTERMEDIATE-LEVEL COMMUNITY SUMMARIES)">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">C2 consists of intermediate-level community summaries, which are sub-communities of C1 if present, or C1 communities projected down. They are used in the Graph RAG approach to answer queries.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="C3 (LOW-LEVEL COMMUNITY SUMMARIES)">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">C3 includes low-level community summaries, which are the greatest in number. They are sub-communities of C2 if present, or otherwise used directly in the Graph RAG approach to answer queries.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="C1">
      <data key="d0">COMMUNITY, INFORMATION SYSTEM</data>
      <data key="d1">C1 refers to a level of community summaries that are used to answer queries. These summaries are sub-communities of C0 if C0 is present, or C0 communities projected down if C0 is not present. C1 communities are at a higher level of detail than C0.</data>
      <data key="d2">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </node>
    <node id="C2">
      <data key="d0">COMMUNITY, INFORMATION SYSTEM</data>
      <data key="d1">C2 refers to a level of community summaries that are used to answer queries. These summaries are sub-communities of C1 if C1 is present, or C1 communities projected down if C1 is not present. C2 communities are at a higher level of detail than C1.</data>
      <data key="d2">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </node>
    <node id="C3">
      <data key="d0">COMMUNITY, INFORMATION SYSTEM</data>
      <data key="d1">C3 refers to a level of community summaries that are used to answer queries. These summaries are sub-communities of C2 if C2 is present, or C2 communities projected down if C2 is not present. C3 communities are the low-level community summaries, greatest in number, and are at a higher level of detail than C2.</data>
      <data key="d2">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </node>
    <node id="TS">
      <data key="d0">INFORMATION PROCESSING, SUMMARIZATION TECHNIQUE</data>
      <data key="d1">TS is a method that uses the same approach as described in subsection 2.6, with the exception that source texts are shuffled and chunked for the map-reduce summarization stages instead of using community summaries.</data>
      <data key="d2">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </node>
    <node id="SS">
      <data key="d0">INFORMATION PROCESSING, SUMMARIZATION TECHNIQUE</data>
      <data key="d1">SS is an implementation of a naive RAG (Retrieval-Augmented Generation) method in which text chunks are retrieved and added to the available context window until the specified token limit is reached.</data>
      <data key="d2">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </node>
    <node id="C0">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </node>
    <node id="RAGAS (RETRIEVAL-AUGMENTED GENERATION ASSESSMENT SYSTEM)">
      <data key="d0">TECHNOLOGY, INFORMATION RETRIEVAL</data>
      <data key="d1">RAGAS, or Retrieval-Augmented Generation Assessment System, is a system that uses LLMs to evaluate the performance of conventional RAG systems. It automatically assesses qualities such as context relevance, faithfulness, and answer relevance.</data>
      <data key="d2">53455f8552b0787cb13c5a03eb550842</data>
    </node>
    <node id="PODCAST DATASET">
      <data key="d0">DATA SET</data>
      <data key="d1">The Podcast dataset is a collection of data used in the context of the graph indexing process. It has a context window size of 600 tokens with 1 gleaning.
The Podcast Dataset is a collection of data used in the research, consisting of 8564 nodes and 20691 edges in the graph created through the indexing process. It is one of the datasets used to evaluate the performance of language models under varying context window sizes.
The Podcast Dataset is a collection of data used in the indexing process, resulting in a graph with 8564 nodes and 20691 edges.
The Podcast Dataset is a collection of audio content, typically consisting of episodes or segments, used in the context of summarization and question answering experiments.&gt;</data>
      <data key="d2">3900d15a5f3ace358fc06038c34cdf79,53455f8552b0787cb13c5a03eb550842,71f14506a6b15dfabd93fd1606a67b73,d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="NEWS DATASET">
      <data key="d0">DATA SET</data>
      <data key="d1">The News dataset is a collection of data used in the context of the graph indexing process. It has a context window size of 600 tokens with 0 gleanings.
The News Dataset is a collection of data used in the research, consisting of 15754 nodes and 19520 edges in the graph created through the indexing process. It is a larger dataset compared to the Podcast Dataset and is used to evaluate the performance of language models under varying context window sizes.
The News Dataset is a collection of data used in the indexing process, resulting in a larger graph with 15754 nodes and 19520 edges.
The News Dataset is a collection of news articles or reports, used in the context of summarization and question answering experiments.&gt;</data>
      <data key="d2">3900d15a5f3ace358fc06038c34cdf79,53455f8552b0787cb13c5a03eb550842,71f14506a6b15dfabd93fd1606a67b73,d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="RAGAS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">53455f8552b0787cb13c5a03eb550842</data>
    </node>
    <node id="WANG ET AL., 2023A">
      <data key="d0">AUTHOR, PUBLICATION</data>
      <data key="d1">Wang et al., 2023a, are authors of a publication that is referenced in the context of head-to-head comparison of competing outputs in the field of LLMs (Large Language Models).</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="LLM AS-A-JUDGE, ZHENG ET AL., 2024">
      <data key="d0">AUTHOR, PUBLICATION</data>
      <data key="d1">LLM as-a-judge, Zheng et al., 2024, are authors of a publication that discusses the use of LLMs as evaluators in head-to-head comparisons of competing outputs.</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="RAGAS, ES ET AL., 2023">
      <data key="d0">AUTHOR, PUBLICATION</data>
      <data key="d1">RAGAS, Es et al., 2023, are authors of a publication that focuses on the evaluation of RAG (Retrieval-Augmented Generation) systems, specifically looking at qualities like context relevance, faithfulness, and answer relevance.</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="GRAPH RAG MECHANISM">
      <data key="d0">METHOD, TECHNIQUE</data>
      <data key="d1">The Graph RAG mechanism is a multi-stage method used for sensemaking activities, which involves the retrieval and integration of information from various sources to answer complex questions.</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="HEAD-TO-HEAD COMPARISON APPROACH">
      <data key="d0">METHOD, TECHNIQUE</data>
      <data key="d1">The head-to-head comparison approach is a method used to evaluate and compare different outputs or methods by directly contrasting them against each other, often using an LLM evaluator.</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="LLM EVALUATOR">
      <data key="d0">TOOL, TECHNOLOGY</data>
      <data key="d1">An LLM evaluator is a Large Language Model used as a tool to assess the performance of different methods or outputs, particularly in the context of sensemaking activities and the evaluation of RAG systems.
The LLM Evaluator is a tool used to assess the quality of answers based on predefined metrics. It compares answers to a question and determines which answer is better according to the metric, providing a rationale for its decision.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f,cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="TARGET METRICS">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Target metrics are specific measures used to evaluate the performance of methods or outputs in sensemaking activities, focusing on qualities such as comprehensiveness, diversity, empowerment, and directness.</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="CONTROL METRIC (DIRECTNESS)">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">The control metric, directness, is a measure used to indicate the validity of the evaluation, specifically assessing how specifically and clearly an answer addresses a question. It is often used as a baseline for comparison.</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="COMPREHENSIVENESS">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Comprehensiveness is a metric that evaluates how much detail an answer provides to cover all aspects and details of a question, indicating the thoroughness of the response.
Comprehensiveness is a measure of how much detail an answer provides to cover all aspects and details of the question. It evaluates the thoroughness of the response in addressing the question's requirements.
Comprehensiveness is a metric used to evaluate the quality of answers or responses generated by language models. It measures how well the generated content covers the topic or question at hand, with a higher win rate indicating better comprehensiveness.
Comprehensiveness is a metric used to measure the completeness of the approaches in covering the content of the datasets.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f,3900d15a5f3ace358fc06038c34cdf79,cbfd4a09b266218f64dc6e6d80f8a77e,d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="DIVERSITY">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Diversity is a metric that assesses how varied and rich an answer is in providing different perspectives and insights on a question, indicating the breadth of information presented.
Diversity is a measure of how varied and rich an answer is in providing different perspectives and insights on the question. It assesses the range of viewpoints and the depth of information presented in the response.
Diversity is a metric used to evaluate the variety of answers or responses generated by language models. It measures the range and uniqueness of the generated content, with a higher win rate indicating greater diversity.
Diversity is a metric used to measure the variety of the approaches in handling different aspects of the datasets.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f,3900d15a5f3ace358fc06038c34cdf79,cbfd4a09b266218f64dc6e6d80f8a77e,d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="EMPOWERMENT">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Empowerment is a metric that evaluates how well an answer helps the reader understand and make informed judgements about a topic, indicating the answer's ability to facilitate understanding and decision-making.
Empowerment is a measure of how well an answer helps the reader understand and make informed judgements about the topic. It evaluates the answer's ability to provide clarity and enable the reader to form educated opinions.
Empowerment is a metric used to evaluate the ability of language models to provide answers that are empowering or enable the user to take action or make decisions. It measures the effectiveness of the generated content in terms of empowerment, with a higher win rate indicating greater empowerment.
Empowerment is a concept that refers to the ability of a method or system to provide users with the information they need to reach an informed understanding. Comparisons showed mixed results for both global approaches versus naive RAG and Graph RAG approaches versus source text summarization.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f,3900d15a5f3ace358fc06038c34cdf79,cbfd4a09b266218f64dc6e6d80f8a77e,ed433e2f5d5387b47376eb0e45ca1c99</data>
    </node>
    <node id="DIRECTNESS">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Directness is a measure of how specifically and clearly an answer addresses the question. It evaluates the answer's relevance and clarity in relation to the question being asked.
Directness is a validity test used to measure the straightforwardness of the responses produced by the approaches.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f,d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="COMPARISON METHOD">
      <data key="d0">PROCEDURE, EVALUATION METHOD</data>
      <data key="d1">The Comparison Method involves providing the LLM with a question, a target metric, and a pair of answers. The LLM then assesses which answer is better according to the metric and returns the winner, or a tie if the answers are fundamentally similar.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f</data>
    </node>
    <node id="STOCHASTICITY">
      <data key="d0">PROPERTY, BEHAVIOR</data>
      <data key="d1">Stochasticity refers to the inherent randomness or variability in the LLM's responses. To account for this, each comparison is run five times and mean scores are used to determine the outcome.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f</data>
    </node>
    <node id="TABLE 2">
      <data key="d0">DATA, EXAMPLE</data>
      <data key="d1">Table 2 is an example of LLM-generated assessment, showing the results of the head-to-head comparisons based on the metrics and conditions evaluated.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f</data>
    </node>
    <node id="FIGURE 4">
      <data key="d0">DATA, VISUALIZATION</data>
      <data key="d1">Figure 4 is a visual representation of head-to-head win rate percentages across two datasets, four metrics, and 125 questions per comparison. It shows the performance of different conditions in relation to the metrics.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f</data>
    </node>
    <node id="HEADWINRATEPERCENTAGES">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Headwin rate percentages represent the success rates of different conditions (row and column conditions) across two datasets, four metrics, and 125 questions per comparison. Each comparison is repeated five times and averaged. The overall winner per dataset and metric is shown in bold. Self-win rates were not computed but are shown as the expected 50% for reference.</data>
      <data key="d2">b83d819b03401fb8332316960610e5d6</data>
    </node>
    <node id="GRAPHRAGCONDITIONS">
      <data key="d0">PROPERTY, TECHNIQUE</data>
      <data key="d1">GraphRAG conditions refer to the conditions under which GraphRAG (a graph-based retrieval-augmented generation model) is applied. These conditions outperformed naive RAG (a retrieval-augmented generation model without a graph index) on comprehensiveness and diversity. Conditions C1-C3 also showed slight improvements in answer comprehensiveness and diversity over TS (global text summarization without a graph index).</data>
      <data key="d2">b83d819b03401fb8332316960610e5d6</data>
    </node>
    <node id="CONTEXTWINDOWSIZE">
      <data key="d0">PROPERTY, PARAMETER</data>
      <data key="d1">Context window size is a parameter that affects the performance of tasks, especially for models like GPT-4 Turbo, which have a large context size of 128k tokens. The effect of context window size on any particular task is unclear, especially for models with a large context size. The potential for information to be &#8220;lost in the middle&#8221; of longer contexts is a concern. The optimum context size for the baseline condition (SS) was determined and then used uniformly for all query-time LLM (large language model) use.</data>
      <data key="d2">b83d819b03401fb8332316960610e5d6</data>
    </node>
    <node id="VARYING CONTEXT WINDOW SIZE">
      <data key="d0">PROPERTY, RESEARCH</data>
      <data key="d1">Varying the context window size is a research variable that was explored to understand its effects on combinations of datasets, questions, and metrics in the context of language model (LLM) use. The goal was to determine the optimum context size for the baseline condition (SS) and apply it uniformly for all query-time LLM use.</data>
      <data key="d2">3900d15a5f3ace358fc06038c34cdf79</data>
    </node>
    <node id="OPTIMUM CONTEXT SIZE">
      <data key="d0">PROPERTY, RESEARCH</data>
      <data key="d1">The optimum context size is the ideal size of the context window that provides the best performance for a given set of conditions, such as datasets, questions, and metrics. In this case, the optimum context size was determined to be 8k tokens for the baseline condition (SS) and was used uniformly for all query-time LLM use.</data>
      <data key="d2">3900d15a5f3ace358fc06038c34cdf79</data>
    </node>
    <node id="WINDOWSIZE">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">The windowsize refers to the size of the window used in the indexing process for the final evaluation, in this case, it is 8k tokens.</data>
      <data key="d2">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="GLOBAL APPROACHES">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">Global Approaches refer to the methods used in the evaluation that consistently outperformed the naive RAG (SS) approach in both comprehensiveness and diversity metrics across datasets.</data>
      <data key="d2">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="NAIVE RAG (SS) APPROACH">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">The Naive RAG (SS) Approach is a method used in the evaluation that was outperformed by global approaches in terms of comprehensiveness and diversity metrics.</data>
      <data key="d2">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="MAP-REDUCE SUMMARIZATION">
      <data key="d0">METHOD, ALGORITHM</data>
      <data key="d1">Map-Reduce Summarization is a resource-intensive approach used for summarizing source texts. It requires the highest number of context tokens among the methods discussed, indicating its computational demands.&gt;
Map-reduce summarization is a technique used in the Graph RAG approach to summarize large amounts of data. It involves breaking down the data into smaller parts (map) and then combining the results (reduce) to produce a summary. This method is used in conjunction with other techniques to support human sensemaking over entire text corpora.</data>
      <data key="d2">5e2933c9646c751e6a60c9de12a255f2,71f14506a6b15dfabd93fd1606a67b73</data>
    </node>
    <node id="SOURCE TEXTS">
      <data key="d0">DOCUMENT, ORIGINAL</data>
      <data key="d1">Source Texts are the original documents or data from which summaries are derived. They are the primary material that undergoes summarization or analysis.&gt;</data>
      <data key="d2">71f14506a6b15dfabd93fd1606a67b73</data>
    </node>
    <node id="SOURCE TEXT SUMMARIZATION">
      <data key="d0">METHOD, TECHNOLOGY</data>
      <data key="d1">Source Text Summarization is a method for creating summaries directly from the original text. It is compared to Graph RAG in terms of context tokens required and comprehensiveness and diversity of the summaries.</data>
      <data key="d2">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </node>
    <node id="LOW-LEVEL COMMUNITY SUMMARIES (C3)">
      <data key="d0">PROPERTY, CATEGORY</data>
      <data key="d1">Low-level Community Summaries (C3) are a category of summaries that are part of the analysis. Graph RAG requires 26-33% fewer context tokens for these summaries compared to source text summarization.</data>
      <data key="d2">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </node>
    <node id="ROOT-LEVEL COMMUNITY SUMMARIES (C0)">
      <data key="d0">PROPERTY, CATEGORY</data>
      <data key="d1">Root-level Community Summaries (C0) are a category of summaries that are part of the analysis. Graph RAG requires over 97% fewer context tokens for these summaries compared to source text summarization.</data>
      <data key="d2">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </node>
    <node id="INFORMED UNDERSTANDING">
      <data key="d0">CONCEPT, INFORMATION PROCESSING</data>
      <data key="d1">Informed Understanding refers to the process by which users reach a comprehensive and knowledgeable comprehension of a subject, often facilitated by specific examples, quotes, and citations.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97</data>
    </node>
    <node id="GRAPH RAG INDEX">
      <data key="d0">PROPERTY, INFORMATION SYSTEM</data>
      <data key="d1">Graph RAG Index is a component of a system designed to enhance the retrieval of relevant information by indexing and organizing data in a graph structure, potentially improving the efficiency and effectiveness of information retrieval.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97</data>
    </node>
    <node id="TUNING ELEMENT EXTRACTION PROMPTS">
      <data key="d0">PROPERTY, INFORMATION RETRIEVAL</data>
      <data key="d1">Tuning Element Extraction Prompts involves refining the prompts used to extract information from documents, which can help in retaining more details in the Graph RAG index, potentially improving the quality of information retrieval.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97</data>
    </node>
    <node id="RAG APPROACHES">
      <data key="d0">CONCEPT, INFORMATION RETRIEVAL</data>
      <data key="d1">RAG Approaches refer to the methods and systems used to retrieve relevant information from external data sources to augment the context window of Large Language Models (LLMs), aiding in answering queries with more comprehensive information.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97</data>
    </node>
    <node id="ADVANCED RAG&lt;||ADVANCED RAG">
      <data key="d0">CONCEPT, INFORMATION RETRIEVAL</data>
      <data key="d1">Advanced RAG encompasses more sophisticated variations of Retrieval-Augmented Generation (RAG) systems that include pre-retrieval, retrieval, and post-retrieval strategies designed to overcome the limitations of Naive RAG, often incorporating iterative and dynamic cycles of retrieval and generation.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97</data>
    </node>
    <node id="MODULAR RAG">
      <data key="d0">PROPERTY, INFORMATION RETRIEVAL</data>
      <data key="d1">Modular RAG is a type of RAG system that includes patterns for iterative and dynamic cycles of interleaved retrieval and generation, designed to enhance the flexibility and adaptability of information retrieval processes.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97</data>
    </node>
    <node id="ADVANCED RAG">
      <data key="d0" />
      <data key="d1">
Advanced RAG (Retrieval-Augmented Generation) is a research area that involves using knowledge graphs as an index for advanced retrieval and analysis of information, as described by Gao et al. in 2023.
Advanced RAG (Retrieval-Augmented Generation) is a form of technology where the index is a knowledge graph, as described by Gao et al. in 2023. This involves using a knowledge graph to augment the generation of text.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97,40f2d6a0270e54743e7ace239369da96,7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="LLM&#8217;S CONTEXT WINDOW">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">LLM&#8217;s context window refers to the limitation of a large language model in processing and understanding text, where the model can only consider a fixed amount of text at a time, typically a few thousand tokens. This context window can affect the model's ability to understand and generate text that is longer than the window size.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="ADVANCED RAG SYSTEMS">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Advanced RAG systems are retrieval-augmented generation systems that include pre-retrieval, retrieval, and post-retrieval strategies designed to overcome the drawbacks of Naive RAG. These systems are more sophisticated and can handle complex tasks by incorporating various strategies for retrieval and generation.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="MODULAR RAG SYSTEMS">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Modular RAG systems are retrieval-augmented generation systems that include patterns for iterative and dynamic cycles of interleaved retrieval and generation. These systems are designed to be flexible and can adapt to different retrieval and generation requirements.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="PARALLEL GENERATION OF COMMUNITY ANSWERS">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Parallel generation of community answers is a technique where community answers are generated simultaneously from community summaries. This approach can improve the efficiency and effectiveness of retrieval and generation by leveraging the power of parallel processing.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="ITERATIVE RETRIEVAL-GENERATION STRATEGY">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Iterative retrieval-generation strategy is a method where retrieval and generation are performed in a series of iterative cycles. This strategy allows for continuous refinement and improvement of the retrieval and generation process.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="FEDERATED RETRIEVAL-GENERATION STRATEGY">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Federated retrieval-generation strategy is a method where retrieval and generation are performed in a distributed and collaborative manner. This strategy allows for the sharing of resources and information across multiple systems or nodes.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="MULTI-DOCUMENT SUMMARIZATION">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Multi-document summarization is a technique for creating a concise and coherent summary of multiple documents. This technique is used to provide a high-level overview of the content of multiple documents, which can be useful for information retrieval and analysis.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="MULTI-HOP QUESTION ANSWERING">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Multi-hop question answering is a technique for answering questions that require reasoning over multiple pieces of information or documents. This technique is used to handle complex questions that cannot be answered by a single piece of information or document.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="HIERARCHICAL INDEX">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Hierarchical index is a data structure that organizes information in a hierarchical manner. This index is used to improve the efficiency and effectiveness of information retrieval by allowing for faster and more accurate search and retrieval of information.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="SUMMARIZATION">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Summarization is a technique for creating a concise and coherent summary of a document or set of documents. This technique is used to provide a high-level overview of the content of a document or set of documents, which can be useful for information retrieval and analysis.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="CAIRE-COVID">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">CAiRE-COVID is a research study conducted by Su et al. in 2020, focusing on the impact of COVID-19 on various aspects of society and health.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="ITRG">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">ITRG is a research study that involves multi-hop question answering, as described by Feng et al. in 2023.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="IR-COT">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">IR-CoT is a research study that deals with multi-hop question answering, as described by Trivedi et al. in 2022.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="DSP">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">DSP is a research study that involves multi-hop question answering, as described by Khattab et al. in 2022.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="RAPTOR">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">RAPTOR is a research study that involves generating a hierarchical index of text chunks by clustering the vectors of text embeddings, as described by Sarthi et al. in 2024.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="TREE OF CLARIFICATIONS">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">Tree of Clarifications is a research study that involves generating a &#8220;tree of clarifications&#8221; to answer multiple interpretations of ambiguous questions, as described by Kim et al. in 2023.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="KNOWLEDGE GRAPH CREATION">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">Knowledge Graph Creation is a research area that involves using LLMs to create knowledge graphs, as described by Trajanoska et al. in 2023.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="KNOWLEDGE GRAPH COMPLETION">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">Knowledge Graph Completion is a research area that involves using LLMs to complete knowledge graphs, as described by Yao et al. in 2023.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="CAUSAL GRAPH EXTRACTION">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">Causal Graph Extraction is a research area that involves using LLMs to extract causal graphs from source texts, as described by Ban et al. in 2023 and Zhang et al. in 2024.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="KAPING">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">KAPING is a research study that involves advanced RAG where the index is a knowledge graph, as described by Baek et al.
KAPING, developed by Baek et al. in 2023, is a system where subsets of the graph structure are the objects of enquiry. This involves querying specific parts of a knowledge graph for information.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96,7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="LLMS FOR KNOWLEDGE GRAPH CREATION">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">LLMs (Large Language Models) are used for the creation of knowledge graphs, as demonstrated by the work of Trajanoska et al. in 2023. This involves the automatic generation of structured knowledge graphs from unstructured text data.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="LLMS FOR KNOWLEDGE GRAPH COMPLETION">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">LLMs are also applied for the completion of knowledge graphs, as shown by the research of Yao et al. in 2023. This involves enhancing existing knowledge graphs by filling in missing information or relationships.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="LLMS FOR CAUSAL GRAPH EXTRACTION">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">LLMs are used for the extraction of causal graphs from source texts, as evidenced by the work of Ban et al. in 2023 and Zhang et al. in 2024. This involves identifying causal relationships between entities in the text.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="G-RETRIEVER">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">G-Retriever, introduced by He et al. in 2024, is a system that retrieves specific parts of a graph structure. This involves querying a graph for specific information or relationships.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="GRAPHTOOLFORMER">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">GraphToolFormer, developed by Zhang in 2023, is a system that uses derived graph metrics as the objects of enquiry. This involves analyzing graph metrics to extract information.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="SURGE">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">SURGE, created by Kang et al. in 2023, is a system where narrative outputs are strongly grounded in the facts of retrieved subgraphs. This involves generating narratives based on the information retrieved from subgraphs.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="FABULA">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">FABULA, developed by Ranade and Joshi in 2023, is a system that serializes retrieved event-plot subgraphs using narrative templates. This involves creating narratives from event plots retrieved from graphs.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="SYSTEM FOR MULTI-HOP QUESTION ANSWERING">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">A system that supports both creation and traversal of text-relationship graphs for multi-hop question answering, as described by Wang et al. in 2023b. This involves answering complex questions that require understanding relationships across multiple pieces of text.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="LANGCHAIN">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">LangChain, as of 2024, is a library that supports a variety of graph databases. This involves providing tools and interfaces for working with graph data.
LangChain is a library that supports the creation and traversal of text-relationship graphs for multi-hop question answering and is compatible with various graph databases for RAG applications.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7,e015335cdcae20e6546fe7cbdef56c1a</data>
    </node>
    <node id="LLAMAINDEX">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">LlamaIndex, as of 2024, is a library that supports a variety of graph databases. This involves providing tools and interfaces for working with graph data.
LlamaIndex is a library that supports the creation and traversal of text-relationship graphs for multi-hop question answering and is compatible with various graph databases for RAG applications.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7,e015335cdcae20e6546fe7cbdef56c1a</data>
    </node>
    <node id="GRAPH-BASED RAG APPLICATIONS">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">A more general class of graph-based RAG (Retrieval-Augmented Generation) applications is emerging. This involves the use of graph data in various applications for retrieval and generation.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="NEO4J">
      <data key="d0">SOFTWARE, DATABASE</data>
      <data key="d1">Neo4J is a graph database that can be used to create and reason over knowledge graphs, and is supported by LangChain and LlamaIndex libraries for RAG applications.</data>
      <data key="d2">e015335cdcae20e6546fe7cbdef56c1a</data>
    </node>
    <node id="NEBULAGRAPH">
      <data key="d0">SOFTWARE, DATABASE</data>
      <data key="d1">NebulaGraph is a graph database that can be used to create and reason over knowledge graphs, and is supported by LangChain and LlamaIndex libraries for RAG applications.</data>
      <data key="d2">e015335cdcae20e6546fe7cbdef56c1a</data>
    </node>
    <node id="EVALUATION APPROACH">
      <data key="d0">METHOD, PROCESS</data>
      <data key="d1">The evaluation approach has limitations, as it has only examined a certain class of sensemaking questions for two corpora in the region of 1 million tokens. More work is needed to understand how performance varies across different ranges of question types, data types, and dataset sizes.</data>
      <data key="d2">e015335cdcae20e6546fe7cbdef56c1a</data>
    </node>
    <node id="SELFCHECKGPT">
      <data key="d0">SOFTWARE, APPLICATION</data>
      <data key="d1">SelfCheckGPT is an application that can be used to compare fabrication rates, which would improve the current analysis of Graph RAG's performance.
SelfCheckGPT is an AI model developed by Manakul et al. in 2023, which can be used for various tasks, including the comparison of fabrication rates, and it represents an approach that can improve the current analysis.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375,e015335cdcae20e6546fe7cbdef56c1a</data>
    </node>
    <node id="1 MILLION TOKENS">
      <data key="d0">QUANTITY, DATA_SIZE</data>
      <data key="d1">The region of 1 million tokens refers to a specific size of data or text, which is a significant amount for analyzing performance in various tasks and models.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="QUESTION TYPES">
      <data key="d0">PROPERTY, TASK</data>
      <data key="d1">Question types represent the various categories or formats of questions that can be asked or generated, which can affect the performance of models or systems in different ways.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="DATA TYPES">
      <data key="d0">PROPERTY, DATA_CLASSIFICATION</data>
      <data key="d1">Data types refer to the classification or categories of data, such as numerical, textual, categorical, etc., which can influence the performance of models or systems depending on their specific capabilities.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="DATASET SIZES">
      <data key="d0">QUANTITY, DATA_SIZE</data>
      <data key="d1">Dataset sizes refer to the volume or scale of datasets used for training or evaluating models, which can impact the performance of models due to the amount of data they have to process.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="END USERS">
      <data key="d0">PERSON, USER</data>
      <data key="d1">End users are the individuals or groups who will ultimately use or benefit from the models, systems, or services, and their feedback is crucial for validating the effectiveness of the sensemaking questions and target metrics.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="GLOBAL SUMMARIZATION">
      <data key="d0">PROPERTY, TEXT_PROCESSING</data>
      <data key="d1">Global summarization is a text processing technique that generates a concise summary of the entire source text, which can be used to provide an overview of the content and can be competitive with the graph index approach in some cases.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="COMPUTE BUDGET">
      <data key="d0">PROPERTY, COST</data>
      <data key="d1">Compute budget refers to the financial resources allocated for computing tasks, which can influence the decision to invest in building a graph index due to the potential costs associated with its creation and maintenance.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="LIFETIME QUERIES">
      <data key="d0">PROPERTY, QUERY_FREQUENCY</data>
      <data key="d1">Lifetime queries refer to the expected number of queries that will be made to a dataset over its lifetime, which can affect the decision to invest in building a graph index due to the potential benefits of faster querying.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="GRAPH-RELATED RAG APPROACHES">
      <data key="d0">PROPERTY, AI_MODEL</data>
      <data key="d1">Graph-related RAG approaches are variations of the Retrieval-Augmented Generation model that utilize graph structures for various tasks, such as summarization or question answering, and they can offer additional benefits beyond the basic graph index.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="FUTURE WORK">
      <data key="d0">PROPERTY, RESEARCH</data>
      <data key="d1">Future work refers to the potential research directions or improvements that can be pursued, which can include refining and adapting the current Graph RAG approach, as well as exploring hybrid RAG schemes that combine different techniques.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="HIERARCHICAL COMMUNITY STRUCTURE">
      <data key="d0">CONCEPT, ORGANIZATION</data>
      <data key="d1">Hierarchical community structure refers to the organization of communities within a graph or text corpus at different levels. This structure supports the refinement and adaptation of the Graph RAG approach by allowing for "roll-up" operations across multiple levels and "drill down" mechanisms that follow the information scent contained in higher-level community summaries.</data>
      <data key="d2">5e2933c9646c751e6a60c9de12a255f2</data>
    </node>
    <node id="EMBEDDING-BASED MATCHING">
      <data key="d0">METHOD, TECHNIQUE</data>
      <data key="d1">Embedding-based matching is a technique used in the Graph RAG approach to match user queries and graph annotations in a more local manner. It involves converting textual information into numerical vectors (embeddings) to compare and find relevant information.</data>
      <data key="d2">5e2933c9646c751e6a60c9de12a255f2</data>
    </node>
    <node id="HYBRID RAG SCHEMES">
      <data key="d0">METHOD, TECHNIQUE</data>
      <data key="d1">Hybrid RAG schemes are a combination of different methods in the Graph RAG approach, which include embedding-based matching against community reports before employing map-reduce summarization mechanisms. These schemes aim to improve the comprehensiveness and diversity of answers by integrating various techniques.</data>
      <data key="d2">5e2933c9646c751e6a60c9de12a255f2</data>
    </node>
    <node id="ANSWER IIVENESS AND DIVERSITY">
      <data key="d0">PROPERTY, QUALITY</data>
      <data key="d1">Answer Iiveness and Diversity refers to the richness and variety of responses or solutions generated by a system or method. It is a measure of the system's capability to produce a wide range of answers or solutions to a given problem or query.</data>
      <data key="d2">e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="GLOBAL BUT GRAPH-FREE APPROACH">
      <data key="d0">METHOD, TECHNIQUE</data>
      <data key="d1">The Global but Graph-free Approach is a method that uses map-reduce source text summarization techniques to process and summarize large datasets without relying on graph structures. It is a computational approach that can handle global queries over large datasets.</data>
      <data key="d2">e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="ENTITY-BASED GRAPH INDEX">
      <data key="d0">DATA STRUCTURE, INDEX</data>
      <data key="d1">The Entity-based Graph Index is a data structure that organizes and indexes information based on entities and their relationships in a graph format. It provides a superior data index for root-level communities, offering better performance and efficiency compared to naive RAG (Retrieval-Augmented Generation) and other global methods.</data>
      <data key="d2">e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="ROOT-LEVEL COMMUNITIES">
      <data key="d0">GROUP, COMMUNITY</data>
      <data key="d1">Root-level Communities are the fundamental or top-level groups within a dataset or graph structure. They represent the highest-level divisions or categories in the data, which can be further analyzed or queried for detailed information.</data>
      <data key="d2">e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="GLOBAL METHODS">
      <data key="d0">METHOD, TECHNIQUE</data>
      <data key="d1">Global Methods refer to computational techniques or algorithms that can handle global queries or operations over large datasets. These methods are designed to process and analyze data at a global scale, often requiring significant computational resources and token costs.</data>
      <data key="d2">e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="TOKEN COST">
      <data key="d0">PROPERTY, COST</data>
      <data key="d1">Token Cost is a measure of the computational or resource cost associated with processing or analyzing data in a graph or dataset. It represents the cost of operations or queries in terms of the number of tokens or computational units required.</data>
      <data key="d2">e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="GRAPHRAG INDEXING">
      <data key="d0">SOFTWARE, TECHNOLOGY</data>
      <data key="d1">GraphRAG Indexing is a suite of data pipeline and transformation tools designed to extract structured data from unstructured text using Large Language Models (LLMs). It includes configurable indexing pipelines composed of workflows, standard and custom steps, prompt templates, and input/output adapters. The suite is capable of entity extraction, relationship detection, community detection, and data embedding into vector spaces. Outputs can be stored in JSON, Parquet, or handled via the Python API.&gt;
GraphRAG Indexing is a system for configuring the GraphRAG indexing engine, which is highly configurable and offers both default and custom configuration modes. It involves setting up the Indexing Engine pipelines for optimal performance and integration. The system supports configuration through the init command, environment variables, and JSON or YAML files for deeper control.</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0,ccd2de9e2219521fbca779843c65af58</data>
    </node>
    <node id="INDEXING PIPELINES">
      <data key="d0">PROCESS, TECHNOLOGY</data>
      <data key="d1">Indexing Pipelines are configurable workflows within the GraphRAG indexing package. They consist of standard and custom steps, prompt templates, and input/output adapters. These pipelines are designed to extract entities, relationships, and claims from raw text, perform community detection, generate summaries and reports, and embed data into vector spaces. Outputs can be stored in various formats or accessed through the Python API.&gt;</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0</data>
    </node>
    <node id="ENTITY EXTRACTION">
      <data key="d0">FUNCTION, TECHNOLOGY</data>
      <data key="d1">Entity Extraction is a function within the GraphRAG indexing suite that identifies and extracts meaningful entities from unstructured text. This process is crucial for understanding the components of the text and is a foundational step in the indexing pipeline. The extracted entities can be further analyzed for relationships and community detection. Outputs are part of the structured data generated by the indexing process.&gt;
Entity Extraction is a configuration section that deals with settings for extracting entities from text, including the LLM (Language Model), parallelization, async mode, prompt file, entity types, max gleanings, and strategy.</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0,abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="RELATIONSHIP DETECTION">
      <data key="d0">FUNCTION, TECHNOLOGY</data>
      <data key="d1">Relationship Detection is a function within the GraphRAG indexing suite that identifies relationships between entities extracted from unstructured text. This process helps in understanding the context and connections within the text, contributing to the generation of structured data. The detected relationships are part of the outputs that can be stored in various formats or accessed through the Python API.&gt;</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0</data>
    </node>
    <node id="COMMUNITY DETECTION">
      <data key="d0">FUNCTION, TECHNOLOGY</data>
      <data key="d1">Community Detection is a function within the GraphRAG indexing suite that identifies groups or communities of related entities within the extracted data. This process aids in summarizing and reporting on the text at multiple levels of granularity. The results of community detection contribute to the structured data outputs of the indexing process.&gt;
Community Detection is a process that identifies groups of related entities in the graph.
Community Detection is an algorithmic process used to identify groups or communities within a larger network or graph, often based on the connectivity patterns of nodes.
Community Detection is a subprocess of Phase 3: Graph Augmentation. It involves generating a hierarchy of entity communities using the Hierarchical Leiden Algorithm to understand the community structure of the graph and provide a way to navigate and summarize the graph at different levels of granularity.</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0,493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08,a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </node>
    <node id="DATA EMBEDDING">
      <data key="d0">FUNCTION, TECHNOLOGY</data>
      <data key="d1">Data Embedding is a function within the GraphRAG indexing suite that transforms extracted entities and text chunks into vector representations. This process is used to embed entities into a graph vector space and text chunks into a textual vector space, facilitating further analysis and storage of the structured data. Outputs can be stored in various formats or accessed through the Python API.&gt;</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0</data>
    </node>
    <node id="OUTPUT FORMATS">
      <data key="d0">PROPERTY, TECHNOLOGY</data>
      <data key="d1">Output Formats refer to the various ways in which the structured data generated by the GraphRAG indexing suite can be stored. These include JSON and Parquet formats, which are commonly used for data storage and analysis. Additionally, the outputs can be handled manually via the Python API, providing flexibility in how the data is accessed and used.&gt;</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0</data>
    </node>
    <node id="CONFIG FILE">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">A Config File is a file that contains settings and parameters for running a pipeline. It can be used with the CLI or the Python API to specify how the pipeline should be executed. The file can be in default config mode or custom config mode, depending on the needs of the user.&gt;
A config file is a document that contains settings and configurations for a software application or system. It is recommended to use a config file for setting up the environment and pipeline for the GraphRAG system. The config file is crucial for specifying the required environment variables and settings for the GraphRAG pipeline to function properly.&gt;
The Config File is a property that allows opting out of the Default Configuration mode and executing a custom configuration. It is specified using the --config flag followed by the path to the configuration file.</data>
      <data key="d2">919cb44d9688a14bf48fa7c98163ed81,9f2cd3d789fd49f220d4cda6b9e8048c,b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="PYTHON API">
      <data key="d0">PROGRAMMING INTERFACE, TOOL</data>
      <data key="d1">The Python API is a programming interface that allows users to run the pipeline using Python code. It provides functions and methods for executing the pipeline and can be used to specify the workflow and parameters for the pipeline.&gt;</data>
      <data key="d2">b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="NODE">
      <data key="d0">EXECUTION ENVIRONMENT, TOOL</data>
      <data key="d1">Node is an execution environment that can be used to run the CLI. It allows users to execute the pipeline commands in a JavaScript environment, providing the necessary runtime and dependencies.&gt;
A Node contains layout information for rendered graph-views of the Entities and Documents which have been embedded and clustered.
Nodes are part of the table that contains layout information for rendered graph-views of the Entities and Documents. They have been embedded and clustered for visualization purposes.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,85e50a4d70697a2c4420e7a9fc82f22d,b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="PIPELINE WORKFLOW REFERENCE">
      <data key="d0">DATA STRUCTURE, CONFIGURATION</data>
      <data key="d1">Pipeline Workflow Reference is a data structure used in the Python API to define the workflow of the pipeline. It contains a list of steps that specify the operations to be performed on the data. Each step can include a verb, arguments, and an optional input.&gt;</data>
      <data key="d2">b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="DERIVE VERB">
      <data key="d0">FUNCTION, CONFIGURATION</data>
      <data key="d1">The Derive Verb is a built-in function used in the Pipeline Workflow Reference to perform operations on the data. It can be used to create new columns by applying an operator to existing columns. The Derive Verb requires arguments such as column names, the operator to be used, and the name of the new column.&gt;</data>
      <data key="d2">b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="COL1">
      <data key="d0">PROPERTY, COLUMN</data>
      <data key="d1">col1 is a column in the dataset with numerical values. It is one of the inputs for the operation defined in the workflow.
col1 is a data column that is used as an input in the derive step of workflow2.</data>
      <data key="d2">76d9dcb9a27c2caea1f46bb5050851c6,f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="COL2">
      <data key="d0">PROPERTY, COLUMN</data>
      <data key="d1">col2 is a column in the dataset with numerical values. It is the second input for the operation defined in the workflow.
col2 is a data column that is used as an input in the derive step of workflow2.</data>
      <data key="d2">76d9dcb9a27c2caea1f46bb5050851c6,f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="COL_MULTIPLIED">
      <data key="d0">PROPERTY, COLUMN</data>
      <data key="d1">col_multiplied is a new column in the dataset that is the result of multiplying the values of col1 and col2.</data>
      <data key="d2">f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="RUN_PIPELINE">
      <data key="d0">FUNCTION, PROCESS</data>
      <data key="d1">run_pipeline is a function that takes a dataset and workflows as input, and asynchronously processes the data according to the workflows. It returns the result of the operations as outputs.</data>
      <data key="d2">f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="WORKFLOWS">
      <data key="d0">PROPERTY, PROCESS</data>
      <data key="d1">workflows is a list of operations to be applied to the dataset. Each operation specifies the source columns, the target column, and the operator to be used.
The Workflows section defines the workflow Directed Acyclic Graph (DAG) for the pipeline, including an array of workflows and their inter-dependencies expressed in steps.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484,f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="OUTPUTS">
      <data key="d0">PROPERTY, DATA</data>
      <data key="d1">outputs is a list that collects the results of the run_pipeline function. Each output is the result of processing the dataset according to the workflows.</data>
      <data key="d2">f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="PIPELINE_RESULT">
      <data key="d0">PROPERTY, DATA</data>
      <data key="d1">pipeline_result is the last element in the outputs list, representing the final result of the pipeline execution.</data>
      <data key="d2">f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="DEFAULT PROMPTS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Default Prompts are the simplest way to start using the GraphRAG system. They are designed to work with minimal configuration and are suitable for out-of-the-box use. These prompts cover various aspects such as entity and relationship extraction, description summarization, claim extraction, and community reports.</data>
      <data key="d2">bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="ENTITY/RELATIONSHIP EXTRACTION">
      <data key="d0">PROPERTY, FUNCTION</data>
      <data key="d1">Entity/Relationship Extraction is a function of the GraphRAG system that identifies and extracts entities and their relationships from input data. This is a fundamental step in the creation of a knowledge graph.
Entity/Relationship Extraction is a procedure for identifying and extracting entities and their relationships from text data. It utilizes prompts and token-replacements to process input text and generate tuples representing individual entities or relationships.</data>
      <data key="d2">6a7157695d90d434b2625c3f05420916,bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="ENTITY/RELATIONSHIP DESCRIPTION SUMMARIZATION">
      <data key="d0">PROPERTY, FUNCTION</data>
      <data key="d1">Entity/Relationship Description Summarization is a function that provides concise summaries of the descriptions of entities and their relationships. This can be useful for understanding the context and significance of the extracted information.</data>
      <data key="d2">bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="CLAIM EXTRACTION">
      <data key="d0">PROPERTY, FUNCTION</data>
      <data key="d1">Claim Extraction is a function that identifies and extracts claims from the input data. This can be particularly useful in scenarios where the validity or truthfulness of statements is important.
Claim Extraction is a procedure for identifying and extracting claims from text data. It uses prompts and token-replacements to process text and identify claims based on the values provided by the extractor.
Claim Extraction is a process that identifies claims or statements about entities in the text.
Claim Extraction is a process that identifies claims or assertions made in a text, often used in information retrieval and text analysis.
Claim Extraction is a process in the pipeline that identifies and extracts claims from the text. It is a specific type of information extraction that focuses on identifying assertions or statements made within the text.
Claim Extraction is a process that identifies positive factual statements with an evaluated status and time-bounds from source TextUnits. These claims are emitted as a primary artifact called Covariates.&gt;</data>
      <data key="d2">10d01d36390b307a63fd5bc97d8682c0,493f38f41b89e767fc23d84e1fa5ba20,6a7157695d90d434b2625c3f05420916,6f92ce3fcd05dd5697ded83586f7bc08,bdb8f9e797229f596744d9636ab857b0,d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </node>
    <node id="AUTO TEMPLATING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Auto Templating is a feature that leverages input data and LLM interactions to create domain adaptive templates for the generation of the knowledge graph. It is highly recommended for better results in an Index Run. The process involves analyzing the input data and automatically generating templates that are tailored to the specific domain or use case.</data>
      <data key="d2">bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="MANUAL CONFIGURATION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Manual Configuration is an advanced use-case feature of the GraphRAG system. It allows users to customize the prompts and templates used in the generation of the knowledge graph manually. This is typically used when the Auto Templating feature does not meet specific requirements or when more control over the process is desired.</data>
      <data key="d2">bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="TEMPLATE GENERATION ALGORITHM">
      <data key="d0">ALGORITHM, PROCESS</data>
      <data key="d1">The Template Generation Algorithm is a process within GraphRAG that is responsible for creating domain adaptive templates for knowledge graph generation. It involves loading inputs, splitting them into chunks (text units), and then running a series of LLM invocations and template substitutions to generate the final prompts. The algorithm can be customized by adjusting various parameters such as the method, limit, language, max tokens, and chunk size. The default values provided by the script are suggested for use, but users can explore and tweak the algorithm for better results. The algorithm is executed as part of the automatic template generation process, which is optional but highly recommended for better Index Run results.</data>
      <data key="d2">9b364093aeecfc789c70fc5bd9503487</data>
    </node>
    <node id="INITIALIZATION PROCESS">
      <data key="d0">PROCESS, TASK</data>
      <data key="d1">The Initialization Process is a task that must be completed before running the automatic template generation in GraphRAG. It involves using the graphrag.index --init command to create the necessary configuration files and default prompts. This process is crucial for setting up the workspace and ensuring that the tool can function properly. For more detailed information about the initialization process, refer to the Init Documentation.</data>
      <data key="d2">9b364093aeecfc789c70fc5bd9503487</data>
    </node>
    <node id="ROOT">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">ROOT is a command-line option that specifies the data project root directory, including the config files (YML, JSON, or .env). It defaults to the current directory if not specified.
root is a required property that specifies the path to the project directory. It is used in the configuration of the graphrag.prompt_tune command to locate the input data and other resources needed for prompt generation.
The root is the base directory or starting point for all relative paths and configurations in the system.
The root argument specifies the data root directory for the data project. This directory should contain an input directory with the input data and an .env file with environment variables.&gt;</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f,abac77a5673e907cf8d65161c2612784,ce9cc3ed2e5f890d02e867ed0b0f8ff9,f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="DOMAIN">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">DOMAIN is a command-line option that allows specifying the domain related to the input data, such as 'space science', 'microbiology', or 'environmental news'. If left empty, the domain will be inferred from the input data.
domain is an optional property that specifies the domain or topic of the input data. It is used in the configuration of the graphrag.prompt_tune command to tailor the prompt generation to a specific subject area.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f,ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="LIMIT">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">LIMIT is a command-line option that sets the limit of text units to load when using random or top selection. The default is 15.
limit is an optional property that specifies the number of text units to select for template generation. It is used in the configuration of the graphrag.prompt_tune command to control the size of the sample used for prompt generation.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f,ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="LANGUAGE">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">LANGUAGE is a command-line option that specifies the language to use for input processing. If it is different from the inputs' language, the LLM will translate. The default is "", meaning it will be automatically detected from the inputs.
language is an optional property that specifies the language of the input data. It is used in the configuration of the graphrag.prompt_tune command to ensure that the prompt generation is tailored to the language of the input documents.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f,ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="MAX_TOKENS">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">MAX_TOKENS is a command-line option that sets the maximum token count for prompt generation. The default is 2000.
The max_tokens is an integer attribute that sets the maximum number of tokens allowed in the output of the LLM (Language Model).</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2,9243633f55cccd0885ba553e14fa5e3f</data>
    </node>
    <node id="CHUNK_SIZE">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">CHUNK_SIZE is a command-line option that determines the size in tokens to use for generating text units from input documents. The default is 200.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f</data>
    </node>
    <node id="NO_ENTITY_TYPES">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">NO_ENTITY_TYPES is a command-line option that enables untyped entity extraction generation. It is recommended for data covering a lot of topics or highly randomized data.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f</data>
    </node>
    <node id="OUTPUT">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">OUTPUT is a command-line option that specifies the folder to save the generated prompts.
output is an optional property that specifies the folder to save the generated prompts. The default is "prompts". It is used in the configuration of the graphrag.prompt_tune command to determine where the output files will be stored.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f,ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="MAX-TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">max-tokens is an optional property that specifies the maximum token count for prompt generation. The default value is 2000. It is used in the configuration of the graphrag.prompt_tune command to control the size of the generated prompts.</data>
      <data key="d2">ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="CHUNK-SIZE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">chunk-size is an optional property that defines the size in tokens to use for generating text units from input documents. The default value is 200. It is used in the configuration of the graphrag.prompt_tune command to determine how input documents are segmented for processing.</data>
      <data key="d2">ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="NO-ENTITY-TYPES">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">no-entity-types is an optional property that, when used, enables untyped entity extraction generation. It is recommended for data that covers a wide range of topics or is highly randomized. It is used in the configuration of the graphrag.prompt_tune command to control the type of entity extraction.</data>
      <data key="d2">ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="CHUNK SIZE PARAMETER">
      <data key="d0">PROPERTY, PARAMETER</data>
      <data key="d1">The Chunk Size Parameter determines the size of text units into which the input data is divided for processing in template generation. It is a crucial setting for managing the granularity of text analysis.</data>
      <data key="d2">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </node>
    <node id="RANDOM SELECTION METHOD">
      <data key="d0">PROPERTY, STRATEGY</data>
      <data key="d1">The Random Selection Method is a strategy used in template generation to select text units randomly. It is the default and recommended option for most use cases, ensuring a diverse and unbiased sample of text units.</data>
      <data key="d2">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </node>
    <node id="TOP SELECTION METHOD">
      <data key="d0">PROPERTY, STRATEGY</data>
      <data key="d1">The Top Selection Method is a strategy used in template generation to select the head n text units. This method is useful for focusing on the most significant or relevant parts of the input data.</data>
      <data key="d2">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </node>
    <node id="ALL SELECTION METHOD">
      <data key="d0">PROPERTY, STRATEGY</data>
      <data key="d1">The All Selection Method is a strategy used in template generation to use all text units for the generation. It is recommended for small datasets where comprehensive analysis is desired, but not usually recommended for larger datasets due to computational constraints.</data>
      <data key="d2">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </node>
    <node id="GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE is an environment variable that specifies the path to the prompt file used for entity extraction during the index run. The default path is "prompts/entity_extraction.txt".
GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE is a configuration setting that specifies the file used for entity extraction prompts. It is set to None, indicating that there is no specific file used.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73,4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </node>
    <node id="GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE is an environment variable that specifies the path to the prompt file used for generating community reports during the index run. The default path is "prompts/community_report.txt".
GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE is a configuration property that indicates the file to use for community report prompts, currently set to None.&gt;</data>
      <data key="d2">4f37c0e9c3c9bac4e5c1c6821eea442e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE is an environment variable that specifies the path to the prompt file used for summarizing descriptions during the index run. The default path is "prompts/summarize_descriptions.txt".
GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE is a configuration setting that specifies the file used for summarizing descriptions. It is set to None, indicating that there is no specific file used.
GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE is a configuration property that indicates the file to use for summarizing descriptions, currently set to None.&gt;</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73,4f37c0e9c3c9bac4e5c1c6821eea442e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="CUSTOM PROMPT FILE">
      <data key="d0">CONCEPT, DOCUMENT</data>
      <data key="d1">A Custom Prompt File is a plaintext document that can be created to override the default prompts used by the GraphRAG indexer. It enables users to specify their own prompts, using token-replacements, to better align with their specific use cases in knowledge discovery.</data>
      <data key="d2">6a7157695d90d434b2625c3f05420916</data>
    </node>
    <node id="TOKEN-REPLACEMENTS">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Token-Replacements is a technique used in customizing prompts for the GraphRAG indexer. It involves replacing placeholders in the form of {token_name} with actual values provided by the extractor, such as input text, entity types, and delimiters, to tailor the prompts to specific needs.</data>
      <data key="d2">6a7157695d90d434b2625c3f05420916</data>
    </node>
    <node id="SUMMARIZE ENTITY/RELATIONSHIP DESCRIPTIONS">
      <data key="d0">CONCEPT, PROCEDURE</data>
      <data key="d1">Summarize Entity/Relationship Descriptions is a procedure that involves summarizing the descriptions of entities or relationships. It uses prompts and token-replacements to process a list of descriptions for an entity or relationship.</data>
      <data key="d2">6a7157695d90d434b2625c3f05420916</data>
    </node>
    <node id="RECORD_DELIMITER">
      <data key="d0">PROPERTY, DELIMITER</data>
      <data key="d1">The record_delimiter is a special character or string used to separate different tuple instances in a text. It helps in identifying the boundaries of individual records or entities within a larger text or data stream.</data>
      <data key="d2">853bfe9a74a916130a20f81506bcaf09</data>
    </node>
    <node id="COMPLETION_DELIMITER">
      <data key="d0">PROPERTY, DELIMITER</data>
      <data key="d1">The completion_delimiter is a marker or indicator used to signal the end of a series of generated data or text. It is particularly useful in scenarios where data is generated in a stream or sequence, and there is a need to determine when the generation process has been completed.</data>
      <data key="d2">853bfe9a74a916130a20f81506bcaf09</data>
    </node>
    <node id="ENTITY_NAME">
      <data key="d0">PROPERTY, IDENTIFIER</data>
      <data key="d1">The entity_name is a unique identifier or label given to an entity or object in a data set or text. It is used to distinguish one entity from another and is crucial for referencing and linking entities in the context of relationships and descriptions.</data>
      <data key="d2">853bfe9a74a916130a20f81506bcaf09</data>
    </node>
    <node id="DESCRIPTION_LIST">
      <data key="d0">PROPERTY, LIST</data>
      <data key="d1">The description_list is a collection of descriptions or attributes associated with an entity or relationship. It provides detailed information about the entity's characteristics, functions, or the nature of the relationship between entities.</data>
      <data key="d2">853bfe9a74a916130a20f81506bcaf09</data>
    </node>
    <node id="INPUT_TEXT">
      <data key="d0">PROPERTY, TEXT</data>
      <data key="d1">The input_text is the text or data provided as input for processing. It can contain information about entities, relationships, or other data points that need to be analyzed, summarized, or extracted.</data>
      <data key="d2">853bfe9a74a916130a20f81506bcaf09</data>
    </node>
    <node id="TUPLE_DELIMITER">
      <data key="d0">PROPERTY, DELIMITER</data>
      <data key="d1">The tuple_delimiter is a character or string used to separate values within a single tuple. A tuple is a collection of related values that represent an individual entity or a relationship between entities.</data>
      <data key="d2">853bfe9a74a916130a20f81506bcaf09</data>
    </node>
    <node id="CONFIGURATION DOCUMENTATION">
      <data key="d0">DOCUMENT, INSTRUCTION</data>
      <data key="d1">The Configuration Documentation provides details on how to change settings related to information discovery. It serves as a guide for users to understand and modify the configuration for better information extraction and processing.&gt;
The Configuration documentation provides detailed information on how to configure GraphRAG, including the use of the init command and other configuration-related commands.
The Configuration documentation provides detailed information on how to configure GraphRAG, including the init command and other configuration options.
The Configuration documentation provides detailed information on how to configure GraphRAG, including the use of .env and config files.</data>
      <data key="d2">12294feb07a1d202b27241eaaf64718b,21cdf11c58927ae505d3d375d1b75c82,32e96c66a531ecd0a8edc7414aec0803,d0f7c236538005bc3056b7daed2401d8</data>
    </node>
    <node id="PROMPT SOURCE">
      <data key="d0">SOURCE, DATA</data>
      <data key="d1">Prompt Source refers to the origin or provider of the input text used to generate the report. It could be a database, a file, or any other data source that contains the information needed for the report generation.&gt;</data>
      <data key="d2">21cdf11c58927ae505d3d375d1b75c82</data>
    </node>
    <node id="TOKENS">
      <data key="d0">PROPERTY, DATA</data>
      <data key="d1">Tokens represent values provided by the extractor, such as the input text. They are key components in the process of generating reports, as they contain the actual data that will be analyzed and presented in the report.&gt;</data>
      <data key="d2">21cdf11c58927ae505d3d375d1b75c82</data>
    </node>
    <node id="GRAPHRAG SYSTEM">
      <data key="d0">SOFTWARE, SYSTEM</data>
      <data key="d1">The GraphRAG system is a software solution designed for indexing and querying text data. It offers various ways to get started, including using the GraphRAG Accelerator solution, installing from PyPI, or using it from source. The system is compatible with Python versions 3.10 to 3.12.</data>
      <data key="d2">84d24b5db902baca7217b5e3bb6ec462</data>
    </node>
    <node id="GRAPHRAG ACCELERATOR SOLUTION">
      <data key="d0">SOFTWARE, SOLUTION</data>
      <data key="d1">The GraphRAG Accelerator solution is a user-friendly package that provides an end-to-end experience with Azure resources for getting started with the GraphRAG system. It is recommended for a quick and easy setup.</data>
      <data key="d2">84d24b5db902baca7217b5e3bb6ec462</data>
    </node>
    <node id="INDEXING PIPELINE OVERVIEW">
      <data key="d0">PROCESS, SYSTEM COMPONENT</data>
      <data key="d1">The Indexing Pipeline Overview is a component of the GraphRAG system that describes the process of indexing text data. It involves setting up a data project, initial configuration, and using the system to index text for later querying.</data>
      <data key="d2">84d24b5db902baca7217b5e3bb6ec462</data>
    </node>
    <node id="QUERY ENGINE OVERVIEW">
      <data key="d0">PROCESS, SYSTEM COMPONENT</data>
      <data key="d1">The Query Engine Overview is a component of the GraphRAG system that describes the process of querying indexed data. It allows users to ask questions about the documents that have been indexed using the system.</data>
      <data key="d2">84d24b5db902baca7217b5e3bb6ec462</data>
    </node>
    <node id="ENVIRONMENT VARIABLES">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Environment variables are variables that are set in the operating system's environment and can be accessed by any program running on the system. They are used to configure the behavior of software applications and systems. In the context of the GraphRAG system, environment variables are necessary for setting up the workspace and specifying the API key for the OpenAI API or Azure OpenAI endpoint.&gt;</data>
      <data key="d2">9f2cd3d789fd49f220d4cda6b9e8048c</data>
    </node>
    <node id="SAMPLE DATASET">
      <data key="d0">DATA, FILE</data>
      <data key="d1">A sample dataset is a collection of data used for testing and demonstration purposes. In this case, the sample dataset is a copy of "A Christmas Carol" by Charles Dickens, obtained from a trusted source and saved as a text file named book.txt in the ./ragtest/input directory.&gt;</data>
      <data key="d2">9f2cd3d789fd49f220d4cda6b9e8048c</data>
    </node>
    <node id="A CHRISTMAS CAROL">
      <data key="d0">LITERATURE, BOOK</data>
      <data key="d1">"A Christmas Carol" is a novella by Charles Dickens, first published in 1843. It is a classic Christmas story that tells the tale of Ebenezer Scrooge, an elderly miser who is visited by the ghost of his former business partner Jacob Marley and the Ghosts of Christmas Past, Present, and Yet to Come. The story has been adapted into numerous films, plays, and other media.&gt;</data>
      <data key="d2">9f2cd3d789fd49f220d4cda6b9e8048c</data>
    </node>
    <node id="GRAPHRAG PIPELINE">
      <data key="d0">SOFTWARE, PROCESS</data>
      <data key="d1">The GraphRAG pipeline is a software system designed for indexing and querying large text datasets. It involves several steps, including setting up the workspace, configuring environment variables, and running the indexing command. The pipeline is capable of creating an index of a text dataset, which can then be used for answering questions and retrieving relevant information.&gt;
GraphRAG pipeline is a software tool designed for indexing and processing graph data. It supports integration with OpenAI and Azure OpenAI APIs for various functionalities. The pipeline requires an API key for authentication and can be configured through settings in the settings.yaml file.</data>
      <data key="d2">5aaa26fbe97dc7573cd1a56d6fb11213,9f2cd3d789fd49f220d4cda6b9e8048c</data>
    </node>
    <node id="OPENAI API">
      <data key="d0">API, SERVICE</data>
      <data key="d1">The OpenAI API is a service provided by OpenAI that allows developers to access and use the capabilities of the OpenAI models, such as text generation, translation, and summarization. It requires an API key for authentication and can be used in OpenAI mode by updating the value of the GRAPHRAG_API_KEY environment variable.&gt;
OpenAI API is a service provided by OpenAI that offers various AI capabilities, such as language models and embeddings. It requires an API key for authentication and can be integrated with the GraphRAG pipeline for enhanced functionality.</data>
      <data key="d2">5aaa26fbe97dc7573cd1a56d6fb11213,9f2cd3d789fd49f220d4cda6b9e8048c</data>
    </node>
    <node id="AZURE OPENAI">
      <data key="d0">API, SERVICE</data>
      <data key="d1">Azure OpenAI is a service provided by Microsoft Azure that offers access to OpenAI models and capabilities. It is an alternative to the OpenAI API and can be used by updating the value of the GRAPHRAG_API_KEY environment variable.&gt;
Azure OpenAI is a service provided by Microsoft Azure that offers AI capabilities similar to the OpenAI API. It requires specific configuration settings in the settings.yaml file for integration with the GraphRAG pipeline, including the API base URL, API version, and deployment name.
Azure OpenAI is a platform provided by Microsoft for accessing and using OpenAI's models and services, specifically designed for enterprise and business use cases. It allows users to customize and deploy AI models in a secure and scalable environment. The platform supports various API versions and requires an API key for authentication.&gt;</data>
      <data key="d2">5aaa26fbe97dc7573cd1a56d6fb11213,7b45dafa74553d3899e2291a3c9fb86e,9f2cd3d789fd49f220d4cda6b9e8048c</data>
    </node>
    <node id="SETTINGS.YAML">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">settings.yaml is a configuration file for the GraphRAG pipeline. It contains settings that can be modified to customize the pipeline's behavior, including the API key and additional settings for Azure OpenAI users.
The settings.yaml file is a configuration settings file created by the init command of GraphRAG. It contains the configuration settings necessary for the operation of GraphRAG.
settings.yaml is the configuration file that references environment variables defined in .env. It is used to set up the GraphRAG system.</data>
      <data key="d2">12294feb07a1d202b27241eaaf64718b,32e96c66a531ecd0a8edc7414aec0803,5aaa26fbe97dc7573cd1a56d6fb11213</data>
    </node>
    <node id="API_BASE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The api_base is a configuration property that specifies the base URL for accessing the Azure OpenAI API. It is required for making API calls to the specified Azure instance. The actual URL should be replaced with the correct instance name.&gt;
The api_base is a string attribute that specifies the base URL for the API used by the LLM (Language Model) service.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2,7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </node>
    <node id="API_VERSION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The api_version is a configuration property that specifies the version of the Azure OpenAI API to be used. It allows customization for different API versions, with the example provided being "2024-02-15-preview".&gt;
The api_version is a string attribute that indicates the version of the API used by the LLM (Language Model) service.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2,7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </node>
    <node id="DEPLOYMENT_NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The deployment_name is a configuration property that specifies the name of the Azure model deployment. It is required for accessing the specific model deployed on the Azure instance.&gt;
The deployment_name is a string attribute that specifies the name of the deployment to use, particularly relevant for Azure-based LLM (Language Model) services.
The deployment_name is a string that specifies the name of the deployment to use, typically within Azure's cognitive services. It is a configuration parameter that identifies the specific deployment for model usage.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2,7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </node>
    <node id="GRAPHRAG CONFIGURATION DOCUMENTATION">
      <data key="d0">DOCUMENTATION</data>
      <data key="d1">The GraphRAG configuration documentation provides detailed information about how to configure GraphRAG, including settings for the API base, version, and deployment name.&gt;</data>
      <data key="d2">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </node>
    <node id="INITIALIZATION DOCUMENTATION">
      <data key="d0">DOCUMENTATION</data>
      <data key="d1">The Initialization documentation offers guidance on how to initialize GraphRAG, which is necessary for setting up the environment and preparing for data indexing and querying.&gt;</data>
      <data key="d2">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </node>
    <node id="CLI DOCUMENTATION">
      <data key="d0">DOCUMENTATION</data>
      <data key="d1">The CLI documentation explains how to use the command-line interface (CLI) for GraphRAG, including commands for indexing data and running the query engine.&gt;</data>
      <data key="d2">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </node>
    <node id="QUERY ENGINE DOCS">
      <data key="d0">DOCUMENTATION, REFERENCE MATERIAL</data>
      <data key="d1">Query Engine Docs are reference materials that provide detailed information on how to use the Local and Global search mechanisms effectively. They offer guidance on leveraging these search methods to extract meaningful insights from data after the Indexer has completed its processing.</data>
      <data key="d2">ae6e91a8cc5773dbd4789773c9ef5a30</data>
    </node>
    <node id="SCROOGE">
      <data key="d0">CHARACTER, PERSON</data>
      <data key="d1">Scrooge is a character within the dataset. Specific questions can be asked about Scrooge to understand his main relationships and characteristics within the context of the story or data being analyzed.</data>
      <data key="d2">ae6e91a8cc5773dbd4789773c9ef5a30</data>
    </node>
    <node id="DEFAULT CONFIGURATION MODE">
      <data key="d0">CONFIGURATION MODE</data>
      <data key="d1">Default Configuration Mode is the simplest way to start using the GraphRAG system. It is designed to work with minimal configuration and is suitable for most users. The mode supports configuration through the init command, environment variables, and JSON or YAML files for more control. It creates a .env and settings.yaml files with necessary configuration settings when initialized.</data>
      <data key="d2">ccd2de9e2219521fbca779843c65af58</data>
    </node>
    <node id="CUSTOM CONFIGURATION MODE">
      <data key="d0">CONFIGURATION MODE</data>
      <data key="d1">Custom Configuration Mode is an advanced use-case for configuring the GraphRAG system. It is not recommended for most users and is primarily used for deeper control over the system's configuration. The mode supports configuration through JSON or YAML files and is detailed in the Custom Configuration Mode documentation.
Custom Configuration Mode refers to an advanced use-case where users can define their own configuration for the Indexing Engine pipelines, instead of using the default configuration. This mode is typically used by experienced users who need to fine-tune the system for specific needs.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc,ccd2de9e2219521fbca779843c65af58</data>
    </node>
    <node id="INIT COMMAND">
      <data key="d0">COMMAND</data>
      <data key="d1">The Init Command is a tool used to initialize the GraphRAG system in the Default Configuration Mode. It creates the necessary configuration files (.env and settings.yaml) in the specified directory and outputs the default LLM prompts used by GraphRAG. It is the easiest way to get started with GraphRAG and is recommended for most users.
The init command is used to initialize the directory with necessary configuration files for GraphRAG, creating .env and settings.yaml files and outputting default LLM prompts.</data>
      <data key="d2">ccd2de9e2219521fbca779843c65af58,d0f7c236538005bc3056b7daed2401d8</data>
    </node>
    <node id=".ENV">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">The .env file is an environment variables file created by the init command of GraphRAG. It contains variables referenced in the settings.yaml file and is essential for the configuration of GraphRAG.
.env is the environment variables file that contains settings referenced in the settings.yaml file. These variables can be used for token replacements in the configuration document using ${ENV_VAR} syntax.</data>
      <data key="d2">12294feb07a1d202b27241eaaf64718b,32e96c66a531ecd0a8edc7414aec0803</data>
    </node>
    <node id="PROMPTS/">
      <data key="d0">DIRECTORY, CONFIGURATION</data>
      <data key="d1">The prompts/ directory contains the default prompts used by GraphRAG. These prompts can be modified or new ones can be generated through the Auto Prompt Tuning command to better adapt to specific data.
prompts/ is the directory containing LLM (Language Model) prompts used by GraphRAG. Users can modify these prompts or use the Auto Prompt Tuning command to generate new prompts adapted to their data.</data>
      <data key="d2">12294feb07a1d202b27241eaaf64718b,32e96c66a531ecd0a8edc7414aec0803</data>
    </node>
    <node id="PYTHON -M GRAPHRAG.INDEX">
      <data key="d0">COMMAND, INITIALIZATION</data>
      <data key="d1">The python -m graphrag.index command is used to initialize GraphRAG in a specified directory. It can create necessary configuration files and output default prompts used by GraphRAG.</data>
      <data key="d2">12294feb07a1d202b27241eaaf64718b</data>
    </node>
    <node id="PROMPT TUNING COMMAND">
      <data key="d0">COMMAND, ACTIVITY</data>
      <data key="d1">The Prompt Tuning command is used to adapt the prompts to your data, ensuring that the language model is fine-tuned for your specific data set and use case.
The Prompt Tuning command is a feature that allows users to adapt the default prompts to their specific data, enhancing the performance of GraphRAG.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,d0f7c236538005bc3056b7daed2401d8</data>
    </node>
    <node id=".ENV FILE">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">The .env file contains environment variables referenced in the settings.yaml file, providing configuration settings for GraphRAG.</data>
      <data key="d2">d0f7c236538005bc3056b7daed2401d8</data>
    </node>
    <node id="SETTINGS.YAML FILE">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">The settings.yaml file contains configuration settings for GraphRAG, including parameters for indexing and other system operations.</data>
      <data key="d2">d0f7c236538005bc3056b7daed2401d8</data>
    </node>
    <node id="PROMPTS FOLDER">
      <data key="d0">DIRECTORY, FILE</data>
      <data key="d1">The prompts folder contains the default prompts used by GraphRAG, which can be modified or adapted through the Auto Prompt Tuning command.</data>
      <data key="d2">d0f7c236538005bc3056b7daed2401d8</data>
    </node>
    <node id="CONFIG.JSON">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">config.json is a JSON configuration file used to set up the default configuration mode for GraphRAG. It can be used alongside .env for environment variable token replacements.
Config.json is a configuration file that contains settings for the LLM (Language Model) service. It includes sections for input and llm, with various fields that define how the service should operate, such as input type, file encoding, and API key. The file is structured as a JSON object.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,f135654a3c057c66b9e5f97a960d302f</data>
    </node>
    <node id="CONFIG.YML">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">config.yml is a YAML configuration file used to set up the default configuration mode for GraphRAG. It can be used alongside .env for environment variable token replacements.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803</data>
    </node>
    <node id="API_KEY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">API_KEY is an environment variable defined in .env that is used for authentication in the LLM (Language Model) configuration. It is referenced in the config.json or config.yml file.
The api_key is a string attribute that holds the OpenAI API key, required for authentication when using OpenAI services.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="INPUT">
      <data key="d0">SECTION, CONFIGURATION</data>
      <data key="d1">input is a section in the configuration file that specifies the input settings for GraphRAG, including the type, file type, encoding, and file pattern.
input is the data input for workflow2, specified as a file type, with details such as base directory, file pattern, and type (file or blob).
The input property describes the configuration for reading data. It includes the type of input (file), the file type (csv), the base directory for CSV files (../data/csv), and a file pattern for matching CSV files. The file pattern uses a regex to match files based on source, year, month, day, and author.
Input is the data or file that is being read and processed. It is specified as a 'file' type with a 'file_type' of 'csv'. The base directory for the CSV files is '../data/csv', and the file pattern used to match the CSV files is a regex that extracts the source, year, month, day, and author from the file name. The specific input for this data is not directly provided in the text but is referenced through the 'input' section.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,3900b87693f02c43b4294e38647eb7cd,6839baed839d7a5e837af1da93e462e5,76d9dcb9a27c2caea1f46bb5050851c6</data>
    </node>
    <node id="TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">type is a property under the input section that defines the input type to use for loading data. The default is file.
The type is a string attribute that specifies the type of LLM (Language Model) to be used, such as openai_chat, azure_openai_chat, openai_embedding, or azure_openai_embedding.
type is a setting that specifies the type of cache or storage to use, such as file, memory, or blob, which affects how data is stored and accessed.
Type is a configuration property that specifies the type of storage or reporting to use, with options including file, memory, or blob.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,647be47c939b4d72f1c0b29a2e0d2cb2,abac77a5673e907cf8d65161c2612784,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="FILE_TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">file_type is a property under the input section that specifies the type of input data to load, either text or csv. The default is text.
The file_type is a property that specifies the type of file being read, which is "csv" for comma-separated values.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,3900b87693f02c43b4294e38647eb7cd</data>
    </node>
    <node id="FILE_ENCODING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">file_encoding is a property under the input section that sets the encoding of the input file. The default is utf-8.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803</data>
    </node>
    <node id="FILE_PATTERN">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">file_pattern is a property under the input section that provides a regex pattern to match input files. The default is not specified in the text.
The file_pattern is a property that contains a regex pattern used to match CSV files. The pattern is '.*[\/](?P&lt;source&gt;[^\/]+)[\/](?P&lt;year&gt;\d{4})-(?P&lt;month&gt;\d{2})-(?P&lt;day&gt;\d{2})_(?P&lt;author&gt;[^_]+)_\d+\.csv$'.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,3900b87693f02c43b4294e38647eb7cd</data>
    </node>
    <node id="API KEY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The API Key is a configuration property used to authenticate and access the LLM (Language Model) service, specifically the OpenAI API. It is stored in the config.json file under the llm section. The value of the API Key is represented by the placeholder ${API_KEY}.</data>
      <data key="d2">f135654a3c057c66b9e5f97a960d302f</data>
    </node>
    <node id="INPUT CONFIGURATION">
      <data key="d0">SECTION, CONFIGURATION</data>
      <data key="d1">The Input Configuration is a section within the config.json file that specifies how input data should be handled. It includes fields for input type, file type, file encoding, file pattern, and additional parameters specific to CSV mode, such as source column, timestamp column, timestamp format, text column, title column, and document attribute columns. It also includes settings for file input from Azure Storage, such as connection string, container name, base directory, and storage account blob URL.</data>
      <data key="d2">f135654a3c057c66b9e5f97a960d302f</data>
    </node>
    <node id="LLM CONFIGURATION">
      <data key="d0">SECTION, CONFIGURATION</data>
      <data key="d1">The LLM Configuration is a section within the config.json file that specifies settings for the LLM (Language Model) service. It includes the API Key field, which is used to authenticate and access the service. Other steps in the system may override this base configuration with their own LLM configuration.</data>
      <data key="d2">f135654a3c057c66b9e5f97a960d302f</data>
    </node>
    <node id="CONNECTION_STRING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The connection_string is a string attribute used specifically for Azure Blob storage, providing the necessary connection details to access the storage service.
connection_string is a string setting that provides the Azure Storage connection string, which is required for using blob storage for cache or storage purposes.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="CONTAINER_NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The container_name is a string attribute specific to Azure Blob storage, indicating the name of the container where the data is stored.
container_name is a string setting that specifies the Azure Storage container name, which is used for organizing and accessing data in blob storage.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="BASE_DIR">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The base_dir is a string attribute that specifies the base directory from which input data should be read, relative to the root directory of the storage.
base_dir is a string setting that defines the base directory to write cache or reports to, relative to the root, which helps in managing the file structure for storage.
The base_dir is a property that specifies the base directory for the CSV files, which is "../data/csv". This directory is relative to the config file.</data>
      <data key="d2">3900b87693f02c43b4294e38647eb7cd,647be47c939b4d72f1c0b29a2e0d2cb2,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="STORAGE_ACCOUNT_BLOB_URL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The storage_account_blob_url is a string attribute that provides the URL of the Azure storage account blob, used for accessing the storage service directly.
storage_account_blob_url is a string setting that specifies the storage account blob URL to use, which is necessary for accessing and managing data in blob storage.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="MODEL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The model is a string attribute that identifies the name of the model to be used for the LLM (Language Model) configuration.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="REQUEST_TIMEOUT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The request_timeout is a float attribute that defines the timeout duration for each request made to the LLM (Language Model) service.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="ORGANIZATION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The organization is a string attribute that identifies the client organization using the LLM (Language Model) service.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="PROXY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The proxy is a string attribute that specifies the URL of the proxy server to be used when making requests to the LLM (Language Model) service.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="COGNITIVE_SERVICES_ENDPOINT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The cognitive_services_endpoint is a string attribute that provides the URL endpoint for cognitive services, used for accessing additional AI services.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="MODEL_SUPPORTS_JSON">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The model_supports_json is a boolean attribute that indicates whether the LLM (Language Model) supports output in JSON format.
The model_supports_json is a boolean flag that indicates whether the model supports JSON-mode output. It is a configuration parameter that enables or disables JSON output for model responses.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="TOKENS_PER_MINUTE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The tokens_per_minute is an integer attribute that sets a throttle limit on the number of tokens processed per minute by the LLM (Language Model) service.
The tokens_per_minute is an integer that sets a leaky-bucket throttle on tokens-per-minute, controlling the rate at which tokens can be used in requests. It is a configuration parameter that manages the rate of token consumption.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="REQUESTS_PER_MINUTE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The requests_per_minute is an integer attribute that sets a throttle limit on the number of requests processed per minute by the LLM (Language Model) service.
The requests_per_minute is an integer that sets a leaky-bucket throttle on requests-per-minute, controlling the rate at which requests can be made. It is a configuration parameter that manages the rate of incoming requests.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="MAX_RETRIES">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The max_retries is an integer attribute that defines the maximum number of retries allowed for failed requests to the LLM (Language Model) service.
The max_retries is an integer that specifies the maximum number of retries to use when a request fails. It is a configuration parameter that determines how many times a failed request should be retried.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="MAX_RETRY_WAIT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The max_retry_wait is a float attribute that specifies the maximum backoff time before retrying a failed request to the LLM (Language Model) service.
The max_retry_wait is a float that specifies the maximum backoff time before retrying a failed request. It is a configuration parameter that determines the maximum wait time before a retry.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="SLEEP_ON_RATE_LIMIT_RECOMMENDATION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The sleep_on_rate_limit_recommendation is a boolean attribute that determines whether the system should pause when it receives a rate limit recommendation from the LLM (Language Model) service.
The sleep_on_rate_limit_recommendation is a boolean flag that indicates whether to adhere to sleep recommendations when rate limits are reached. It is a configuration parameter that controls behavior when rate limits are encountered.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="URL ENDPOINT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The url endpoint is the specific address used to access cognitive services, such as Azure's deployment for machine learning models. It is a configuration parameter that determines where requests are sent.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="CONCURRENT_REQUESTS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The concurrent_requests is an integer that specifies the number of open requests to allow at once. It is a configuration parameter that manages the concurrency of requests.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="TEMPERATURE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The temperature is a float that specifies the temperature to use in generating completions. It is a configuration parameter that influences the randomness of model outputs.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="TOP_P">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The top_p is a float that specifies the top-p value to use in generating completions. It is a configuration parameter that influences the diversity of model outputs.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="N">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The n is an integer that specifies the number of completions to generate. It is a configuration parameter that determines the quantity of completions returned by the model.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="STAGGER">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The stagger is a float that specifies the threading stagger value for parallelization. It is a configuration parameter that influences the scheduling of threads.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="NUM_THREADS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The num_threads is an integer that specifies the maximum number of work threads for parallelization. It is a configuration parameter that determines the number of threads used for processing.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="ASYNC_MODE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The async_mode is a string that specifies the async mode to use, either asyncio or threaded. It is a configuration parameter that determines the asynchronous processing mode.&gt;
async_mode is a top-level configuration that determines whether the system operates in asynchronous mode, allowing for concurrent processing of tasks.
async_mode is a configuration setting that determines the asynchronous mode for processing. It can be found in the Async Mode top-level configuration and is referenced in multiple sections including entity extraction, summarization, and claim extraction strategies. This setting is crucial for defining how tasks are handled concurrently.&gt;
async_mode is a configuration field within community_reports that refers to the Async Mode top-level configuration. It is used to specify settings related to asynchronous processing in the system.
ASYNC_MODE is a configuration property set to asyncio, indicating that the system is configured to use asyncio for asynchronous operations.</data>
      <data key="d2">1b24101de07b1c195448240237b84b37,3c66b7e86b3675fce14fe0047ae731aa,53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,9cbd4e21339eeed5e22a638e52a094cb,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="BATCH_SIZE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The batch_size is an integer that specifies the maximum batch size to use for embeddings. It is a configuration parameter that determines the size of batches for processing embeddings.&gt;
batch_size is an integer setting that defines the maximum number of items to be processed in a single batch, which can affect the efficiency and resource usage of the system.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="BATCH_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The batch_max_tokens is an integer that specifies the maximum batch #-tokens for embeddings. It is a configuration parameter that determines the maximum number of tokens processed in a batch for embeddings.&gt;
batch_max_tokens is an integer setting that specifies the maximum number of tokens allowed in a batch, which helps in managing the computational resources and processing time.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="TARGET">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The target is a string that determines which set of embeddings to emit, either required or all. It is a configuration parameter that specifies the output of embeddings.&gt;
target is a setting that determines which set of embeddings to emit, either required or all, influencing the output of the text-embedding process.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="SKIP">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The skip is a list of strings that specifies which embeddings to skip. It is a configuration parameter that allows certain embeddings to be excluded from processing.&gt;
skip is a list of strings that specifies which embeddings to skip during the text-embedding process, allowing for customization and optimization of the output.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="STRATEGY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The strategy is a dictionary that fully overrides the text-embedding strategy. It is a configuration parameter that allows for custom strategies in text embedding.&gt;
strategy is a dictionary setting that allows for the full override of the text-embedding strategy, providing flexibility in how text is processed and embedded.
Strategy is a configuration property used in entity extraction, specifying the strategy to fully override the entity extraction process.
strategy is a dictionary setting that allows for the full override of the default strategies used in entity extraction, summarization, and claim extraction. This setting is critical for customizing the algorithms and rules applied to these processes.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,9cbd4e21339eeed5e22a638e52a094cb,abac77a5673e907cf8d65161c2612784,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="SIZE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The size is an integer that specifies the max chunk size in processing. It is a configuration parameter that determines the size of chunks used in processing.&gt;
size is an integer setting that defines the maximum chunk size in tokens, which is used in the chunking process to manage the size of text segments for processing.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="PARALLELIZATION">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">parallelization is a top-level configuration that specifies how tasks are distributed across multiple processors or threads to improve performance and efficiency.
Parallelization is a configuration property used in entity extraction, specifying the parallelization settings for processing text.
parallelization is a configuration setting that defines how tasks are processed in parallel. It can be found in the Parallelization top-level configuration and is referenced in multiple sections. This setting is important for optimizing the efficiency of processing tasks concurrently.&gt;
parallelization is a configuration field within community_reports that refers to the Parallelization top-level configuration. It is used to specify settings related to parallel processing in the system.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,9cbd4e21339eeed5e22a638e52a094cb,abac77a5673e907cf8d65161c2612784,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="OVERLAP">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">overlap is an integer setting that specifies the chunk overlap in tokens, which helps in maintaining context across adjacent chunks during the chunking process.</data>
      <data key="d2">d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="GROUP_BY_COLUMNS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">group_by_columns is a list of strings that specifies fields to group documents by before chunking, which can help in organizing and processing related documents together.</data>
      <data key="d2">d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="CACHE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">cache is a section that includes settings for the type of cache to use, such as file, memory, or blob, and additional parameters for configuring the cache, such as connection strings and directories.
cache is a configuration section that allows you to define the cache strategy for the pipeline. It includes the type of cache to use, which can be crucial for optimizing performance and managing resources in the pipeline.</data>
      <data key="d2">d27237468a1b9e89110eeeca8080f63c,e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="STORAGE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">storage is a section that includes settings for the type of storage to use, such as file, memory, or blob, and additional parameters for configuring the storage, such as connection strings and directories.
Storage refers to the configuration section that deals with storage settings, including the type of storage (file, memory, or blob), connection string (for blob type), container name (for blob type), and base directory for writing reports relative to the root.
storage is a configuration section that allows you to define the output strategy for the pipeline. It includes options for the type of storage to use, such as file, memory, or blob, and specific settings for each type, such as base_dir for file storage or connection_string and container_name for blob storage.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784,d27237468a1b9e89110eeeca8080f63c,e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="REPORTING">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">reporting is a section that includes settings related to the generation and management of reports, which can include details on how reports are stored and accessed.
Reporting refers to the configuration section that deals with reporting settings, including the type of reporting (file, console, or blob), connection string (for blob type), container name (for blob type), and base directory for writing reports relative to the root.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="STORAGE ACCOUNT BLOB URL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The storage account blob URL is a configuration property used to specify the URL of the Azure Storage account blob. It is used in blob storage type configurations.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="FIELDS">
      <data key="d0">CONCEPT, CONFIGURATION</data>
      <data key="d1">Fields represent the specific configuration options or parameters within a given section, such as storage or reporting.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="CONNECTION STRING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The connection string is a configuration property used for blob storage type, specifying the Azure Storage connection string.
The Connection String is a configuration property (type: blob only) that specifies the connection string to use for blob storage.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484,abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="BASE DIR">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Base Dir is a configuration property that specifies the base directory to write reports to, relative to the root.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="ASYNC MODE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Async Mode is a configuration property used in entity extraction, specifying whether to process text asynchronously.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="PROMPT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Prompt is a configuration property used in entity extraction, specifying the prompt file to use for guiding the text analysis.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="ENTITY TYPES">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Entity Types is a configuration property used in entity extraction, specifying the types of entities to identify.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="MAX GLEANINGS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Max Gleanings is a configuration property used in entity extraction, specifying the maximum number of gleaning cycles to use.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="SUMMARIZE DESCRIPTIONS">
      <data key="d0">CONCEPT, CONFIGURATION</data>
      <data key="d1">Summarize Descriptions is a configuration section that deals with settings for summarizing descriptions, including the LLM (Language Model), parallelization, async mode, and prompt.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="CONTAINER NAME">
      <data key="d0" />
      <data key="d1">
The Container Name is a configuration property (type: blob only) that defines the container to use for blob storage.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484,abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="PROMPT STR">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">prompt str is a configuration setting that specifies the prompt file to use for various processes such as entity extraction, summarization, and claim extraction. This setting is essential for defining the initial input or guidance for the AI or machine learning models involved in these tasks.&gt;
prompt str is a configuration field within community_reports that specifies the prompt file to use for generating reports. It is a string value that points to a file.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="ENTITY_TYPES">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">entity_types is a list of strings that defines the types of entities to identify during the entity extraction process. This setting is crucial for specifying the categories of information to be recognized and extracted from the text.&gt;</data>
      <data key="d2">9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="MAX_GLEANINGS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">max_gleanings is an integer setting that determines the maximum number of gleaning cycles to use in the entity extraction process. This setting is important for controlling the depth and thoroughness of the information extraction from the text.&gt;</data>
      <data key="d2">9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="SUMMARIZE_DESCRIPTIONS">
      <data key="d0">PROCESS, FUNCTION</data>
      <data key="d1">summarize_descriptions is a process or function that involves summarizing the descriptions of entities. It utilizes the llm (language model), parallelization, and async_mode settings, and has its own strategy for summarization. This process is essential for generating concise and informative summaries of entity descriptions.&gt;</data>
      <data key="d2">9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="MAX_LENGTH">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">max_length is an integer setting that specifies the maximum number of output tokens per summarization or report. This setting is crucial for controlling the length of the generated text to ensure it is within acceptable limits.&gt;
max_length is a configuration field within community_reports that specifies the maximum number of output tokens per report. It is an integer value that limits the size of the report output.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="CLAIM_EXTRACTION">
      <data key="d0">PROCESS, FUNCTION</data>
      <data key="d1">claim_extraction is a process or function that involves identifying and extracting claims from text. It has a boolean setting 'enabled' to toggle its functionality, and utilizes the llm, parallelization, and async_mode settings. This process is essential for identifying assertions or statements of fact within the text.&gt;</data>
      <data key="d2">9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="COMMUNITY_REPORTS">
      <data key="d0">PROCESS, FUNCTION</data>
      <data key="d1">community_reports is a process or function that generates reports based on community data. It utilizes the llm, parallelization, and async_mode settings, and has its own strategy for generating reports. This process is crucial for compiling and analyzing data from community sources.&gt;
community_reports is a configuration section that includes various fields for managing community reports, such as the LLM, parallelization, async_mode, prompt, max_length, max_input_length, and strategy dict settings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="GLEANINGS INT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">gleanings int is a configuration parameter that specifies the maximum number of gleaning cycles to use in the system. It is an integer value.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="STRATEGY DICT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">strategy dict is a configuration parameter that allows for the full override of the claim extraction strategy or other strategies in the system. It is a dictionary that can contain various settings and rules.
The strategy dict is a configuration setting that fully overrides the default strategy for embedding graphs. It allows for customizing the graph embedding process according to specific requirements or preferences.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="MAX_INPUT_LENGTH">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">max_input_length is a configuration field within community_reports that specifies the maximum number of input tokens to use when generating reports. It is an integer value that limits the size of the input for report generation.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="CLUSTER_GRAPH">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">cluster_graph is a configuration section that includes various fields for managing cluster graphs, such as max_cluster_size, strategy dict, and other settings related to graph clustering.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="MAX_CLUSTER_SIZE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">max_cluster_size is a configuration field within cluster_graph that specifies the maximum cluster size to emit. It is an integer value that limits the size of clusters in the graph.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="EMBED_GRAPH">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">embed_graph is a configuration section that includes various fields for managing graph embeddings, such as enabled, num_walks, walk_length, window_size, iterations, random_seed, and strategy dict settings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="ENABLED BOOL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">enabled bool is a configuration field within embed_graph that specifies whether to enable graph embeddings. It is a boolean value that determines if the graph embeddings feature is turned on or off.
The enabled bool is a configuration property that determines whether a specific feature or functionality, such as UMAP layouts, is activated or deactivated in the system.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="NUM_WALKS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">num_walks is a configuration field within embed_graph that specifies the node2vec number of walks. It is an integer value that determines the number of walks performed in the graph for node2vec embeddings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="WALK_LENGTH">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">walk_length is a configuration field within embed_graph that specifies the node2vec walk length. It is an integer value that determines the length of each walk in the graph for node2vec embeddings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="WINDOW_SIZE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">window_size is a configuration field within embed_graph that specifies the node2vec window size. It is an integer value that determines the size of the window for context in node2vec embeddings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="ITERATIONS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">iterations is a configuration field within embed_graph that specifies the node2vec number of iterations. It is an integer value that determines the number of iterations for node2vec embeddings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="RANDOM_SEED">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">random_seed is a configuration field within embed_graph that specifies the node2vec random seed. It is an integer value that determines the seed for the random number generator used in node2vec embeddings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="UMAP">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">umap is a configuration section that includes various fields for managing UMAP layouts, such as enabled bool and other settings related to UMAP visualization.
UMAP (Uniform Manifold Approximation and Projection) is an algorithm for dimension reduction and visualization. In the context of the text, it refers to a configuration option that enables or disables UMAP layouts for data visualization.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="SNAPSHOTS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">snapshots is a configuration section that includes various fields for managing snapshots, such as graphml, raw_entities, top_level_nodes, and other settings related to snapshot generation.
Snapshots refer to a set of configuration options related to emitting various types of snapshots, such as graphml, raw entity, and top-level-node snapshots, which are used for data preservation and analysis.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="GRAPHML">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">graphml is a configuration field within snapshots that specifies whether to emit graphml snapshots. It is a boolean value that determines if graphml snapshots are generated.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="RAW_ENTITIES">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">raw_entities is a configuration field within snapshots that specifies whether to emit raw entity snapshots. It is a boolean value that determines if raw entity snapshots are generated.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="TOP_LEVEL_NODES">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">top_level_nodes is a configuration field within snapshots that specifies whether to emit top-level-node snapshots. It is a boolean value that determines if top-level-node snapshots are generated.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="ENCODING_MODEL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">encoding_model is a configuration field that specifies the text encoding model to use. It is a string value that determines the model used for text encoding. The default is cl100k_base.
The encoding_model is a configuration setting that specifies the text encoding model to be used. The default model is cl100k_base, but other models can be selected based on the specific requirements of the text encoding process.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="SKIP_WORKFLOWS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">skip_workflows is a configuration field that specifies which workflow names to skip. It is a list of strings that contains the names of workflows that should be skipped in the system.
The skip_workflows is a configuration property that allows specifying a list of workflow names to be skipped during the execution of a pipeline. This can be useful for skipping unnecessary or problematic workflows.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="NODE2VEC RANDOM SEED">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The node2vec random seed is a configuration parameter that determines the initial conditions for the node2vec algorithm, which is used for generating embeddings in graph structures. It is crucial for reproducibility and can affect the resulting embeddings.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="GRAPHML BOOL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The graphml bool is a configuration property that controls whether graphml snapshots are emitted. Graphml is a file format for graphs and networks, used for data exchange and visualization.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="RAW_ENTITIES BOOL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The raw_entities bool is a configuration property that determines if raw entity snapshots are emitted. Raw entity snapshots capture the state of entities in their raw form for later analysis.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="TOP_LEVEL_NODES BOOL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The top_level_nodes bool is a configuration property that specifies whether top-level-node snapshots are emitted. These snapshots capture the state of the top-level nodes in the system.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="INDEXING ENGINE EXAMPLES">
      <data key="d0">PROPERTY, EXAMPLES</data>
      <data key="d1">Indexing Engine Examples are a set of examples provided in the examples directory that demonstrate how to use the Indexing Engine with custom configuration. These examples are useful for learning and adapting the engine to various use-cases.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="EXTENDS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The extends property is a configuration option that allows for extending or overriding existing configurations. It is used to include or modify configurations from other sections or files, facilitating modular and flexible configuration management.
extends is a configuration directive that allows you to extend a base configuration file or multiple base configuration files. This directive is used to inherit settings from one or more base configurations, typically found in pipeline configuration files.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc,e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="RUN.PY">
      <data key="d0">FILE, CODE</data>
      <data key="d1">run.py is a Python script file that is used to run examples, typically found in the examples directory of a project. It can be executed using the Python interpreter with the PYTHONPATH environment variable set to the project's root directory.</data>
      <data key="d2">e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="POETRY SHELL">
      <data key="d0">COMMAND, ENVIRONMENT</data>
      <data key="d1">poetry shell is a command used to activate a virtual environment with the required dependencies for a project. It is often used in conjunction with the Python API and pipeline configuration files to ensure that the correct environment is set up for running examples.</data>
      <data key="d2">e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="PYTHONPATH">
      <data key="d0">ENVIRONMENT VARIABLE, PATH</data>
      <data key="d1">PYTHONPATH is an environment variable that specifies the directory where Python looks for modules and packages. Setting PYTHONPATH to the project's root directory ensures that the Python interpreter can find and import the necessary modules and scripts when running examples.</data>
      <data key="d2">e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="ROOT_DIR">
      <data key="d0">CONFIGURATION, PATH</data>
      <data key="d1">root_dir is a configuration directive that sets the root directory for the pipeline. All data inputs and outputs are assumed to be relative to this path, which is crucial for specifying the correct location of data files and output directories.</data>
      <data key="d2">e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="STORAGE TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The Storage Type is a configuration property that determines the type of storage to use for data. The options are file, memory, and blob. This property is essential for specifying how data will be stored and accessed within the system.&gt;("entity"</data>
      <data key="d2">763b51b68ecc9b69bc8014cf6f59fd33</data>
    </node>
    <node id="REPORTING TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The Reporting Type is a configuration property that specifies the type of reporting to use, with options including file, memory, and blob.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </node>
    <node id="BASE DIRECTORY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The Base Directory is a configuration property (type: file only) that defines the base directory to store the reports in, relative to the config root.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </node>
    <node id="WORKFLOW NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The Workflow Name is a property used to reference a specific workflow in other parts of the config, establishing dependencies between workflows.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </node>
    <node id="STEPS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Steps are the DataShaper operations that a workflow comprises. Steps can define inputs in the form of workflow:&lt;workflow_name&gt; to establish dependencies on the output of other workflows.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </node>
    <node id="INPUT TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The Input Type is a configuration property that specifies the type of input to use, with options including file or blob.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </node>
    <node id="FILE TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The File Type is a field that discriminates between different input types, with options including csv and text.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </node>
    <node id="WORKFLOW2">
      <data key="d0">WORKFLOW, PROCESS</data>
      <data key="d1">workflow2 is a process that includes steps to derive data, with dependencies on other workflows, specifically workflow1. It operates on columns such as col1 and col2.</data>
      <data key="d2">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </node>
    <node id="WORKFLOW1">
      <data key="d0">WORKFLOW, PROCESS</data>
      <data key="d1">workflow1 is a process that serves as a source for workflow2, establishing a dependency through the derive step.</data>
      <data key="d2">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </node>
    <node id="DERIVE">
      <data key="d0">ACTION, PROCESS</data>
      <data key="d1">derive is an action or process step within workflow2 that uses col1 and col2 as inputs and has a dependency on workflow1.</data>
      <data key="d2">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </node>
    <node id="TIMESTAMP_COLUMN">
      <data key="d0">PROPERTY, DATA_COLUMN</data>
      <data key="d1">The timestamp_column is a property in the CSV data that contains the timestamp of the data entries. It is formatted according to the timestamp_format specified in the configuration. The format used is "%Y%m%d%H%M%S".</data>
      <data key="d2">3900b87693f02c43b4294e38647eb7cd</data>
    </node>
    <node id="TIMESTAMP_FORMAT">
      <data key="d0">PROPERTY, DATA_FORMAT</data>
      <data key="d1">The timestamp_format is a property that defines the format of the timestamps in the timestamp_column. It is specified as "%Y%m%d%H%M%S", which represents the year, month, day, hour, minute, and second in the timestamp.</data>
      <data key="d2">3900b87693f02c43b4294e38647eb7cd</data>
    </node>
    <node id="SOURCE_COLUMN">
      <data key="d0">PROPERTY, DATA_COLUMN</data>
      <data key="d1">The source_column is a property that indicates the column containing the source or author of the data. It is specified as "author".</data>
      <data key="d2">3900b87693f02c43b4294e38647eb7cd</data>
    </node>
    <node id="TEXT_COLUMN">
      <data key="d0">PROPERTY, DATA_COLUMN</data>
      <data key="d1">The text_column is a property that indicates the column containing the text of the data. It is specified as "message".</data>
      <data key="d2">3900b87693f02c43b4294e38647eb7cd</data>
    </node>
    <node id="AUTHOR">
      <data key="d0">PERSON, ROLE</data>
      <data key="d1">The Author is the person or entity responsible for creating or originating the data. The author's name is stored in the 'author' column of the CSV file. The author's role could be a writer, a researcher, or any other individual or organization contributing to the data. The specific author for this data is not directly provided in the text but is referenced through the 'author' column in the CSV file.</data>
      <data key="d2">6839baed839d7a5e837af1da93e462e5</data>
    </node>
    <node id="MESSAGE">
      <data key="d0">TEXT, COMMUNICATION</data>
      <data key="d1">The Message is the content or text of the data that is being communicated. It is stored in the 'message' column of the CSV file. The message could be a written document, a note, or any form of textual communication. The specific content of the message is not provided in the text but is referenced through the 'message' column in the CSV file.</data>
      <data key="d2">6839baed839d7a5e837af1da93e462e5</data>
    </node>
    <node id="DATE">
      <data key="d0">TIME, DATE</data>
      <data key="d1">The Date is the timestamp associated with the data, indicating when the data was created or recorded. It is stored in the 'date(yyyyMMddHHmmss)' column of the CSV file. The date format is specified as '%Y%m%d%H%M%S', which represents the year, month, day, hour, minute, and second. The specific date for this data is not directly provided in the text but is referenced through the 'date(yyyyMMddHHmmss)' column in the CSV file.</data>
      <data key="d2">6839baed839d7a5e837af1da93e462e5</data>
    </node>
    <node id="POST PROCESS">
      <data key="d0">PROCESS, DATA_MANIPULATION</data>
      <data key="d1">Post Process refers to a set of steps that are applied to the data after it is read from the CSV file and before it is used in the workflow. The post-process steps can include filtering, transformation, or any other data manipulation necessary for the workflow. The specific post-process steps mentioned are filtering based on the 'title' column with the value 'My document'.
The Post Process is a set of steps to process data before it goes into the workflow. It includes a filter verb that targets the "title" column with the value "My document".</data>
      <data key="d2">6839baed839d7a5e837af1da93e462e5,765d8a78606fe81a03a0da4f7ff231fa</data>
    </node>
    <node id="CSV FILE PATTERN">
      <data key="d0">PROPERTY, REGEX</data>
      <data key="d1">The CSV File Pattern is a regular expression used to match CSV files, specifically looking for the format that includes day and author information. The pattern is: {2})-(?P&lt;day&gt;\d{2})_(?P&lt;author&gt;[^_]+)_\d+\.csv$.</data>
      <data key="d2">765d8a78606fe81a03a0da4f7ff231fa</data>
    </node>
    <node id="FILE FILTER">
      <data key="d0">PROPERTY, FILTER</data>
      <data key="d1">The File Filter is a set of criteria used to further filter files based on named groups from the CSV File Pattern. It includes filters for year, month, and potentially day. Currently, it filters files for the year 2023 and month 06.</data>
      <data key="d2">765d8a78606fe81a03a0da4f7ff231fa</data>
    </node>
    <node id="CONFIGURATION TEMPLATE">
      <data key="d0">PROPERTY, TEMPLATE</data>
      <data key="d1">The Configuration Template is a template for a .env file used in the Indexing Pipeline execution. It includes settings for LLM (Language Model), API key, API base, API version, and text generation settings. The LLM type is set to "azure_openai_chat".</data>
      <data key="d2">765d8a78606fe81a03a0da4f7ff231fa</data>
    </node>
    <node id="GRAPHRAG_API_BASE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_API_BASE is a configuration setting that specifies the base URL for the GraphRAG API. It is particularly important for Azure OpenAI users, as it should be set to the domain of the Azure OpenAI service. This setting ensures that all API requests are directed to the correct endpoint. It is a required setting for Azure OpenAI users.
GRAPHRAG_API_BASE is a configuration property that specifies the base URL for the LLM (Language Model) service API. It determines the endpoint where the system will send requests to access the LLM service. The value "http://&lt;domain&gt;.openai.azure.com" is a placeholder for the actual domain. This property is essential for directing the system to the correct LLM service endpoint.&gt;</data>
      <data key="d2">3da10b454f926a257b9fdf5d2487c0a5,8ac79ce92be1254dfda9a10eb54ab703</data>
    </node>
    <node id="GRAPHRAG_API_VERSION">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_API_VERSION is a configuration setting that specifies the API version to be used when interacting with the GraphRAG service. It is a required setting for Azure OpenAI users and should be set to the appropriate API version provided by the service. This setting ensures compatibility and correct functionality when making API calls.
GRAPHRAG_API_VERSION is a configuration property that specifies the version of the LLM (Language Model) service API. It determines which version of the API the system will use to access the LLM service. The value "api_version" is a placeholder for the actual API version. This property is crucial for ensuring compatibility between the system and the LLM service.&gt;
GRAPHRAG_API_VERSION is a configuration property that specifies the API version to be used when interacting with the Azure OpenAI service. It is set to "api_version" by default and is crucial for compatibility and functionality.&gt;</data>
      <data key="d2">3da10b454f926a257b9fdf5d2487c0a5,7b45dafa74553d3899e2291a3c9fb86e,8ac79ce92be1254dfda9a10eb54ab703</data>
    </node>
    <node id="GRAPHRAG_LLM_DEPLOYMENT_NAME">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_LLM_DEPLOYMENT_NAME is a configuration setting that specifies the name of the deployment for the language model being used. It is set to 'gpt-4-turbo-preview' in the provided configuration. This setting is crucial for identifying the specific model deployment to be used for text generation.</data>
      <data key="d2">3da10b454f926a257b9fdf5d2487c0a5</data>
    </node>
    <node id="GRAPHRAG_LLM_MODEL_SUPPORTS_JSON">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_LLM_MODEL_SUPPORTS_JSON is a configuration setting that indicates whether the language model supports JSON input. It is set to True in the provided configuration. This setting is important for determining the format of the input data that can be processed by the model.
GRAPHRAG_LLM_MODEL_SUPPORTS_JSON is a configuration property that indicates whether the language model being used supports JSON input or output. It is set to True by default, suggesting that the model can handle JSON data.&gt;</data>
      <data key="d2">3da10b454f926a257b9fdf5d2487c0a5,7b45dafa74553d3899e2291a3c9fb86e</data>
    </node>
    <node id="GRAPHRAG_INPUT_TYPE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_INPUT_TYPE is a configuration setting that determines the type of input data to be processed. It can be set to 'text' for processing text data or 'file' for processing file data. This setting is crucial for specifying the format of the input data that will be used for text generation or embedding.
GRAPHRAG_INPUT_TYPE is a configuration property that specifies the type of input data, set to "file" for file-based input. It determines how input data is processed and interpreted by the system. The value "file" indicates that the input data is expected to be in a file format. This property is crucial for configuring the input data source for the system.&gt;</data>
      <data key="d2">3da10b454f926a257b9fdf5d2487c0a5,8ac79ce92be1254dfda9a10eb54ab703</data>
    </node>
    <node id="GRAPHRAG_INPUT_FILE_TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_INPUT_FILE_TYPE is a configuration property that specifies the type of files to be processed, set to "text" for text files. It determines the format of the input files that the system will handle. The value "text" indicates that the system is configured to process text files. This property is essential for specifying the file type that the system should expect and process.&gt;</data>
      <data key="d2">8ac79ce92be1254dfda9a10eb54ab703</data>
    </node>
    <node id="GRAPHRAG_INPUT_FILE_PATTERN">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_INPUT_FILE_PATTERN is a configuration property that specifies the pattern for matching input files, set to ".*\.txt$". It determines which files will be selected for processing based on their names. The value ".*\.txt$" indicates that the system is configured to process files with a .txt extension. This property is crucial for filtering the input files based on their names.&gt;</data>
      <data key="d2">8ac79ce92be1254dfda9a10eb54ab703</data>
    </node>
    <node id="GRAPHRAG_INPUT_SOURCE_COLUMN">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_INPUT_SOURCE_COLUMN is a configuration property that specifies the column name in the input data that contains the source information. It determines which column in the input data should be used for source information. The value "source" indicates that the system is configured to use the "source" column for source information. This property is essential for identifying the source of the input data.&gt;</data>
      <data key="d2">8ac79ce92be1254dfda9a10eb54ab703</data>
    </node>
    <node id="GRAPHRAG_LLM_API_KEY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_API_KEY is a configuration property that holds the API key required for authentication when using the Azure OpenAI service. It is set to "your_api_key" as a placeholder, indicating that a valid API key should be provided by the user.&gt;</data>
      <data key="d2">7b45dafa74553d3899e2291a3c9fb86e</data>
    </node>
    <node id="GRAPHRAG_LLM_API_VERSION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_API_VERSION is a configuration property that specifies the API version to be used when interacting with the Azure OpenAI service for language model requests. It is set to "api_version" by default and is crucial for compatibility and functionality.&gt;</data>
      <data key="d2">7b45dafa74553d3899e2291a3c9fb86e</data>
    </node>
    <node id="HRAG_LLM_DEPLOYMENT_NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">HRAG_LLM_DEPLOYMENT_NAME is a configuration property that specifies the name of the deployment for the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is currently set to None, indicating that no specific deployment name is provided.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_MAX_TOKENS is a configuration property that specifies the maximum number of tokens allowed for a request to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 4000, indicating that the maximum number of tokens for a request is 4000.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_REQUEST_TIMEOUT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_REQUEST_TIMEOUT is a configuration property that specifies the timeout duration for a request to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 180, indicating that the request will time out after 180 seconds.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_THREAD_STAGGER">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_THREAD_STAGGER is a configuration property that specifies the stagger time between threads for processing requests to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 0.3, indicating that there will be a 0.3-second stagger between threads.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_CONCURRENT_REQUESTS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_CONCURRENT_REQUESTS is a configuration property that specifies the number of concurrent requests allowed to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 25, indicating that 25 concurrent requests are allowed.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_TPM">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_TPM is a configuration property that specifies the target tokens per minute for requests to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 0, indicating that no specific target tokens per minute is set.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_RPM">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_RPM is a configuration property that specifies the target requests per minute for requests to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 0, indicating that no specific target requests per minute is set.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_MAX_RETRY_WAIT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_MAX_RETRY_WAIT is a configuration property that specifies the maximum wait time between retries for a request to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 10, indicating that the maximum wait time between retries is 10 seconds.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION is a configuration property that specifies whether the system should sleep when it receives a rate limit recommendation from the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to True, indicating that the system will sleep when it receives a rate limit recommendation.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_API_KEY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_EMBEDDING_API_KEY is a configuration property that specifies the API key for the embedding service in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to "your_api_key", indicating that a specific API key is provided for the embedding service.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_API_VERSION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_EMBEDDING_API_VERSION is a configuration property that specifies the API version for the embedding service in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to "api_version", indicating that a specific API version is provided for the embedding service.&gt;)&lt;|COMPLETE|&gt;
GRAPHRAG_EMBEDDING_API_VERSION is a configuration variable set to "api_version" for Azure OpenAI users. It specifies the API version for embedding requests when GRAPHRAG_API_VERSION is not set.&gt;</data>
      <data key="d2">485c17007ccb3102887eaa47d6a6100f,9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="APHRAG_API_KEY">
      <data key="d0">VARIABLE, CONFIGURATION</data>
      <data key="d1">APHRAG_API_KEY is a configuration variable that is not set. It is likely used for authentication or access to certain services or APIs.&gt;</data>
      <data key="d2">485c17007ccb3102887eaa47d6a6100f</data>
    </node>
    <node id="HRAG_EMBEDDING_THREAD_COUNT">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">HRAG_EMBEDDING_THREAD_COUNT is a configuration setting that specifies the number of threads used for embedding operations. It is set to None, indicating that the default or system-determined number of threads will be used.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_THREAD_STAGGER">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_EMBEDDING_THREAD_STAGGER is a configuration setting that determines the stagger or delay between embedding threads. It is set to 50, indicating a 50 millisecond delay between starting threads.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS is a configuration setting that specifies the number of concurrent requests for embedding operations. It is set to 25, indicating that up to 25 requests can be processed simultaneously.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_TPM">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_EMBEDDING_TPM is a configuration setting that represents the transactions per minute for embedding operations. It is set to 0, indicating that there is no specific limit.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_RPM">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_EMBEDDING_RPM is a configuration setting that represents the requests per minute for embedding operations. It is set to 0, indicating that there is no specific limit.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT is a configuration setting that determines the maximum wait time between retries for embedding operations. It is set to 10, indicating a 10-second maximum wait.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION is a configuration setting that indicates whether the system should sleep or pause when rate limits are reached during embedding operations. It is set to True, indicating that the system will sleep.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_INPUT_ENCODING">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_INPUT_ENCODING is a configuration setting that specifies the encoding used for input data. It is set to utf-8, indicating that the system uses UTF-8 encoding for input data.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_CHUNK_SIZE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_CHUNK_SIZE is a configuration setting that determines the size of data chunks for processing. It is set to 1200, indicating that data is processed in chunks of 1200 units.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_CHUNK_OVERLAP">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_CHUNK_OVERLAP is a configuration setting that specifies the overlap between data chunks. It is set to 100, indicating that there is a 100-unit overlap between chunks.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_CHUNK_BY_COLUMNS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_CHUNK_BY_COLUMNS is a configuration setting that indicates the column(s) used for chunking data. It is set to id, indicating that data is chunked based on the 'id' column.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS is a configuration setting that determines the maximum number of gleanings or entities extracted. It is set to 1, indicating that only one entity will be extracted.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES is a configuration setting that specifies the types of entities to be extracted. It is set to organization, person, event, geo, indicating that entities of these types will be extracted.
GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES is a configuration property that specifies the types of entities to be extracted, including organization, person, event, and geo.&gt;</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH is a configuration setting that determines the maximum length of summarized descriptions. It is set to 500, indicating that descriptions will be summarized to a maximum of 500 characters.
GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH is a configuration property that sets the maximum length for summarized descriptions, currently set to 500.&gt;</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION is a configuration setting that provides a description for claim extraction. It is set to "Any claims or facts that could be relevant to threat analysis.", indicating the criteria for claim extraction.
GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION is a configuration property that describes the type of claims or facts to be extracted for threat analysis.&gt;</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE is a configuration setting that specifies the file used for claim extraction prompts. It is set to None, indicating that there is no specific file used.
GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE is a configuration property that indicates the file to use for claim extraction prompts, currently set to None.&gt;</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GLEANINGS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GLEANINGS is a configuration property with a value of 1, indicating a setting or a state in the system related to data processing or analysis.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS is a configuration property that sets the maximum number of gleanings for claim extraction, currently set to 1.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_COMMUNITY_REPORT_MAX_LENGTH">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_COMMUNITY_REPORT_MAX_LENGTH is a configuration property that sets the maximum length for community reports, currently set to 1500.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_STORAGE_TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_STORAGE_TYPE is a configuration property that specifies the type of storage system to use, currently set to file.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_STORAGE_CONNECTION_STRING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_STORAGE_CONNECTION_STRING is a configuration property that provides the connection string for the storage system, currently set to None.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_STORAGE_CONTAINER_NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_STORAGE_CONTAINER_NAME is a configuration property that specifies the name of the storage container, currently set to None.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_STORAGE_BASE_DIR">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_STORAGE_BASE_DIR is a configuration property that indicates the base directory for the storage system, currently set to None.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CACHE_TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_CACHE_TYPE is a configuration property that specifies the type of cache system to use, currently set to file.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CACHE_CONNECTION_STRING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_CACHE_CONNECTION_STRING is a configuration property that provides the connection string for the cache system, currently set to None.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CACHE_CONTAINER_NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_CACHE_CONTAINER_NAME is a configuration property that specifies the name of the cache container, currently set to None.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CACHE_BASE_DIR">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_CACHE_BASE_DIR is a configuration property that indicates the base directory for the cache system, currently set to None.&gt;
GRAPHRAG_CACHE_BASE_DIR is a configuration setting that specifies the base directory for caching. It is currently set to None, indicating that no specific base directory has been defined for caching.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_REPORTING_TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_REPORTING_TYPE is a configuration property that specifies the type of reporting system to use, currently set to file.&gt;
GRAPHRAG_REPORTING_TYPE is a configuration setting that specifies the type of reporting mechanism to be used. It is currently set to 'file', indicating that reporting will be done through files.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_REPORTING_CONNECTION_STRING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_REPORTING_CONNECTION_STRING is a configuration property that provides the connection string for the reporting system, currently set to None.&gt;
GRAPHRAG_REPORTING_CONNECTION_STRING is a configuration setting that specifies the connection string for reporting. It is currently set to None, indicating that no specific connection string has been defined for reporting.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_REPORTING_CONTAINER_NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_REPORTING_CONTAINER_NAME is a configuration property that specifies the name of the reporting container, currently set to None.&gt;
GRAPHRAG_REPORTING_CONTAINER_NAME is a configuration setting that specifies the name of the container for reporting purposes. It is currently set to None, indicating that no specific container name has been defined for reporting.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_REPORTING_BASE_DIR">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_REPORTING_BASE_DIR is a configuration property that indicates the base directory for the reporting system, currently set to None.&gt;
GRAPHRAG_REPORTING_BASE_DIR is a configuration setting that specifies the base directory for reporting. It is currently set to None, indicating that no specific base directory has been defined for reporting.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_NODE2VEC_ENABLED">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_NODE2VEC_ENABLED is a configuration property that indicates whether the Node2Vec algorithm is enabled, currently set to False.&gt;
GRAPHRAG_NODE2VEC_ENABLED is a configuration setting that specifies whether the Node2Vec algorithm is enabled. It is currently set to False, indicating that the Node2Vec algorithm is not enabled.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_NODE2VEC_NUM_WALKS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_NODE2VEC_NUM_WALKS is a configuration property that sets the number of walks for the Node2Vec algorithm, currently set to 10.&gt;
GRAPHRAG_NODE2VEC_NUM_WALKS is a configuration setting that specifies the number of walks for the Node2Vec algorithm. It is currently set to 10, indicating that 10 walks will be performed.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="APHRAG_CACHE_CONTAINER_NAME">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">APHRAG_CACHE_CONTAINER_NAME is a configuration setting that specifies the name of the container for caching purposes. It is currently set to None, indicating that no specific container name has been defined.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_NODE2VEC_WALK_LENGTH">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_NODE2VEC_WALK_LENGTH is a configuration setting that specifies the length of each walk for the Node2Vec algorithm. It is currently set to 40, indicating that each walk will have a length of 40.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_NODE2VEC_WINDOW_SIZE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_NODE2VEC_WINDOW_SIZE is a configuration setting that specifies the window size for the Node2Vec algorithm. It is currently set to 2, indicating that the window size is 2.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_NODE2VEC_ITERATIONS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_NODE2VEC_ITERATIONS is a configuration setting that specifies the number of iterations for the Node2Vec algorithm. It is currently set to 3, indicating that the algorithm will be run for 3 iterations.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_NODE2VEC_RANDOM_SEED">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_NODE2VEC_RANDOM_SEED is a configuration setting that specifies the random seed for the Node2Vec algorithm. It is currently set to 597832, indicating that this number will be used as the random seed.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_SNAPSHOT_GRAPHML">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_SNAPSHOT_GRAPHML is a configuration setting that specifies whether to take a snapshot in GraphML format. It is currently set to False, indicating that no GraphML snapshot will be taken.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_SNAPSHOT_RAW_ENTITIES">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_SNAPSHOT_RAW_ENTITIES is a configuration setting that specifies whether to take a snapshot of raw entities. It is currently set to False, indicating that no snapshot of raw entities will be taken.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES is a configuration setting that specifies whether to take a snapshot of top-level nodes. It is currently set to False, indicating that no snapshot of top-level nodes will be taken.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_ASYNC_MODE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_ASYNC_MODE is a configuration setting that specifies the asynchronous mode to be used. It is currently set to 'asyncio', indicating that asyncio will be used for asynchronous operations.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_ENCODING_MODEL">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_ENCODING_MODEL is a configuration setting that specifies the encoding model to be used. It is currently set to 'cl100k_base', indicating that the 'cl100k_base' model will be used for encoding.&gt;
GRAPHRAG_ENCODING_MODEL is a configuration property set to cl100k_base, indicating the encoding model used for graph representation.</data>
      <data key="d2">1b24101de07b1c195448240237b84b37,79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_MAX_CLUSTER_SIZE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_MAX_CLUSTER_SIZE is a configuration setting that specifies the maximum size of a cluster. It is currently set to 10, indicating that the maximum cluster size is 10.&gt;
GRAPHRAG_MAX_CLUSTER_SIZE is a configuration property set to 10, indicating the maximum size of a cluster in the graph representation.</data>
      <data key="d2">1b24101de07b1c195448240237b84b37,79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_ENTITY_RESOLUTION_ENABLED">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_ENTITY_RESOLUTION_ENABLED is a configuration setting that specifies whether entity resolution is enabled. It is currently set to False, indicating that entity resolution is not enabled.&gt;
GRAPHRAG_ENTITY_RESOLUTION_ENABLED is a configuration property set to False, indicating that entity resolution is disabled in the graph representation.</data>
      <data key="d2">1b24101de07b1c195448240237b84b37,79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_SKIP_WORKFLOWS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_SKIP_WORKFLOWS is a configuration setting that specifies the workflows to be skipped. It is currently set to None, indicating that no specific workflows have been defined to be skipped.&gt;
GRAPHRAG_SKIP_WORKFLOWS is a configuration property set to None, indicating that no workflows are skipped in the graph representation.</data>
      <data key="d2">1b24101de07b1c195448240237b84b37,79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_UMAP_ENABLED">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_UMAP_ENABLED is a configuration setting that specifies whether UMAP (Uniform Manifold Approximation and Projection) is enabled. It is currently set to False, indicating that UMAP is not enabled.&gt;)&lt;|COMPLETE|&gt;
GRAPHRAG_UMAP_ENABLED is a configuration property set to False, indicating that UMAP (Uniform Manifold Approximation and Projection) is disabled in the graph representation.</data>
      <data key="d2">1b24101de07b1c195448240237b84b37,79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="DOCUMENT">
      <data key="d0">DOCUMENT, DATA</data>
      <data key="d1">A Document is an input document into the system, representing individual rows in a CSV or individual .txt files.
Documents are the primary containers for information in the text processing pipeline. They can have a one-to-many relationship with TextUnits, which are smaller chunks of text derived from them. In some cases, the relationship can be many-to-many when multiple documents are needed to form a meaningful analysis unit, such as in the case of short documents like Tweets or chat logs.</data>
      <data key="d2">81f57cf867ea246ad9a6e794ed613375,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </node>
    <node id="TEXTUNIT">
      <data key="d0">TEXT, DATA</data>
      <data key="d1">A TextUnit is a chunk of text to analyze. The size of these chunks, their overlap, and whether they adhere to any data boundaries can be configured. A common use case is to set CHUNK_BY_COLUMNS to id so that there is a 1-to-many relationship between documents and TextUnits instead of a many-to-many.
TextUnits are smaller chunks of text derived from Documents. They are the basic units for processing in the pipeline, and each TextUnit is text-embedded and passed into the next phase of the pipeline for further analysis.
A TextUnit is a segment of text that is processed individually in the pipeline for graph extraction, entity and relationship extraction, and other analysis tasks. It is the basic unit of information that is passed through various stages of processing.</data>
      <data key="d2">10d01d36390b307a63fd5bc97d8682c0,81f57cf867ea246ad9a6e794ed613375,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </node>
    <node id="ENTITY">
      <data key="d0">ENTITY, DATA</data>
      <data key="d1">An Entity is an entity extracted from a TextUnit. These represent people, places, events, or some other entity-model that is provided.
Entities are information elements extracted from the text during the Graph Extraction phase. They represent significant concepts, objects, or subjects within the text and are characterized by a name, type, and description.</data>
      <data key="d2">81f57cf867ea246ad9a6e794ed613375,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </node>
    <node id="RELATIONSHIP">
      <data key="d0">RELATIONSHIP, DATA</data>
      <data key="d1">A Relationship is a relationship between two entities. These are generated from the covariates.
Relationships are links between Entities that represent the connections or interactions between them. They are also extracted during the Graph Extraction phase and contribute to the structure of the graph being built from the text.</data>
      <data key="d2">81f57cf867ea246ad9a6e794ed613375,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </node>
    <node id="COVARIATE">
      <data key="d0">PROPERTY, DATA</data>
      <data key="d1">A Covariate is extracted claim information, which contains statements about entities which may be time-bound.
Covariates are extracted claim information that contains statements about entities which may be time-bound. They are used to provide context and details about entities in the text.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </node>
    <node id="COMMUNITY REPORT">
      <data key="d0">REPORT, DATA</data>
      <data key="d1">A Community Report is generated once entities are created. Hierarchical community detection is performed on them, and reports are generated for each community in this hierarchy.
Community Reports are generated after hierarchical community detection is performed on the entities. These reports provide insights and summaries for each community in the hierarchy.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </node>
    <node id="DEFAULT CONFIGURATION WORKFLOW">
      <data key="d0">PROCESS, WORKFLOW</data>
      <data key="d1">The Default Configuration Workflow is a series of steps that transform text documents into the GraphRAG Knowledge Model. It includes major phases such as Network Visualization, Document Processing, Community Summarization, Graph Augmentation, Graph Extraction, and Compose TextUnits.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20</data>
    </node>
    <node id="NETWORK VISUALIZATION">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Network Visualization is a phase in the workflow where the network of entities and their relationships are visualized in a graph format.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20</data>
    </node>
    <node id="DOCUMENT PROCESSING">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Document Processing is a phase in the workflow where documents are processed for further analysis and transformation into the GraphRAG Knowledge Model.
Document Processing is the phase in the workflow where the Documents table for the knowledge model is created, involving augmenting, linking to TextUnits, and averaging embeddings.
Document Processing is a phase in the workflow where documents are processed to create the Documents table for the knowledge model. This phase includes various steps such as Augment, Link to TextUnits, and Avg. Embedding.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,493f38f41b89e767fc23d84e1fa5ba20,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="COMMUNITY SUMMARIZATION">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Community Summarization is a process that summarizes the communities detected in the graph.Community Summarization is a phase in the workflow where communities are summarized to provide insights and reports.
Community Summarization is a process that involves summarizing the characteristics and insights of a community, often derived from social network analysis or community detection algorithms.
Community Summarization is a process that involves generating and summarizing community reports, embedding communities, and emitting community tables. It aims to provide a high-level understanding of the graph at various levels of granularity.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,5b2968b8f1c891d47ecbe641c3391663,6f92ce3fcd05dd5697ded83586f7bc08</data>
      <data key="d3">PROPERTY, INFORMATION</data>
    </node>
    <node id="GRAPH AUGMENTATION">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Graph Augmentation is a phase in the workflow where the graph is enhanced with additional information and relationships.
Graph Augmentation is a process that enhances the understanding of the community structure of entities and relationships by augmenting the graph with additional information. This process occurs after the extraction of entities and relationships.&gt;</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </node>
    <node id="GRAPH EXTRACTION">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Graph Extraction is a phase in the workflow where entities and relationships are extracted from the text to form a graph.
Graph Extraction is a phase in the pipeline where each TextUnit is analyzed to extract graph primitives such as Entities, Relationships, and Claims. This process involves using the entity_extract and claim_extract verbs to identify and extract these components from the raw text.
Graph Extraction is a process in the pipeline that involves analyzing TextUnits to identify entities and relationships within the text. The output is a subgraph for each TextUnit, containing a list of entities and relationships.</data>
      <data key="d2">10d01d36390b307a63fd5bc97d8682c0,493f38f41b89e767fc23d84e1fa5ba20,81f57cf867ea246ad9a6e794ed613375</data>
    </node>
    <node id="COMPOSE TEXTUNITS">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Compose TextUnits is the first phase in the workflow where input documents are transformed into TextUnits, which are chunks of text used for graph analysis.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20</data>
    </node>
    <node id="UMAP DOCUMENTS">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Umap Documents is a process that maps documents into a lower-dimensional space for visualization and analysis.
Umap Documents is the process of applying UMAP dimensionality reduction to documents, generating a 2D representation to visualize the relationships between documents in a 2D space.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,56506e2d064c0732efa3cf418057edfd</data>
    </node>
    <node id="UMAP ENTITIES">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Umap Entities is a process that maps entities into a lower-dimensional space for visualization and analysis.
Umap Entities is the process of applying UMAP dimensionality reduction to entities, generating a 2D representation to visualize the relationships between entities in a 2D space.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,56506e2d064c0732efa3cf418057edfd</data>
    </node>
    <node id="NODES TABLE">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">The Nodes Table contains information about the nodes in the graph, including their layout and relationships.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20</data>
    </node>
    <node id="LINK TO TEXTUNITS">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Link to TextUnits is a property that connects nodes to the corresponding TextUnits in the graph.
Link to TextUnits is a process in Document Processing where documents are linked to their constituent text units, enabling detailed analysis and understanding of document structure.
Link to TextUnits is a process that connects each document to the text-units created in the first phase of the workflow, establishing the relationship between documents and text-units.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,493f38f41b89e767fc23d84e1fa5ba20,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="DOCUMENT EMBEDDING">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Document Embedding is a process that converts documents into numerical vectors for analysis and comparison.
Document Embedding is a property or algorithm that generates a vector representation of documents by averaging embeddings of document slices, weighted by token-count. This representation helps in understanding the relationship between documents and supports network visualization.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="DOCUMENT GRAPH CREATION">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Document Graph Creation is a process that creates a graph representation of the documents and their relationships.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20</data>
    </node>
    <node id="DOCUMENT TABLES">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Document Tables contain information about the documents, including metadata and analysis results.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20</data>
    </node>
    <node id="COMMUNITY EMBEDDING">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Community Embedding is a process that maps communities into a lower-dimensional space for visualization and analysis.
Community Embedding refers to the representation of community structures in a vector space, often used for machine learning tasks such as clustering or classification.
Community Embedding is a vector representation of communities, generated by creating text embeddings of the community report, the community report summary, and the title of the community report.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="COMMUNITY TABLES">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Community Tables contain information about the communities, including their members and properties.
Community Tables are data structures that organize information about communities, potentially including details about members, relationships, and community attributes.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="GRAPH EMBEDDING">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Graph Embedding is a process that converts the graph into a numerical representation for analysis and visualization.
Graph Embedding is a technique that converts a graph into a vector space, where nodes are represented as points and edges as distances between points, facilitating various graph analysis tasks.
Graph Embedding is a subprocess of Phase 3: Graph Augmentation. It involves generating a vector representation of the graph using the Node2Vec algorithm to understand the implicit structure of the graph and provide an additional vector-space for searching related concepts during the query phase.
Graph Embedding is a step in the process where a vector representation of the graph is generated using the Node2Vec algorithm. This allows for understanding the implicit structure of the graph and provides a vector-space for searching related concepts during the query phase.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,5b2968b8f1c891d47ecbe641c3391663,6f92ce3fcd05dd5697ded83586f7bc08,a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </node>
    <node id="AUGMENTED GRAPH TABLES">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Augmented Graph Tables contain information about the graph after augmentation, including additional entities and relationships.
Augmented Graph Tables are enhanced data structures that store information about graphs, potentially including additional metadata or analytical results.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="ENTITY &amp; RELATIONSHIP EXTRACTION">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Entity &amp; Relationship Extraction is a process that identifies entities and their relationships from the text.
Entity &amp; Relationship Extraction is a process that identifies entities and the relationships between them from text or other data sources, often used in natural language processing and information extraction.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="ENTITY &amp; RELATIONSHIP SUMMARIZATION">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Entity &amp; Relationship Summarization is a process that summarizes the entities and relationships identified in the text.
Entity &amp; Relationship Summarization is a process that summarizes the entities and relationships identified in a dataset, providing a concise overview of the key entities and their interactions.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="ENTITY RESOLUTION">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Entity Resolution is a process that resolves entities that refer to the same real-world object or concept.
Entity Resolution is a process that identifies and merges duplicate or similar entities in a dataset, improving data quality and consistency.
Entity Resolution is an optional process in the pipeline that aims to identify and merge entities that represent the same real-world entity but have different names. This process takes a conservative, non-destructive approach to ensure no information is lost.
Entity Resolution is a process used to resolve entities that represent the same real-world entity but have different names. The current implementation is destructive, merging entities into a single entity and updating their relationships. Future implementations aim to be non-destructive, allowing end-users to undo indexing-side resolutions and add their own resolutions.&gt;</data>
      <data key="d2">10d01d36390b307a63fd5bc97d8682c0,493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08,d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </node>
    <node id="GRAPH TABLES">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Graph Tables contain information about the graph, including nodes, edges, and properties.
Graph Tables are data structures that represent graph data, potentially including nodes, edges, and attributes, facilitating graph analysis and visualization.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="DOCUMENTS">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Documents are the input texts that are transformed and analyzed in the workflow.
Documents are collections of text or other data that are treated as individual units for processing, analysis, or storage.
The Documents table is a data structure created during the Document Processing phase of the workflow. It holds information about documents that are part of the knowledge model.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="TEXT UNITS">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Text Units are the basic units of text that are used for analysis and processing in the workflow.
Text Units are the basic units of text that are processed in text analysis workflows, often derived from documents and used as the basis for further analysis or extraction.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="EMBED">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Embed is a process that converts information into a numerical representation for analysis and comparison.
Embed refers to the process of converting data into a vector representation, often used in machine learning and natural language processing.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="DATAFLOW OVERVIEW">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Dataflow Overview is a description of the flow of data and processes in the workflow, from input to output.
Dataflow Overview is a description of the flow of data through a system or process, often used to understand the sequence of operations or transformations applied to data.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="PHASE 1: COMPOSE TEXTUNITS">
      <data key="d0">PROCESS, ANALYSIS</data>
      <data key="d1">Phase 1: Compose TextUnits is the initial step in a workflow where input documents are transformed into TextUnits, which are chunks of text used for graph extraction techniques and as source references for extracted knowledge items.</data>
      <data key="d2">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="CLAIM">
      <data key="d0">CLAIM, INFORMATION_STATEMENT</data>
      <data key="d1">Claims are statements or assertions extracted from the text during the Graph Extraction phase. They represent the assertions made within the text and are a component of the graph being built.</data>
      <data key="d2">81f57cf867ea246ad9a6e794ed613375</data>
    </node>
    <node id="GRAPH SUMMARIZATION">
      <data key="d0">PROCESS, INFORMATION SUMMARIZATION</data>
      <data key="d1">Graph Summarization is a process that consolidates multiple subgraphs into a single graph by merging entities and relationships with the same name and type, creating arrays of their descriptions for a comprehensive view.</data>
      <data key="d2">10d01d36390b307a63fd5bc97d8682c0</data>
    </node>
    <node id="COVARIATES">
      <data key="d0">ARTIFACT, DATA</data>
      <data key="d1">Covariates are the primary artifacts emitted from the Claim Extraction process. They represent positive factual statements with an evaluated status and time-bounds.&gt;
Covariates are the primary artifacts emitted from the Claim Extraction &amp; Emission process. They represent claims that are positive factual statements with an evaluated status and time-bounds.</data>
      <data key="d2">a6bcb4514cb6de67e3d74ad0ea62452d,d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </node>
    <node id="CLAIM EXTRACTION &amp; EMISSION">
      <data key="d0">PROCESS, WORKFLOW</data>
      <data key="d1">Claim Extraction &amp; Emission is a process where claims are extracted from the source TextUnits. These claims are positive factual statements with an evaluated status and time-bounds, and they are emitted as a primary artifact called Covariates.</data>
      <data key="d2">a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </node>
    <node id="PHASE 3: GRAPH AUGMENTATION">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Phase 3: Graph Augmentation is a process where the graph of entities and relationships is augmented with additional information to understand their community structure. This is done through Community Detection and Graph Embedding.</data>
      <data key="d2">a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </node>
    <node id="GRAPH TABLES EMISSION">
      <data key="d0">SUBPROCESS, DATA EMISSION</data>
      <data key="d1">Graph Tables Emission is a subprocess of Phase 3: Graph Augmentation. It involves emitting the final entities and relationships in a tabular format after the graph augmentation steps are complete.
Graph Tables Emission is the process of emitting the final Entities and Relationships tables after their text fields are text-embedded, following the completion of graph augmentation steps.</data>
      <data key="d2">5b2968b8f1c891d47ecbe641c3391663,a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </node>
    <node id="NODE2VEC ALGORITHM">
      <data key="d0">ALGORITHM, TOOL</data>
      <data key="d1">Node2Vec is an algorithm used for generating vector representations of nodes in a graph, facilitating the understanding of the graph's structure and enabling vector-space search for related concepts.</data>
      <data key="d2">5b2968b8f1c891d47ecbe641c3391663</data>
    </node>
    <node id="COMMUNITY TABLES EMISSION">
      <data key="d0">ACTION, PROCESS</data>
      <data key="d1">Community Tables Emission is the process of generating and outputting the Communities and CommunityReports tables as part of the workflow.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888</data>
    </node>
    <node id="AUGMENT">
      <data key="d0">ACTION, PROCESS</data>
      <data key="d1">Augment is a process in Document Processing where additional information is added to the documents, enhancing their content and context.
Augment is a process that adds additional fields to the Documents output when the workflow is operating on CSV data. These fields should exist on the incoming CSV tables.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="AVG. EMBEDDING">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Avg. Embedding is a property calculated during Document Processing, representing the average vector embedding of a document, which is used for various analytical purposes.
Avg. Embedding is a property or algorithm that generates a vector representation of documents using an average embedding of document slices. This helps in understanding the implicit relationship between documents and supports network visualization.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="DOCUMENT TABLE EMISSION">
      <data key="d0">ACTION, PROCESS</data>
      <data key="d1">Document Table Emission is the process of generating and outputting the Documents table as part of the Document Processing phase.
Document Table Emission is the process of emitting the Documents table into the knowledge model, making the document data available for further processing or analysis.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="AUGMENT WITH COLUMNS (CSV ONLY)">
      <data key="d0">ACTION, PROCESS</data>
      <data key="d1">Augment with Columns (CSV Only) is a process specific to CSV data in Document Processing, where additional fields are added to the Documents output based on the incoming CSV tables.
Augment with Columns (CSV Only) is a function that allows the addition of extra fields to the Documents output when processing CSV data. This function requires configuration and relies on fields present in the incoming CSV tables.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="COMMUNITIES">
      <data key="d0">TABLE, DATA STRUCTURE</data>
      <data key="d1">The Communities table is a data structure that holds information about various communities, likely in the context of a knowledge model or database.</data>
      <data key="d2">827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="COMMUNITYREPORTS">
      <data key="d0">TABLE, DATA STRUCTURE</data>
      <data key="d1">The CommunityReports table is a data structure that contains reports or data related to communities, possibly including statistics, summaries, or other relevant information.</data>
      <data key="d2">827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="DOCUMENTS TABLE EMISSION">
      <data key="d0">FUNCTION, PROCESS</data>
      <data key="d1">Documents Table Emission is the function or process of emitting the Documents table into the knowledge model, making the document data available for further processing or analysis.
Documents Table Emission is the process of integrating the documents into the knowledge model, allowing for the representation of documents within the model.</data>
      <data key="d2">56506e2d064c0732efa3cf418057edfd,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="PHASE 6: NETWORK VISUALIZATION">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Phase 6: Network Visualization is a phase in the workflow where steps are taken to support the visualization of high-dimensional vector spaces within existing graphs. This phase aims to represent the relationships between documents in a network format.
Phase 6: Network Visualization is a step in the workflow where high-dimensional vector spaces are visualized within existing graphs, aiding in the understanding of relationships between documents and entities.</data>
      <data key="d2">56506e2d064c0732efa3cf418057edfd,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="ENTITY-RELATIONSHIP GRAPH">
      <data key="d0">CONCEPT, GRAPH</data>
      <data key="d1">The Entity-Relationship graph is a logical graph that represents the relationships between entities and their attributes, providing a structured view of the data.</data>
      <data key="d2">56506e2d064c0732efa3cf418057edfd</data>
    </node>
    <node id="DOCUMENT GRAPH">
      <data key="d0">CONCEPT, GRAPH</data>
      <data key="d1">The Document graph is a logical graph that represents the relationships between documents, providing a structured view of the document space.</data>
      <data key="d2">56506e2d064c0732efa3cf418057edfd</data>
    </node>
    <node id="NODES TABLE EMISSION">
      <data key="d0">ACTION, PROCESS</data>
      <data key="d1">Nodes Table Emission is the process of emitting a table of nodes, which includes information about whether the node is a document or an entity, and the UMAP coordinates for visualization.</data>
      <data key="d2">56506e2d064c0732efa3cf418057edfd</data>
    </node>
    <node id="NETWORK VISUALIZATION WORKFLOWS">
      <data key="d0">CONCEPT, PROCESS</data>
      <data key="d1">Network Visualization Workflows are the processes and steps taken to visualize high-dimensional vector spaces within graphs, enabling a 2D representation of the graph and the understanding of relationships between nodes.</data>
      <data key="d2">56506e2d064c0732efa3cf418057edfd</data>
    </node>
    <node id="DISCRIMINATOR">
      <data key="d0">PROPERTY, CLASSIFICATION</data>
      <data key="d1">The Discriminator is a property that classifies whether a node represents a document or an entity in a data structure. It is a binary indicator that helps in distinguishing between different types of nodes within a system.</data>
      <data key="d2">2011f03f21e526cf9277c27bf3e68242</data>
    </node>
    <node id="UMAP COORDINATES">
      <data key="d0">PROPERTY, LOCATION</data>
      <data key="d1">UMAP (Uniform Manifold Approximation and Projection) Coordinates are a set of values that represent the location of a node in a reduced dimensional space. They are used for visualizing high-dimensional data in a lower-dimensional format, typically for the purpose of data exploration and machine learning tasks.</data>
      <data key="d2">2011f03f21e526cf9277c27bf3e68242</data>
    </node>
    <node id="GRAPHRAG INDEXER CLI">
      <data key="d0">SOFTWARE, TOOL</data>
      <data key="d1">The GraphRAG Indexer CLI is a command-line interface tool that enables no-code usage of the GraphRAG Indexer, facilitating the indexing process for data projects without the need for programming. It provides various options for configuring the indexing process.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="CLI ARGUMENTS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">CLI Arguments are parameters that can be passed to the GraphRAG Indexer CLI to customize its behavior. These include options for verbosity, specifying the root directory, initializing the project, resuming a previous run, custom configuration, progress reporting, and output formats.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="VERBOSE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The verbose argument, when added, increases the amount of logging information during the execution of the GraphRAG Indexer CLI.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="INIT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The init argument initializes the data project directory at the specified root with bootstrap configuration and prompt-overrides.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="RESUME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The resume argument, when specified with a timestamp, allows the pipeline to resume a prior run. The parquet files from the prior run are loaded as inputs, and the workflows that generated those files are skipped.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="CONFIG">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The config argument allows opting out of the Default Configuration mode and executing a custom configuration specified in a config file.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="REPORTER">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The reporter argument specifies the progress reporter to use during the execution of the GraphRAG Indexer CLI. The default is rich, and valid values include rich, print, and none.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="EMIT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The emit argument specifies the table output formats for the GraphRAG Indexer CLI. It can include types such as json, csv, and parquet.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="TIMESTAMPED OUTPUT FOLDER">
      <data key="d0">PROPERTY, FOLDER</data>
      <data key="d1">The Timestamped Output Folder is a property that specifies the folder where the output of the process is stored, named with a timestamp, for example, "20240105-143721".</data>
      <data key="d2">919cb44d9688a14bf48fa7c98163ed81</data>
    </node>
    <node id="PROGRESS REPORTER">
      <data key="d0">PROPERTY, REPORTER</data>
      <data key="d1">The Progress Reporter is a property that specifies the type of progress reporter to use during the process. The default is 'rich', but it can be changed to 'print' or 'none' using the --reporter flag.</data>
      <data key="d2">919cb44d9688a14bf48fa7c98163ed81</data>
    </node>
    <node id="TABLE OUTPUT FORMATS">
      <data key="d0">PROPERTY, FORMATS</data>
      <data key="d1">The Table Output Formats is a property that specifies the formats in which the pipeline should emit the table output. The default is 'parquet', but it can be changed to 'csv' or 'json' using the --emit flag, with formats separated by commas.</data>
      <data key="d2">919cb44d9688a14bf48fa7c98163ed81</data>
    </node>
    <node id="CACHING MECHANISM">
      <data key="d0">PROPERTY, MECHANISM</data>
      <data key="d1">The Caching Mechanism is a feature that can be disabled using the --nocache flag. This is useful for debugging and development but should not be used in production as it can affect performance.</data>
      <data key="d2">919cb44d9688a14bf48fa7c98163ed81</data>
    </node>
    <edge source="PYTHON 3.10-3.12" target="POETRY">
      <data key="d4">1.0</data>
      <data key="d5">Python 3.10-3.12 is the environment in which Poetry operates, providing the necessary runtime for executing Poetry commands and managing dependencies for GraphRAG</data>
      <data key="d6">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </edge>
    <edge source="PYTHON 3.10-3.12" target="GRAPHRAG SYSTEM">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG system is compatible with Python versions 3.10 to 3.12, which are the supported versions for running the system</data>
      <data key="d6">84d24b5db902baca7217b5e3bb6ec462</data>
    </edge>
    <edge source="POETRY" target="INDEXING ENGINE">
      <data key="d4">1.0</data>
      <data key="d5">Poetry is used to execute the Indexing Engine, which is a part of GraphRAG, by running the 'poe index' command</data>
      <data key="d6">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </edge>
    <edge source="POETRY" target="QUERY ENGINE">
      <data key="d4">1.0</data>
      <data key="d5">Poetry is used to execute the Query Engine, which is a part of GraphRAG, by running the 'poe query' command</data>
      <data key="d6">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </edge>
    <edge source="POETRY" target="LIFECYCLE SCRIPTS">
      <data key="d4">1.0</data>
      <data key="d5">Poetry is used to manage and execute the Lifecycle Scripts, which are essential for building, testing, and executing the GraphRAG package</data>
      <data key="d6">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </edge>
    <edge source="AZURITE" target="UNIT AND SMOKE TESTS">
      <data key="d4">1.0</data>
      <data key="d5">Azurite is used in unit and smoke tests to emulate Azure resources, providing a testing environment for GraphRAG</data>
      <data key="d6">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </edge>
    <edge source="QUERY ENGINE" target="INDEXING PIPELINE">
      <data key="d4">1.0</data>
      <data key="d5">The Query Engine and the Indexing Pipeline are related as they are the two main components of the Graph RAG library, working together to process and retrieve information. The Indexing Pipeline prepares data for the Query Engine to perform searches and generate answers</data>
      <data key="d6">f8cf53ce98a8bc52581f7907ad98ef70</data>
    </edge>
    <edge source="QUERY ENGINE" target="LOCAL SEARCH">
      <data key="d4">1.0</data>
      <data key="d5">The Query Engine uses the Local Search method to generate answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents</data>
      <data key="d6">f8cf53ce98a8bc52581f7907ad98ef70</data>
    </edge>
    <edge source="QUERY ENGINE" target="GLOBAL SEARCH">
      <data key="d4">1.0</data>
      <data key="d5">The Query Engine uses the Global Search method to generate answers by searching over all AI-generated community reports in a map-reduce fashion. This method is resource-intensive but gives good responses for questions that require an understanding of the dataset as a whole</data>
      <data key="d6">f8cf53ce98a8bc52581f7907ad98ef70</data>
    </edge>
    <edge source="QUERY ENGINE" target="QUESTION GENERATION">
      <data key="d4">1.0</data>
      <data key="d5">The Query Engine uses the Question Generation functionality to take a list of user queries and generate the next candidate questions. This is useful for generating follow-up questions in a conversation or for generating a list of questions for the investigator to dive deeper into the dataset</data>
      <data key="d6">f8cf53ce98a8bc52581f7907ad98ef70</data>
    </edge>
    <edge source="QUERY ENGINE" target="CLI DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The CLI documentation is related to the Query Engine, as it provides guidance on how to use the command-line interface to run queries against the indexed data, including examples of different query methods.</data>
      <data key="d6">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE QUERY">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe query command, which runs the Query CLI for executing queries through the command line interface.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY BUILD">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry build command, which builds a wheel file and other distributable artifacts for the package.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE TEST">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe test command, which runs all tests for the package to ensure its functionality and integrity.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE TEST_UNIT">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe test_unit command, which runs unit tests to verify the correctness of individual units of code.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE TEST_INTEGRATION">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe test_integration command, which runs integration tests to verify the interaction between different parts of the system.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE TEST_SMOKE">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe test_smoke command, which runs smoke tests to ensure the system is in a stable state before more extensive testing.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE CHECK">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe check command, which performs a suite of static checks across the package, including formatting, documentation formatting, linting, security patterns, and type-checking.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE FIX">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe fix command, which applies any available auto-fixes to the package, typically limited to formatting fixes.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE FIX_UNSAFE">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe fix_unsafe command, which applies any available auto-fixes to the package, including those that may be unsafe.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE FORMAT">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe format command, which explicitly runs the formatter across the package to ensure consistent code style and formatting.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="TROUBLESHOOTING">
      <data key="d4">1.0</data>
      <data key="d5">The CLI may be used in conjunction with the Troubleshooting section, which provides solutions to common problems encountered when using the software or executing commands.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="RUNTIMEERROR" target="LLVM_CONFIG">
      <data key="d4">1.0</data>
      <data key="d5">The RuntimeError encountered during the 'poetry install' process is related to the LLVM_CONFIG environment variable, as the error message suggests setting this variable to the path of the llvm-config executable. This relationship indicates that the error is due to a missing or incorrect configuration of the LLVM environment.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="RUNTIMEERROR" target="LLVM-9">
      <data key="d4">1.0</data>
      <data key="d5">The RuntimeError encountered during the 'poetry install' process is related to the llvm-9 software package, as the error message suggests installing llvm-9 and llvm-9-dev. This relationship indicates that the error is due to the absence of the required LLVM components.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="RUNTIMEERROR" target="LLVM-9-DEV">
      <data key="d4">1.0</data>
      <data key="d5">The RuntimeError encountered during the 'poetry install' process is related to the llvm-9-dev software package, as the error message suggests installing llvm-9 and llvm-9-dev. This relationship indicates that the error is due to the absence of the required LLVM development components.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="RUNTIMEERROR" target="POETRY INSTALL">
      <data key="d4">1.0</data>
      <data key="d5">The RuntimeError encountered during the 'poetry install' process is directly related to the execution of the 'poetry install' command. This relationship indicates that the error occurs as a result of running this command, which is intended to install project dependencies.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="RUNTIMEERROR" target="PYTHON.H">
      <data key="d4">1.0</data>
      <data key="d5">The RuntimeError encountered during the 'poetry install' process is related to the missing Python.h file, as indicated by the error message "numba/_pymodule.h:6:10: fatal error: Python.h: No such file or directory". This relationship indicates that the error is due to the absence of the required Python header file.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="RUNTIMEERROR" target="PYTHON3.10-DEV">
      <data key="d4">1.0</data>
      <data key="d5">The RuntimeError encountered during the 'poetry install' process is related to the python3.10-dev software package, as the error message suggests installing python3.10-dev to resolve the issue with the missing Python.h file. This relationship indicates that the error is due to the absence of the required Python development components.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="GRAPHRAG_LLM_THREAD_COUNT" target="GRAPHRAG_EMBEDDING_THREAD_COUNT">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_LLM_THREAD_COUNT and GRAPHRAG_EMBEDDING_THREAD_COUNT environment variables are related to each other as they both determine the level of concurrency in the GraphRAG system. Modifying these values can help reduce the concurrency and improve system stability, as they are both set to 50 by default, which can lead to high concurrency and potential performance issues.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_THREAD_COUNT" target="GRAPHRAG_EMBEDDING_THREAD_STAGGER">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_EMBEDDING_THREAD_STAGGER setting is related to the GRAPHRAG_EMBEDDING_THREAD_COUNT as it determines the delay between starting threads, which can affect the efficiency and resource usage when multiple threads are specified</data>
      <data key="d6">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_THREAD_COUNT" target="GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS setting is related to the GRAPHRAG_EMBEDDING_THREAD_COUNT as it specifies the number of concurrent requests that can be processed, which can be influenced by the number of threads available for processing</data>
      <data key="d6">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </edge>
    <edge source="GRAPHRAG" target="GPT-4 TURBO">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG uses GPT-4 Turbo to generate knowledge graphs from text data. The model's ability to create detailed and structured representations of information is essential for the GraphRAG process.</data>
      <data key="d6">e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="MICROSOFT RESEARCH BLOG POST">
      <data key="d4">1.0</data>
      <data key="d5">The Microsoft Research Blog Post provides information about GraphRAG and its applications. It is a resource for users who want to learn more about how GraphRAG can enhance the ability of LLMs to reason about private data.</data>
      <data key="d6">e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="GITHUB REPOSITORY">
      <data key="d4">1.0</data>
      <data key="d5">The GitHub Repository is a resource for developers who want to access, contribute to, and collaborate on the GraphRAG project. It contains the source code and documentation for GraphRAG.</data>
      <data key="d6">e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="GRAPHRAG ARXIV">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG Arxiv is a repository for research papers and documentation related to GraphRAG. It is a resource for users who want to learn more about the theory, implementation, and applications of GraphRAG.</data>
      <data key="d6">e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="SOLUTION ACCELERATOR">
      <data key="d4">1.0</data>
      <data key="d5">The Solution Accelerator is a package that provides a user-friendly end-to-end experience with Azure resources for quickstarting the GraphRAG system. It is recommended for users who want to start using GraphRAG with ease.</data>
      <data key="d6">e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="GET STARTED GUIDE">
      <data key="d4">2.0</data>
      <data key="d5">The Get Started guide is a resource that provides instructions on how to start using GraphRAG. It is a step-by-step guide for users who are new to the system.
The Get Started guide is a resource provided by GraphRAG to help users understand and begin using the software effectively. It is a starting point for learning about the software's features and functionalities.</data>
      <data key="d6">32603b739bed06b4695b0cc3915b2c4b,e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="INDEXER">
      <data key="d4">2.0</data>
      <data key="d5">The Indexer is a sub-system of GraphRAG that is responsible for extracting information from raw text and building a knowledge graph. It plays a crucial role in the GraphRAG process by creating a structured representation of information.
The Indexer is a subsystem of GraphRAG that is essential for the software's ability to organize and retrieve data efficiently. It works in conjunction with the main software to provide improved question-and-answer performance.</data>
      <data key="d6">32603b739bed06b4695b0cc3915b2c4b,e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="QUERY PACKAGE">
      <data key="d4">2.0</data>
      <data key="d5">The Query package is a sub-system of GraphRAG that is used for querying the knowledge graph. It allows users to search for information within the graph and retrieve relevant results.
The Query package is a subsystem of GraphRAG that enables users to interact with the indexed data and retrieve information. It is a critical component for the software's functionality and user interaction.</data>
      <data key="d6">32603b739bed06b4695b0cc3915b2c4b,e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="SOLUTION ACCELERATOR PACKAGE">
      <data key="d4">1.0</data>
      <data key="d5">The Solution Accelerator package includes GraphRAG, which is a component that enhances the user experience by providing advanced RAG capabilities for better handling of complex information and reasoning about private datasets.</data>
      <data key="d6">32603b739bed06b4695b0cc3915b2c4b</data>
    </edge>
    <edge source="GRAPHRAG" target="BASELINE RAG">
      <data key="d4">2.0</data>
      <data key="d5">GraphRAG is an advanced version of Baseline RAG, designed to overcome the limitations of the latter in handling complex information and reasoning about private datasets. It provides substantial improvements in question-and-answer performance and reasoning capabilities.
GraphRAG is developed to address the limitations of Baseline RAG, particularly in connecting disparate pieces of information and understanding summarized semantic concepts over large data collections. GraphRAG shows substantial improvement in these areas, demonstrating intelligence or mastery that outperforms Baseline RAG.</data>
      <data key="d6">32603b739bed06b4695b0cc3915b2c4b,d441b136505c273cf3577b6867e872e4</data>
    </edge>
    <edge source="GRAPHRAG" target="LLMS">
      <data key="d4">1.0</data>
      <data key="d5">LLMs are used in GraphRAG to create a knowledge graph from an input corpus. The LLMs extract entities, relationships, and key claims from the TextUnits, which are then used to build the graph and enhance the query responses in GraphRAG.</data>
      <data key="d6">d441b136505c273cf3577b6867e872e4</data>
    </edge>
    <edge source="GRAPHRAG" target="LEIDEN TECHNIQUE">
      <data key="d4">2.0</data>
      <data key="d5">The Leiden technique is used in GraphRAG for hierarchical clustering of the graph. This method helps in visually representing the clustering of entities and relationships in the graph, which is a crucial step in the GraphRAG process.
GraphRAG employs the Leiden technique for hierarchical clustering of the graph, aiding in the visualization and analysis of entities and their communities.</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7,d441b136505c273cf3577b6867e872e4</data>
    </edge>
    <edge source="GRAPHRAG" target="TEXTUNITS">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG process involves slicing the input corpus into TextUnits, which are used as analyzable units for entity and relationship extraction.</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7</data>
    </edge>
    <edge source="GRAPHRAG" target="LLM">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG utilizes LLM for entity, relationship, and key claim extraction from the TextUnits.</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7</data>
    </edge>
    <edge source="GRAPHRAG" target="COMMUNITY SUMMARIES">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG generates Community Summaries from the bottom-up to provide a holistic understanding of each community and its constituents.</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7</data>
    </edge>
    <edge source="GRAPHRAG" target="GLOBAL SEARCH">
      <data key="d4">2.0</data>
      <data key="d5">Global Search is a query mode within the GraphRAG process that allows for reasoning about holistic questions concerning the corpus by leveraging the community summaries.
Global Search utilizes the structure of the LLM-generated knowledge graph created by GraphRAG to identify themes and semantic clusters within the dataset, enabling more effective aggregation of information</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7,3e143a60e2aeb57eb418a68d1484bbb3</data>
    </edge>
    <edge source="GRAPHRAG" target="LOCAL SEARCH">
      <data key="d4">2.0</data>
      <data key="d5">Local Search is a query mode within the GraphRAG process that enables reasoning about specific entities by fanning out to their neighbors and associated concepts.
Local Search is a technique that can be applied using GraphRAG to reason about specific entities by exploring their neighbors and associated concepts in a graph</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7,849698743b07680402ff8572b1c6c469</data>
    </edge>
    <edge source="GRAPHRAG" target="PROMPT TUNING">
      <data key="d4">2.0</data>
      <data key="d5">Prompt Tuning is a recommended technique for optimizing the use of GraphRAG with specific data, involving fine-tuning prompts according to the documentation to achieve the best possible results.
Prompt Tuning is recommended for optimizing the performance of GraphRAG, by fine-tuning prompts to better suit the specific data and tasks at hand</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7,849698743b07680402ff8572b1c6c469</data>
    </edge>
    <edge source="GRAPHRAG" target="LLM-GENERATED KNOWLEDGE GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG utilizes the LLM-generated knowledge graph to organize data into semantic clusters and summarize themes, enabling it to respond to user queries with relevant information</data>
      <data key="d6">812b3414c467da0b62f7932d2adcbad4</data>
    </edge>
    <edge source="GRAPHRAG" target="GLOBAL SEARCH METHOD">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG employs the global search method to process user queries, using LLM-generated community reports as context data to generate intermediate and final responses</data>
      <data key="d6">812b3414c467da0b62f7932d2adcbad4</data>
    </edge>
    <edge source="GRAPHRAG" target="EVALUATION APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG's performance has been evaluated using an approach that has limitations, as it has only examined a certain class of sensemaking questions for two corpora in the region of 1 million tokens</data>
      <data key="d6">e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="GRAPHRAG" target="TEMPLATE GENERATION ALGORITHM">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG utilizes the Template Generation Algorithm to create domain adaptive templates for the generation of knowledge graphs. The algorithm is a core component of GraphRAG, enabling it to adapt to different domains and improve the results of Index Runs by generating customized prompts based on the input data and specified parameters.</data>
      <data key="d6">9b364093aeecfc789c70fc5bd9503487</data>
    </edge>
    <edge source="GRAPHRAG" target="INITIALIZATION PROCESS">
      <data key="d4">1.0</data>
      <data key="d5">The Initialization Process is a prerequisite for using GraphRAG effectively. Before running the automatic template generation, the workspace must be initialized using the graphrag.index --init command. This process creates the necessary configuration files and default prompts, allowing GraphRAG to function properly and ensuring that the tool is ready for use.</data>
      <data key="d6">9b364093aeecfc789c70fc5bd9503487</data>
    </edge>
    <edge source="GRAPHRAG" target=".ENV">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG uses the .env file to store environment variables that are referenced in the settings.yaml file, essential for the proper configuration and operation of the software.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="GRAPHRAG" target="SETTINGS.YAML">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG uses the settings.yaml file to store configuration settings necessary for its operation. This file is created during the initialization process and is referenced by the software.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="GRAPHRAG" target="PROMPTS/">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG uses the prompts/ directory to store default prompts that can be modified or new ones generated through the Auto Prompt Tuning command. These prompts are essential for the software's operations.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="INDEXER" target="COMMUNITY_LEVEL">
      <data key="d4">1.0</data>
      <data key="d5">The Indexer uses the community_level parameter to determine which level of community reports to load from the Leiden community hierarchy, affecting the granularity of the data processed.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="RESPONSE_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The Indexer's output format and type are influenced by the response_type parameter, which dictates the structure of the generated reports.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The method parameter influences how the Indexer answers queries, either using a local or global approach, impacting the scope and methodology of the analysis.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GRAPHRAG_API_KEY">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_API_KEY environment variable is required for the Indexer to execute, providing the necessary API Key for model execution.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GRAPHRAG_LLM_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_LLM_MODEL environment variable specifies the model the Indexer uses for Chat Completions, affecting the conversational capabilities of the tool.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GRAPHRAG_EMBEDDING_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_EMBEDDING_MODEL environment variable determines the model the Indexer uses for Embeddings, impacting the representation and processing of data.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GRAPHRAG_LLM_API_BASE">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_LLM_API_BASE environment variable can be set to customize the API Base URL for the Indexer's LLM operation, allowing for flexibility in deployment environments.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GRAPHRAG_LLM_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_LLM_TYPE environment variable defines the LLM operation type used by the Indexer, influencing the method of interaction with the language model.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GRAPHRAG_LLM_MAX_RETRIES">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_LLM_MAX_RETRIES environment variable sets the maximum number of retries the Indexer will attempt when a request fails, affecting the tool's resilience and reliability.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GLOBAL SEARCH">
      <data key="d4">1.0</data>
      <data key="d5">The Indexer is a prerequisite for using the Global Search method, as it processes the data to make it searchable and ready for high-level questions</data>
      <data key="d6">ae6e91a8cc5773dbd4789773c9ef5a30</data>
    </edge>
    <edge source="INDEXER" target="LOCAL SEARCH">
      <data key="d4">1.0</data>
      <data key="d5">The Indexer is a prerequisite for using the Local Search method, as it processes the data to make it searchable and ready for specific questions about particular aspects or entities within the dataset</data>
      <data key="d6">ae6e91a8cc5773dbd4789773c9ef5a30</data>
    </edge>
    <edge source="LLMS" target="ENTITY GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">LLMs are capable of understanding the rich descriptive text associated with homogeneous nodes in the entity graph, which is crucial for global, query-focused summarization</data>
      <data key="d6">6dace8e490674ac8e031aed987a63789</data>
    </edge>
    <edge source="LLMS" target="RAGAS">
      <data key="d4">1.0</data>
      <data key="d5">LLMs are used by RAGAS to evaluate the performance of conventional RAG systems, automatically assessing qualities such as context relevance, faithfulness, and answer relevance.</data>
      <data key="d6">53455f8552b0787cb13c5a03eb550842</data>
    </edge>
    <edge source="LLMS" target="GRAPH RAG">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG and LLMs are connected through the use of large language models in the creation and analysis of graph indexes for advanced information retrieval.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="LLMS" target="KNOWLEDGE GRAPH CREATION">
      <data key="d4">1.0</data>
      <data key="d5">LLMs are used in the creation of knowledge graphs, as described in the research study by Trajanoska et al., indicating a direct connection between LLMs and knowledge graph creation techniques.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="LLM" target="GLOBAL SEARCH">
      <data key="d4">1.0</data>
      <data key="d5">The LLM is used in Global Search to summarize themes identified in the dataset's semantic clusters, responding to user queries that require understanding of the entire dataset</data>
      <data key="d6">3e143a60e2aeb57eb418a68d1484bbb3</data>
    </edge>
    <edge source="LLM" target="GLOBALSEARCH CLASS">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class uses the LLM for response generation, following the map-reduce process.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="LLM" target="PARALLELIZATION">
      <data key="d4">1.0</data>
      <data key="d5">llm and parallelization are related because the language model's configuration can influence how tasks are distributed across multiple processors or threads, affecting the overall performance and efficiency of the system.</data>
      <data key="d6">d27237468a1b9e89110eeeca8080f63c</data>
    </edge>
    <edge source="LLM" target="ASYNC_MODE">
      <data key="d4">1.0</data>
      <data key="d5">llm and async_mode are related because the language model's operation in asynchronous mode can impact how the model processes tasks concurrently, potentially affecting the throughput and response time of the system.</data>
      <data key="d6">d27237468a1b9e89110eeeca8080f63c</data>
    </edge>
    <edge source="LLM" target="ENTITY EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the LLM property, specifying the language model to use for text analysis</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="LLM" target="SUMMARIZE_DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The llm setting is used in the summarize_descriptions process, specifying the language model to be used for generating summaries</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="LLM" target="CLAIM_EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The llm setting is used in the claim_extraction process, specifying the language model to be used for identifying claims</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="LLM" target="COMMUNITY_REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The llm setting is used in the community_reports process, specifying the language model to be used for generating reports</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="LLM" target="ENTITY RESOLUTION">
      <data key="d4">1.0</data>
      <data key="d5">LLM is used in the process of Entity Resolution to determine which entities should be merged based on their representation of the same real-world entity but with different names. The current implementation is destructive, but future implementations aim to be non-destructive.</data>
      <data key="d6">d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </edge>
    <edge source="LLM" target="CLAIM EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">LLM is used in the Claim Extraction process to identify positive factual statements with an evaluated status and time-bounds from source TextUnits. These claims are emitted as Covariates.</data>
      <data key="d6">d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="CORPUS">
      <data key="d4">1.0</data>
      <data key="d5">The corpus can be summarized or analyzed to create community summaries, which provide condensed versions of the information contained in the corpus</data>
      <data key="d6">849698743b07680402ff8572b1c6c469</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="LOCAL TO GLOBAL GRAPH RAG APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">The Local to Global Graph RAG Approach utilizes Community Summaries to generate partial responses to a given question. These summaries are pre-generated for groups of closely-related entities and are combined into a final response.</data>
      <data key="d6">f76c18c7582167c3626f8741c2c9374f</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="ENTITY KNOWLEDGE GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The entity knowledge graph is used to generate community summaries for groups of closely-related entities, which are then used to generate partial responses to questions</data>
      <data key="d6">b149708d0b4ac3ff417565739ea6b03b</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="GRAPHCOMMUNITIES">
      <data key="d4">1.0</data>
      <data key="d5">Community Summaries are created for each community in the Leiden hierarchy, providing a detailed report of the structure and semantics of the dataset</data>
      <data key="d6">a660289d2bf43f25d3524d35cd2d9a96</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="GLOBAL QUERIES">
      <data key="d4">1.0</data>
      <data key="d5">Community summaries are used to answer global queries by providing structured information that can be searched or analyzed to address the broader themes or questions posed by the user</data>
      <data key="d6">93d4d4effbf989e6ef1c4c3b4f42494e</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="USER QUERY">
      <data key="d4">1.0</data>
      <data key="d5">A user query can be answered using community summaries, as the summaries provide relevant information that can be matched to the query to provide detailed or thematic responses</data>
      <data key="d6">93d4d4effbf989e6ef1c4c3b4f42494e</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="PODCAST DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The Podcast Dataset's graph structure influences the number of community summaries at different levels of the graph community hierarchy</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="NEWS DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The News Dataset's graph structure influences the number of community summaries at different levels of the graph community hierarchy</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="SOURCE TEXTS">
      <data key="d4">1.0</data>
      <data key="d5">Community Summaries are derived from Source Texts, representing a condensed version of the original content. The relationship indicates that summaries are created to provide a more concise representation of the source material.</data>
      <data key="d6">71f14506a6b15dfabd93fd1606a67b73</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="GRAPH RAG">
      <data key="d4">2.0</data>
      <data key="d5">Graph RAG is related to Community Summaries in that it uses these summaries to enhance the question answering process, providing improvements in answer comprehensiveness and diversity. The relationship strength is high due to the direct use of summaries in the Graph RAG method.
Graph RAG incorporates community summaries as a kind of self-memory for generation-augmented retrieval. This relationship indicates that community summaries are an integral part of Graph RAG, enhancing its retrieval capabilities by providing additional context and information.</data>
      <data key="d6">71f14506a6b15dfabd93fd1606a67b73,7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="GRAPH INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The decision to invest in building a graph index depends on the value obtained from other aspects of the graph index, including the generic community summaries</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the consideration of the generic community summaries</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GLOBAL SEARCH" target="QUERY ENGINE DOCS">
      <data key="d4">1.0</data>
      <data key="d5">The Global Search method is described and explained in the Query Engine Docs, which provide detailed information on how to use this search technique effectively to ask high-level questions about the dataset</data>
      <data key="d6">ae6e91a8cc5773dbd4789773c9ef5a30</data>
    </edge>
    <edge source="LOCAL SEARCH" target="QUESTION GENERATION">
      <data key="d4">2.0</data>
      <data key="d5">Question Generation is related to Local Search as both are functionalities that can be used to generate and answer questions based on input documents and structured data from a knowledge graph
Question Generation uses the same context-building approach as in local search to extract and prioritize relevant data</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4,364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="LOCAL SEARCH" target="ENTITY-BASED REASONING">
      <data key="d4">1.0</data>
      <data key="d5">Local Search is related to Entity-based Reasoning as Entity-based Reasoning is an approach used within the Local Search method to reason about information based on entities and their relationships</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="LOCAL SEARCH" target="QUESTION GENERATION FUNCTION">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search method uses the Question Generation Function to generate context for the search, which is then used to find semantically similar content within the dataset</data>
      <data key="d6">3e143a60e2aeb57eb418a68d1484bbb3</data>
    </edge>
    <edge source="LOCAL SEARCH" target="QUERY ENGINE DOCS">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search method is described and explained in the Query Engine Docs, which provide detailed information on how to use this search technique effectively to ask specific questions about particular aspects or entities within the dataset</data>
      <data key="d6">ae6e91a8cc5773dbd4789773c9ef5a30</data>
    </edge>
    <edge source="PROMPT TUNING" target="AUTO TEMPLATING">
      <data key="d4">1.0</data>
      <data key="d5">Prompt Tuning includes the Auto Templating feature, which is a method for creating domain adaptive templates for the generation of the knowledge graph. This relationship indicates that Auto Templating is a part of the broader Prompt Tuning capabilities</data>
      <data key="d6">bdb8f9e797229f596744d9636ab857b0</data>
    </edge>
    <edge source="PROMPT TUNING" target="MANUAL CONFIGURATION">
      <data key="d4">1.0</data>
      <data key="d5">Prompt Tuning includes the option for Manual Configuration, which allows for advanced customization of the prompts and templates used in the generation of the knowledge graph. This relationship indicates that Manual Configuration is an alternative method within the Prompt Tuning feature set</data>
      <data key="d6">bdb8f9e797229f596744d9636ab857b0</data>
    </edge>
    <edge source="PROMPT TUNING" target="GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE is a configuration variable that can be modified during Prompt Tuning to customize the entity extraction process</data>
      <data key="d6">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </edge>
    <edge source="PROMPT TUNING" target="GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE is a configuration variable that can be modified during Prompt Tuning to customize the community report generation process</data>
      <data key="d6">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </edge>
    <edge source="PROMPT TUNING" target="GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE is a configuration variable that can be modified during Prompt Tuning to customize the summarization of descriptions process</data>
      <data key="d6">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </edge>
    <edge source="PROMPT TUNING" target="GRAPHRAG INDEXER">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Indexer allows for the customization of its default prompts through the process of Prompt Tuning, enabling better alignment with specific use cases in knowledge discovery</data>
      <data key="d6">6a7157695d90d434b2625c3f05420916</data>
    </edge>
    <edge source="PROMPT TUNING" target="CUSTOM PROMPT FILE">
      <data key="d4">1.0</data>
      <data key="d5">A Custom Prompt File is created as part of the Prompt Tuning process to override default prompts, allowing for the customization of the GraphRAG indexer's behavior</data>
      <data key="d6">6a7157695d90d434b2625c3f05420916</data>
    </edge>
    <edge source="INDEXING PIPELINE" target="CLI DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The CLI documentation is related to the Indexing pipeline, as it explains how to use the command-line interface to run the indexing process, including the specific Python script and parameters required.</data>
      <data key="d6">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </edge>
    <edge source="INDEXING PIPELINE" target="PROMPT TUNING COMMAND">
      <data key="d4">1.0</data>
      <data key="d5">The Prompt Tuning command can be run before or after the Indexing Pipeline, as it adapts the prompts to better fit the data being indexed</data>
      <data key="d6">d0f7c236538005bc3056b7daed2401d8</data>
    </edge>
    <edge source="INDEXING PIPELINE" target="CONFIGURATION DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The Indexing Pipeline is a process that can be run after configuring GraphRAG as detailed in the Configuration documentation, allowing for the indexing of data for use with the system.</data>
      <data key="d6">32e96c66a531ecd0a8edc7414aec0803</data>
    </edge>
    <edge source="QUESTION GENERATION" target="GLOBAL SEARCH DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The Global Search documentation is related to Question Generation as it provides more information about the functionality that Question Generation is a part of, which is searching and retrieving information across various sources</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="QUESTION GENERATION" target="STRUCTURED DATA">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation combines structured data from a knowledge graph to generate candidate questions related to specific entities</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="UNSTRUCTURED DATA">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation combines unstructured data from input documents to generate candidate questions related to specific entities</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="KNOWLEDGE GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation uses structured data from the knowledge graph to generate candidate questions</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="OPENAI MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation uses an OpenAI model for response generation</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="CONTEXT BUILDER">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation uses a context builder to prepare context data from collections of knowledge model objects</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="SYSTEM PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation uses a system prompt template to generate candidate questions</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="LLM PARAMETERS">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation uses LLM parameters to customize the behavior of the LLM call</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="CONTEXT BUILDER PARAMETERS">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation uses context builder parameters to customize the context-building process</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="ENTITY-BASED REASONING" target="EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Entity-based Reasoning is related to Embedding as Embedding is a technique used in the Entity-based Reasoning approach to represent entities and text in a numerical format that can be processed by machine learning models</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="ENTITY-TEXT UNIT MAPPING" target="RANKING + FILTERING">
      <data key="d4">1.0</data>
      <data key="d5">Entity-Text Unit Mapping is related to Ranking + Filtering as Ranking + Filtering is a technique used after Entity-Text Unit Mapping to prioritize and select the most relevant text units based on their relevance to a user query</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="RANKING + FILTERING" target="ENTITY-REPORT MAPPING">
      <data key="d4">1.0</data>
      <data key="d5">Entity-Report Mapping is related to Ranking + Filtering as Ranking + Filtering is a technique used after Entity-Report Mapping to prioritize and select the most relevant community reports based on their relevance to a user query</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="RANKING + FILTERING" target="ENTITY-ENTITY RELATIONSHIPS">
      <data key="d4">1.0</data>
      <data key="d5">Entity-Entity Relationships are related to Ranking + Filtering as Ranking + Filtering is a technique used to prioritize and select the most relevant entity relationships based on their relevance to a user query</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="RANKING + FILTERING" target="ENTITY-COVARIATE MAPPINGS">
      <data key="d4">1.0</data>
      <data key="d5">Entity-Covariate Mappings are related to Ranking + Filtering as Ranking + Filtering is a technique used to prioritize and select the most relevant covariates based on their relevance to a user query</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="USER QUERY" target="LOCAL SEARCH DATAFLOW">
      <data key="d4">1.0</data>
      <data key="d5">The User Query initiates the Local Search Dataflow, guiding the search process and influencing the prioritization of entities, relationships, and covariates</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="USER QUERY" target="GLOBAL SEARCH METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The global search method is triggered by a user query, which serves as the input for generating responses based on the LLM-generated knowledge graph and semantic clusters</data>
      <data key="d6">812b3414c467da0b62f7932d2adcbad4</data>
    </edge>
    <edge source="CONVERSATION HISTORY" target="LOCAL SEARCH DATAFLOW">
      <data key="d4">1.0</data>
      <data key="d5">The Conversation History provides context for the Local Search Dataflow, influencing the prioritization of entities, relationships, and covariates based on previous interactions</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="CONVERSATION HISTORY" target="GLOBAL SEARCH METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The global search method can take conversation history as additional input to provide more context for generating informed and relevant responses to user queries</data>
      <data key="d6">812b3414c467da0b62f7932d2adcbad4</data>
    </edge>
    <edge source="LOCAL SEARCH DATAFLOW" target="PRIORITIZED TEXT UNITS">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search Dataflow extracts and prioritizes the Prioritized Text Units, selecting relevant text chunks from the input documents</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="LOCAL SEARCH DATAFLOW" target="PRIORITIZED COMMUNITY REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search Dataflow extracts and prioritizes the Prioritized Community Reports, selecting relevant reports from the knowledge graph</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="LOCAL SEARCH DATAFLOW" target="PRIORITIZED ENTITIES">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search Dataflow identifies and prioritizes the Prioritized Entities, serving as access points into the knowledge graph</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="LOCAL SEARCH DATAFLOW" target="PRIORITIZED RELATIONSHIPS">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search Dataflow identifies and prioritizes the Prioritized Relationships, providing context about the connections between entities</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="LOCAL SEARCH DATAFLOW" target="PRIORITIZED COVARIATES">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search Dataflow identifies and prioritizes the Prioritized Covariates, providing additional details about the entities and their attributes</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="LOCALSEARCH CLASS" target="QUESTION GENERATION METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The LocalSearch class is related to the Question Generation method as it uses a similar context-building approach to extract and prioritize relevant information for generating questions related to specific entities</data>
      <data key="d6">25797740f434cc2bf16365fc498791f6</data>
    </edge>
    <edge source="LOCALSEARCH CLASS" target="CONFIGURATION PARAMETERS">
      <data key="d4">1.0</data>
      <data key="d5">The Configuration Parameters are essential for the LocalSearch class as they define how the class operates, including the model, context builder, and various settings that affect the search and response generation process</data>
      <data key="d6">25797740f434cc2bf16365fc498791f6</data>
    </edge>
    <edge source="LLM COMPLETION STREAMING EVENTS" target="CALLBACKS">
      <data key="d4">1.0</data>
      <data key="d5">Callbacks can be used to handle LLM Completion Streaming Events, providing custom processing or reactions to the information generated by the LLM during the completion of tasks or queries</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="CONTEXT BUILDER" target="GLOBALSEARCH CLASS">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class utilizes the Context Builder to prepare context data from community reports for the map-reduce process.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="CALLBACKS" target="LLM CALL">
      <data key="d4">1.0</data>
      <data key="d5">The LLM call can trigger callbacks, which are optional functions that handle custom events, such as completion streaming events, for real-time processing or logging.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="FEW-SHOT EXAMPLES">
      <data key="d4">1.0</data>
      <data key="d5">The LLM is trained using few-shot examples specialized to various domains, which helps it to understand and perform tasks in those specific fields</data>
      <data key="d6">805a07a8f9c2ed5da2d9a61356aafa77</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="SECONDARY EXTRACTION PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The LLM uses the secondary extraction prompt to gather additional information or covariates associated with the extracted node instances, enhancing the detail and context of the extracted data</data>
      <data key="d6">805a07a8f9c2ed5da2d9a61356aafa77</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="GLEANING PROCESS">
      <data key="d4">1.0</data>
      <data key="d5">The LLM employs the gleaning process to ensure that all entities are detected, improving the completeness and accuracy of the data extraction</data>
      <data key="d6">805a07a8f9c2ed5da2d9a61356aafa77</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="ELEMENT SUMMARIES">
      <data key="d4">1.0</data>
      <data key="d5">The LLM generates element summaries, which are descriptions of entities, relationships, and claims represented in source texts, providing meaningful and concise information</data>
      <data key="d6">805a07a8f9c2ed5da2d9a61356aafa77</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="SUMMARIZATION QUERIES">
      <data key="d4">1.0</data>
      <data key="d5">LLM is used to generate summarization queries that require understanding of the entire corpus, aiding in the evaluation of RAG systems for data sensemaking tasks</data>
      <data key="d6">8e69f04648f5fc24c299591365f1aa68</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="ACTIVITY-CENTERED APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">The Activity-Centered Approach utilizes the LLM to identify users, tasks, and generate questions for evaluation</data>
      <data key="d6">a739018eb63cbb6c26b779bd37afc233</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="EVALUATION DATASETS">
      <data key="d4">1.0</data>
      <data key="d5">The LLM generates questions for the Evaluation Datasets, resulting in 125 test questions per dataset when N = 5</data>
      <data key="d6">a739018eb63cbb6c26b779bd37afc233</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="COMMUNITY REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">Community Reports are generated using the LLM (Language Model) to summarize the distinct information within each community, offering insights from high-level to low-level perspectives</data>
      <data key="d6">5b2968b8f1c891d47ecbe641c3391663</data>
    </edge>
    <edge source="GLOBAL SEARCH METHOD" target="AGGREGATED INTERMEDIATE RESPONSES">
      <data key="d4">1.0</data>
      <data key="d5">The global search method produces aggregated intermediate responses by filtering and selecting the most important points from intermediate responses, which are then used to generate the final response</data>
      <data key="d6">812b3414c467da0b62f7932d2adcbad4</data>
    </edge>
    <edge source="GRAPH'S COMMUNITY HIERARCHY" target="COMMUNITY REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The Graph's Community Hierarchy influences the level of detail in Community Reports, which are segmented into text chunks during the map step of the map-reduce process.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="MAP-REDUCE PROCESS" target="GLOBALSEARCH CLASS">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class implements the Map-Reduce Process for generating responses from community reports.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="COMMUNITY REPORTS" target="INTERMEDIATE RESPONSE">
      <data key="d4">1.0</data>
      <data key="d5">Community Reports are segmented into text chunks, which are used to produce an Intermediate Response containing points rated for importance.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="COMMUNITY REPORTS" target="DEFAULT PROMPTS">
      <data key="d4">1.0</data>
      <data key="d5">Default Prompts include the function of generating Community Reports, which provide insights and summaries based on the input data. This relationship indicates that Community Reports are one of the outputs provided by the Default Prompts</data>
      <data key="d6">bdb8f9e797229f596744d9636ab857b0</data>
    </edge>
    <edge source="COMMUNITY REPORTS" target="CONFIGURATION DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The Configuration Documentation is related to Community Reports as it provides guidelines on how to configure the settings that affect the generation of these reports</data>
      <data key="d6">21cdf11c58927ae505d3d375d1b75c82</data>
    </edge>
    <edge source="COMMUNITY REPORTS" target="PROMPT SOURCE">
      <data key="d4">1.0</data>
      <data key="d5">The Prompt Source is related to Community Reports as it is the origin of the input text that is used to generate the reports</data>
      <data key="d6">21cdf11c58927ae505d3d375d1b75c82</data>
    </edge>
    <edge source="COMMUNITY REPORTS" target="TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">Tokens are related to Community Reports as they contain the data, such as the input text, that is analyzed and presented in the reports</data>
      <data key="d6">21cdf11c58927ae505d3d375d1b75c82</data>
    </edge>
    <edge source="COMMUNITY REPORTS" target="DOCUMENT PROCESSING">
      <data key="d4">1.0</data>
      <data key="d5">Community Reports are processed in the Document Processing phase, where they are augmented, linked to TextUnits, and their embeddings are averaged to create the Documents table</data>
      <data key="d6">3e292d936b7efa377ba9530456cfd888</data>
    </edge>
    <edge source="INTERMEDIATE RESPONSE" target="FINAL RESPONSE">
      <data key="d4">1.0</data>
      <data key="d5">The Intermediate Response, containing points rated for importance, is aggregated during the reduce step to generate the Final Response.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="GLOBALSEARCH CLASS" target="MAP SYSTEM PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class uses the Map System Prompt as a template for the map step of the map-reduce process.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="GLOBALSEARCH CLASS" target="REDUCE SYSTEM PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class uses the Reduce System Prompt as a template for the reduce step of the map-reduce process.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="GLOBALSEARCH CLASS" target="RESPONSE TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class includes the Response Type as a parameter to specify the format of the final response.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="GLOBALSEARCH CLASS" target="ALLOW GENERAL KNOWLEDGE">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class includes the Allow General Knowledge setting to control whether general knowledge is included in the final response.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="MAP_SYSTEM_PROMPT" target="REDUCE_SYSTEM_PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The map_system_prompt and reduce_system_prompt are related as they both serve as templates guiding the processing of data in the map and reduce stages respectively</data>
      <data key="d6">e442fbb7a67e97ebc4de131b25c639e1</data>
    </edge>
    <edge source="REDUCE_SYSTEM_PROMPT" target="RESPONSE_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The response_type is related to the reduce_system_prompt as it influences the structure and format of the output generated during the reduce stage</data>
      <data key="d6">e442fbb7a67e97ebc4de131b25c639e1</data>
    </edge>
    <edge source="RESPONSE_TYPE" target="GRAPHRAG QUERY CLI">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG query CLI's output is customized by response_type, which dictates the format and type of response generated by the query engine.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="ALLOW_GENERAL_KNOWLEDGE" target="GENERAL_KNOWLEDGE_INCLUSION_PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The allow_general_knowledge setting is related to the general_knowledge_inclusion_prompt as enabling the former triggers the inclusion of the latter in the reduce_system_prompt</data>
      <data key="d6">e442fbb7a67e97ebc4de131b25c639e1</data>
    </edge>
    <edge source="MAX_DATA_TOKENS" target="CONTEXT_BUILDER_PARAMS">
      <data key="d4">1.0</data>
      <data key="d5">The max_data_tokens property is related to the context_builder_params as it sets a limit on the amount of data that can be processed, which the context_builder must adhere to when building the context window</data>
      <data key="d6">e442fbb7a67e97ebc4de131b25c639e1</data>
    </edge>
    <edge source="REDUCE_LLM_PARAMS" target="LLM CALL">
      <data key="d4">1.0</data>
      <data key="d5">The LLM call is configured by reduce_llm_params, which specifies additional parameters for the reduce stage, influencing the output of the LLM call.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="CONTEXT_BUILDER_PARAMS" target="LLM CALL">
      <data key="d4">1.0</data>
      <data key="d5">The LLM call's effectiveness in the map stage is enhanced by context_builder_params, which customize the context window for better understanding of the input data.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="CONCURRENT_COROUTINES" target="LLM CALL">
      <data key="d4">1.0</data>
      <data key="d5">The performance of the LLM call in the map stage is influenced by concurrent_coroutines, which determines the level of parallelism in processing tasks.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="GRAPHRAG QUERY CLI" target="DATA">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG query CLI uses data, typically in .parquet files, as input for queries and responses, enabling searches and generation of outputs based on indexed data.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="GRAPHRAG QUERY CLI" target="COMMUNITY_LEVEL">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG query CLI's functionality is influenced by community_level, which specifies the level of detail in community reports, affecting the granularity of the search results.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="METHOD" target="ROOT">
      <data key="d4">2.0</data>
      <data key="d5">ROOT and METHOD are related as ROOT specifies the project root directory, which may contain configuration files that influence the method of document selection
The root property specifies the path to the project directory, which contains the input data that is processed using the document selection method specified by the method property. The method property determines how text units are selected from the input data for template generation.</data>
      <data key="d6">9243633f55cccd0885ba553e14fa5e3f,ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </edge>
    <edge source="METHOD" target="LIMIT">
      <data key="d4">1.0</data>
      <data key="d5">METHOD and LIMIT are related as the method chosen for selecting documents (all, random, or top) can affect how the limit of text units is applied</data>
      <data key="d6">9243633f55cccd0885ba553e14fa5e3f</data>
    </edge>
    <edge source="GRAPHRAG_API_KEY" target="GRAPHRAG PIPELINE">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG pipeline uses the GRAPHRAG_API_KEY for authentication when making requests to the OpenAI API or Azure OpenAI endpoint. This key is essential for the pipeline to access and utilize the AI services provided by these APIs.</data>
      <data key="d6">5aaa26fbe97dc7573cd1a56d6fb11213</data>
    </edge>
    <edge source="GRAPHRAG_API_KEY" target="SETTINGS.YAML">
      <data key="d4">1.0</data>
      <data key="d5">The settings.yaml file includes the GRAPHRAG_API_KEY, which is an environment variable that holds the API key for the OpenAI API or Azure OpenAI endpoint. This key is essential for the GraphRAG pipeline to authenticate and access the respective API services.</data>
      <data key="d6">5aaa26fbe97dc7573cd1a56d6fb11213</data>
    </edge>
    <edge source="GRAPHRAG_API_KEY" target="GRAPHRAG_API_BASE">
      <data key="d4">2.0</data>
      <data key="d5">GRAPHRAG_API_KEY and GRAPHRAG_API_BASE are related because the API key is used for authentication when making requests to the API base URL. The API key is essential for accessing the services provided by the API base. The strength of this relationship is high because both settings are required for successful API interaction.
GRAPHRAG_API_KEY and GRAPHRAG_API_BASE are related because GRAPHRAG_API_KEY is used for authentication to access the LLM service, and GRAPHRAG_API_BASE specifies the base URL for the LLM service API. Together, they determine the endpoint and the authentication method for accessing the LLM service.</data>
      <data key="d6">3da10b454f926a257b9fdf5d2487c0a5,8ac79ce92be1254dfda9a10eb54ab703</data>
    </edge>
    <edge source="GRAPHRAG_API_KEY" target="GRAPHRAG_API_VERSION">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_API_KEY and GRAPHRAG_API_VERSION are related because GRAPHRAG_API_KEY is used for authentication to access the LLM service, and GRAPHRAG_API_VERSION specifies the version of the LLM service API. Together, they ensure compatibility between the system and the LLM service by specifying the correct version of the API to use.</data>
      <data key="d6">8ac79ce92be1254dfda9a10eb54ab703</data>
    </edge>
    <edge source="GRAPHRAG_LLM_API_BASE" target="GRAPHRAG_LLM_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LLM_API_BASE and GRAPHRAG_LLM_TYPE are related as they both configure the LLM operations, with the API Base URL and the type of operation, respectively</data>
      <data key="d6">1ef6439b7c457ba43993467ff734eedf</data>
    </edge>
    <edge source="GRAPHRAG_LLM_API_BASE" target="GRAPHRAG_LLM_MAX_RETRIES">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LLM_API_BASE and GRAPHRAG_LLM_MAX_RETRIES are related as they both pertain to the configuration of LLM operations, with the API Base URL and the maximum number of retries for failed requests, respectively</data>
      <data key="d6">1ef6439b7c457ba43993467ff734eedf</data>
    </edge>
    <edge source="GRAPHRAG_LLM_API_BASE" target="AZURE OPENAI">
      <data key="d4">1.0</data>
      <data key="d5">The Azure OpenAI service requires the GRAPHRAG_LLM_API_BASE configuration to determine the base URL for API requests. This relationship is necessary for establishing a connection to the service and sending requests to the correct endpoint.</data>
      <data key="d6">7b45dafa74553d3899e2291a3c9fb86e</data>
    </edge>
    <edge source="GRAPHRAG_LLM_TYPE" target="GRAPHRAG_LLM_DEPLOYMENT_NAME">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LLM_TYPE and GRAPHRAG_LLM_DEPLOYMENT_NAME are related because the language model type and the deployment name together determine the specific language model to be used for text generation. The deployment name is crucial for identifying the correct model within the specified type. The strength of this relationship is high because both settings are required for successful text generation.</data>
      <data key="d6">3da10b454f926a257b9fdf5d2487c0a5</data>
    </edge>
    <edge source="GRAPHRAG_LLM_TYPE" target="GRAPHRAG_LLM_MODEL_SUPPORTS_JSON">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LLM_TYPE and GRAPHRAG_LLM_MODEL_SUPPORTS_JSON are related because the language model type and the model's support for JSON input together determine the format of the input data that can be processed by the model. The model's support for JSON is crucial for ensuring that the input data is in a compatible format. The strength of this relationship is high because both settings are required for successful text generation.</data>
      <data key="d6">3da10b454f926a257b9fdf5d2487c0a5</data>
    </edge>
    <edge source="GRAPHRAG_LLM_TYPE" target="GRAPHRAG_INPUT_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_INPUT_TYPE and GRAPHRAG_LLM_TYPE are related because the input data type and the language model type together determine the compatibility of the input data with the model being used for text generation. The input data type is crucial for ensuring that the data is in a format that can be processed by the specified model. The strength of this relationship is high because both settings are required for successful text generation.</data>
      <data key="d6">3da10b454f926a257b9fdf5d2487c0a5</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_API_BASE" target="GRAPHRAG_EMBEDDING_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_EMBEDDING_API_BASE and GRAPHRAG_EMBEDDING_TYPE are related as they both configure the embedding operations, with the API Base URL and the type of embedding client, respectively</data>
      <data key="d6">1ef6439b7c457ba43993467ff734eedf</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_API_BASE" target="GRAPHRAG_EMBEDDING_MAX_RETRIES">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_EMBEDDING_API_BASE and GRAPHRAG_EMBEDDING_MAX_RETRIES are related as they both pertain to the configuration of embedding operations, with the API Base URL and the maximum number of retries for failed requests, respectively</data>
      <data key="d6">1ef6439b7c457ba43993467ff734eedf</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_API_BASE" target="APHRAG_API_KEY">
      <data key="d4">1.0</data>
      <data key="d5">APHRAG_API_KEY is related to GRAPHRAG_EMBEDDING_API_BASE as both are configuration variables that may be required for accessing and using the embedding API. The absence of APHRAG_API_KEY might affect the functionality or access to the API specified by GRAPHRAG_EMBEDDING_API_BASE.</data>
      <data key="d6">485c17007ccb3102887eaa47d6a6100f</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_TYPE" target="GRAPHRAG_LLM_DEPLOYMENT_NAME">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_EMBEDDING_TYPE and GRAPHRAG_LLM_DEPLOYMENT_NAME are related because the embedding model type and the deployment name together determine the specific embedding model to be used for text embedding. The deployment name is crucial for identifying the correct model within the specified type. The strength of this relationship is high because both settings are required for successful text embedding.</data>
      <data key="d6">3da10b454f926a257b9fdf5d2487c0a5</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_MAX_RETRIES" target="GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_EMBEDDING_MAX_RETRIES setting is related to the GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT as they both determine the retry mechanism for embedding operations, with the number of retries and the wait time between retries</data>
      <data key="d6">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </edge>
    <edge source="GRAPHRAG_LOCAL_SEARCH_TEXT_UNIT_PROP" target="GRAPHRAG_LOCAL_SEARCH_COMMUNITY_PROP">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LOCAL_SEARCH_TEXT_UNIT_PROP and GRAPHRAG_LOCAL_SEARCH_COMMUNITY_PROP are related as they both configure the proportions of the context window dedicated to related text units and community reports, respectively</data>
      <data key="d6">1ef6439b7c457ba43993467ff734eedf</data>
    </edge>
    <edge source="GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS" target="GRAPHRAG_LOCAL_SEARCH_LLM_MAX_TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS and GRAPHRAG_LOCAL_SEARCH_LLM_MAX_TOKENS are related as they both are configuration properties that can be adjusted based on the token limit of the model being used, affecting the local search context window and language model token limits respectively</data>
      <data key="d6">2efb1fec56fe3b0543d395dd541295c3</data>
    </edge>
    <edge source="GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS" target="GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS and GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS are related as they both are configuration properties that can be adjusted based on the token limit of the model being used, affecting the local and global search context window token limits respectively</data>
      <data key="d6">2efb1fec56fe3b0543d395dd541295c3</data>
    </edge>
    <edge source="GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS" target="GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">Both GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS and GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS are related as they both pertain to the token limits for global search and should be adjusted based on the model's token limit</data>
      <data key="d6">2049798d3000849f8bec3e88c0006807</data>
    </edge>
    <edge source="GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS" target="GRAPHRAG_GLOBAL_SEARCH_REDUCE_MAX_TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS and GRAPHRAG_GLOBAL_SEARCH_REDUCE_MAX_TOKENS are related as they both pertain to the token limits for global search and should be adjusted based on the model's token limit</data>
      <data key="d6">2049798d3000849f8bec3e88c0006807</data>
    </edge>
    <edge source="GRAPHRAG KNOWLEDGE MODEL" target="GRAPHRAG INDEXER">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Knowledge Model is aligned with the outputs of the GraphRAG Indexer, which means the data processed by the Indexer is structured according to the Knowledge Model for compatibility with the GraphRAG system</data>
      <data key="d6">25e04f0e9a961dcdc3f6eae6df7807b2</data>
    </edge>
    <edge source="GRAPHRAG INDEXER" target="GRAPHRAG QUERY ENGINE">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Indexer's outputs are loaded into a database system, which is then accessed by the GraphRAG Query Engine to process queries based on the indexed data</data>
      <data key="d6">25e04f0e9a961dcdc3f6eae6df7807b2</data>
    </edge>
    <edge source="DATASHAPER" target="GRAPHRAG INDEXING PIPELINE">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Indexing Pipeline is built on top of DataShaper, utilizing its data processing capabilities to transform and index data for the GraphRAG system</data>
      <data key="d6">25e04f0e9a961dcdc3f6eae6df7807b2</data>
    </edge>
    <edge source="DATASHAPER" target="WORKFLOW">
      <data key="d4">1.0</data>
      <data key="d5">DataShaper provides the framework for defining and executing Workflows, which are sequences of steps (verbs) that transform data tables through a pipeline</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="WORKFLOW" target="VERB">
      <data key="d4">1.0</data>
      <data key="d5">A Workflow in DataShaper is composed of Verbs, which are steps that model relational concepts and transform input data tables</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="WORKFLOW GRAPHS">
      <data key="d4">1.0</data>
      <data key="d5">Workflow Graphs are a fundamental concept in the GraphRAG Indexing Pipeline, where they are used to represent the series of interdependent workflows that form the data indexing process.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="PREPARE">
      <data key="d4">1.0</data>
      <data key="d5">The Prepare step is part of the GraphRAG Indexing Pipeline, where it is used to prepare data for subsequent processing tasks.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="CHUNK">
      <data key="d4">1.0</data>
      <data key="d5">The Chunk step is part of the GraphRAG Indexing Pipeline, where it is used to break down data into smaller, manageable pieces for more efficient processing.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="EXTRACTGRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The ExtractGraph step is part of the GraphRAG Indexing Pipeline, where it is used to extract graph structures from data.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="EMBEDDOCUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">The EmbedDocuments step is part of the GraphRAG Indexing Pipeline, where it is used to convert documents into numerical representations for machine learning or information retrieval.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="GENERATEREPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The GenerateReports step is part of the GraphRAG Indexing Pipeline, where it is used to create reports based on the processed data.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="EMBEDGRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The EmbedGraph step is part of the GraphRAG Indexing Pipeline, where it is used to convert graph structures into numerical representations for machine learning algorithms.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="ENTITYRESOLUTION">
      <data key="d4">1.0</data>
      <data key="d5">The EntityResolution step is part of the GraphRAG Indexing Pipeline, where it is used to resolve ambiguities in data, such as identifying and merging duplicate entities.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="SAMPLE WORKFLOW DAG">
      <data key="d4">1.0</data>
      <data key="d5">The Sample Workflow DAG is a visual representation of the directed acyclic graph (DAG) that is part of the GraphRAG Indexing Pipeline, showing the dependencies between different workflows.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="DATAFRAME MESSAGE FORMAT">
      <data key="d4">1.0</data>
      <data key="d5">The Dataframe Message Format is used in the GraphRAG Indexing Pipeline as the primary unit of communication between workflows and workflow steps, facilitating data-centric and table-centric data processing.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="LLM CACHING">
      <data key="d4">1.0</data>
      <data key="d5">LLM Caching is a technique used in the GraphRAG Indexing Pipeline to improve the resilience of the indexer to network issues by caching the results of Large Language Model (LLM) interactions.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="VERB" target="INPUT TABLE">
      <data key="d4">1.0</data>
      <data key="d5">A Verb in DataShaper operates on an Input Table, transforming it according to the verb's specific function</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="VERB" target="OUTPUT TABLE">
      <data key="d4">1.0</data>
      <data key="d5">A Verb in DataShaper produces an Output Table, which is the result of the transformation applied to the Input Table</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="GRAPHRAG'S INDEXING PIPELINE" target="LLM-BASED WORKFLOW STEPS">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG's Indexing Pipeline implements custom LLM-based Workflow Steps that utilize Large Language Models to perform data enrichment and extraction tasks</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="GRAPHRAG'S INDEXING PIPELINE" target="WORKFLOW GRAPHS">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG's Indexing Pipeline uses Workflow Graphs to represent the complex interdependencies between multiple workflows, forming a DAG for scheduling processing</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="WORKFLOW GRAPHS" target="DAG (DIRECTED ACYCLIC GRAPH)">
      <data key="d4">1.0</data>
      <data key="d5">Workflow Graphs in GraphRAG's Indexing Pipeline are represented as a DAG (Directed Acyclic Graph) to manage the dependencies between different workflow steps</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="PREPARE" target="CHUNK">
      <data key="d4">1.0</data>
      <data key="d5">Prepare may be followed by Chunk in a workflow, where data is prepared and then divided into smaller, manageable pieces</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="CHUNK" target="EXTRACTGRAPH">
      <data key="d4">1.0</data>
      <data key="d5">Chunk may be followed by ExtractGraph in a workflow, where data chunks are analyzed to extract graph structures</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="CHUNK" target="DOCUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">Chunk is related to Documents as chunks are often derived from documents, with a user-configurable size, and there is a strict 1-to-many relationship between Documents and TextUnits by default</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="EXTRACTGRAPH" target="EMBEDDOCUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">ExtractGraph may be followed by EmbedDocuments in a workflow, where graph structures are used to inform the embedding of documents into a vector space</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="EMBEDDOCUMENTS" target="GENERATEREPORTS">
      <data key="d4">1.0</data>
      <data key="d5">EmbedDocuments may be followed by GenerateReports in a workflow, where embedded documents are used to generate reports based on the processed data</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="GENERATEREPORTS" target="EMBEDGRAPH">
      <data key="d4">1.0</data>
      <data key="d5">GenerateReports may be followed by EmbedGraph in a workflow, where reports are used to inform the embedding of graph structures into a vector space</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="EMBEDGRAPH" target="ENTITYRESOLUTION">
      <data key="d4">1.0</data>
      <data key="d5">EmbedGraph may be followed by EntityResolution in a workflow, where embedded graph structures are used to resolve entities within the data</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="GRAPHRAG LIBRARY" target="LLM INTERACTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG library is designed to facilitate and optimize LLM interactions by addressing common issues such as network latency and throttling errors through the implementation of a caching mechanism.</data>
      <data key="d6">6335601c6ec22bd6f15c8b69c26f854b</data>
    </edge>
    <edge source="GRAPHRAG LIBRARY" target="CACHING">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG library incorporates caching as a core feature to improve the efficiency and reliability of LLM interactions by storing and reusing results when the same input set is used.</data>
      <data key="d6">6335601c6ec22bd6f15c8b69c26f854b</data>
    </edge>
    <edge source="LLM INTERACTIONS" target="CACHING">
      <data key="d4">1.0</data>
      <data key="d5">Caching is a strategy used in LLM interactions to enhance system performance by storing and reusing results, which helps in mitigating network issues and improving the efficiency of the interactions.</data>
      <data key="d6">6335601c6ec22bd6f15c8b69c26f854b</data>
    </edge>
    <edge source="LOCAL TO GLOBAL GRAPH RAG APPROACH" target="RETRIEVAL-AUGMENTED GENERATION (RAG)">
      <data key="d4">1.0</data>
      <data key="d5">The Local to Global Graph RAG Approach builds upon the concept of Retrieval-Augmented Generation (RAG) by addressing its limitations in handling global questions. It extends RAG's capabilities to include query-focused summarization tasks over large text corpora.</data>
      <data key="d6">f76c18c7582167c3626f8741c2c9374f</data>
    </edge>
    <edge source="LOCAL TO GLOBAL GRAPH RAG APPROACH" target="QUERY-FOCUSED SUMMARIZATION (QFS)">
      <data key="d4">1.0</data>
      <data key="d5">The Local to Global Graph RAG Approach incorporates elements of Query-Focused Summarization (QFS) to scale to the quantities of text indexed by typical RAG systems. It aims to combine the strengths of QFS and RAG to handle both the generality of user questions and the quantity of source text.</data>
      <data key="d6">f76c18c7582167c3626f8741c2c9374f</data>
    </edge>
    <edge source="LOCAL TO GLOBAL GRAPH RAG APPROACH" target="ENTITY KNOWLEDGE GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The Local to Global Graph RAG Approach uses an Entity Knowledge Graph as a foundational component. The graph is derived from source documents and is used to identify closely-related entities for generating community summaries.</data>
      <data key="d6">f76c18c7582167c3626f8741c2c9374f</data>
    </edge>
    <edge source="RETRIEVAL-AUGMENTED GENERATION (RAG)" target="QUERY-FOCUSED SUMMARIZATION (QFS)">
      <data key="d4">1.0</data>
      <data key="d5">Retrieval-Augmented Generation (RAG) is a specific approach within the broader task of query-focused summarization, but it is designed for situations where answers are contained locally within regions of text, whereas QFS focuses on generating summaries in response to user queries</data>
      <data key="d6">c7669e6a1add9a2829b09196256b1492</data>
    </edge>
    <edge source="QUERY-FOCUSED SUMMARIZATION (QFS)" target="ABSTRACTIVE SUMMARIZATION">
      <data key="d4">2.0</data>
      <data key="d5">Query-Focused Summarization (QFS) particularly focuses on abstractive summarization, which generates natural language summaries that are not just concatenated excerpts, but rather new sentences that convey the meaning of the original text
Query-Focused Summarization (QFS) is a specific type of Abstractive Summarization that focuses on generating summaries relevant to specific queries, rather than generic summaries. It requires the creation of new sentences that are tailored to the information sought by the query.</data>
      <data key="d6">85eff07c379a9dc24db0edb983acf3c9,c7669e6a1add9a2829b09196256b1492</data>
    </edge>
    <edge source="QUERY-FOCUSED SUMMARIZATION (QFS)" target="EXTRACTIVE SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Query-Focused Summarization (QFS) contrasts with extractive summarization, as it focuses on generating new sentences in response to user queries, rather than selecting and concatenating existing sentences</data>
      <data key="d6">c7669e6a1add9a2829b09196256b1492</data>
    </edge>
    <edge source="ENTITY KNOWLEDGE GRAPH" target="GRAPH-BASED TEXT INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The graph-based text index method involves the creation of an entity knowledge graph from source documents, which is used to generate community summaries for groups of closely-related entities</data>
      <data key="d6">b149708d0b4ac3ff417565739ea6b03b</data>
    </edge>
    <edge source="GRAPH-BASED TEXT INDEX" target="GLOBAL SENSEMAKING QUESTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The graph-based text index method is particularly effective for answering global sensemaking questions over datasets in the 1 million token range, as it improves the comprehensiveness and diversity of generated answers</data>
      <data key="d6">b149708d0b4ac3ff417565739ea6b03b</data>
    </edge>
    <edge source="GLOBAL SENSEMAKING QUESTIONS" target="GRAPH RAG">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG leads to substantial improvements over a naive RAG baseline for answering global sensemaking questions, as it uses a graph-based text index to improve the comprehensiveness and diversity of generated answers</data>
      <data key="d6">b149708d0b4ac3ff417565739ea6b03b</data>
    </edge>
    <edge source="GRAPH RAG" target="SOURCE TEXTS">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG is compared against Source Texts in terms of summarization and question answering effectiveness. Graph RAG generally provides a small but consistent improvement in answer comprehensiveness and diversity over source texts, especially with community summaries.</data>
      <data key="d6">71f14506a6b15dfabd93fd1606a67b73</data>
    </edge>
    <edge source="GRAPH RAG" target="SOURCE TEXT SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG requires fewer context tokens for both low-level (C3) and root-level (C0) community summaries compared to source text summarization, indicating a scalability advantage</data>
      <data key="d6">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </edge>
    <edge source="GRAPH RAG" target="LOW-LEVEL COMMUNITY SUMMARIES (C3)">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG requires 26-33% fewer context tokens for low-level community summaries (C3) compared to source text summarization, showing a scalability advantage</data>
      <data key="d6">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </edge>
    <edge source="GRAPH RAG" target="ROOT-LEVEL COMMUNITY SUMMARIES (C0)">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG requires over 97% fewer context tokens for root-level community summaries (C0) compared to source text summarization, demonstrating a significant scalability advantage</data>
      <data key="d6">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </edge>
    <edge source="GRAPH RAG" target="EMPOWERMENT">
      <data key="d4">1.0</data>
      <data key="d5">Empowerment comparisons showed mixed results for Graph RAG versus naive RAG and source text summarization, indicating that Graph RAG may need tuning to retain more details in the index to better empower users</data>
      <data key="d6">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </edge>
    <edge source="GRAPH RAG" target="PARALLEL GENERATION OF COMMUNITY ANSWERS">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG uses parallel generation of community answers as a technique for improving the efficiency and effectiveness of retrieval and generation. This relationship indicates that parallel generation of community answers is a key feature of Graph RAG, enabling faster and more accurate retrieval and generation.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="ITERATIVE RETRIEVAL-GENERATION STRATEGY">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG incorporates iterative retrieval-generation strategy as a method for continuous refinement and improvement of the retrieval and generation process. This relationship indicates that iterative retrieval-generation strategy is a core component of Graph RAG, allowing for iterative cycles of retrieval and generation.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="FEDERATED RETRIEVAL-GENERATION STRATEGY">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG uses federated retrieval-generation strategy as a method for sharing resources and information across multiple systems or nodes. This relationship indicates that federated retrieval-generation strategy is a key feature of Graph RAG, enabling distributed and collaborative retrieval and generation.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="MULTI-DOCUMENT SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG bears resemblance to multi-document summarization techniques, which are used for creating a concise and coherent summary of multiple documents. This relationship indicates that multi-document summarization is a related concept to Graph RAG, sharing similarities in the handling of multiple documents or data sources.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="MULTI-HOP QUESTION ANSWERING">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG is related to multi-hop question answering techniques, which are used for answering questions that require reasoning over multiple pieces of information or documents. This relationship indicates that multi-hop question answering is a related concept to Graph RAG, sharing similarities in the handling of complex questions and information retrieval.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="HIERARCHICAL INDEX">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG uses a hierarchical index as a data structure for organizing information in a hierarchical manner. This relationship indicates that hierarchical index is a key component of Graph RAG, improving the efficiency and effectiveness of information retrieval.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG incorporates summarization techniques for creating a concise and coherent summary of a document or set of documents. This relationship indicates that summarization is a related concept to Graph RAG, sharing similarities in the handling of document summarization and information retrieval.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="TREE OF CLARIFICATIONS">
      <data key="d4">1.0</data>
      <data key="d5">Tree of Clarifications and Graph RAG are both research studies that involve advanced information retrieval and analysis, with Tree of Clarifications focusing on ambiguous questions and Graph RAG on self-generated graph indexes.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="GRAPH RAG" target="SELFCHECKGPT">
      <data key="d4">1.0</data>
      <data key="d5">Comparing fabrication rates using approaches like SelfCheckGPT can improve the current analysis and potentially enhance the performance of Graph RAG</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH RAG" target="GRAPH INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The graph index is a key component in the Graph RAG approach, which consistently achieves the best head-to-head results against other methods</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH RAG" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the Graph RAG model</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="LARGE LANGUAGE MODELS (LLMS)" target="SCIENTIFIC DISCOVERY">
      <data key="d4">1.0</data>
      <data key="d5">Large language models are being used to automate human-like sensemaking in the domain of scientific discovery, where the process of making new and significant contributions to scientific knowledge is supported by the use of artificial intelligence</data>
      <data key="d6">b149708d0b4ac3ff417565739ea6b03b</data>
    </edge>
    <edge source="LARGE LANGUAGE MODELS (LLMS)" target="INTELLIGENCE ANALYSIS">
      <data key="d4">1.0</data>
      <data key="d5">Large language models are being used to automate human-like sensemaking in the domain of intelligence analysis, where the process of gathering, processing, and analyzing information to support decision-making is supported by the use of artificial intelligence</data>
      <data key="d6">b149708d0b4ac3ff417565739ea6b03b</data>
    </edge>
    <edge source="AUTOMATED SENSEMAKING" target="HUMAN-LED SENSEMAKING">
      <data key="d4">1.0</data>
      <data key="d5">Automated sensemaking complements human-led sensemaking by providing tools and techniques to support humans in understanding complex domains and refining their mental model of the data</data>
      <data key="d6">c7669e6a1add9a2829b09196256b1492</data>
    </edge>
    <edge source="ABSTRACTIVE SUMMARIZATION" target="EXTRACTIVE SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Abstractive Summarization is distinct from Extractive Summarization in that it generates new sentences to convey the meaning of the original text, whereas Extractive Summarization selects and reorders existing sentences from the source text. Abstractive Summarization is more flexible and can provide a more coherent summary, while Extractive Summarization is more literal and may not capture the essence of the text as effectively.</data>
      <data key="d6">85eff07c379a9dc24db0edb983acf3c9</data>
    </edge>
    <edge source="TRANSFORMER ARCHITECTURE" target="LLMS (LARGE LANGUAGE MODELS)">
      <data key="d4">1.0</data>
      <data key="d5">The Transformer Architecture is foundational to the development of LLMs (Large Language Models), which have shown substantial improvements in various natural language processing tasks, including summarization. The transformer's self-attention mechanism allows LLMs to process and generate text more effectively, making them highly capable in summarization tasks.</data>
      <data key="d6">85eff07c379a9dc24db0edb983acf3c9</data>
    </edge>
    <edge source="LLMS (LARGE LANGUAGE MODELS)" target="GPT (GENERATIVE PRE-TRAINED TRANSFORMER)">
      <data key="d4">1.0</data>
      <data key="d5">GPT (Generative Pre-trained Transformer) is a type of LLM (Large Language Model) that has been particularly effective in summarization tasks. It uses the transformer architecture to generate coherent and contextually relevant text, making it a powerful tool for summarization and other NLP tasks.</data>
      <data key="d6">85eff07c379a9dc24db0edb983acf3c9</data>
    </edge>
    <edge source="LLMS (LARGE LANGUAGE MODELS)" target="LLAMA">
      <data key="d4">1.0</data>
      <data key="d5">Llama is a type of LLM (Large Language Model) that is designed to handle complex language tasks, including summarization. It is based on the transformer architecture and can generate human-like text, making it a valuable resource for summarization and other NLP tasks.</data>
      <data key="d6">85eff07c379a9dc24db0edb983acf3c9</data>
    </edge>
    <edge source="LLMS (LARGE LANGUAGE MODELS)" target="GEMINI">
      <data key="d4">1.0</data>
      <data key="d5">Gemini is a type of LLM (Large Language Model) that is capable of understanding and generating text, making it suitable for various NLP tasks, including summarization. It is based on the transformer architecture and can process and generate text effectively, making it a powerful tool for summarization tasks.</data>
      <data key="d6">85eff07c379a9dc24db0edb983acf3c9</data>
    </edge>
    <edge source="BROWN ET AL., 2020" target="LLAMA (TOUVRON ET AL., 2023)">
      <data key="d4">1.0</data>
      <data key="d5">Brown et al., 2020, and Llama (Touvron et al., 2023) are related in that they both discuss the use of in-context learning for summarization tasks within the context window of LLMs</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="LLAMA (TOUVRON ET AL., 2023)" target="GEMINI (ANIL ET AL., 2023)">
      <data key="d4">1.0</data>
      <data key="d5">Llama (Touvron et al., 2023) and Gemini (Anil et al., 2023) are related in that they both explore the application of in-context learning for summarization tasks, focusing on different series that can summarize content within the context window of LLMs</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="IN-CONTEXT LEARNING" target="QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">In-context learning is related to query-focused abstractive summarization in that it is a technique used by LLMs to adapt their responses based on the context provided, which is essential for generating summaries in response to specific queries</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION" target="LLM CONTEXT WINDOW LIMITS">
      <data key="d4">1.0</data>
      <data key="d5">Query-focused abstractive summarization is constrained by LLM context window limits, as the volumes of text in entire corpora can greatly exceed the processing capacity of LLMs</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION" target="NAIVE RAG (RETRIEVAL-AUGMENTED GENERATION)">
      <data key="d4">1.0</data>
      <data key="d5">Naive RAG (Retrieval-Augmented Generation) is related to query-focused abstractive summarization in that it is a technique that directly retrieves text chunks for summarization, which may not be sufficient for summarization tasks over large corpora</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION" target="GRAPH RAG APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">The Graph RAG approach is related to query-focused abstractive summarization in that it is a technique based on global summarization of an LLM-derived knowledge graph, which can address the limitations of naive RAG for summarization tasks over large corpora</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="LLM CONTEXT WINDOW LIMITS" target="INFORMATION LOSS IN LONGER CONTEXTS">
      <data key="d4">1.0</data>
      <data key="d5">LLM context window limits are related to information loss in longer contexts, as the middle part of very long texts may not receive adequate attention, leading to potential loss of information</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="NAIVE RAG (RETRIEVAL-AUGMENTED GENERATION)" target="ENTITY-BASED GRAPH INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The Entity-based Graph Index achieves superior performance compared to Naive RAG (Retrieval-Augmented Generation), offering a more efficient and effective data index for root-level communities and global queries.</data>
      <data key="d6">e31d2d134cf501c93f9445914d7350f9</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="STRUCTURED RETRIEVAL AND TRAVERSAL">
      <data key="d4">1.0</data>
      <data key="d5">Structured retrieval and traversal is related to the Graph RAG approach in that it exploits the structured nature of graph indexes for efficient information retrieval and navigation, which is a key component of the Graph RAG approach</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="GRAPH MODULARITY">
      <data key="d4">1.0</data>
      <data key="d5">Graph modularity is related to the Graph RAG approach in that it is a property of graphs that allows them to be partitioned into modular communities of closely-related nodes, which is a key aspect of the Graph RAG approach</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="COMMUNITY DETECTION ALGORITHMS">
      <data key="d4">1.0</data>
      <data key="d5">Community detection algorithms are related to the Graph RAG approach in that they are techniques used to identify groups of closely-related nodes in a graph, which can be useful for summarization tasks within the Graph RAG approach</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="TEXT CHUNKS">
      <data key="d4">1.0</data>
      <data key="d5">The Graph RAG approach involves processing text chunks extracted from source documents. The granularity of these chunks affects the efficiency and quality of the information retrieval and augmentation process in the Graph RAG technique.</data>
      <data key="d6">d2399fd0aae5bd200639806ca87184f8</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="EMBEDDING-BASED MATCHING">
      <data key="d4">1.0</data>
      <data key="d5">The Graph RAG approach utilizes embedding-based matching as one of its methods to refine and adapt the current approach by operating in a more local manner, matching user queries and graph annotations</data>
      <data key="d6">5e2933c9646c751e6a60c9de12a255f2</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="HYBRID RAG SCHEMES">
      <data key="d4">1.0</data>
      <data key="d5">The Graph RAG approach can be enhanced by implementing hybrid RAG schemes that combine embedding-based matching against community reports before employing map-reduce summarization mechanisms, improving the comprehensiveness and diversity of answers</data>
      <data key="d6">5e2933c9646c751e6a60c9de12a255f2</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="MAP-REDUCE SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The Graph RAG approach uses map-reduce summarization as a mechanism to summarize large amounts of data, which is particularly useful for global queries over the same dataset, providing summaries of root-level communities in the entity-based graph index</data>
      <data key="d6">5e2933c9646c751e6a60c9de12a255f2</data>
    </edge>
    <edge source="STRUCTURED RETRIEVAL AND TRAVERSAL" target="LLM-DERIVED KNOWLEDGE GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The LLM-derived knowledge graph is related to structured retrieval and traversal because the graph structure facilitates efficient and effective information retrieval and navigation. The inherent modularity of the graph and the use of community detection algorithms enable the identification of relevant information for specific queries.</data>
      <data key="d6">d39abd5380fb3fe0468ea1e122512091</data>
    </edge>
    <edge source="COMMUNITY DETECTION ALGORITHMS" target="MODULARITY">
      <data key="d4">1.0</data>
      <data key="d5">Modularity is related to community detection algorithms because these algorithms are designed to identify and optimize the modularity of a graph by partitioning it into communities of closely-related nodes. The algorithms aim to maximize the modularity score, which measures the quality of the partitioning.</data>
      <data key="d6">d39abd5380fb3fe0468ea1e122512091</data>
    </edge>
    <edge source="COMMUNITY DETECTION ALGORITHMS" target="ENTITY GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">Community Detection Algorithms are applied to the entity graph to partition it into communities of closely-related entities</data>
      <data key="d6">6dace8e490674ac8e031aed987a63789</data>
    </edge>
    <edge source="COMMUNITY DETECTION ALGORITHMS" target="LEIDEN ALGORITHM">
      <data key="d4">1.0</data>
      <data key="d5">The Leiden Algorithm is a type of Community Detection Algorithm that is specifically used in the text for its ability to efficiently recover the hierarchical community structure of large-scale graphs</data>
      <data key="d6">a660289d2bf43f25d3524d35cd2d9a96</data>
    </edge>
    <edge source="LOUVAIN" target="LEIDEN">
      <data key="d4">1.0</data>
      <data key="d5">Louvain is related to Leiden because Leiden is an extension of the Louvain algorithm for community detection. It builds upon the Louvain algorithm by adding a refinement step that allows for the detection of smaller communities, improving the resolution and stability of the partitioning.</data>
      <data key="d6">d39abd5380fb3fe0468ea1e122512091</data>
    </edge>
    <edge source="QUERY-FOCUSED SUMMARIZATION" target="MAP-REDUCE APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">Query-focused summarization is related to the map-reduce approach because the map-reduce approach can be used to implement query-focused summarization on large datasets. The map step involves using each community summary to answer the query independently and in parallel, while the reduce step summarizes all relevant partial answers into a final global answer.</data>
      <data key="d6">d39abd5380fb3fe0468ea1e122512091</data>
    </edge>
    <edge source="ACTIVITY-CENTERED SENSEMAKING QUESTIONS" target="PODCAST TRANSCRIPTS">
      <data key="d4">1.0</data>
      <data key="d5">Activity-centered sensemaking questions are related to podcast transcripts because the questions can be derived from short descriptions of podcast transcripts to help users understand the content and context of the podcasts. The questions are intended to guide the user in making sense of the underlying data and its implications.</data>
      <data key="d6">d39abd5380fb3fe0468ea1e122512091</data>
    </edge>
    <edge source="ACTIVITY-CENTERED SENSEMAKING QUESTIONS" target="NEWS ARTICLES">
      <data key="d4">1.0</data>
      <data key="d5">Activity-centered sensemaking questions are related to news articles because the questions can be derived from short descriptions of news articles to help users understand the events and issues reported in the news. The questions are intended to guide the user in making sense of the underlying data and its implications.</data>
      <data key="d6">d39abd5380fb3fe0468ea1e122512091</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The dataset is related to podcast transcripts as they are a specific type of data within the dataset. Podcast transcripts are used for understanding tech leaders' views on policy and regulation.</data>
      <data key="d6">aed2ea39de8a027cc818c7f4557f0514</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="TECH JOURNALIST">
      <data key="d4">1.0</data>
      <data key="d5">A tech journalist is related to podcast transcripts as they use these transcripts to find insights and trends in the tech industry, specifically focusing on tech leaders' views on policy and regulation.</data>
      <data key="d6">aed2ea39de8a027cc818c7f4557f0514</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="NEWS ARTICLES">
      <data key="d4">1.0</data>
      <data key="d5">Both Podcast Transcripts and News Articles are datasets selected for evaluation in the one million token range, representative of the kind of corpora that users may encounter in their real-world activities</data>
      <data key="d6">5d04129d46662571f635a4e63cb4d6b7</data>
    </edge>
    <edge source="NEWS ARTICLES" target="DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The dataset is related to news articles as they are a specific type of data within the dataset. News articles are used for teaching about health and wellness.</data>
      <data key="d6">aed2ea39de8a027cc818c7f4557f0514</data>
    </edge>
    <edge source="NEWS ARTICLES" target="EDUCATOR">
      <data key="d4">2.0</data>
      <data key="d5">An educator is related to news articles as they use these articles to incorporate current affairs into curricula, particularly for teaching about health and wellness.
News articles can be used by educators to incorporate current affairs into curricula, enhancing students' understanding of various topics</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc,aed2ea39de8a027cc818c7f4557f0514</data>
    </edge>
    <edge source="NEWS ARTICLES" target="CURRENT POLICIES">
      <data key="d4">1.0</data>
      <data key="d5">News articles often discuss current policies and their implications, providing insights into how policies are perceived and how they might evolve</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="DIVERSE ACTIVITY-CENTERED SENSE-MAKING QUESTIONS" target="REAL-WORLD DATASETS">
      <data key="d4">1.0</data>
      <data key="d5">Diverse activity-centered sense-making questions are generated from real-world datasets, which include podcast transcripts and news articles. These datasets provide the context and content necessary for developing questions that aim to be comprehensive, diverse, and empowering.</data>
      <data key="d6">d2399fd0aae5bd200639806ca87184f8</data>
    </edge>
    <edge source="REAL-WORLD DATASETS" target="COMPREHENSIVENESS, DIVERSITY, EMPOWERMENT">
      <data key="d4">1.0</data>
      <data key="d5">Real-world datasets are used to create sense-making questions that aim to be comprehensive, diverse, and empowering. The datasets serve as the foundation for developing questions that foster understanding of broad issues and themes.</data>
      <data key="d6">d2399fd0aae5bd200639806ca87184f8</data>
    </edge>
    <edge source="COMPREHENSIVENESS, DIVERSITY, EMPOWERMENT" target="HIERARCHICAL LEVEL OF COMMUNITY SUMMARIES">
      <data key="d4">1.0</data>
      <data key="d5">The target qualities of comprehensiveness, diversity, and empowerment are assessed by exploring the impact of varying the hierarchical level of community summaries used to answer queries. This relationship helps in evaluating the effectiveness of different summarization techniques in meeting these criteria.</data>
      <data key="d6">d2399fd0aae5bd200639806ca87184f8</data>
    </edge>
    <edge source="HIERARCHICAL LEVEL OF COMMUNITY SUMMARIES" target="NAIVE RAG">
      <data key="d4">1.0</data>
      <data key="d5">The hierarchical level of community summaries is compared to naive RAG to evaluate its performance in terms of comprehensiveness and diversity. This comparison helps in understanding the advantages and limitations of each approach in summarization tasks.</data>
      <data key="d6">d2399fd0aae5bd200639806ca87184f8</data>
    </edge>
    <edge source="HIERARCHICAL LEVEL OF COMMUNITY SUMMARIES" target="GLOBAL MAP-REDUCE SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The hierarchical level of community summaries is compared to global map-reduce summarization to assess its effectiveness in providing comprehensive and diverse insights. This comparison is crucial for determining the best summarization technique based on the target qualities.</data>
      <data key="d6">d2399fd0aae5bd200639806ca87184f8</data>
    </edge>
    <edge source="NAIVE RAG" target="MAP-REDUCE SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Naive RAG and Map-Reduce Summarization are related in the context of summarization and question answering, as both are methods used to process and generate responses from source texts. However, Naive RAG is noted for its directness and simplicity, whereas Map-Reduce Summarization is more resource-intensive.</data>
      <data key="d6">71f14506a6b15dfabd93fd1606a67b73</data>
    </edge>
    <edge source="NAIVE RAG" target="ADVANCED RAG">
      <data key="d4">1.0</data>
      <data key="d5">Advanced RAG builds upon the foundational principles of Naive RAG, addressing its limitations by incorporating more sophisticated retrieval and generation strategies, leading to improved information retrieval and context augmentation</data>
      <data key="d6">38feec52b8bfbd3fd8e03635acdaec97</data>
    </edge>
    <edge source="TEXT CHUNKS" target="SOURCE DOCUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">Source Documents are split into Text Chunks for processing by LLM prompts. The granularity of this split affects the efficiency and effectiveness of information extraction</data>
      <data key="d6">e7caf4256ddea71533af1c4c50444146</data>
    </edge>
    <edge source="TEXT CHUNKS" target="LLM PROMPTS">
      <data key="d4">1.0</data>
      <data key="d5">Text Chunks are processed by LLM Prompts to extract graph index elements. The size of the text chunks impacts the number of LLM calls and the recall of the extraction process</data>
      <data key="d6">e7caf4256ddea71533af1c4c50444146</data>
    </edge>
    <edge source="TEXT CHUNKS" target="ENTITY REFERENCES">
      <data key="d4">1.0</data>
      <data key="d5">The number of Entity References extracted is influenced by the size of the Text Chunks. Smaller chunks tend to extract more references, but the process needs to balance recall and precision</data>
      <data key="d6">e7caf4256ddea71533af1c4c50444146</data>
    </edge>
    <edge source="TEXT CHUNKS" target="GRAPH NODES">
      <data key="d4">1.0</data>
      <data key="d5">Text Chunks are the source from which Graph Nodes are identified and extracted. The relationship indicates that the processing of text chunks leads to the identification of entities represented as graph nodes.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="TEXT CHUNKS" target="GRAPH EDGES">
      <data key="d4">1.0</data>
      <data key="d5">Text Chunks are the source from which Graph Edges are identified and extracted. The relationship indicates that the processing of text chunks leads to the identification of relationships between entities, represented as graph edges.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="TEXT CHUNKS" target="LLM PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Prompt is used to process Text Chunks for the identification of entities and relationships. The relationship indicates that the prompt guides the extraction of information from text chunks.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="LLM PROMPTS" target="GRAPH INDEX">
      <data key="d4">1.0</data>
      <data key="d5">LLM Prompts are used to construct the Graph Index by identifying and extracting instances of graph nodes and edges from the Text Chunks</data>
      <data key="d6">e7caf4256ddea71533af1c4c50444146</data>
    </edge>
    <edge source="GRAPH INDEX" target="PODCAST DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The Graph Index was created using a context window size of 600 tokens with 1 gleaning for the Podcast dataset, indicating that the Podcast dataset was used in the indexing process.</data>
      <data key="d6">53455f8552b0787cb13c5a03eb550842</data>
    </edge>
    <edge source="GRAPH INDEX" target="NEWS DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The Graph Index was created using a context window size of 600 tokens with 0 gleanings for the News dataset, indicating that the News dataset was used in the indexing process.</data>
      <data key="d6">53455f8552b0787cb13c5a03eb550842</data>
    </edge>
    <edge source="GRAPH INDEX" target="GLOBAL SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The decision to invest in building a graph index depends on multiple factors, including the performance of global summarization of source texts, which can be competitive in many cases</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH INDEX" target="COMPUTE BUDGET">
      <data key="d4">1.0</data>
      <data key="d5">The decision to invest in building a graph index depends on the compute budget, as it can influence the feasibility of creating and maintaining the index</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH INDEX" target="LIFETIME QUERIES">
      <data key="d4">1.0</data>
      <data key="d5">The decision to invest in building a graph index depends on the expected number of lifetime queries per dataset, as it can affect the value obtained from the index</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH INDEX" target="GRAPH-RELATED RAG APPROACHES">
      <data key="d4">1.0</data>
      <data key="d5">The decision to invest in building a graph index depends on the value obtained from other aspects of the graph index, including the use of other graph-related RAG approaches</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH INDEX" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the graph index</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH NODES" target="COVARIATE PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The Covariate Prompt is used to associate additional attributes with Graph Nodes. The relationship indicates that the prompt helps in enriching the information associated with the nodes extracted from the text.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="LLM PROMPT" target="FEW-SHOT EXAMPLES">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Prompt utilizes Few-Shot Examples for in-context learning, enhancing its ability to identify entities and relationships specific to the domain of the document corpus.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="LLM PROMPT" target="NAMED ENTITIES">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Prompt is used to identify Named Entities within the text chunks. The relationship indicates that the prompt facilitates the extraction of named entities from the text.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="LLM PROMPT" target="SPECIALIZED KNOWLEDGE">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Prompt can be tailored to incorporate Specialized Knowledge by using domain-specific few-shot examples. The relationship indicates that the prompt can be adapted to better handle specialized knowledge.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="ELEMENT SUMMARIES" target="LLM (LARGE LANGUAGE MODEL)">
      <data key="d4">1.0</data>
      <data key="d5">The LLM generates Element Summaries by processing Element Instances, creating condensed representations of information for each graph element, such as entity nodes, relationship edges, and claim covariates</data>
      <data key="d6">a73d3e7b661743b7583d8a0fd412b6a7</data>
    </edge>
    <edge source="ELEMENT SUMMARIES" target="ELEMENT INSTANCES">
      <data key="d4">1.0</data>
      <data key="d5">Element Summaries are derived from Element Instances, as the LLM processes these instances to create condensed representations of information for each graph element</data>
      <data key="d6">a73d3e7b661743b7583d8a0fd412b6a7</data>
    </edge>
    <edge source="ENTITY GRAPH" target="CONNECTIVITY">
      <data key="d4">1.0</data>
      <data key="d5">Connectivity is essential for the resilience of the entity graph approach to variations in entity names, ensuring that all variations are connected to a shared set of closely-related entities</data>
      <data key="d6">6dace8e490674ac8e031aed987a63789</data>
    </edge>
    <edge source="ENTITY GRAPH" target="RICH DESCRIPTIVE TEXT">
      <data key="d4">1.0</data>
      <data key="d5">Rich Descriptive Text is used in the entity graph to provide detailed information about homogeneous nodes, aligning with the capabilities of LLMs</data>
      <data key="d6">6dace8e490674ac8e031aed987a63789</data>
    </edge>
    <edge source="ENTITY GRAPH" target="KNOWLEDGE GRAPHS">
      <data key="d4">1.0</data>
      <data key="d5">The entity graph is differentiated from typical knowledge graphs by its use of rich descriptive text and its focus on global, query-focused summarization</data>
      <data key="d6">6dace8e490674ac8e031aed987a63789</data>
    </edge>
    <edge source="ENTITY GRAPH" target="LEIDEN ALGORITHM">
      <data key="d4">1.0</data>
      <data key="d5">The Leiden Algorithm is used in the entity graph pipeline for community detection, due to its efficiency in handling large-scale graphs</data>
      <data key="d6">6dace8e490674ac8e031aed987a63789</data>
    </edge>
    <edge source="LEIDEN ALGORITHM" target="GRAPHCOMMUNITIES">
      <data key="d4">1.0</data>
      <data key="d5">The Leiden Algorithm is used to generate GraphCommunities by partitioning the graph into communities of nodes with stronger connections to one another than to the other nodes in the graph</data>
      <data key="d6">a660289d2bf43f25d3524d35cd2d9a96</data>
    </edge>
    <edge source="CHUNKS OF PRE-SPECIFIED TOKEN SIZE" target="INTERMEDIATE ANSWERS">
      <data key="d4">1.0</data>
      <data key="d5">Chunks of pre-specified token size are related to intermediate answers as they are the input for generating these answers. The process ensures that each chunk is analyzed independently to produce relevant intermediate answers.</data>
      <data key="d6">aed2ea39de8a027cc818c7f4557f0514</data>
    </edge>
    <edge source="INTERMEDIATE ANSWERS" target="GLOBAL ANSWER">
      <data key="d4">1.0</data>
      <data key="d5">Intermediate answers are related to the global answer as they are sorted and combined to create the final context used for generating the global answer. The helpfulness score of each intermediate answer determines its inclusion in the global answer.</data>
      <data key="d6">aed2ea39de8a027cc818c7f4557f0514</data>
    </edge>
    <edge source="DATASET" target="RUN_PIPELINE">
      <data key="d4">1.0</data>
      <data key="d5">run_pipeline function takes the dataset as input and processes it according to the workflows specified</data>
      <data key="d6">f3a07680cbe8ab1f6055369da05f4f38</data>
    </edge>
    <edge source="EDUCATOR" target="HEALTH LITERACY">
      <data key="d4">1.0</data>
      <data key="d5">Educators can use health articles and current affairs to highlight the importance of health literacy, teaching students how to understand and use health information effectively</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="PRIVACY LAWS" target="TECHNOLOGY DEVELOPMENT">
      <data key="d4">1.0</data>
      <data key="d5">Privacy laws can impact technology development by setting restrictions on data usage and influencing the design and implementation of products and services</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="TECHNOLOGY DEVELOPMENT" target="ETHICAL CONSIDERATIONS">
      <data key="d4">1.0</data>
      <data key="d5">Technology development is influenced by ethical considerations, which ensure that innovations are developed and used responsibly, considering the impact on individuals and society</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="COLLABORATIONS" target="TECH COMPANIES">
      <data key="d4">1.0</data>
      <data key="d5">Collaborations between tech companies and governments can lead to the development of new technologies, policies, or solutions that benefit society</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="HEALTH EDUCATION CURRICULA" target="PREVENTIVE MEDICINE">
      <data key="d4">1.0</data>
      <data key="d5">Health education curricula can include topics on preventive medicine, teaching students about practices that prevent diseases and promote health</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="HEALTH ARTICLES" target="PUBLIC HEALTH PRIORITIES">
      <data key="d4">1.0</data>
      <data key="d5">Health articles can provide insights into public health priorities based on the topics they cover and the emphasis they place on certain health issues</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="HOTPOTQA" target="MULTIHOP-RAG">
      <data key="d4">1.0</data>
      <data key="d5">HotPotQA, MultiHop-RAG, and MT-Bench are all benchmark datasets for open-domain question answering, with a focus on explicit fact retrieval rather than summarization for data sensemaking</data>
      <data key="d6">5d04129d46662571f635a4e63cb4d6b7</data>
    </edge>
    <edge source="HOTPOTQA" target="DATA SENSEMAKING">
      <data key="d4">1.0</data>
      <data key="d5">HotPotQA, as a benchmark dataset for open-domain question answering, does not target data sensemaking as its questions focus on explicit fact retrieval rather than summarization for understanding data</data>
      <data key="d6">8e69f04648f5fc24c299591365f1aa68</data>
    </edge>
    <edge source="MULTIHOP-RAG" target="DATA SENSEMAKING">
      <data key="d4">1.0</data>
      <data key="d5">MultiHop-RAG, as a benchmark dataset for open-domain question answering, does not target data sensemaking as its questions focus on explicit fact retrieval rather than summarization for understanding data</data>
      <data key="d6">8e69f04648f5fc24c299591365f1aa68</data>
    </edge>
    <edge source="MT-BENCH" target="DATA SENSEMAKING">
      <data key="d4">1.0</data>
      <data key="d5">MT-Bench, as a benchmark dataset for open-domain question answering, does not target data sensemaking as its questions focus on explicit fact retrieval rather than summarization for understanding data</data>
      <data key="d6">8e69f04648f5fc24c299591365f1aa68</data>
    </edge>
    <edge source="DATA SENSEMAKING" target="SUMMARIZATION QUERIES">
      <data key="d4">1.0</data>
      <data key="d5">Summarization queries are essential for data sensemaking as they aim to extract a high-level understanding of dataset contents, facilitating the process of inspecting, engaging with, and contextualizing data within real-world activities</data>
      <data key="d6">8e69f04648f5fc24c299591365f1aa68</data>
    </edge>
    <edge source="SUMMARIZATION QUERIES" target="RAG (RETRIEVAL-AUGMENTED GENERATION) SYSTEMS">
      <data key="d4">1.0</data>
      <data key="d5">RAG systems are evaluated using summarization queries to assess their effectiveness in global sensemaking tasks, where a high-level understanding of dataset contents is required rather than details of specific texts</data>
      <data key="d6">8e69f04648f5fc24c299591365f1aa68</data>
    </edge>
    <edge source="EVALUATION DATASETS" target="GRAPH RAG (RETRIEVAL-AUGMENTED GENERATION)">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG uses summaries from different levels of graph communities to answer queries in the Evaluation Datasets</data>
      <data key="d6">a739018eb63cbb6c26b779bd37afc233</data>
    </edge>
    <edge source="EVALUATION DATASETS" target="TEXT SUMMARIZATION METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The Text Summarization Method is applied to the Evaluation Datasets to generate summaries for comparison</data>
      <data key="d6">a739018eb63cbb6c26b779bd37afc233</data>
    </edge>
    <edge source="EVALUATION DATASETS" target="NAIVE SEMANTIC SEARCH RAG APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">The Naive Semantic Search RAG Approach is used on the Evaluation Datasets as a baseline for comparison</data>
      <data key="d6">a739018eb63cbb6c26b779bd37afc233</data>
    </edge>
    <edge source="C1" target="C2">
      <data key="d4">1.0</data>
      <data key="d5">C1 communities are higher-level summaries that can be projected down to form C2 communities, which are sub-communities of C1 if C1 is present. This relationship indicates a hierarchical structure in the community summaries.</data>
      <data key="d6">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </edge>
    <edge source="C1" target="C0">
      <data key="d4">1.0</data>
      <data key="d5">C0 communities can be projected down to form C1 communities if C0 is present. This relationship indicates a hierarchical structure in the community summaries, with C1 being a sub-community of C0.</data>
      <data key="d6">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </edge>
    <edge source="C2" target="C3">
      <data key="d4">1.0</data>
      <data key="d5">C2 communities are higher-level summaries that can be projected down to form C3 communities, which are sub-communities of C2 if C2 is present. This relationship indicates a hierarchical structure in the community summaries.</data>
      <data key="d6">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </edge>
    <edge source="PODCAST DATASET" target="NEWS DATASET">
      <data key="d4">2.0</data>
      <data key="d5">The Podcast Dataset and the News Dataset are related as they are both datasets used in the research to evaluate the performance of language models under varying context window sizes. The News Dataset is larger than the Podcast Dataset, with more nodes and edges in the graph created through the indexing process.
The Podcast Dataset and the News Dataset are related in the context of summarization and question answering experiments. Both datasets are used to evaluate the effectiveness of summarization methods, with specific levels of community summaries achieving notable results in terms of comprehensiveness and diversity.</data>
      <data key="d6">3900d15a5f3ace358fc06038c34cdf79,71f14506a6b15dfabd93fd1606a67b73</data>
    </edge>
    <edge source="PODCAST DATASET" target="WINDOWSIZE">
      <data key="d4">1.0</data>
      <data key="d5">The windowsize of 8k tokens is used in the indexing process for the Podcast Dataset, affecting the resulting graph's structure and size</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="NEWS DATASET" target="WINDOWSIZE">
      <data key="d4">1.0</data>
      <data key="d5">The windowsize of 8k tokens is used in the indexing process for the News Dataset, affecting the resulting graph's structure and size</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="WANG ET AL., 2023A" target="LLM AS-A-JUDGE, ZHENG ET AL., 2024">
      <data key="d4">1.0</data>
      <data key="d5">Wang et al., 2023a, and LLM as-a-judge, Zheng et al., 2024, are related through their contributions to the field of LLMs, particularly in the context of head-to-head comparisons of competing outputs</data>
      <data key="d6">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </edge>
    <edge source="RAGAS, ES ET AL., 2023" target="GRAPH RAG MECHANISM">
      <data key="d4">1.0</data>
      <data key="d5">RAGAS, Es et al., 2023, and the Graph RAG mechanism are related in the context of evaluating RAG systems, where the Graph RAG mechanism is a multi-stage method that could potentially be assessed using the evaluation criteria discussed in RAGAS, Es et al., 2023</data>
      <data key="d6">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </edge>
    <edge source="HEAD-TO-HEAD COMPARISON APPROACH" target="LLM EVALUATOR">
      <data key="d4">1.0</data>
      <data key="d5">The head-to-head comparison approach is related to the LLM evaluator, as the LLM evaluator is used as a tool to implement the head-to-head comparison approach in evaluating different methods or outputs</data>
      <data key="d6">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </edge>
    <edge source="LLM EVALUATOR" target="COMPREHENSIVENESS">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Evaluator uses Comprehensiveness as one of the metrics to assess the quality of answers. It evaluates how well the answers cover all aspects and details of the question.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="LLM EVALUATOR" target="DIVERSITY">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Evaluator uses Diversity as one of the metrics to assess the quality of answers. It evaluates how varied and rich the answers are in providing different perspectives and insights on the question.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="LLM EVALUATOR" target="EMPOWERMENT">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Evaluator uses Empowerment as one of the metrics to assess the quality of answers. It evaluates how well the answers help the reader understand and make informed judgements about the topic.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="LLM EVALUATOR" target="DIRECTNESS">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Evaluator uses Directness as one of the metrics to assess the quality of answers. It evaluates how specifically and clearly the answers address the question.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="LLM EVALUATOR" target="TABLE 2">
      <data key="d4">1.0</data>
      <data key="d5">Table 2 is an output of the LLM Evaluator, showing an example of the assessment generated by the tool based on the metrics and conditions evaluated.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="TARGET METRICS" target="CONTROL METRIC (DIRECTNESS)">
      <data key="d4">1.0</data>
      <data key="d5">The target metrics are related to the control metric (directness) in the context of evaluation, as the control metric serves as a baseline for assessing the validity of the target metrics in evaluating the performance of methods or outputs</data>
      <data key="d6">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </edge>
    <edge source="COMPREHENSIVENESS" target="OPTIMUM CONTEXT SIZE">
      <data key="d4">1.0</data>
      <data key="d5">The optimum context size (8k) was found to have the best performance on the comprehensiveness metric, with an average win rate of 58.1%. This indicates that the 8k context window size provides the most comprehensive answers or responses among the tested sizes.</data>
      <data key="d6">3900d15a5f3ace358fc06038c34cdf79</data>
    </edge>
    <edge source="COMPREHENSIVENESS" target="GLOBAL APPROACHES">
      <data key="d4">1.0</data>
      <data key="d5">Global Approaches achieved higher comprehensiveness win rates for both Podcast transcripts and News articles</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="DIVERSITY" target="OPTIMUM CONTEXT SIZE">
      <data key="d4">1.0</data>
      <data key="d5">The optimum context size (8k) was found to have comparable performance on the diversity metric, with an average win rate of 52.4%. This indicates that the 8k context window size provides answers or responses with a range and uniqueness similar to those generated with larger context sizes.</data>
      <data key="d6">3900d15a5f3ace358fc06038c34cdf79</data>
    </edge>
    <edge source="DIVERSITY" target="GLOBAL APPROACHES">
      <data key="d4">1.0</data>
      <data key="d5">Global Approaches achieved higher diversity win rates for both Podcast transcripts and News articles</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="EMPOWERMENT" target="OPTIMUM CONTEXT SIZE">
      <data key="d4">1.0</data>
      <data key="d5">The optimum context size (8k) was found to have comparable performance on the empowerment metric, with an average win rate of 51.3%. This indicates that the 8k context window size provides answers or responses that are as empowering as those generated with larger context sizes.</data>
      <data key="d6">3900d15a5f3ace358fc06038c34cdf79</data>
    </edge>
    <edge source="DIRECTNESS" target="NAIVE RAG (SS) APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">The Naive RAG (SS) Approach produced the most direct responses across all comparisons, as measured by the directness validity test</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="COMPARISON METHOD" target="STOCHASTICITY">
      <data key="d4">1.0</data>
      <data key="d5">The Comparison Method accounts for the stochasticity of LLMs by running each comparison five times and using mean scores to determine the outcome.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="COMPARISON METHOD" target="FIGURE 4">
      <data key="d4">1.0</data>
      <data key="d5">Figure 4 visualizes the results of the head-to-head comparisons conducted using the Comparison Method, showing the win rate percentages across different conditions and metrics.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="HEADWINRATEPERCENTAGES" target="GRAPHRAGCONDITIONS">
      <data key="d4">1.0</data>
      <data key="d5">The headwin rate percentages are related to the performance of GraphRAG conditions, as they measure the success rates of these conditions across datasets, metrics, and questions. GraphRAG conditions outperformed naive RAG on comprehensiveness and diversity.</data>
      <data key="d6">b83d819b03401fb8332316960610e5d6</data>
    </edge>
    <edge source="GRAPHRAGCONDITIONS" target="CONTEXTWINDOWSIZE">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG conditions are related to the context window size, as varying the context window size can affect the performance of GraphRAG conditions. The optimum context size for the baseline condition (SS) was determined and then used for all query-time LLM use, which can impact the performance of GraphRAG conditions.</data>
      <data key="d6">b83d819b03401fb8332316960610e5d6</data>
    </edge>
    <edge source="VARYING CONTEXT WINDOW SIZE" target="OPTIMUM CONTEXT SIZE">
      <data key="d4">1.0</data>
      <data key="d5">The relationship between varying the context window size and determining the optimum context size is that the optimum context size was found through testing different context window sizes (8k, 16k, 32k, 64k) and evaluating their performance on comprehensiveness, diversity, and empowerment metrics. The smallest context window size (8k) was found to be universally better for comprehensiveness and comparable for diversity and empowerment, leading to its selection as the optimum context size.</data>
      <data key="d6">3900d15a5f3ace358fc06038c34cdf79</data>
    </edge>
    <edge source="GLOBAL APPROACHES" target="NAIVE RAG (SS) APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">Global Approaches outperformed the Naive RAG (SS) Approach in terms of comprehensiveness and diversity metrics across datasets</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="INFORMED UNDERSTANDING" target="TUNING ELEMENT EXTRACTION PROMPTS">
      <data key="d4">1.0</data>
      <data key="d5">Tuning Element Extraction Prompts can contribute to a higher level of Informed Understanding by ensuring that more detailed information is retained in the Graph RAG index, thus providing a richer context for comprehension</data>
      <data key="d6">38feec52b8bfbd3fd8e03635acdaec97</data>
    </edge>
    <edge source="GRAPH RAG INDEX" target="RAG APPROACHES">
      <data key="d4">1.0</data>
      <data key="d5">RAG Approaches, including Naive RAG and Advanced RAG, are integral to the functionality of the Graph RAG Index, as they determine how information is retrieved and integrated into the index to support informed understanding</data>
      <data key="d6">38feec52b8bfbd3fd8e03635acdaec97</data>
    </edge>
    <edge source="ADVANCED RAG" target="CAUSAL GRAPH EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Causal Graph Extraction and Advanced RAG are both research studies that involve the use of LLMs in the context of advanced information retrieval and analysis, suggesting a shared interest in graph-based techniques.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="ADVANCED RAG" target="KAPING">
      <data key="d4">1.0</data>
      <data key="d5">Advanced RAG and KAPING are both research studies that involve the use of knowledge graphs as an index for advanced retrieval and analysis of information, indicating a connection in their research focus and methodology.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="ADVANCED RAG" target="LLMS FOR CAUSAL GRAPH EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">LLMs for causal graph extraction and Advanced RAG are related as they both involve the use of LLMs for extracting information from text, with causal graph extraction focusing on causal relationships and Advanced RAG focusing on retrieval-augmented generation</data>
      <data key="d6">7383c69e93bb8c8648181f5355d2c9a7</data>
    </edge>
    <edge source="LLM&#8217;S CONTEXT WINDOW" target="ADVANCED RAG SYSTEMS">
      <data key="d4">1.0</data>
      <data key="d5">Advanced RAG systems are designed to overcome the limitations of LLM&#8217;s context window by incorporating pre-retrieval, retrieval, and post-retrieval strategies. This relationship indicates that Advanced RAG systems are an evolution of LLM&#8217;s context window, addressing its drawbacks and enhancing its capabilities.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="ADVANCED RAG SYSTEMS" target="MODULAR RAG SYSTEMS">
      <data key="d4">1.0</data>
      <data key="d5">Modular RAG systems are a type of Advanced RAG systems that include patterns for iterative and dynamic cycles of interleaved retrieval and generation. This relationship indicates that Modular RAG systems are a specific implementation of Advanced RAG systems, focusing on modularity and flexibility.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="CAIRE-COVID" target="ITRG">
      <data key="d4">1.0</data>
      <data key="d5">Both CAiRE-COVID and ITRG are research studies that involve the use of LLMs and RAG, but CAiRE-COVID focuses on the impact of COVID-19, while ITRG deals with multi-hop question answering.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="ITRG" target="IR-COT">
      <data key="d4">1.0</data>
      <data key="d5">ITRG and IR-CoT are both research studies that deal with multi-hop question answering, indicating a connection in their research focus and methodology.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="IR-COT" target="DSP">
      <data key="d4">1.0</data>
      <data key="d5">IR-CoT and DSP are both research studies that deal with multi-hop question answering, suggesting a shared interest in advanced question answering techniques.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="RAPTOR" target="TREE OF CLARIFICATIONS">
      <data key="d4">1.0</data>
      <data key="d5">RAPTOR and Tree of Clarifications are both research studies that involve advanced text processing techniques, with RAPTOR focusing on text embeddings and Tree of Clarifications on answering ambiguous questions.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="KNOWLEDGE GRAPH CREATION" target="KNOWLEDGE GRAPH COMPLETION">
      <data key="d4">1.0</data>
      <data key="d5">Knowledge Graph Creation and Knowledge Graph Completion are both research studies that involve the use of LLMs in the context of knowledge graphs, suggesting a connection in their research focus and methodology.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="KNOWLEDGE GRAPH COMPLETION" target="CAUSAL GRAPH EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Knowledge Graph Completion and Causal Graph Extraction are both research studies that involve the use of LLMs in the context of graph analysis, indicating a connection in their research focus and methodology.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="KAPING" target="G-RETRIEVER">
      <data key="d4">1.0</data>
      <data key="d5">KAPING and G-Retriever are related as they both involve querying specific parts of a graph structure, with KAPING focusing on subsets of the graph and G-Retriever focusing on retrieving specific parts of the graph</data>
      <data key="d6">7383c69e93bb8c8648181f5355d2c9a7</data>
    </edge>
    <edge source="LLMS FOR KNOWLEDGE GRAPH CREATION" target="LLMS FOR KNOWLEDGE GRAPH COMPLETION">
      <data key="d4">1.0</data>
      <data key="d5">LLMs for knowledge graph creation and LLMs for knowledge graph completion are related as they both involve the use of LLMs in the context of knowledge graphs, with creation focusing on the initial generation and completion focusing on enhancing existing graphs</data>
      <data key="d6">7383c69e93bb8c8648181f5355d2c9a7</data>
    </edge>
    <edge source="GRAPHTOOLFORMER" target="SURGE">
      <data key="d4">1.0</data>
      <data key="d5">GraphToolFormer and SURGE are related as they both involve the use of graph data, with GraphToolFormer focusing on derived graph metrics and SURGE focusing on generating narratives based on retrieved subgraphs</data>
      <data key="d6">7383c69e93bb8c8648181f5355d2c9a7</data>
    </edge>
    <edge source="FABULA" target="SYSTEM FOR MULTI-HOP QUESTION ANSWERING">
      <data key="d4">1.0</data>
      <data key="d5">FABULA and the system for multi-hop question answering are related as they both involve the use of graph data for generating narratives or answering questions, with FABULA focusing on event plots and the system for multi-hop question answering focusing on complex question answering</data>
      <data key="d6">7383c69e93bb8c8648181f5355d2c9a7</data>
    </edge>
    <edge source="LANGCHAIN" target="LLAMAINDEX">
      <data key="d4">2.0</data>
      <data key="d5">LangChain and LlamaIndex are related as they both are libraries that support a variety of graph databases, providing tools and interfaces for working with graph data
LangChain and LlamaIndex are both libraries that support the creation and traversal of text-relationship graphs for multi-hop question answering and are compatible with various graph databases for RAG applications</data>
      <data key="d6">7383c69e93bb8c8648181f5355d2c9a7,e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="LANGCHAIN" target="NEO4J">
      <data key="d4">1.0</data>
      <data key="d5">LangChain supports the use of Neo4J, a graph database that can be used to create and reason over knowledge graphs</data>
      <data key="d6">e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="LANGCHAIN" target="NEBULAGRAPH">
      <data key="d4">1.0</data>
      <data key="d5">LangChain supports the use of NebulaGraph, a graph database that can be used to create and reason over knowledge graphs</data>
      <data key="d6">e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="LLAMAINDEX" target="NEO4J">
      <data key="d4">1.0</data>
      <data key="d5">LlamaIndex supports the use of Neo4J, a graph database that can be used to create and reason over knowledge graphs</data>
      <data key="d6">e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="LLAMAINDEX" target="NEBULAGRAPH">
      <data key="d4">1.0</data>
      <data key="d5">LlamaIndex supports the use of NebulaGraph, a graph database that can be used to create and reason over knowledge graphs</data>
      <data key="d6">e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="EVALUATION APPROACH" target="SELFCHECKGPT">
      <data key="d4">1.0</data>
      <data key="d5">Using SelfCheckGPT to compare fabrication rates would improve the current analysis of Graph RAG's performance</data>
      <data key="d6">e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="1 MILLION TOKENS" target="QUESTION TYPES">
      <data key="d4">1.0</data>
      <data key="d5">Understanding how performance varies across different ranges of question types is crucial for analyzing the effectiveness of models or systems when dealing with a region of 1 million tokens</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="1 MILLION TOKENS" target="DATA TYPES">
      <data key="d4">1.0</data>
      <data key="d5">Understanding how performance varies across different data types is crucial for analyzing the effectiveness of models or systems when dealing with a region of 1 million tokens</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="1 MILLION TOKENS" target="DATASET SIZES">
      <data key="d4">1.0</data>
      <data key="d5">Understanding how performance varies across different dataset sizes is crucial for analyzing the effectiveness of models or systems when dealing with a region of 1 million tokens</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="QUESTION TYPES" target="DATA TYPES">
      <data key="d4">1.0</data>
      <data key="d5">Understanding how performance varies across different data types is crucial for analyzing the effectiveness of models or systems when dealing with various question types</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="QUESTION TYPES" target="DATASET SIZES">
      <data key="d4">1.0</data>
      <data key="d5">Understanding how performance varies across different dataset sizes is crucial for analyzing the effectiveness of models or systems when dealing with various question types</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="QUESTION TYPES" target="END USERS">
      <data key="d4">1.0</data>
      <data key="d5">Validating sensemaking questions and target metrics with end users is crucial for understanding how performance varies across different question types</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="DATA TYPES" target="DATASET SIZES">
      <data key="d4">1.0</data>
      <data key="d5">Understanding how performance varies across different dataset sizes is crucial for analyzing the effectiveness of models or systems when dealing with various data types</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="DATA TYPES" target="END USERS">
      <data key="d4">1.0</data>
      <data key="d5">Validating sensemaking questions and target metrics with end users is crucial for understanding how performance varies across different data types</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="DATASET SIZES" target="END USERS">
      <data key="d4">1.0</data>
      <data key="d5">Validating sensemaking questions and target metrics with end users is crucial for understanding how performance varies across different dataset sizes</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GLOBAL SUMMARIZATION" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the global summarization technique</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="COMPUTE BUDGET" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the consideration of the compute budget</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="LIFETIME QUERIES" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the consideration of the expected number of lifetime queries per dataset</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH-RELATED RAG APPROACHES" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the consideration of other graph-related RAG approaches</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="ANSWER IIVENESS AND DIVERSITY" target="GLOBAL BUT GRAPH-FREE APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">The Answer Iiveness and Diversity can be compared to the results obtained from a Global but Graph-free Approach, which uses map-reduce source text summarization. The comparison helps to evaluate the effectiveness and efficiency of different methods in generating diverse and rich answers or solutions.</data>
      <data key="d6">e31d2d134cf501c93f9445914d7350f9</data>
    </edge>
    <edge source="ENTITY-BASED GRAPH INDEX" target="ROOT-LEVEL COMMUNITIES">
      <data key="d4">1.0</data>
      <data key="d5">The Entity-based Graph Index provides summaries of root-level communities, which are the fundamental or top-level groups within a dataset. The summaries help to improve the data index and facilitate global queries over the same dataset.</data>
      <data key="d6">e31d2d134cf501c93f9445914d7350f9</data>
    </edge>
    <edge source="ENTITY-BASED GRAPH INDEX" target="GLOBAL METHODS">
      <data key="d4">1.0</data>
      <data key="d5">The Entity-based Graph Index achieves competitive performance to other Global Methods at a fraction of the token cost. This indicates that the Entity-based Graph Index is a more cost-effective and efficient method for handling global queries over large datasets.</data>
      <data key="d6">e31d2d134cf501c93f9445914d7350f9</data>
    </edge>
    <edge source="GRAPHRAG INDEXING" target="INDEXING PIPELINES">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG Indexing includes Indexing Pipelines as a core component of its suite, which are responsible for the extraction and transformation of data from unstructured text into structured formats. The pipelines are configurable and can be customized to meet specific needs, making them integral to the overall functionality of GraphRAG Indexing.</data>
      <data key="d6">251e8d332b451d900df961cbe215bca0</data>
    </edge>
    <edge source="GRAPHRAG INDEXING" target="DEFAULT CONFIGURATION MODE">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Indexing system includes the Default Configuration Mode as one of its configuration options, which is designed for simplicity and ease of use for most users</data>
      <data key="d6">ccd2de9e2219521fbca779843c65af58</data>
    </edge>
    <edge source="GRAPHRAG INDEXING" target="CUSTOM CONFIGURATION MODE">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Indexing system includes the Custom Configuration Mode as an advanced configuration option, which is designed for users who require deeper control over the system's configuration</data>
      <data key="d6">ccd2de9e2219521fbca779843c65af58</data>
    </edge>
    <edge source="INDEXING PIPELINES" target="ENTITY EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Indexing Pipelines utilize Entity Extraction as one of their functions to identify and extract meaningful entities from unstructured text. This step is essential for the subsequent analysis and transformation of data within the pipelines.</data>
      <data key="d6">251e8d332b451d900df961cbe215bca0</data>
    </edge>
    <edge source="INDEXING PIPELINES" target="RELATIONSHIP DETECTION">
      <data key="d4">1.0</data>
      <data key="d5">Indexing Pipelines incorporate Relationship Detection to identify relationships between entities extracted from text. This function is crucial for understanding the context and connections within the data, enhancing the structured outputs of the pipelines.</data>
      <data key="d6">251e8d332b451d900df961cbe215bca0</data>
    </edge>
    <edge source="INDEXING PIPELINES" target="COMMUNITY DETECTION">
      <data key="d4">1.0</data>
      <data key="d5">Indexing Pipelines use Community Detection to identify groups of related entities within the extracted data. This function aids in summarizing and reporting on the text at multiple levels of granularity, contributing to the structured data outputs of the pipelines.</data>
      <data key="d6">251e8d332b451d900df961cbe215bca0</data>
    </edge>
    <edge source="INDEXING PIPELINES" target="DATA EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Indexing Pipelines employ Data Embedding to transform extracted entities and text chunks into vector representations. This process facilitates further analysis and storage of the structured data, making it an integral part of the pipelines' functionality.</data>
      <data key="d6">251e8d332b451d900df961cbe215bca0</data>
    </edge>
    <edge source="INDEXING PIPELINES" target="OUTPUT FORMATS">
      <data key="d4">1.0</data>
      <data key="d5">Indexing Pipelines generate structured data that can be stored in various Output Formats, including JSON and Parquet. These formats, along with the option to access outputs via the Python API, provide flexibility in how the data is handled and used.</data>
      <data key="d6">251e8d332b451d900df961cbe215bca0</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="FIELDS">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section contains the Fields, which are specific configuration options for entity extraction settings</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="PARALLELIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the Parallelization property, specifying the parallelization settings for processing text</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="ASYNC MODE">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the Async Mode property, specifying whether to process text asynchronously</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the Prompt property, specifying the prompt file to use for guiding the text analysis</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="ENTITY TYPES">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the Entity Types property, specifying the types of entities to identify</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="MAX GLEANINGS">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the Max Gleanings property, specifying the maximum number of gleaning cycles to use</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="STRATEGY">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the Strategy property, specifying the strategy to fully override the entity extraction process</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="COMMUNITY SUMMARIZATION">
      <data key="d4">2.0</data>
      <data key="d5">Community Detection is related to Community Summarization, as the identification of groups of related entities in the graph is used in the summarization of communities
Community Summarization is related to Community Detection as the former process often relies on the latter to identify and summarize the characteristics of communities within a network</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="COMMUNITY TABLES">
      <data key="d4">1.0</data>
      <data key="d5">Community Tables are related to Community Detection as they are often used to store and organize the results of community detection algorithms, including details about the communities and their members</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="PHASE 3: GRAPH AUGMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">Community Detection is a subprocess of Phase 3: Graph Augmentation, aimed at understanding the community structure of the graph</data>
      <data key="d6">a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </edge>
    <edge source="CONFIG FILE" target="TIMESTAMPED OUTPUT FOLDER">
      <data key="d4">1.0</data>
      <data key="d5">The Timestamped Output Folder is related to the Config File as the Config File may contain settings that determine the naming and location of the Timestamped Output Folder</data>
      <data key="d6">919cb44d9688a14bf48fa7c98163ed81</data>
    </edge>
    <edge source="CONFIG FILE" target="PROGRESS REPORTER">
      <data key="d4">1.0</data>
      <data key="d5">The Config File is related to the Progress Reporter as it may contain settings that determine the type of Progress Reporter to be used during the process</data>
      <data key="d6">919cb44d9688a14bf48fa7c98163ed81</data>
    </edge>
    <edge source="CONFIG FILE" target="TABLE OUTPUT FORMATS">
      <data key="d4">1.0</data>
      <data key="d5">The Config File is related to the Table Output Formats as it may contain settings that determine the formats in which the pipeline should emit the table output</data>
      <data key="d6">919cb44d9688a14bf48fa7c98163ed81</data>
    </edge>
    <edge source="CONFIG FILE" target="CACHING MECHANISM">
      <data key="d4">1.0</data>
      <data key="d5">The Config File is related to the Caching Mechanism as it may contain settings that determine whether the Caching Mechanism is enabled or disabled</data>
      <data key="d6">919cb44d9688a14bf48fa7c98163ed81</data>
    </edge>
    <edge source="NODE" target="ENTITY">
      <data key="d4">1.0</data>
      <data key="d5">Nodes contain layout information for rendered graph-views of the entities, showing how entities are embedded and clustered</data>
      <data key="d6">85e50a4d70697a2c4420e7a9fc82f22d</data>
    </edge>
    <edge source="NODE" target="COMMUNITY REPORT">
      <data key="d4">1.0</data>
      <data key="d5">Community Reports are related to the Nodes in the graph, as they provide insights and summaries for the communities represented by the nodes</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="NODE" target="DEFAULT CONFIGURATION WORKFLOW">
      <data key="d4">1.0</data>
      <data key="d5">Nodes are part of the Default Configuration Workflow, as they are used in the visualization and analysis of the graph created by the workflow</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="COL1" target="COL_MULTIPLIED">
      <data key="d4">1.0</data>
      <data key="d5">The values in col1 are used as one of the inputs to calculate the values in col_multiplied, which is the result of multiplying col1 and col2</data>
      <data key="d6">f3a07680cbe8ab1f6055369da05f4f38</data>
    </edge>
    <edge source="COL1" target="WORKFLOW2">
      <data key="d4">1.0</data>
      <data key="d5">In the workflow2 process, col1 is used as an input in the derive step to process data</data>
      <data key="d6">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </edge>
    <edge source="COL2" target="COL_MULTIPLIED">
      <data key="d4">1.0</data>
      <data key="d5">The values in col2 are used as one of the inputs to calculate the values in col_multiplied, which is the result of multiplying col1 and col2</data>
      <data key="d6">f3a07680cbe8ab1f6055369da05f4f38</data>
    </edge>
    <edge source="COL2" target="WORKFLOW2">
      <data key="d4">1.0</data>
      <data key="d5">In the workflow2 process, col2 is used as an input in the derive step to process data</data>
      <data key="d6">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </edge>
    <edge source="RUN_PIPELINE" target="WORKFLOWS">
      <data key="d4">1.0</data>
      <data key="d5">run_pipeline function uses the workflows to determine how to process the dataset</data>
      <data key="d6">f3a07680cbe8ab1f6055369da05f4f38</data>
    </edge>
    <edge source="RUN_PIPELINE" target="OUTPUTS">
      <data key="d4">1.0</data>
      <data key="d5">The results of the run_pipeline function are collected in the outputs list</data>
      <data key="d6">f3a07680cbe8ab1f6055369da05f4f38</data>
    </edge>
    <edge source="WORKFLOWS" target="WORKFLOW NAME">
      <data key="d4">1.0</data>
      <data key="d5">The Workflows property is related to the Workflow Name property, as the workflow name is used to define and reference specific workflows within the workflows section</data>
      <data key="d6">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </edge>
    <edge source="OUTPUTS" target="PIPELINE_RESULT">
      <data key="d4">1.0</data>
      <data key="d5">The last element in the outputs list, pipeline_result, represents the final result of the pipeline execution</data>
      <data key="d6">f3a07680cbe8ab1f6055369da05f4f38</data>
    </edge>
    <edge source="DEFAULT PROMPTS" target="ENTITY/RELATIONSHIP EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Default Prompts include the function of Entity/Relationship Extraction, which is a fundamental step in the creation of a knowledge graph. This relationship indicates that Entity/Relationship Extraction is one of the functions provided by the Default Prompts</data>
      <data key="d6">bdb8f9e797229f596744d9636ab857b0</data>
    </edge>
    <edge source="DEFAULT PROMPTS" target="ENTITY/RELATIONSHIP DESCRIPTION SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Default Prompts include the function of Entity/Relationship Description Summarization, which provides concise summaries of entity and relationship descriptions. This relationship indicates that Entity/Relationship Description Summarization is one of the functions provided by the Default Prompts</data>
      <data key="d6">bdb8f9e797229f596744d9636ab857b0</data>
    </edge>
    <edge source="DEFAULT PROMPTS" target="CLAIM EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Default Prompts include the function of Claim Extraction, which identifies and extracts claims from the input data. This relationship indicates that Claim Extraction is one of the functions provided by the Default Prompts</data>
      <data key="d6">bdb8f9e797229f596744d9636ab857b0</data>
    </edge>
    <edge source="ENTITY/RELATIONSHIP EXTRACTION" target="TOKEN-REPLACEMENTS">
      <data key="d4">1.0</data>
      <data key="d5">Entity/Relationship Extraction utilizes Token-Replacements to process input text and generate tuples representing entities or relationships, enhancing the flexibility and specificity of the extraction process</data>
      <data key="d6">6a7157695d90d434b2625c3f05420916</data>
    </edge>
    <edge source="CLAIM EXTRACTION" target="ENTITY RESOLUTION">
      <data key="d4">1.0</data>
      <data key="d5">Entity Resolution is related to Claim Extraction, as the resolution of entities that refer to the same real-world object or concept is used in the identification of claims or statements about entities in the text</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="CLAIM EXTRACTION" target="GRAPH TABLES">
      <data key="d4">1.0</data>
      <data key="d5">Claim Extraction is related to Graph Tables, as the identification of claims or statements about entities in the text is used in the creation of the graph, which is represented in the Graph Tables</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="CLAIM EXTRACTION" target="TEXT UNITS">
      <data key="d4">1.0</data>
      <data key="d5">Claim Extraction is related to Text Units as the process often operates on Text Units to identify claims or assertions made in the text</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="CLAIM EXTRACTION" target="COVARIATES">
      <data key="d4">1.0</data>
      <data key="d5">Claim Extraction results in the emission of Covariates, which are positive factual statements with an evaluated status and time-bounds.</data>
      <data key="d6">d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </edge>
    <edge source="ROOT" target="DOMAIN">
      <data key="d4">1.0</data>
      <data key="d5">The root property specifies the path to the project directory, which contains the input data for the specified domain. The domain property is used to tailor the prompt generation to a specific subject area, which is determined by the input data located in the project directory specified by the root property.</data>
      <data key="d6">ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </edge>
    <edge source="ROOT" target="LIMIT">
      <data key="d4">1.0</data>
      <data key="d5">The root property specifies the path to the project directory, which contains the input data that is processed using the limit property to control the number of text units selected for template generation. The limit property determines the size of the sample used for prompt generation, which is taken from the input data located in the project directory specified by the root property.</data>
      <data key="d6">ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </edge>
    <edge source="ROOT" target="LANGUAGE">
      <data key="d4">1.0</data>
      <data key="d5">The root property specifies the path to the project directory, which contains the input data that is processed in the language specified by the language property. The language property ensures that the prompt generation is tailored to the language of the input documents, which are located in the project directory specified by the root property.</data>
      <data key="d6">ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </edge>
    <edge source="ROOT" target="STORAGE ACCOUNT BLOB URL">
      <data key="d4">1.0</data>
      <data key="d5">The Root is the base for the Storage Account Blob URL, as the URL is relative to the root directory specified in the configuration</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ROOT" target="CLI ARGUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">The root argument is a CLI Argument that specifies the data root directory for the GraphRAG Indexer CLI, which contains the input data and environment variables.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="DOMAIN" target="LANGUAGE">
      <data key="d4">1.0</data>
      <data key="d5">DOMAIN and LANGUAGE are related as specifying the domain can help in determining the appropriate language for input processing, especially if the domain is associated with a specific language or if translation is needed</data>
      <data key="d6">9243633f55cccd0885ba553e14fa5e3f</data>
    </edge>
    <edge source="LANGUAGE" target="MAX_TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">The LANGUAGE option can influence the MAX_TOKENS setting, as the language used for input processing might affect the tokenization process and thus the maximum token count for prompt generation</data>
      <data key="d6">9243633f55cccd0885ba553e14fa5e3f</data>
    </edge>
    <edge source="MAX_TOKENS" target="MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The model and max_tokens are related because the model defines the LLM (Language Model) to be used, and the max_tokens sets the limit on the output size of that model</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="CHUNK_SIZE" target="OUTPUT">
      <data key="d4">1.0</data>
      <data key="d5">CHUNK_SIZE and OUTPUT are related as the size of chunks generated from input documents can impact the number and size of prompts saved in the output folder</data>
      <data key="d6">9243633f55cccd0885ba553e14fa5e3f</data>
    </edge>
    <edge source="CHUNK SIZE PARAMETER" target="RANDOM SELECTION METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The Chunk Size Parameter influences the Random Selection Method by determining the size of text units that are randomly selected for template generation</data>
      <data key="d6">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </edge>
    <edge source="CHUNK SIZE PARAMETER" target="TOP SELECTION METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The Chunk Size Parameter influences the Top Selection Method by determining the size of text units from which the head n units are selected for template generation</data>
      <data key="d6">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </edge>
    <edge source="CHUNK SIZE PARAMETER" target="ALL SELECTION METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The Chunk Size Parameter influences the All Selection Method by determining the size of text units that are all used for template generation</data>
      <data key="d6">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </edge>
    <edge source="CUSTOM PROMPT FILE" target="TOKEN-REPLACEMENTS">
      <data key="d4">1.0</data>
      <data key="d5">Token-Replacements are a key feature of the Custom Prompt File, enabling the customization of prompts through the use of placeholders that are replaced with actual values</data>
      <data key="d6">6a7157695d90d434b2625c3f05420916</data>
    </edge>
    <edge source="TOKEN-REPLACEMENTS" target="SUMMARIZE ENTITY/RELATIONSHIP DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">Summarize Entity/Relationship Descriptions uses Token-Replacements to process a list of descriptions for an entity or relationship, facilitating the summarization of these descriptions</data>
      <data key="d6">6a7157695d90d434b2625c3f05420916</data>
    </edge>
    <edge source="RECORD_DELIMITER" target="TUPLE_DELIMITER">
      <data key="d4">1.0</data>
      <data key="d5">The record_delimiter is used to separate tuple instances, and the tuple_delimiter is used to separate values within a tuple. These delimiters work together to structure and organize data in a clear and readable format</data>
      <data key="d6">853bfe9a74a916130a20f81506bcaf09</data>
    </edge>
    <edge source="ENTITY_NAME" target="DESCRIPTION_LIST">
      <data key="d4">1.0</data>
      <data key="d5">The entity_name is associated with the description_list, as the descriptions provide detailed information about the entity identified by the entity_name</data>
      <data key="d6">853bfe9a74a916130a20f81506bcaf09</data>
    </edge>
    <edge source="ENTITY_NAME" target="INPUT_TEXT">
      <data key="d4">1.0</data>
      <data key="d5">The input_text contains the entity_name, which is a unique identifier for an entity within the text. The entity_name is extracted from the input_text for further processing and analysis</data>
      <data key="d6">853bfe9a74a916130a20f81506bcaf09</data>
    </edge>
    <edge source="CONFIGURATION DOCUMENTATION" target=".ENV">
      <data key="d4">1.0</data>
      <data key="d5">The Configuration documentation provides guidance on how to properly configure the .env file, which is essential for the operation of GraphRAG.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="CONFIGURATION DOCUMENTATION" target="SETTINGS.YAML">
      <data key="d4">1.0</data>
      <data key="d5">The Configuration documentation provides guidance on how to properly configure the settings.yaml file, which is essential for the operation of GraphRAG.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="CONFIGURATION DOCUMENTATION" target="PROMPTS/">
      <data key="d4">1.0</data>
      <data key="d5">The Configuration documentation provides guidance on how to properly configure the prompts/ directory, including how to modify existing prompts or generate new ones through the Auto Prompt Tuning command.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="GRAPHRAG SYSTEM" target="GRAPHRAG ACCELERATOR SOLUTION">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Accelerator Solution is a recommended way to get started with the GraphRAG system, providing a user-friendly end-to-end experience with Azure resources for setting up and using the system</data>
      <data key="d6">84d24b5db902baca7217b5e3bb6ec462</data>
    </edge>
    <edge source="GRAPHRAG SYSTEM" target="INDEXING PIPELINE OVERVIEW">
      <data key="d4">1.0</data>
      <data key="d5">The Indexing Pipeline Overview is a component of the GraphRAG system that describes the process of indexing text data, which is a key functionality of the system</data>
      <data key="d6">84d24b5db902baca7217b5e3bb6ec462</data>
    </edge>
    <edge source="GRAPHRAG SYSTEM" target="QUERY ENGINE OVERVIEW">
      <data key="d4">1.0</data>
      <data key="d5">The Query Engine Overview is a component of the GraphRAG system that describes the process of querying indexed data, which is another key functionality of the system</data>
      <data key="d6">84d24b5db902baca7217b5e3bb6ec462</data>
    </edge>
    <edge source="GRAPHRAG PIPELINE" target="SETTINGS.YAML">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG pipeline relies on the settings.yaml file for configuration. This file contains settings that can be modified to customize the pipeline's behavior, including the API key and additional settings for Azure OpenAI users.</data>
      <data key="d6">5aaa26fbe97dc7573cd1a56d6fb11213</data>
    </edge>
    <edge source="GRAPHRAG PIPELINE" target="OPENAI API">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG pipeline can be configured to use the OpenAI API for various functionalities, such as language models and embeddings. The pipeline requires an API key for authentication, which is specified in the .env file.</data>
      <data key="d6">5aaa26fbe97dc7573cd1a56d6fb11213</data>
    </edge>
    <edge source="GRAPHRAG PIPELINE" target="AZURE OPENAI">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG pipeline can be configured to use Azure OpenAI for AI capabilities. Azure OpenAI users need to set specific variables in the settings.yaml file, including the API base URL, API version, and deployment name, to integrate with the pipeline.</data>
      <data key="d6">5aaa26fbe97dc7573cd1a56d6fb11213</data>
    </edge>
    <edge source="AZURE OPENAI" target="GRAPHRAG_API_VERSION">
      <data key="d4">1.0</data>
      <data key="d5">The Azure OpenAI service requires the GRAPHRAG_API_VERSION configuration to specify the API version for compatibility and functionality. This relationship is essential for ensuring that the service requests are processed correctly.</data>
      <data key="d6">7b45dafa74553d3899e2291a3c9fb86e</data>
    </edge>
    <edge source="AZURE OPENAI" target="GRAPHRAG_LLM_API_KEY">
      <data key="d4">1.0</data>
      <data key="d5">The Azure OpenAI service uses the GRAPHRAG_LLM_API_KEY for authentication. This relationship is critical for accessing and using the service, as the API key verifies the user's identity and permissions.</data>
      <data key="d6">7b45dafa74553d3899e2291a3c9fb86e</data>
    </edge>
    <edge source="AZURE OPENAI" target="GRAPHRAG_LLM_API_VERSION">
      <data key="d4">1.0</data>
      <data key="d5">The Azure OpenAI service uses the GRAPHRAG_LLM_API_VERSION configuration to specify the API version for language model requests. This relationship is crucial for compatibility and functionality, ensuring that the service requests are processed correctly.</data>
      <data key="d6">7b45dafa74553d3899e2291a3c9fb86e</data>
    </edge>
    <edge source="AZURE OPENAI" target="GRAPHRAG_LLM_MODEL_SUPPORTS_JSON">
      <data key="d4">1.0</data>
      <data key="d5">The Azure OpenAI service's functionality is influenced by the GRAPHRAG_LLM_MODEL_SUPPORTS_JSON configuration, which indicates whether the language model can handle JSON data. This relationship is important for determining the capabilities of the service and how it can be used in various applications.</data>
      <data key="d6">7b45dafa74553d3899e2291a3c9fb86e</data>
    </edge>
    <edge source="SETTINGS.YAML" target="PYTHON -M GRAPHRAG.INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The python -m graphrag.index command creates the settings.yaml file when the --init option is used. This file contains the configuration settings necessary for the operation of GraphRAG.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="SETTINGS.YAML" target=".ENV">
      <data key="d4">1.0</data>
      <data key="d5">.env is referenced in settings.yaml for environment variable settings, allowing for token replacements in the configuration document using ${ENV_VAR} syntax.</data>
      <data key="d6">32e96c66a531ecd0a8edc7414aec0803</data>
    </edge>
    <edge source="API_BASE" target="GRAPHRAG CONFIGURATION DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The api_base is a configuration property that is documented in the GraphRAG configuration documentation, providing guidance on how to set up the base URL for accessing the Azure OpenAI API.</data>
      <data key="d6">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </edge>
    <edge source="API_BASE" target="REQUEST_TIMEOUT">
      <data key="d4">1.0</data>
      <data key="d5">The request_timeout and api_base are related because the request_timeout defines the timeout for requests made to the API specified by api_base</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="API_VERSION" target="GRAPHRAG CONFIGURATION DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The api_version is a configuration property that is documented in the GraphRAG configuration documentation, offering information on how to specify the version of the Azure OpenAI API to be used.</data>
      <data key="d6">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </edge>
    <edge source="API_VERSION" target="ORGANIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The api_version and organization are related because the api_version specifies the version of the API used by the LLM (Language Model) service, which is associated with the organization using the service</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="DEPLOYMENT_NAME" target="GRAPHRAG CONFIGURATION DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The deployment_name is a configuration property that is documented in the GraphRAG configuration documentation, explaining how to set the name of the Azure model deployment for accessing the model.</data>
      <data key="d6">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </edge>
    <edge source="DEPLOYMENT_NAME" target="MODEL_SUPPORTS_JSON">
      <data key="d4">1.0</data>
      <data key="d5">The deployment_name and model_supports_json are related because the deployment_name specifies the deployment to use for Azure-based LLM (Language Model) services, and model_supports_json indicates whether the model deployed supports JSON output</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="DEPLOYMENT_NAME" target="URL ENDPOINT">
      <data key="d4">1.0</data>
      <data key="d5">The url endpoint is related to the deployment_name as the endpoint is specific to the deployment specified by the deployment_name. The deployment_name determines the correct endpoint to use for accessing the model.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="GRAPHRAG CONFIGURATION DOCUMENTATION" target="INITIALIZATION DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG configuration documentation is related to the Initialization documentation, as both provide information necessary for setting up the environment and preparing for data indexing and querying.</data>
      <data key="d6">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION MODE" target="INIT COMMAND">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Mode supports the use of the Init Command to easily set up the GraphRAG system with the necessary configuration files</data>
      <data key="d6">ccd2de9e2219521fbca779843c65af58</data>
    </edge>
    <edge source="CUSTOM CONFIGURATION MODE" target="ENCODING_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The encoding_model is related to Custom Configuration Mode as it is a configuration setting that can be customized in the advanced use-case of Custom Configuration Mode</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="CUSTOM CONFIGURATION MODE" target="SKIP_WORKFLOWS">
      <data key="d4">1.0</data>
      <data key="d5">The skip_workflows is related to Custom Configuration Mode as it is a configuration property that can be defined in the advanced use-case of Custom Configuration Mode to skip specific workflows</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="INIT COMMAND" target=".ENV FILE">
      <data key="d4">1.0</data>
      <data key="d5">The init command creates the .env file, which contains environment variables necessary for the configuration of GraphRAG</data>
      <data key="d6">d0f7c236538005bc3056b7daed2401d8</data>
    </edge>
    <edge source="INIT COMMAND" target="SETTINGS.YAML FILE">
      <data key="d4">1.0</data>
      <data key="d5">The init command creates the settings.yaml file, which contains the configuration settings for GraphRAG</data>
      <data key="d6">d0f7c236538005bc3056b7daed2401d8</data>
    </edge>
    <edge source="INIT COMMAND" target="PROMPTS FOLDER">
      <data key="d4">1.0</data>
      <data key="d5">The init command creates the prompts folder, which contains the default prompts used by GraphRAG and can be modified or adapted</data>
      <data key="d6">d0f7c236538005bc3056b7daed2401d8</data>
    </edge>
    <edge source=".ENV" target="PYTHON -M GRAPHRAG.INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The python -m graphrag.index command creates the .env file when the --init option is used. This file is necessary for storing environment variables referenced in the settings.yaml file.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source=".ENV" target="CONFIG.JSON">
      <data key="d4">1.0</data>
      <data key="d5">.env is used alongside config.json for environment variable token replacements, allowing for dynamic configuration settings in GraphRAG.</data>
      <data key="d6">32e96c66a531ecd0a8edc7414aec0803</data>
    </edge>
    <edge source=".ENV" target="CONFIG.YML">
      <data key="d4">1.0</data>
      <data key="d5">.env is used alongside config.yml for environment variable token replacements, allowing for dynamic configuration settings in GraphRAG.</data>
      <data key="d6">32e96c66a531ecd0a8edc7414aec0803</data>
    </edge>
    <edge source="PROMPTS/" target="PYTHON -M GRAPHRAG.INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The python -m graphrag.index command creates the prompts/ directory when the --init option is used. This directory contains default prompts used by GraphRAG and can be modified or new ones generated through the Auto Prompt Tuning command.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="PROMPTS/" target="PROMPT TUNING COMMAND">
      <data key="d4">1.0</data>
      <data key="d5">The prompts/ directory contains default prompts that can be modified or adapted using the Prompt Tuning command, enhancing the performance of GraphRAG for specific data sets.</data>
      <data key="d6">32e96c66a531ecd0a8edc7414aec0803</data>
    </edge>
    <edge source="CONFIG.JSON" target="API KEY">
      <data key="d4">1.0</data>
      <data key="d5">The API Key is a part of the Config.json file, specifically located within the llm section. It is a critical configuration property for accessing the LLM service.</data>
      <data key="d6">f135654a3c057c66b9e5f97a960d302f</data>
    </edge>
    <edge source="CONFIG.JSON" target="INPUT CONFIGURATION">
      <data key="d4">1.0</data>
      <data key="d5">The Input Configuration is a section within the Config.json file that defines how input data should be processed. It includes various fields that are necessary for configuring the input handling process.</data>
      <data key="d6">f135654a3c057c66b9e5f97a960d302f</data>
    </edge>
    <edge source="CONFIG.JSON" target="LLM CONFIGURATION">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Configuration is a section within the Config.json file that specifies settings for the LLM (Language Model) service. It includes the API Key and other parameters that are essential for the operation of the LLM service.</data>
      <data key="d6">f135654a3c057c66b9e5f97a960d302f</data>
    </edge>
    <edge source="API_KEY" target="TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The api_key and type are related because the api_key is required for authentication when using the specified type of LLM (Language Model) service, such as OpenAI or Azure OpenAI</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="INPUT" target="WORKFLOW2">
      <data key="d4">1.0</data>
      <data key="d5">The input data for workflow2 is specified as a file type, with details such as base directory and file pattern, which are necessary for the process to execute</data>
      <data key="d6">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </edge>
    <edge source="INPUT" target="FILE_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The input property is related to the file_type as the file_type specifies the type of file that the input property is configured to read</data>
      <data key="d6">3900b87693f02c43b4294e38647eb7cd</data>
    </edge>
    <edge source="INPUT" target="BASE_DIR">
      <data key="d4">1.0</data>
      <data key="d5">The input property is related to the base_dir as the base_dir specifies the directory from which the input property reads the files</data>
      <data key="d6">3900b87693f02c43b4294e38647eb7cd</data>
    </edge>
    <edge source="INPUT" target="FILE_PATTERN">
      <data key="d4">1.0</data>
      <data key="d5">The input property is related to the file_pattern as the file_pattern is used by the input property to identify and select the files to be read</data>
      <data key="d6">3900b87693f02c43b4294e38647eb7cd</data>
    </edge>
    <edge source="INPUT" target="SOURCE_COLUMN">
      <data key="d4">1.0</data>
      <data key="d5">The input property is related to the source_column as the source_column is a part of the input configuration that specifies which column contains the source or author of the data</data>
      <data key="d6">3900b87693f02c43b4294e38647eb7cd</data>
    </edge>
    <edge source="INPUT" target="TEXT_COLUMN">
      <data key="d4">1.0</data>
      <data key="d5">The input property is related to the text_column as the text_column is a part of the input configuration that specifies which column contains the text of the data</data>
      <data key="d6">3900b87693f02c43b4294e38647eb7cd</data>
    </edge>
    <edge source="INPUT" target="POST PROCESS">
      <data key="d4">1.0</data>
      <data key="d5">The Input is related to the Post Process as the post-process steps are applied to the input data after it is read from the CSV file. The relationship is established through the 'post_process' section in the input that specifies the filtering step based on the 'title' column with the value 'My document'</data>
      <data key="d6">6839baed839d7a5e837af1da93e462e5</data>
    </edge>
    <edge source="TYPE" target="FIELDS">
      <data key="d4">1.0</data>
      <data key="d5">The Fields include the Type property, which specifies the type of storage to use</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="CONNECTION_STRING" target="CONTAINER_NAME">
      <data key="d4">1.0</data>
      <data key="d5">The connection_string and container_name are related because the connection_string provides the necessary details to access the Azure Blob storage, while the container_name specifies the exact container within that storage where data is stored</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="BASE_DIR" target="STORAGE_ACCOUNT_BLOB_URL">
      <data key="d4">1.0</data>
      <data key="d5">The base_dir and storage_account_blob_url are related because the base_dir specifies the directory from which to read input data, and the storage_account_blob_url provides the direct access point to the Azure storage account blob where the data is located</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="PROXY" target="COGNITIVE_SERVICES_ENDPOINT">
      <data key="d4">1.0</data>
      <data key="d5">The proxy and cognitive_services_endpoint are related because the proxy can be used to route requests to the cognitive_services_endpoint, which provides additional AI services</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="MODEL_SUPPORTS_JSON" target="URL ENDPOINT">
      <data key="d4">1.0</data>
      <data key="d5">The model_supports_json is related to the url endpoint as it indicates whether the model at the specified endpoint supports JSON-mode output. This affects how requests are formatted and how responses are interpreted.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="TOKENS_PER_MINUTE" target="REQUESTS_PER_MINUTE">
      <data key="d4">2.0</data>
      <data key="d5">The tokens_per_minute and requests_per_minute are related because they both set throttle limits on the LLM (Language Model) service, with tokens_per_minute controlling the number of tokens processed and requests_per_minute controlling the number of requests processed
The tokens_per_minute is related to the requests_per_minute as both parameters control the rate at which requests can be made and tokens can be used. They work together to manage the rate of consumption and processing of requests and tokens.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="REQUESTS_PER_MINUTE" target="SLEEP_ON_RATE_LIMIT_RECOMMENDATION">
      <data key="d4">1.0</data>
      <data key="d5">The sleep_on_rate_limit_recommendation is related to the requests_per_minute as it controls the behavior when the rate limit set by requests_per_minute is reached. It determines whether the system should sleep according to recommendations when the rate limit is exceeded.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="MAX_RETRIES" target="MAX_RETRY_WAIT">
      <data key="d4">2.0</data>
      <data key="d5">The max_retries and max_retry_wait are related because they both control the retry mechanism for failed requests to the LLM (Language Model) service, with max_retries setting the maximum number of retries and max_retry_wait specifying the maximum backoff time before retrying
The max_retries is related to the max_retry_wait as they both determine the retry mechanism for failed requests. The max_retries specifies the number of retries, while the max_retry_wait specifies the maximum wait time before each retry.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="MAX_RETRIES" target="CONCURRENT_REQUESTS">
      <data key="d4">1.0</data>
      <data key="d5">The concurrent_requests is related to the max_retries as they both manage the handling of requests. The concurrent_requests specifies the number of requests that can be open at once, while the max_retries determines how many times a failed request should be retried.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="TEMPERATURE" target="TOP_P">
      <data key="d4">1.0</data>
      <data key="d5">The temperature is related to the top_p as they both influence the generation of completions. The temperature affects the randomness of the output, while the top_p affects the diversity of the output by controlling the selection of likely completions.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="TEMPERATURE" target="N">
      <data key="d4">1.0</data>
      <data key="d5">The n is related to the temperature as they both determine the output of the model. The n specifies the number of completions to generate, while the temperature influences the randomness of those completions.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="STAGGER" target="NUM_THREADS">
      <data key="d4">1.0</data>
      <data key="d5">The stagger is related to the num_threads as they both influence the parallelization of processing. The stagger specifies the threading stagger value, while the num_threads determines the number of threads used for processing.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="ASYNC_MODE" target="SUMMARIZE_DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The async_mode setting is used in the summarize_descriptions process, indicating how the summarization tasks are handled asynchronously</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="ASYNC_MODE" target="CLAIM_EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The async_mode setting is used in the claim_extraction process, indicating how the claim extraction tasks are handled asynchronously</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="ASYNC_MODE" target="COMMUNITY_REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The async_mode setting is used in the community_reports process, indicating how the report generation tasks are handled asynchronously</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="ASYNC_MODE" target="GRAPHRAG_MAX_CLUSTER_SIZE">
      <data key="d4">1.0</data>
      <data key="d5">ASYNC_MODE and GRAPHRAG_MAX_CLUSTER_SIZE are related as they both are configuration properties that can affect the performance and behavior of the system</data>
      <data key="d6">1b24101de07b1c195448240237b84b37</data>
    </edge>
    <edge source="BATCH_SIZE" target="BATCH_MAX_TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">batch_size and batch_max_tokens are related because they both define parameters for batch processing, with batch_size controlling the number of items and batch_max_tokens controlling the token limit, which together influence the efficiency and resource usage of the system.</data>
      <data key="d6">d27237468a1b9e89110eeeca8080f63c</data>
    </edge>
    <edge source="TARGET" target="STRATEGY">
      <data key="d4">1.0</data>
      <data key="d5">target and strategy are related because the strategy for text-embedding can be fully overridden, which includes how the target set of embeddings is emitted, affecting the output and customization of the text-embedding process.</data>
      <data key="d6">d27237468a1b9e89110eeeca8080f63c</data>
    </edge>
    <edge source="STRATEGY" target="SUMMARIZE_DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The strategy setting is used in the summarize_descriptions process, allowing for the customization of the summarization algorithm</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="STRATEGY" target="CLAIM_EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The strategy setting is used in the claim_extraction process, allowing for the customization of the claim extraction algorithm</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="STRATEGY" target="COMMUNITY_REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The strategy setting is used in the community_reports process, allowing for the customization of the report generation algorithm</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="SIZE" target="OVERLAP">
      <data key="d4">1.0</data>
      <data key="d5">size and overlap are related because they both influence the chunking process, with size defining the maximum chunk size and overlap specifying the overlap between chunks, which together impact the context preservation and processing efficiency of text segments.</data>
      <data key="d6">d27237468a1b9e89110eeeca8080f63c</data>
    </edge>
    <edge source="PARALLELIZATION" target="SUMMARIZE_DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The parallelization setting is used in the summarize_descriptions process, defining how tasks are processed in parallel</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="PARALLELIZATION" target="CLAIM_EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The parallelization setting is used in the claim_extraction process, defining how tasks are processed in parallel</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="PARALLELIZATION" target="COMMUNITY_REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The parallelization setting is used in the community_reports process, defining how tasks are processed in parallel</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="CACHE" target="STORAGE">
      <data key="d4">2.0</data>
      <data key="d5">cache and storage are related because they both involve settings for managing data, with cache focusing on temporary storage for quick access and storage dealing with long-term data management, potentially affecting the performance and reliability of the system.
storage and cache are related as they both deal with data management in the pipeline, with storage focusing on the output strategy and cache on the caching strategy, which can impact performance and resource management</data>
      <data key="d6">d27237468a1b9e89110eeeca8080f63c,e01c546120a27319dcbdf7a6b89bab26</data>
    </edge>
    <edge source="STORAGE" target="STORAGE ACCOUNT BLOB URL">
      <data key="d4">1.0</data>
      <data key="d5">The Storage section includes the Storage Account Blob URL as a configuration property for blob storage type</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="STORAGE" target="FIELDS">
      <data key="d4">1.0</data>
      <data key="d5">The Storage section contains the Fields, which are specific configuration options for storage settings</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="STORAGE" target="ROOT_DIR">
      <data key="d4">1.0</data>
      <data key="d5">root_dir is related to the storage configuration as it specifies the base directory for storing data, which is a critical component of the storage strategy</data>
      <data key="d6">e01c546120a27319dcbdf7a6b89bab26</data>
    </edge>
    <edge source="REPORTING" target="STORAGE ACCOUNT BLOB URL">
      <data key="d4">1.0</data>
      <data key="d5">The Reporting section includes the Storage Account Blob URL as a configuration property for blob reporting type</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="REPORTING" target="FIELDS">
      <data key="d4">1.0</data>
      <data key="d5">The Reporting section contains the Fields, which are specific configuration options for reporting settings</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="FIELDS" target="CONNECTION STRING">
      <data key="d4">1.0</data>
      <data key="d5">The Fields include the Connection String property, which is used for blob storage type</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="FIELDS" target="CONTAINER NAME">
      <data key="d4">1.0</data>
      <data key="d5">The Fields include the Container Name property, which is used for blob storage type</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="FIELDS" target="BASE DIR">
      <data key="d4">1.0</data>
      <data key="d5">The Fields include the Base Dir property, which specifies the base directory for writing reports</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="FIELDS" target="SUMMARIZE DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The Summarize Descriptions section contains the Fields, which are specific configuration options for summarizing descriptions settings</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="CONNECTION STRING" target="REPORTING TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The Reporting Type property is related to the Connection String property, as the connection string is only relevant when the reporting type is set to blob</data>
      <data key="d6">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </edge>
    <edge source="CONTAINER NAME" target="REPORTING TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The Reporting Type property is related to the Container Name property, as the container name is only relevant when the reporting type is set to blob</data>
      <data key="d6">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </edge>
    <edge source="PROMPT STR" target="SUMMARIZE_DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The prompt str setting is used in the summarize_descriptions process, specifying the prompt file to guide the summarization</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="PROMPT STR" target="CLAIM_EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The prompt str setting is used in the claim_extraction process, specifying the prompt file to guide the claim extraction</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="PROMPT STR" target="COMMUNITY_REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The prompt str setting is used in the community_reports process, specifying the prompt file to guide the report generation</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="SUMMARIZE_DESCRIPTIONS" target="MAX_LENGTH">
      <data key="d4">1.0</data>
      <data key="d5">The max_length setting is used in the summarize_descriptions process, controlling the length of the generated summaries</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="COMMUNITY_REPORTS" target="STRATEGY DICT">
      <data key="d4">1.0</data>
      <data key="d5">The community_reports section can be fully customized by the strategy dict, allowing for the override of default settings and strategies for community reports</data>
      <data key="d6">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </edge>
    <edge source="STRATEGY DICT" target="CLUSTER_GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The cluster_graph section can be fully customized by the strategy dict, allowing for the override of default settings and strategies for cluster graphs</data>
      <data key="d6">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </edge>
    <edge source="STRATEGY DICT" target="EMBED_GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The embed_graph section can be fully customized by the strategy dict, allowing for the override of default settings and strategies for graph embeddings</data>
      <data key="d6">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </edge>
    <edge source="STRATEGY DICT" target="NODE2VEC RANDOM SEED">
      <data key="d4">1.0</data>
      <data key="d5">The node2vec random seed is related to the strategy dict as it can be a parameter within the strategy dict that influences the node2vec algorithm's behavior and results</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="ENABLED BOOL" target="UMAP">
      <data key="d4">1.0</data>
      <data key="d5">The UMAP is related to the enabled bool as the enabled bool determines whether UMAP layouts are enabled or disabled in the system</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="SNAPSHOTS" target="GRAPHML BOOL">
      <data key="d4">1.0</data>
      <data key="d5">The snapshots are related to the graphml bool as the graphml bool is a sub-property of snapshots that controls the emission of graphml snapshots</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="SNAPSHOTS" target="RAW_ENTITIES BOOL">
      <data key="d4">1.0</data>
      <data key="d5">The snapshots are related to the raw_entities bool as the raw_entities bool is a sub-property of snapshots that determines if raw entity snapshots are emitted</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="SNAPSHOTS" target="TOP_LEVEL_NODES BOOL">
      <data key="d4">1.0</data>
      <data key="d5">The snapshots are related to the top_level_nodes bool as the top_level_nodes bool is a sub-property of snapshots that specifies whether top-level-node snapshots are emitted</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="EXTENDS" target="ROOT_DIR">
      <data key="d4">1.0</data>
      <data key="d5">extends can influence the root_dir configuration by inheriting settings from base configurations, which may include the root directory path for the pipeline</data>
      <data key="d6">e01c546120a27319dcbdf7a6b89bab26</data>
    </edge>
    <edge source="RUN.PY" target="PYTHONPATH">
      <data key="d4">1.0</data>
      <data key="d5">run.py requires the PYTHONPATH environment variable to be set to the project's root directory in order to find and import the necessary modules and scripts when executing the script</data>
      <data key="d6">e01c546120a27319dcbdf7a6b89bab26</data>
    </edge>
    <edge source="RUN.PY" target="POETRY SHELL">
      <data key="d4">1.0</data>
      <data key="d5">poetry shell is used to activate the virtual environment with the required dependencies before running run.py, ensuring that the correct environment is set up for the execution of the script</data>
      <data key="d6">e01c546120a27319dcbdf7a6b89bab26</data>
    </edge>
    <edge source="REPORTING TYPE" target="BASE DIRECTORY">
      <data key="d4">1.0</data>
      <data key="d5">The Reporting Type property is related to the Base Directory property, as the base directory is only relevant when the reporting type is set to file</data>
      <data key="d6">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </edge>
    <edge source="WORKFLOW NAME" target="STEPS">
      <data key="d4">1.0</data>
      <data key="d5">The Workflow Name property is related to the Steps property, as steps are part of a specific workflow and can establish dependencies on other workflows through the input property</data>
      <data key="d6">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </edge>
    <edge source="INPUT TYPE" target="FILE TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The Input Type property is related to the File Type property, as the file type is only relevant when the input type is set to file</data>
      <data key="d6">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </edge>
    <edge source="WORKFLOW2" target="WORKFLOW1">
      <data key="d4">1.0</data>
      <data key="d5">workflow2 has a dependency on workflow1, as indicated in the derive step's input source</data>
      <data key="d6">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </edge>
    <edge source="WORKFLOW2" target="DERIVE">
      <data key="d4">1.0</data>
      <data key="d5">The derive step is a part of the workflow2 process, used to process data from col1 and col2 with a dependency on workflow1</data>
      <data key="d6">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </edge>
    <edge source="TIMESTAMP_COLUMN" target="TIMESTAMP_FORMAT">
      <data key="d4">1.0</data>
      <data key="d5">The timestamp_column is related to the timestamp_format as the timestamp_format specifies how the timestamps in the timestamp_column are structured and should be interpreted</data>
      <data key="d6">3900b87693f02c43b4294e38647eb7cd</data>
    </edge>
    <edge source="AUTHOR" target="MESSAGE">
      <data key="d4">1.0</data>
      <data key="d5">The Author is related to the Message as the author is the creator or originator of the message. The relationship is established through the 'author' column in the CSV file that links the author to the message</data>
      <data key="d6">6839baed839d7a5e837af1da93e462e5</data>
    </edge>
    <edge source="MESSAGE" target="DATE">
      <data key="d4">1.0</data>
      <data key="d5">The Message is related to the Date as the date indicates when the message was created or recorded. The relationship is established through the 'date(yyyyMMddHHmmss)' column in the CSV file that provides the timestamp for the message</data>
      <data key="d6">6839baed839d7a5e837af1da93e462e5</data>
    </edge>
    <edge source="POST PROCESS" target="FILE FILTER">
      <data key="d4">1.0</data>
      <data key="d5">The File Filter is related to the Post Process as the filtered files are then processed by the post processing steps, which can include further filtering based on specific columns and values</data>
      <data key="d6">765d8a78606fe81a03a0da4f7ff231fa</data>
    </edge>
    <edge source="CSV FILE PATTERN" target="FILE FILTER">
      <data key="d4">1.0</data>
      <data key="d5">The CSV File Pattern is related to the File Filter as the filter uses the named groups from the pattern to apply additional filtering criteria to the files</data>
      <data key="d6">765d8a78606fe81a03a0da4f7ff231fa</data>
    </edge>
    <edge source="GRAPHRAG_API_BASE" target="GRAPHRAG_API_VERSION">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_API_BASE and GRAPHRAG_API_VERSION are related because the API base URL and the API version together determine the endpoint for making API requests. The API version is crucial for ensuring compatibility with the API base. The strength of this relationship is high because both settings are required for successful API interaction.</data>
      <data key="d6">3da10b454f926a257b9fdf5d2487c0a5</data>
    </edge>
    <edge source="GRAPHRAG_INPUT_TYPE" target="GRAPHRAG_INPUT_FILE_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_INPUT_TYPE and GRAPHRAG_INPUT_FILE_TYPE are related because GRAPHRAG_INPUT_TYPE specifies the type of input data, and GRAPHRAG_INPUT_FILE_TYPE specifies the type of files to be processed. When GRAPHRAG_INPUT_TYPE is set to "file", GRAPHRAG_INPUT_FILE_TYPE becomes relevant as it determines the format of the input files that the system will handle.</data>
      <data key="d6">8ac79ce92be1254dfda9a10eb54ab703</data>
    </edge>
    <edge source="GRAPHRAG_INPUT_TYPE" target="GRAPHRAG_INPUT_FILE_PATTERN">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_INPUT_TYPE and GRAPHRAG_INPUT_FILE_PATTERN are related because GRAPHRAG_INPUT_TYPE specifies the type of input data, and GRAPHRAG_INPUT_FILE_PATTERN specifies the pattern for matching input files. When GRAPHRAG_INPUT_TYPE is set to "file", GRAPHRAG_INPUT_FILE_PATTERN becomes relevant as it determines which files will be selected for processing based on their names.</data>
      <data key="d6">8ac79ce92be1254dfda9a10eb54ab703</data>
    </edge>
    <edge source="GRAPHRAG_INPUT_TYPE" target="GRAPHRAG_INPUT_SOURCE_COLUMN">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_INPUT_TYPE and GRAPHRAG_INPUT_SOURCE_COLUMN are related because GRAPHRAG_INPUT_TYPE specifies the type of input data, and GRAPHRAG_INPUT_SOURCE_COLUMN specifies the column name in the input data that contains the source information. When GRAPHRAG_INPUT_TYPE is set to "file", GRAPHRAG_INPUT_SOURCE_COLUMN becomes relevant as it determines which column in the input data should be used for source information.</data>
      <data key="d6">8ac79ce92be1254dfda9a10eb54ab703</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_TPM" target="GRAPHRAG_EMBEDDING_RPM">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_EMBEDDING_TPM setting is related to the GRAPHRAG_EMBEDDING_RPM as both represent rate limits for embedding operations, with TPM focusing on transactions and RPM on requests</data>
      <data key="d6">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </edge>
    <edge source="GRAPHRAG_STORAGE_TYPE" target="GRAPHRAG_STORAGE_CONNECTION_STRING">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_STORAGE_TYPE property is related to the GRAPHRAG_STORAGE_CONNECTION_STRING property, as the type of storage system dictates the format and requirements of the connection string</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_STORAGE_TYPE" target="GRAPHRAG_STORAGE_CONTAINER_NAME">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_STORAGE_TYPE property is related to the GRAPHRAG_STORAGE_CONTAINER_NAME property, as the type of storage system may require a specific container name for organization and identification</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_STORAGE_TYPE" target="GRAPHRAG_STORAGE_BASE_DIR">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_STORAGE_TYPE property is related to the GRAPHRAG_STORAGE_BASE_DIR property, as the type of storage system determines the base directory structure for storing data</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_CACHE_TYPE" target="GRAPHRAG_CACHE_CONNECTION_STRING">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_CACHE_TYPE property is related to the GRAPHRAG_CACHE_CONNECTION_STRING property, as the type of cache system dictates the format and requirements of the connection string</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_CACHE_TYPE" target="GRAPHRAG_CACHE_CONTAINER_NAME">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_CACHE_TYPE property is related to the GRAPHRAG_CACHE_CONTAINER_NAME property, as the type of cache system may require a specific container name for organization and identification</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_CACHE_TYPE" target="GRAPHRAG_CACHE_BASE_DIR">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_CACHE_TYPE property is related to the GRAPHRAG_CACHE_BASE_DIR property, as the type of cache system determines the base directory structure for storing cached data</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_REPORTING_TYPE" target="GRAPHRAG_REPORTING_CONNECTION_STRING">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_REPORTING_TYPE property is related to the GRAPHRAG_REPORTING_CONNECTION_STRING property, as the type of reporting system dictates the format and requirements of the connection string</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_REPORTING_TYPE" target="GRAPHRAG_REPORTING_CONTAINER_NAME">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_REPORTING_TYPE property is related to the GRAPHRAG_REPORTING_CONTAINER_NAME property, as the type of reporting system may require a specific container name for organization and identification</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_REPORTING_TYPE" target="GRAPHRAG_REPORTING_BASE_DIR">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_REPORTING_TYPE property is related to the GRAPHRAG_REPORTING_BASE_DIR property, as the type of reporting system determines the base directory structure for storing reports</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_ENCODING_MODEL" target="GRAPHRAG_MAX_CLUSTER_SIZE">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_ENCODING_MODEL and GRAPHRAG_MAX_CLUSTER_SIZE are related as they both influence the structure and efficiency of the graph representation</data>
      <data key="d6">1b24101de07b1c195448240237b84b37</data>
    </edge>
    <edge source="DOCUMENT" target="TEXTUNIT">
      <data key="d4">2.0</data>
      <data key="d5">Documents can be split into multiple TextUnits, depending on the configuration, creating a 1-to-many relationship between documents and TextUnits
Documents are divided into smaller TextUnits for processing, establishing a one-to-many or many-to-many relationship depending on the nature of the documents and the analysis requirements</data>
      <data key="d6">81f57cf867ea246ad9a6e794ed613375,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </edge>
    <edge source="TEXTUNIT" target="ENTITY">
      <data key="d4">1.0</data>
      <data key="d5">Entities are extracted from TextUnits, representing people, places, events, or other entity-models</data>
      <data key="d6">85e50a4d70697a2c4420e7a9fc82f22d</data>
    </edge>
    <edge source="TEXTUNIT" target="GRAPH EXTRACTION">
      <data key="d4">2.0</data>
      <data key="d5">TextUnits are processed during the Graph Extraction phase to identify and extract Entities, Relationships, and Claims, forming the basis of the graph structure
The TextUnit is processed by the Graph Extraction process to identify entities and relationships within the text, creating a subgraph for each TextUnit.</data>
      <data key="d6">10d01d36390b307a63fd5bc97d8682c0,81f57cf867ea246ad9a6e794ed613375</data>
    </edge>
    <edge source="ENTITY" target="RELATIONSHIP">
      <data key="d4">1.0</data>
      <data key="d5">Relationships are generated between entities based on the covariates, indicating connections between different entities</data>
      <data key="d6">85e50a4d70697a2c4420e7a9fc82f22d</data>
    </edge>
    <edge source="ENTITY" target="COVARIATE">
      <data key="d4">1.0</data>
      <data key="d5">Covariates contain statements about entities, which may be time-bound, providing context for the entities</data>
      <data key="d6">85e50a4d70697a2c4420e7a9fc82f22d</data>
    </edge>
    <edge source="ENTITY" target="COMMUNITY REPORT">
      <data key="d4">1.0</data>
      <data key="d5">Community Reports are generated by performing hierarchical community detection on entities, providing insights into the structure of the data</data>
      <data key="d6">85e50a4d70697a2c4420e7a9fc82f22d</data>
    </edge>
    <edge source="ENTITY" target="GRAPH EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">During the Graph Extraction phase, Entities are extracted from the text within each TextUnit, contributing to the graph being built</data>
      <data key="d6">81f57cf867ea246ad9a6e794ed613375</data>
    </edge>
    <edge source="RELATIONSHIP" target="GRAPH EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">During the Graph Extraction phase, Relationships are extracted from the text within each TextUnit, contributing to the graph being built</data>
      <data key="d6">81f57cf867ea246ad9a6e794ed613375</data>
    </edge>
    <edge source="COVARIATE" target="COMMUNITY REPORT">
      <data key="d4">1.0</data>
      <data key="d5">Covariates are used in the generation of Community Reports, as they provide context and details about entities which are summarized in the reports</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION WORKFLOW" target="NETWORK VISUALIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Workflow includes Network Visualization as one of its phases, where the network of entities and their relationships are visualized</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION WORKFLOW" target="DOCUMENT PROCESSING">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Workflow includes Document Processing as one of its phases, where documents are processed for further analysis and transformation</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION WORKFLOW" target="COMMUNITY SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Workflow includes Community Summarization as one of its phases, where communities are summarized to provide insights and reports</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION WORKFLOW" target="GRAPH AUGMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Workflow includes Graph Augmentation as one of its phases, where the graph is enhanced with additional information and relationships</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION WORKFLOW" target="GRAPH EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Workflow includes Graph Extraction as one of its phases, where entities and relationships are extracted from the text to form a graph</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION WORKFLOW" target="COMPOSE TEXTUNITS">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Workflow includes Compose TextUnits as one of its phases, where input documents are transformed into TextUnits for graph analysis</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DOCUMENT PROCESSING" target="COMMUNITY EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Community Embedding is a part of the Document Processing phase, where vector representations of communities are created and used in the processing of documents</data>
      <data key="d6">3e292d936b7efa377ba9530456cfd888</data>
    </edge>
    <edge source="DOCUMENT PROCESSING" target="COMMUNITY TABLES EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Community Tables Emission is a process within the Document Processing phase, where the Communities and CommunityReports tables are generated and emitted</data>
      <data key="d6">3e292d936b7efa377ba9530456cfd888</data>
    </edge>
    <edge source="DOCUMENT PROCESSING" target="DOCUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">The Documents table is created during the Document Processing phase, which involves various steps such as Augment, Link to TextUnits, and Avg. Embedding.</data>
      <data key="d6">827fd80da359cf05b091c24e465dd05d</data>
    </edge>
    <edge source="DOCUMENT PROCESSING" target="LINK TO TEXTUNITS">
      <data key="d4">1.0</data>
      <data key="d5">Link to TextUnits is a step within the Document Processing phase that connects documents to text-units, establishing the relationship between these entities.</data>
      <data key="d6">827fd80da359cf05b091c24e465dd05d</data>
    </edge>
    <edge source="DOCUMENT PROCESSING" target="PHASE 6: NETWORK VISUALIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The Document Processing phase is related to Phase 6: Network Visualization as the processed documents are used to support network visualization of high-dimensional vector spaces within existing graphs.</data>
      <data key="d6">827fd80da359cf05b091c24e465dd05d</data>
    </edge>
    <edge source="COMMUNITY SUMMARIZATION" target="COMMUNITY EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Community Summarization is related to Community Embedding, as the summarization of communities is based on the mapping of communities into a lower-dimensional space for visualization and analysis</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="COMMUNITY SUMMARIZATION" target="GRAPH TABLES EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Community Summarization builds upon the data from Graph Tables Emission, generating reports and summaries for communities at different levels of granularity to provide a high-level understanding of the graph</data>
      <data key="d6">5b2968b8f1c891d47ecbe641c3391663</data>
    </edge>
    <edge source="COMMUNITY SUMMARIZATION" target="NODE2VEC ALGORITHM">
      <data key="d4">1.0</data>
      <data key="d5">Community Summarization utilizes the vector representations generated by the Node2Vec algorithm to summarize communities and understand the graph at various levels of granularity</data>
      <data key="d6">5b2968b8f1c891d47ecbe641c3391663</data>
    </edge>
    <edge source="GRAPH EXTRACTION" target="CLAIM">
      <data key="d4">1.0</data>
      <data key="d5">During the Graph Extraction phase, Claims are extracted from the text within each TextUnit, contributing to the graph being built</data>
      <data key="d6">81f57cf867ea246ad9a6e794ed613375</data>
    </edge>
    <edge source="GRAPH EXTRACTION" target="GRAPH SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The output of Graph Extraction, which are subgraphs for each TextUnit, are combined and processed by Graph Summarization to create a single graph with merged entities and relationships.</data>
      <data key="d6">10d01d36390b307a63fd5bc97d8682c0</data>
    </edge>
    <edge source="UMAP DOCUMENTS" target="DOCUMENT EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Umap Documents is related to Document Embedding, as it maps documents into a lower-dimensional space for visualization and analysis, which is a part of the embedding process</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="UMAP DOCUMENTS" target="PHASE 6: NETWORK VISUALIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Umap Documents is a part of Phase 6: Network Visualization, as it is a process that generates a 2D representation of the document space for visualization.</data>
      <data key="d6">56506e2d064c0732efa3cf418057edfd</data>
    </edge>
    <edge source="UMAP ENTITIES" target="ENTITY &amp; RELATIONSHIP EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Umap Entities is related to Entity &amp; Relationship Extraction, as it maps entities into a lower-dimensional space for visualization and analysis, which is based on the entities and relationships identified in the text</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="UMAP ENTITIES" target="PHASE 6: NETWORK VISUALIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Umap Entities is a part of Phase 6: Network Visualization, as it is a process that generates a 2D representation of the entity space for visualization.</data>
      <data key="d6">56506e2d064c0732efa3cf418057edfd</data>
    </edge>
    <edge source="NODES TABLE" target="LINK TO TEXTUNITS">
      <data key="d4">1.0</data>
      <data key="d5">The Nodes Table is related to the Link to TextUnits, as it contains information about the nodes in the graph, including their layout and relationships, which are connected to the corresponding TextUnits</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DOCUMENT EMBEDDING" target="DOCUMENT GRAPH CREATION">
      <data key="d4">1.0</data>
      <data key="d5">Document Embedding is related to Document Graph Creation, as the embedding process converts documents into numerical vectors for analysis and comparison, which is used in the creation of the document graph</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DOCUMENT EMBEDDING" target="AVG. EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Avg. Embedding is a part of the Document Embedding process, which generates a vector representation of documents using an average of embeddings of document slices.</data>
      <data key="d6">827fd80da359cf05b091c24e465dd05d</data>
    </edge>
    <edge source="DOCUMENT GRAPH CREATION" target="DOCUMENT TABLES">
      <data key="d4">1.0</data>
      <data key="d5">Document Graph Creation is related to Document Tables, as the creation of the document graph is based on the information about the documents, including metadata and analysis results, which are stored in the Document Tables</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="COMMUNITY EMBEDDING" target="COMMUNITY TABLES">
      <data key="d4">1.0</data>
      <data key="d5">Community Embedding is related to Community Tables, as the mapping of communities into a lower-dimensional space for visualization and analysis is based on the information about the communities, including their members and properties, which are stored in the Community Tables</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="COMMUNITY EMBEDDING" target="GRAPH EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Community Embedding is related to Graph Embedding as both involve the representation of network structures in a vector space, with Community Embedding focusing specifically on community structures</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="GRAPH EMBEDDING" target="AUGMENTED GRAPH TABLES">
      <data key="d4">1.0</data>
      <data key="d5">Graph Embedding is related to Augmented Graph Tables, as the conversion of the graph into a numerical representation for analysis and visualization is based on the information about the graph after augmentation, including additional entities and relationships, which are stored in the Augmented Graph Tables</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="GRAPH EMBEDDING" target="GRAPH TABLES">
      <data key="d4">1.0</data>
      <data key="d5">Graph Tables are related to Graph Embedding as they are often used to store and organize the results of graph embedding techniques, including details about nodes, edges, and attributes</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="GRAPH EMBEDDING" target="PHASE 3: GRAPH AUGMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">Graph Embedding is a subprocess of Phase 3: Graph Augmentation, aimed at understanding the implicit structure of the graph</data>
      <data key="d6">a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </edge>
    <edge source="GRAPH EMBEDDING" target="NODE2VEC ALGORITHM">
      <data key="d4">1.0</data>
      <data key="d5">Graph Embedding uses the Node2Vec algorithm to generate vector representations of the graph, which is essential for understanding the graph's structure and searching for related concepts</data>
      <data key="d6">5b2968b8f1c891d47ecbe641c3391663</data>
    </edge>
    <edge source="GRAPH EMBEDDING" target="GRAPH TABLES EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Graph Tables Emission follows the Graph Embedding step, where the final Entities and Relationships tables are emitted after text embedding, providing a structured representation of the graph data</data>
      <data key="d6">5b2968b8f1c891d47ecbe641c3391663</data>
    </edge>
    <edge source="ENTITY &amp; RELATIONSHIP EXTRACTION" target="ENTITY &amp; RELATIONSHIP SUMMARIZATION">
      <data key="d4">2.0</data>
      <data key="d5">Entity &amp; Relationship Extraction is related to Entity &amp; Relationship Summarization, as the identification of entities and their relationships from the text is used in the summarization of the entities and relationships
Entity &amp; Relationship Summarization is related to Entity &amp; Relationship Extraction as the former process often relies on the results of the latter to provide a concise overview of the entities and relationships identified</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="ENTITY &amp; RELATIONSHIP EXTRACTION" target="TEXT UNITS">
      <data key="d4">1.0</data>
      <data key="d5">Entity &amp; Relationship Extraction is related to Text Units as the process often operates on Text Units to identify entities and relationships within the text</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="ENTITY &amp; RELATIONSHIP SUMMARIZATION" target="ENTITY RESOLUTION">
      <data key="d4">1.0</data>
      <data key="d5">Entity &amp; Relationship Summarization is related to Entity Resolution, as the summarization of the entities and relationships identified in the text is used in the resolution of entities that refer to the same real-world object or concept</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="ENTITY RESOLUTION" target="DOCUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">Entity Resolution is related to Documents as the process often operates on data from documents to identify and merge duplicate or similar entities</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="ENTITY RESOLUTION" target="GRAPH SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The graph produced by Graph Summarization is further processed by Entity Resolution to identify and merge entities that represent the same real-world entity but have different names.</data>
      <data key="d6">10d01d36390b307a63fd5bc97d8682c0</data>
    </edge>
    <edge source="ENTITY RESOLUTION" target="COVARIATES">
      <data key="d4">1.0</data>
      <data key="d5">Entity Resolution does not directly produce Covariates, but the resolved entities and their relationships can be part of the context for Claim Extraction, which emits Covariates.</data>
      <data key="d6">d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </edge>
    <edge source="DOCUMENTS" target="TEXT UNITS">
      <data key="d4">1.0</data>
      <data key="d5">Text Units are related to Documents as Text Units are derived from documents and used as the basis for further analysis or extraction, with a strict 1-to-many relationship by default</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="COVARIATES" target="CLAIM EXTRACTION &amp; EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Covariates are the primary artifacts emitted from the Claim Extraction &amp; Emission process, representing claims that are positive factual statements with an evaluated status and time-bounds</data>
      <data key="d6">a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </edge>
    <edge source="PHASE 3: GRAPH AUGMENTATION" target="GRAPH TABLES EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Graph Tables Emission is a subprocess of Phase 3: Graph Augmentation, involving emitting the final entities and relationships in a tabular format</data>
      <data key="d6">a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </edge>
    <edge source="AUGMENT" target="AUGMENT WITH COLUMNS (CSV ONLY)">
      <data key="d4">1.0</data>
      <data key="d5">The Augment process is related to Augment with Columns (CSV Only) as it allows for the addition of extra fields to the Documents output when processing CSV data.</data>
      <data key="d6">827fd80da359cf05b091c24e465dd05d</data>
    </edge>
    <edge source="DOCUMENT TABLE EMISSION" target="DOCUMENTS TABLE EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Document Table Emission is the process of emitting the Documents table into the knowledge model, which is a part of the Documents Table Emission function.</data>
      <data key="d6">827fd80da359cf05b091c24e465dd05d</data>
    </edge>
    <edge source="DOCUMENTS TABLE EMISSION" target="PHASE 6: NETWORK VISUALIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The Documents Table Emission is a prerequisite for Phase 6: Network Visualization, as it provides the data necessary for network visualization and understanding the relationships between documents.</data>
      <data key="d6">56506e2d064c0732efa3cf418057edfd</data>
    </edge>
    <edge source="PHASE 6: NETWORK VISUALIZATION" target="ENTITY-RELATIONSHIP GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">Phase 6: Network Visualization involves the Entity-Relationship graph, as it is one of the logical graphs that are visualized to understand the relationships between entities and their attributes.</data>
      <data key="d6">56506e2d064c0732efa3cf418057edfd</data>
    </edge>
    <edge source="PHASE 6: NETWORK VISUALIZATION" target="DOCUMENT GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">Phase 6: Network Visualization involves the Document graph, as it is one of the logical graphs that are visualized to understand the relationships between documents.</data>
      <data key="d6">56506e2d064c0732efa3cf418057edfd</data>
    </edge>
    <edge source="PHASE 6: NETWORK VISUALIZATION" target="NODES TABLE EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Nodes Table Emission is a part of Phase 6: Network Visualization, as it is a process that emits a table of nodes for visualization, including information about whether the node is a document or an entity and the UMAP coordinates.</data>
      <data key="d6">56506e2d064c0732efa3cf418057edfd</data>
    </edge>
    <edge source="DISCRIMINATOR" target="UMAP COORDINATES">
      <data key="d4">1.0</data>
      <data key="d5">The Discriminator and UMAP Coordinates are related in that the Discriminator can help in interpreting the UMAP Coordinates by providing context on whether the coordinates represent a document or an entity in the data structure</data>
      <data key="d6">2011f03f21e526cf9277c27bf3e68242</data>
    </edge>
    <edge source="GRAPHRAG INDEXER CLI" target="CLI ARGUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Indexer CLI utilizes CLI Arguments to customize its functionality and behavior, allowing users to configure various aspects of the indexing process.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="CLI ARGUMENTS" target="VERBOSE">
      <data key="d4">1.0</data>
      <data key="d5">The verbose argument is a CLI Argument that can be used to increase the logging level during the execution of the GraphRAG Indexer CLI.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="CLI ARGUMENTS" target="INIT">
      <data key="d4">1.0</data>
      <data key="d5">The init argument is a CLI Argument that initializes the data project directory for the GraphRAG Indexer CLI, setting up the necessary configuration and overrides.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="CLI ARGUMENTS" target="RESUME">
      <data key="d4">1.0</data>
      <data key="d5">The resume argument is a CLI Argument that enables the GraphRAG Indexer CLI to resume a previous run, loading parquet files as inputs and skipping certain workflows.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="CLI ARGUMENTS" target="CONFIG">
      <data key="d4">1.0</data>
      <data key="d5">The config argument is a CLI Argument that allows the GraphRAG Indexer CLI to execute a custom configuration, overriding the default settings.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="CLI ARGUMENTS" target="REPORTER">
      <data key="d4">1.0</data>
      <data key="d5">The reporter argument is a CLI Argument that specifies the progress reporting mechanism for the GraphRAG Indexer CLI.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="CLI ARGUMENTS" target="EMIT">
      <data key="d4">1.0</data>
      <data key="d5">The emit argument is a CLI Argument that determines the output formats for the tables generated by the GraphRAG Indexer CLI.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
  </graph>
</graphml>