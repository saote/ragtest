<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d6" for="edge" attr.name="source_id" attr.type="string" />
  <key id="d5" for="edge" attr.name="description" attr.type="string" />
  <key id="d4" for="edge" attr.name="weight" attr.type="double" />
  <key id="d3" for="node" attr.name="entity_type" attr.type="string" />
  <key id="d2" for="node" attr.name="source_id" attr.type="string" />
  <key id="d1" for="node" attr.name="description" attr.type="string" />
  <key id="d0" for="node" attr.name="type" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="PYTHON 3.10-3.12">
      <data key="d0">SOFTWARE, VERSION</data>
      <data key="d1">The Python programming language versions 3.10 to 3.12 are the supported and required versions for running and developing the GraphRAG system. These versions serve as the foundation for executing the library and its associated scripts, ensuring compatibility and optimal performance of the GraphRAG system.</data>
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0,84d24b5db902baca7217b5e3bb6ec462</data>
    </node>
    <node id="POETRY">
      <data key="d0">SOFTWARE, TOOL</data>
      <data key="d1">Poetry, a versatile package manager for Python projects, plays a crucial role in managing dependencies and virtual environments. It enables users to run the command-line interface (CLI) smoothly by setting up the required environment and installing necessary dependencies. This tool is indispensable for GraphRAG, as it facilitates the installation and management of dependencies, ensuring the project's seamless execution.</data>
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0,b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="AZURITE">
      <data key="d0">SOFTWARE, EMULATOR</data>
      <data key="d1">Azurite is an emulator for Azure resources, used in unit and smoke tests to simulate Azure environments for testing purposes.</data>
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </node>
    <node id="INDEXING ENGINE">
      <data key="d0">SOFTWARE, ENGINE</data>
      <data key="d1">The Indexing Engine is a component of GraphRAG that can be executed using Poetry to process and index data for the graph database.</data>
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </node>
    <node id="QUERY ENGINE">
      <data key="d0">SOFTWARE, ENGINE</data>
      <data key="d1">The Query Engine is a pivotal component of the GraphRAG system, serving as the retrieval module within the Graph RAG Library. It operates in tandem with the Indexing Pipeline, forming one of the two main components of the library. The Engine is designed to facilitate the querying of the graph database, enabling users to ask questions and retrieve information from the indexed data through various methods. It supports Global search for high-level questions, which is resource-intensive but provides comprehensive responses by understanding the dataset as a whole, and Local search for more specific inquiries, which is suitable for questions requiring an understanding of specific entities mentioned in the documents. The Engine combines relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents and searches over all AI-generated community reports in a map-reduce fashion to generate answers. Additionally, it features question generation functionality, which is beneficial for creating follow-up questions in a conversation or for generating a list of questions to help investigators delve deeper into the dataset. The Query Engine can be executed using Poetry, enhancing its usability and flexibility in querying the graph database.</data>
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0,7c1bad237a1ef86cb41b6c5dbad4ffc3,f8cf53ce98a8bc52581f7907ad98ef70</data>
    </node>
    <node id="LIFECYCLE SCRIPTS">
      <data key="d0">SOFTWARE, SCRIPTS</data>
      <data key="d1">Lifecycle Scripts are a set of scripts managed by Poetry and poethepoet, used for various tasks such as building, testing, and executing the GraphRAG package.</data>
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </node>
    <node id="UNIT AND SMOKE TESTS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </node>
    <node id="CLI">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">CLI, or Command Line Interface, is a versatile tool enabling users to interact with software by inputting text commands. Specifically, in the context provided, CLI serves as the interface for executing tasks using the poetry tool. This tool facilitates the operation of a pipeline by allowing users to provide commands and arguments. It can be seamlessly integrated with a Config File, either in default or custom config mode, to enhance its functionality. Depending on the user's environment, the CLI can be accessed through either Poetry or Node, offering flexibility in usage.</data>
      <data key="d2">563caa38fe33c495449888d62950b959,b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="POETRY RUN POE QUERY">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe query is a command used to run the Query CLI, which allows for executing queries through the command line interface.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY BUILD">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry build is a command that invokes the build process, creating a wheel file and other distributable artifacts for the package.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE TEST">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe test is a command that executes all tests for the package, ensuring its functionality and integrity.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE TEST_UNIT">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe test_unit is a command that specifically executes unit tests, which are tests that verify the correctness of individual units of code.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE TEST_INTEGRATION">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe test_integration is a command that executes integration tests, which are tests that verify the interaction between different parts of the system.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE TEST_SMOKE">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe test_smoke is a command that executes smoke tests, which are a preliminary set of tests to ensure that the system is in a stable state before more extensive testing.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE CHECK">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poe check is a command that performs a suite of static checks across the package, including formatting, documentation formatting, linting, security patterns, and type-checking.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE FIX">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe fix is a command that applies any available auto-fixes to the package, typically limited to formatting fixes.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE FIX_UNSAFE">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe fix_unsafe is a command that applies any available auto-fixes to the package, including those that may be unsafe, potentially altering the code in ways that could affect its functionality.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="POETRY RUN POE FORMAT">
      <data key="d0">COMMAND, TOOL</data>
      <data key="d1">poetry run poe format is a command that explicitly runs the formatter across the package, ensuring consistent code style and formatting.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="TROUBLESHOOTING">
      <data key="d0">SECTION, DOCUMENT</data>
      <data key="d1">Troubleshooting is a section that provides solutions to common problems encountered when using the software or executing commands. It includes specific steps to resolve issues such as missing dependencies or configuration errors.</data>
      <data key="d2">563caa38fe33c495449888d62950b959</data>
    </node>
    <node id="RUNTIMEERROR">
      <data key="d0">ERROR, SOFTWARE_ISSUE</data>
      <data key="d1">A RuntimeError occurred when executing the command 'poetry install', indicating that 'llvm-config' failed to execute. The error suggests that the user should point the LLVM_CONFIG environment variable to the path for llvm-config. This issue is related to the LLVM configuration and is a common software error encountered during the installation process.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="LLVM_CONFIG">
      <data key="d0">ENVIRONMENT_VARIABLE, CONFIGURATION</data>
      <data key="d1">LLVM_CONFIG is an environment variable that needs to be set to the path of the llvm-config executable. This variable is crucial for the proper functioning of LLVM-related tools and components in the software development environment.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="LLVM-9">
      <data key="d0">SOFTWARE, DEVELOPMENT_TOOL</data>
      <data key="d1">llvm-9 is a version of the LLVM compiler infrastructure, which is a collection of modular and reusable compiler and toolchain technologies. It is required for the installation process to proceed without errors related to the 'llvm-config' executable.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="LLVM-9-DEV">
      <data key="d0">SOFTWARE, DEVELOPMENT_TOOL</data>
      <data key="d1">llvm-9-dev is a development package for the LLVM compiler infrastructure version 9. It contains headers and libraries necessary for building software that uses LLVM, and is required for the installation process to proceed without errors related to the 'llvm-config' executable.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="POETRY INSTALL">
      <data key="d0">COMMAND, INSTALLATION_PROCESS</data>
      <data key="d1">poetry install is a command used in the poetry package manager to install all dependencies for a project as specified in the pyproject.toml file. This command is essential for setting up the development environment for a Python project.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="PYTHON.H">
      <data key="d0">FILE, HEADER_FILE</data>
      <data key="d1">Python.h is a header file that provides the definitions and declarations for the Python C API. It is required for building C extensions for Python and is missing when the error "numba/_pymodule.h:6:10: fatal error: Python.h: No such file or directory" occurs during the 'poetry install' process.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="PYTHON3.10-DEV">
      <data key="d0">SOFTWARE, DEVELOPMENT_TOOL</data>
      <data key="d1">python3.10-dev is a development package for Python version 3.10. It contains headers, libraries, and other resources necessary for building Python extensions and applications. It is required to resolve the error related to the missing Python.h file during the 'poetry install' process.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </node>
    <node id="GRAPHRAG_LLM_THREAD_COUNT">
      <data key="d0">ENVIRONMENT_VARIABLE, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_THREAD_COUNT is a configuration property and environment variable within the GraphRAG system, specifically impacting the Large Language Model (LLM) component. This variable determines the number of threads allocated for processing requests in the Human-in-the-Loop Request Augmentation Gateway (HRAG). By default, GRAPHRAG_LLM_THREAD_COUNT is set to 50, enabling high concurrency. However, this default setting may lead to potential performance issues due to the high number of threads. Modifying GRAPHRAG_LLM_THREAD_COUNT can help in reducing concurrency and enhancing system stability.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205,9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_THREAD_COUNT">
      <data key="d0">ENVIRONMENT_VARIABLE, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_EMBEDDING_THREAD_COUNT is an environment variable pivotal in the Motor Control and Drive Systems domain, specifically within the context of the GraphRAG system. This variable dictates the number of threads allocated to the embedding component, a crucial aspect of the system's performance and efficiency. By default, GRAPHRAG_EMBEDDING_THREAD_COUNT is set to 50, which, while promoting high concurrency, may inadvertently trigger performance issues due to excessive thread usage. Adjusting this variable can significantly mitigate concurrency-related problems, thereby enhancing the stability and overall performance of the GraphRAG system. This understanding is essential for professionals in the Motor Control and Drive Systems domain looking to optimize their systems and avoid potential bottlenecks.</data>
      <data key="d2">0dc1f5e4f8fb5903f12acf8e141fb205,2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG">
      <data key="d0">PRODUCT, TECHNOLOGY</data>
      <data key="d1">GraphRAG (Graph Retrieval-Augmented Generation) is a sophisticated software tool developed by Microsoft Research, designed to enhance the capabilities of Large Language Models (LLMs) in reasoning about complex information, particularly within private datasets. It operates by extracting a knowledge graph from the input corpus, which is then augmented with community summaries and graph machine learning outputs to improve query responses. GraphRAG demonstrates a significant enhancement in answering intricate questions compared to other approaches.

The process involves indexing and slicing the input corpus into TextUnits, followed by entity and relationship extraction. Hierarchical clustering using the Leiden technique is applied to identify themes and semantic clusters, enabling a more effective global search. Summaries of each community are generated to provide a holistic understanding of the dataset, facilitating the summarization of themes in response to user queries.

GraphRAG is a structured, hierarchical approach to Retrieval Augmented Generation (RAG) that leverages the natural modularity of graphs to partition data for global summarization. It is distinct from other graph-based RAG systems by its ability to create domain adaptive templates for the generation of knowledge graphs. This feature improves the results of an Index Run by adapting to specific domains through the use of templates.

The tool requires an initialized workspace with the graphrag.index --init command to function properly. The initialization process creates necessary configuration files and default prompts. Users can customize the template generation algorithm by tweaking various parameters, allowing for greater flexibility and adaptability to different domains.

GraphRAG is a system for managing and processing graph-based data, offering features such as indexing, querying, and adapting prompts for better data handling. It is a software product that enhances the question-and-answer performance of Language Models (LMs) by using knowledge graphs, aiming to provide better insights and understanding compared to Baseline RAG. GraphRAG is a tool that enables the analysis of LLM-generated knowledge graphs, providing insights into the structure and themes of datasets.</data>
      <data key="d2">12294feb07a1d202b27241eaaf64718b,32603b739bed06b4695b0cc3915b2c4b,369b39fdfd649d6df32a5d7b4cc559b7,3e143a60e2aeb57eb418a68d1484bbb3,812b3414c467da0b62f7932d2adcbad4,849698743b07680402ff8572b1c6c469,9b364093aeecfc789c70fc5bd9503487,d0f7c236538005bc3056b7daed2401d8,d441b136505c273cf3577b6867e872e4,e015335cdcae20e6546fe7cbdef56c1a,e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="GPT-4 TURBO">
      <data key="d0">PRODUCT, TECHNOLOGY</data>
      <data key="d1">GPT-4 Turbo is a version of the GPT-4 model that is used to generate knowledge graphs from text data. It is capable of creating detailed and structured representations of information, which can be used in various applications including GraphRAG.</data>
      <data key="d2">e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="MICROSOFT RESEARCH BLOG POST">
      <data key="d0">ONLINE_RESOURCE</data>
      <data key="d1">The Microsoft Research Blog Post is an online resource that provides information about GraphRAG and how it can be used to enhance the ability of Large Language Models (LLMs) to reason about private data. It offers insights into the GraphRAG system and its applications.</data>
      <data key="d2">e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="GITHUB REPOSITORY">
      <data key="d0">ONLINE_RESOURCE</data>
      <data key="d1">The GitHub Repository is an online resource where the source code and documentation for GraphRAG can be found. It is a platform for developers to access, contribute to, and collaborate on the GraphRAG project.</data>
      <data key="d2">e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="GRAPHRAG ARXIV">
      <data key="d0">ONLINE_RESOURCE</data>
      <data key="d1">GraphRAG Arxiv is an online resource that contains research papers and documentation related to GraphRAG. It is a repository for academic and technical papers that discuss the theory, implementation, and applications of GraphRAG.</data>
      <data key="d2">e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="SOLUTION ACCELERATOR">
      <data key="d0">PRODUCT, TECHNOLOGY</data>
      <data key="d1">The Solution Accelerator is a package that provides a user-friendly end-to-end experience with Azure resources for quickstarting the GraphRAG system. It is recommended for users who want to start using GraphRAG with ease.</data>
      <data key="d2">e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="GET STARTED GUIDE">
      <data key="d0">ONLINE_RESOURCE</data>
      <data key="d1">The "GET STARTED GUIDE" is a comprehensive resource designed to assist new users in effectively utilizing the GraphRAG software. Available both as a document and an online resource, it offers clear instructions and information to help users understand the basic functionalities and setup process of the system. The guide provides a step-by-step approach, making it easy for beginners to navigate through the initial stages of using GraphRAG.</data>
      <data key="d2">32603b739bed06b4695b0cc3915b2c4b,e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="INDEXER">
      <data key="d0">PRODUCT, TECHNOLOGY</data>
      <data key="d1">The Indexer is a sophisticated software tool and a critical sub-system of GraphRAG, designed to process, organize, and prepare data for efficient search, analysis, and retrieval. It excels in handling and processing large datasets by extracting information from raw text, indexing it, and building a structured knowledge graph represented in .parquet output files. These files contain structured data that is accessible for both Local and Global search methods, ensuring that the data is ready for querying and analysis. The Indexer's role is pivotal in the GraphRAG process, as it creates a structured representation of information, enabling the software to manage and process extensive data sets effectively.</data>
      <data key="d2">32603b739bed06b4695b0cc3915b2c4b,8c70a7321fb0e945054d226a8c69abee,ae6e91a8cc5773dbd4789773c9ef5a30,e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="QUERY PACKAGE">
      <data key="d0">PRODUCT, TECHNOLOGY</data>
      <data key="d1">The Query package, a crucial sub-system of GraphRAG, serves as a powerful tool for users seeking to explore and extract information from the knowledge graph. It facilitates the search process within the indexed data, allowing users to interact effectively with the graph and retrieve pertinent results. This subsystem ensures that users can efficiently query the knowledge graph, making it an essential component for anyone aiming to harness the full potential of GraphRAG's data resources.</data>
      <data key="d2">32603b739bed06b4695b0cc3915b2c4b,e6fa3bdaf65c92df6b3430f02804321a</data>
    </node>
    <node id="SOLUTION ACCELERATOR PACKAGE">
      <data key="d0">PRODUCT, SOFTWARE</data>
      <data key="d1">The Solution Accelerator package is a software product designed to offer a user-friendly experience in managing and utilizing Azure resources, streamlining the process for end-users.</data>
      <data key="d2">32603b739bed06b4695b0cc3915b2c4b</data>
    </node>
    <node id="BASELINE RAG">
      <data key="d0">PRODUCT, SOFTWARE</data>
      <data key="d1">Baseline RAG (Retrieval-Augmented Generation) is a foundational technology in LLM-based tools that enhances the outputs of Language Models (LMs) by incorporating real-world information through a vector search method. This technique identifies semantically similar text content within a dataset, aiming to solve problems in communications by connecting disparate pieces of information. However, Baseline RAG has limitations in handling complex information and reasoning about private datasets. It struggles with queries that require aggregation of information across the dataset, as it relies on finding text that is semantically similar to the query, rather than traversing through shared attributes for new insights. Additionally, Baseline RAG performs poorly in understanding summarized semantic concepts over large data collections or singular large documents. Despite these challenges, Baseline RAG remains a crucial approach in leveraging vector similarity to augment language model outputs.</data>
      <data key="d2">32603b739bed06b4695b0cc3915b2c4b,3e143a60e2aeb57eb418a68d1484bbb3,d441b136505c273cf3577b6867e872e4</data>
    </node>
    <node id="LLMS">
      <data key="d0">TECHNOLOGY, ARTIFICIAL INTELLIGENCE</data>
      <data key="d1">LLMS, also known as Large Language Models (LLMs), are sophisticated AI models designed to process and generate text that closely mimics human language. These models are widely utilized in research studies and various applications due to their ability to understand complex linguistic patterns. In the context of GraphRAG, LLMs play a pivotal role in constructing a knowledge graph from an input corpus. They excel in tasks such as entity and relationship extraction, as well as hierarchical clustering, making them indispensable tools for analyzing and structuring large volumes of textual data.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96,53455f8552b0787cb13c5a03eb550842,6dace8e490674ac8e031aed987a63789,d441b136505c273cf3577b6867e872e4</data>
    </node>
    <node id="LEIDEN TECHNIQUE">
      <data key="d0">METHOD, ALGORITHM</data>
      <data key="d1">The Leiden Technique is a sophisticated algorithm employed in the GraphRAG process for hierarchical clustering of graphs. This method, a refined version of the Louvain method, is instrumental in visually representing the clustering of entities and relationships within the graph. In the visual representation, entities are depicted as circles, where the size of the circle corresponds to the degree of the entity, and the color signifies its community, providing a comprehensive and intuitive understanding of the graph's structure and dynamics.</data>
      <data key="d2">369b39fdfd649d6df32a5d7b4cc559b7,d441b136505c273cf3577b6867e872e4</data>
    </node>
    <node id="TEXTUNITS">
      <data key="d0">DATA UNIT</data>
      <data key="d1">TextUnits are the analyzable units created by slicing up the input corpus in the GraphRAG process. They serve as the basis for entity, relationship, and key claim extraction and provide fine-grained references into the outputs.</data>
      <data key="d2">369b39fdfd649d6df32a5d7b4cc559b7</data>
    </node>
    <node id="LLM">
      <data key="d0">TECHNOLOGY, TOOL</data>
      <data key="d1">LLM, or Language Learning Model, is a pivotal technology in the domain of text analysis, serving multiple functions such as summarization, entity resolution, and claim extraction. It is employed to capture unique information from various descriptions, reconcile entities that denote the same real-world entity but are known by different names, and distill claims from source TextUnits. LLM acts as a configuration property in entity extraction, where it specifies the language model to be utilized for analyzing text. This technology is integral to the GraphRAG process, aiding in the identification of entities, relationships, and key claims within TextUnits. The 'llm' configuration setting is critical, as it determines the AI or machine learning model that will process the text in various processes such as entity extraction, summarization, and claim extraction. Additionally, 'llm' is a configuration field within community_reports, referring to the top-level LLM configuration. This setting encompasses parameters that define the operational aspects of the language model and its application within the system.</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc,369b39fdfd649d6df32a5d7b4cc559b7,3e143a60e2aeb57eb418a68d1484bbb3,53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,9cbd4e21339eeed5e22a638e52a094cb,abac77a5673e907cf8d65161c2612784,d27237468a1b9e89110eeeca8080f63c,d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </node>
    <node id="COMMUNITY SUMMARIES">
      <data key="d0">DATA, OUTPUT</data>
      <data key="d1">Community Summaries are a pivotal component within the GraphRAG framework, serving as a kind of self-memory for generation-augmented retrieval that facilitates future generation cycles. These summaries are pre-generated for groups of closely-related entities identified in the entity knowledge graph, designed to scale to very large datasets and independently useful for understanding the global structure and semantics of the dataset. They are created for each community in the Leiden hierarchy, allowing for answering questions at different hierarchical levels, which may offer varying balances of detail and scope for general sensemaking questions. The summaries are categorized into different levels (C0-C3), with C0 being the root-level summary and C3 the low-level summary, providing a more concise representation of the original content.

The summaries are prepared by shuffling and dividing them into chunks of pre-specified token size to ensure even distribution of relevant information. They are ranked based on the number of tokens they contain, and shorter summaries may replace longer ones to fit within the context window. The summaries are used to generate community answers, which are then combined into a global answer. The global answer is generated by mapping community answers, generating intermediate answers for each chunk, and reducing them to a single global answer. The intermediate answers are scored by an LLM on a scale of 0-100 based on their helpfulness in answering the target question, with answers scoring 0 being filtered out. The global answer is the final result of this process, making the community summaries a crucial element in the generation of a comprehensive and contextually relevant response to user queries.

For leaf-level communities, element summaries are added to the LLM context window until the token limit is reached. For higher-level communities, if all element summaries fit within the token limit, they are summarized as is; otherwise, sub-community summaries are substituted for their associated element summaries to fit within the context window. Community summaries are condensed versions of information or data, often created by or for a community, which can be used to provide an overview or key insights from a larger set of data or documents. They are generated for leaf-level and higher-level communities to provide an overview of the elements (nodes, edges, covariates) within a corpus, prioritized based on the prominence of the elements, and used to make sense of a corpus or to answer global queries.</data>
      <data key="d2">369b39fdfd649d6df32a5d7b4cc559b7,7040ba36a7c09899a355d14a30d65375,71f14506a6b15dfabd93fd1606a67b73,7da3d8d244b67f09425a4a7783e4bb55,849698743b07680402ff8572b1c6c469,93d4d4effbf989e6ef1c4c3b4f42494e,9b52298451f8936974ab08a129b0b92e,a660289d2bf43f25d3524d35cd2d9a96,b149708d0b4ac3ff417565739ea6b03b,d08fc91bbfe9749abab38a99a1a88dc6,f76c18c7582167c3626f8741c2c9374f</data>
    </node>
    <node id="GLOBAL SEARCH">
      <data key="d0">FUNCTION, QUERY MODE</data>
      <data key="d1">Global Search is a comprehensive query method designed to answer complex, high-level questions about a dataset by aggregating information across the entire dataset. This method leverages the structure of an LLM-generated knowledge graph to identify themes and semantic clusters, enabling the summarization of these themes in response to user queries. Global Search is particularly useful after the Indexer has processed the data, as it allows for the exploration of the dataset's overall content and structure. In the context of the GraphRAG process, Global Search serves as a query mode that facilitates reasoning about holistic questions concerning the corpus by utilizing community summaries. The Query Engine employs Global Search to generate answers by searching over all AI-generated community reports in a map-reduce fashion, which, although resource-intensive, often yields high-quality responses for questions that require an understanding of the dataset as a whole.</data>
      <data key="d2">369b39fdfd649d6df32a5d7b4cc559b7,3e143a60e2aeb57eb418a68d1484bbb3,ae6e91a8cc5773dbd4789773c9ef5a30,f8cf53ce98a8bc52581f7907ad98ef70</data>
    </node>
    <node id="LOCAL SEARCH">
      <data key="d0">FUNCTION, QUERY MODE</data>
      <data key="d1">Local Search is a sophisticated method employed in the GraphRAG process and the Query Engine, designed to answer specific questions about particular aspects or entities within a dataset. This technique combines structured data from the knowledge graph with unstructured data from input documents to augment the context of a language model with relevant entity information at query time. It is particularly adept at providing detailed information about specific characters, themes, or relationships within the data, offering a more targeted approach to data exploration.

Local Search enables reasoning about specific entities by exploring their immediate neighbors and associated concepts in a graph or network. It uses a context builder class to generate context for the search and can be enhanced with various parameters such as llm_params and context_builder_params. This method is suitable for questions that require an understanding of specific entities mentioned in the documents, as it can extract and prioritize relevant structured and unstructured data, including entities, relationships, and other data records, which are then used to generate context for question generation.

However, Local Search is limited in its ability to aggregate information across the entire dataset, focusing instead on finding information that is semantically similar to a query within a specific context. By fanning out to the neighbors and associated concepts of a particular entity, Local Search offers a powerful tool for data exploration and question answering in specialized professional networks and beyond.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4,364624242a84e1859e758069d914d8c8,369b39fdfd649d6df32a5d7b4cc559b7,3e143a60e2aeb57eb418a68d1484bbb3,849698743b07680402ff8572b1c6c469,ae6e91a8cc5773dbd4789773c9ef5a30,f8cf53ce98a8bc52581f7907ad98ef70</data>
    </node>
    <node id="PROMPT TUNING">
      <data key="d0">FUNCTION, TECHNIQUE</data>
      <data key="d1">Prompt Tuning is a pivotal feature of the GraphRAG indexing engine, designed to optimize and customize the prompts used in the generation of knowledge graphs. This technique is highly recommended for enhancing the performance of GraphRAG with specific datasets, as it allows for the creation of domain-adaptive templates that can significantly improve the outcomes of Index Runs. The process entails loading inputs, dividing them into text units, and executing a series of Large Language Model (LLM) invocations and template substitutions to generate refined prompts. Users are provided with default values by the system for ease of use, but the real power of Prompt Tuning lies in its ability to fine-tune these prompts according to the guidelines detailed in the GraphRAG documentation. By specifying a custom prompt file in plaintext and employing token replacements, users can tailor the prompts to their specific use cases, thereby boosting the effectiveness of knowledge discovery and natural language processing tasks. This customization ensures that the models perform optimally in the context of graph-based reasoning, making Prompt Tuning an essential tool for achieving the best possible results in knowledge graph creation.</data>
      <data key="d2">369b39fdfd649d6df32a5d7b4cc559b7,4f37c0e9c3c9bac4e5c1c6821eea442e,6a7157695d90d434b2625c3f05420916,849698743b07680402ff8572b1c6c469,bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="CORPUS">
      <data key="d0">DOCUMENT, DATA</data>
      <data key="d1">The corpus refers to a collection of documents or data that is being analyzed or processed, often used in natural language processing tasks to train models or extract information.</data>
      <data key="d2">849698743b07680402ff8572b1c6c469</data>
    </node>
    <node id="INDEXING PIPELINE">
      <data key="d0">SOFTWARE, MODULE</data>
      <data key="d1">The Indexing Pipeline is a crucial system component within the Graph RAG library, designed to index data for efficient searchability and accessibility. It operates alongside the Query Engine, the other primary component of the library. The Indexing Pipeline prepares data by creating an index, enabling the Query Engine to perform searches and generate answers. This process can be initiated after setting up the workspace and adapting prompts. It involves executing a Python script with parameters such as the root directory and model configuration to structure the data for subsequent querying. As one of the two main components of the Graph RAG library, the Indexing Pipeline plays a pivotal role in making data searchable and ready for various operations and analyses.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,7c1bad237a1ef86cb41b6c5dbad4ffc3,d0f7c236538005bc3056b7daed2401d8,f8cf53ce98a8bc52581f7907ad98ef70</data>
    </node>
    <node id="QUESTION GENERATION">
      <data key="d0">FUNCTION, PROCESS</data>
      <data key="d1">Question Generation is a sophisticated feature of the Query Engine designed to enhance data exploration and conversation flow. This functionality takes a list of user queries and generates the next set of candidate questions, combining structured data from a knowledge graph with unstructured data from input documents. It is particularly useful for creating follow-up questions that represent important or urgent information content or themes in the data, enabling investigators to explore a dataset more thoroughly and dive deeper into its nuances. Detailed information on how this feature works can be found at the Question Generation documentation page.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4,364624242a84e1859e758069d914d8c8,f8cf53ce98a8bc52581f7907ad98ef70</data>
    </node>
    <node id="GLOBAL SEARCH DOCUMENTATION">
      <data key="d0">DOCUMENT, REFERENCE</data>
      <data key="d1">The Global Search documentation provides detailed information about the Global Search functionality, which is a tool for searching and retrieving information across various sources. It is referenced as a source for more information about the topic discussed in the text.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="ENTITY-BASED REASONING">
      <data key="d0">CONCEPT, APPROACH</data>
      <data key="d1">Entity-based Reasoning is an approach used in the local search method. It involves the use of entities and their relationships to reason about information in the context of a user query. This approach is effective for answering questions that require knowledge about specific entities and their properties.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="EMBEDDING">
      <data key="d0">PROPERTY, TECHNIQUE</data>
      <data key="d1">Embedding is a technique used in the local search method to represent entities and text in a numerical format that can be processed by machine learning models. It is a key component in the Entity-Text Unit Mapping process.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="ENTITY-TEXT UNIT MAPPING">
      <data key="d0">PROPERTY, TECHNIQUE</data>
      <data key="d1">Entity-Text Unit Mapping is a process in the local search method that associates entities with relevant text units. This mapping is used to identify and prioritize text units that are relevant to a user query.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="RANKING + FILTERING">
      <data key="d0">PROPERTY, TECHNIQUE</data>
      <data key="d1">Ranking + Filtering is a technique used in the local search method to prioritize and select the most relevant entities, text units, community reports, relationships, and covariates based on their relevance to a user query.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="ENTITY-REPORT MAPPING">
      <data key="d0">PROPERTY, TECHNIQUE</data>
      <data key="d1">Entity-Report Mapping is a process in the local search method that associates entities with relevant community reports. This mapping is used to identify and prioritize community reports that are relevant to a user query.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="ENTITY-ENTITY RELATIONSHIPS">
      <data key="d0">PROPERTY, CONCEPT</data>
      <data key="d1">Entity-Entity Relationships are connections between entities that are identified and prioritized in the local search method. These relationships are used to understand the connections between entities and to answer questions that require knowledge about these connections.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="ENTITY-COVARIATE MAPPINGS">
      <data key="d0">PROPERTY, TECHNIQUE</data>
      <data key="d1">Entity-Covariate Mappings is a process in the local search method that associates entities with relevant covariates. This mapping is used to identify and prioritize covariates that are relevant to a user query.</data>
      <data key="d2">364624242a84e1859e758069d914d8c8</data>
    </node>
    <node id="USER QUERY">
      <data key="d0">INPUT, QUERY</data>
      <data key="d1">A User Query represents a specific question, request, or command initiated by a user, serving as the catalyst for information retrieval and navigation within the GraphRAG system and community summaries. This query can be directed towards seeking answers based on the dataset and its themes, and it often includes conversation history to provide additional context for a more refined search. As the starting point for entity and relationship extraction, the User Query plays a pivotal role in initiating the local search process within the knowledge graph, enabling the exploration of the corpus and identification of relevant information.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea,812b3414c467da0b62f7932d2adcbad4,93d4d4effbf989e6ef1c4c3b4f42494e</data>
    </node>
    <node id="CONVERSATION HISTORY">
      <data key="d0">INPUT, CONTEXT</data>
      <data key="d1">Conversation History is a comprehensive record of all previous interactions or queries made by the user to the system. This data serves as a critical context for the current user query, enabling the system to generate more informed and relevant responses. By leveraging the conversation history, GraphRAG can better understand the user's needs and preferences, leading to enhanced search and response generation processes. This historical data is essential for maintaining continuity in conversations and ensuring that the system's responses are tailored to the user's specific requirements.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea,812b3414c467da0b62f7932d2adcbad4</data>
    </node>
    <node id="LOCAL SEARCH DATAFLOW">
      <data key="d0">PROCESS, ALGORITHM</data>
      <data key="d1">Local Search Dataflow is the method used to identify and prioritize entities, relationships, and covariates from the knowledge graph based on the user query and conversation history. It extracts relevant details and text chunks from the input documents and prioritizes them for fitting within a single context window.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="KNOWLEDGE GRAPH">
      <data key="d0">DATABASE, INFORMATION</data>
      <data key="d1">The KNOWLEDGE GRAPH is a sophisticated database that leverages graph theory to organize data into a network of entities and their relationships. This structure enables the storage and representation of structured data in a highly accessible and analyzable format, making it an indispensable resource for AI applications such as question generation. The KNOWLEDGE GRAPH acts as the central information hub for the local search method, supplying the necessary context and details to formulate a response to user queries. Its richly interconnected data model facilitates efficient querying and analysis, enhancing the capabilities of AI systems that rely on it.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4,3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="PRIORITIZED TEXT UNITS">
      <data key="d0">OUTPUT, INFORMATION</data>
      <data key="d1">Prioritized Text Units are the relevant text chunks extracted from the raw input documents and prioritized by the local search method. They are associated with the identified entities and are used to generate a response to the user query.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="PRIORITIZED COMMUNITY REPORTS">
      <data key="d0">OUTPUT, INFORMATION</data>
      <data key="d1">Prioritized Community Reports are the relevant reports extracted from the knowledge graph and prioritized by the local search method. They are associated with the identified entities and are used to generate a response to the user query.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="PRIORITIZED ENTITIES">
      <data key="d0">OUTPUT, INFORMATION</data>
      <data key="d1">Prioritized Entities are the entities identified from the knowledge graph and prioritized by the local search method. They serve as access points into the knowledge graph and are used to extract further relevant details.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="PRIORITIZED RELATIONSHIPS">
      <data key="d0">OUTPUT, INFORMATION</data>
      <data key="d1">Prioritized Relationships are the relationships identified from the knowledge graph and prioritized by the local search method. They provide context and details about the connections between entities.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="PRIORITIZED COVARIATES">
      <data key="d0">OUTPUT, INFORMATION</data>
      <data key="d1">Prioritized Covariates are the covariates identified from the knowledge graph and prioritized by the local search method. They provide additional details about the entities and their attributes.</data>
      <data key="d2">3a0742c280217fe600b9af2d06b58eea</data>
    </node>
    <node id="LOCALSEARCH CLASS">
      <data key="d0">CLASS, INFORMATION_RETRIEVAL</data>
      <data key="d1">The LocalSearch class is a component designed for searching and retrieving information from a collection of knowledge model objects. It prioritizes and filters candidate data sources to fit within a single context window of a pre-defined size, which is then used to generate a response to a user query. Key parameters include the OpenAI model object (llm), context builder object (context_builder), system prompt, response type, llm_params, context_builder_params, and callbacks for handling LLM's completion streaming events.</data>
      <data key="d2">25797740f434cc2bf16365fc498791f6</data>
    </node>
    <node id="QUESTION GENERATION METHOD">
      <data key="d0">METHOD, INFORMATION_EXTRACTION</data>
      <data key="d1">The Question Generation method is a process that combines structured data from the knowledge graph with unstructured data from input documents to generate candidate questions related to specific entities. This method uses the same context-building approach as the LocalSearch class to extract and prioritize relevant structured and unstructured information.</data>
      <data key="d2">25797740f434cc2bf16365fc498791f6</data>
    </node>
    <node id="CONFIGURATION PARAMETERS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Configuration parameters for the LocalSearch class include llm (the OpenAI model object), context_builder (the context builder object), system_prompt (the prompt template for generating search responses), response_type (the desired format of the response), llm_params (additional parameters for the LLM call), context_builder_params (additional parameters for the context_builder), and callbacks (optional functions for handling LLM's completion streaming events).</data>
      <data key="d2">25797740f434cc2bf16365fc498791f6</data>
    </node>
    <node id="LLM COMPLETION STREAMING EVENTS">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">LLM Completion Streaming Events refer to the real-time updates or responses generated by Large Language Models (LLM) during the completion of tasks or queries. These events can be handled by custom event handlers to process or react to the information as it is being generated.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="ENTITY-BASED QUESTION GENERATION">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Entity-based Question Generation is a specific approach within the question generation method that focuses on generating questions related to specific entities. It utilizes both structured and unstructured data to create context and generate relevant questions.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="STRUCTURED DATA">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">Structured Data refers to the organized and formatted data that is typically stored in databases or knowledge graphs. This data is well-organized and can be easily searched and analyzed, making it useful for various applications including question generation.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="UNSTRUCTURED DATA">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">Unstructured Data refers to the data that does not have a predefined format or organization. It includes text documents, images, and other forms of data that are not easily searchable or analyzed without prior processing. Unstructured data is often used in conjunction with structured data to provide a more comprehensive context for question generation.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="OPENAI MODEL">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">An OpenAI Model is a type of artificial intelligence model developed by OpenAI. It can be used for various tasks, including response generation in the context of question generation.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="CONTEXT BUILDER">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">The Context Builder, a crucial component in the GlobalSearch class, serves a dual purpose in the realm of question generation and data analysis. Primarily, it prepares context data by meticulously extracting and prioritizing relevant information from collections of knowledge model objects, akin to the context-building approach employed in local search. Additionally, the Context Builder plays a significant role in the map-reduce process by processing community reports, ensuring that the data is optimized for further analysis and question generation. This versatile component is pivotal in bridging the gap between raw data and actionable insights, facilitating a more efficient and targeted exploration of knowledge within specialized professional networks.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4,1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="SYSTEM PROMPT">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">A System Prompt is a template used to generate candidate questions in the question generation process. It can be customized to fit specific needs and is used as a starting point for the LLM to generate follow-up questions.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="LLM PARAMETERS">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">LLM Parameters are additional parameters that can be passed to the LLM call during the question generation process. These parameters can include settings such as temperature and max_tokens, which affect the behavior and output of the LLM.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="CONTEXT BUILDER PARAMETERS">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Context Builder Parameters are additional parameters that can be passed to the Context Builder object when building context for the question generation prompt. These parameters can be used to customize the context-building process to better fit specific needs.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4</data>
    </node>
    <node id="CALLBACKS">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Callbacks are optional functions designed to handle custom events, specifically focusing on completion streaming events from the LLM (Language Model). These callbacks enable real-time processing, logging, and custom reactions to the information generated by the LLM during task completion or query responses. They provide a mechanism for monitoring, intervention, and custom processing during the various processing stages, enhancing the flexibility and control over the LLM's operations.</data>
      <data key="d2">1415949832ba3fee570ea961998a8ac4,e0cc1cf05b92456e09100790815186fe,e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="LLM (LANGUAGE MODEL)">
      <data key="d0">TOOL, TECHNOLOGY</data>
      <data key="d1">LLM, or Large Language Model, is a sophisticated AI tool designed to generate human-like text based on given prompts. This model is widely utilized in various applications, including entity extraction, relationship detection, and abstractive summarization. LLM can create meaningful summaries of concepts that may be implied but not explicitly stated in the source texts, making it an invaluable asset in understanding the information contained within different communities from both high-level and low-level perspectives. In the context of the GlobalSearch class and data sensemaking tasks, LLM is employed for response generation, identifying users and tasks, and automating the generation of summarization queries to evaluate RAG systems. Its capabilities extend to generating text and questions, facilitating its use in Local Search and Global Search processes to produce context and responses.</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc,3e143a60e2aeb57eb418a68d1484bbb3,5b2968b8f1c891d47ecbe641c3391663,805a07a8f9c2ed5da2d9a61356aafa77,8e69f04648f5fc24c299591365f1aa68,a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="QUESTION GENERATION FUNCTION">
      <data key="d0">FUNCTION, ACTIVITY</data>
      <data key="d1">The Question Generation Function is a process that generates candidate questions based on a given context. It can be customized with parameters such as system_prompt, llm_params, and context_builder_params, and can use callback functions for custom event handling.&gt;</data>
      <data key="d2">3e143a60e2aeb57eb418a68d1484bbb3</data>
    </node>
    <node id="LLM-GENERATED KNOWLEDGE GRAPH">
      <data key="d0">DATA STRUCTURE</data>
      <data key="d1">The LLM-generated knowledge graph is a data structure that represents the relationships and themes within a dataset. It is used by GraphRAG to organize data into semantic clusters and summarize information.</data>
      <data key="d2">812b3414c467da0b62f7932d2adcbad4</data>
    </node>
    <node id="SEMANTIC CLUSTERS">
      <data key="d0">DATA ORGANIZATION</data>
      <data key="d1">Semantic clusters are groups of related data within the LLM-generated knowledge graph. They are organized by GraphRAG to provide a structured representation of the dataset, allowing for the summarization of themes and meaningful responses to user queries.</data>
      <data key="d2">812b3414c467da0b62f7932d2adcbad4</data>
    </node>
    <node id="GLOBAL SEARCH METHOD">
      <data key="d0">METHOD, PROCEDURE</data>
      <data key="d1">The global search method is a process used by GraphRAG to generate responses to user queries. It involves using a collection of LLM-generated community reports as context data, segmenting them into text chunks, and producing intermediate responses. The most important points from these intermediate responses are then aggregated to generate the final response.</data>
      <data key="d2">812b3414c467da0b62f7932d2adcbad4</data>
    </node>
    <node id="AGGREGATED INTERMEDIATE RESPONSES">
      <data key="d0">OUTPUT, RESULT</data>
      <data key="d1">Aggregated intermediate responses are the combined results of the intermediate responses generated during the global search method. They are formed by filtering and selecting the most important points from the intermediate responses, which are then used to generate the final response.</data>
      <data key="d2">812b3414c467da0b62f7932d2adcbad4</data>
    </node>
    <node id="GRAPH'S COMMUNITY HIERARCHY">
      <data key="d0">CONCEPT, HIERARCHY</data>
      <data key="d1">The Graph's Community Hierarchy is a structured organization of communities within a graph, which can be used as context data for generating responses in a map-reduce manner. Different levels of the hierarchy provide varying degrees of detail in community reports.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="MAP-REDUCE PROCESS">
      <data key="d0">CONCEPT, PROCESS</data>
      <data key="d1">The Map-Reduce Process is a computational paradigm used to process large data sets. In the context of generating responses, it involves segmenting community reports into text chunks, producing intermediate responses with points rated for importance, and aggregating the most important points to generate the final response.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="COMMUNITY REPORTS">
      <data key="d0">DATA, DOCUMENT</data>
      <data key="d1">Community Reports are comprehensive summaries generated by the GraphRAG system, tailored to the needs of specific communities or user groups. These reports offer insights and data from a community or a particular context, segmented into text chunks during the map step of the map-reduce process. They include tables of entities and relationships extracted from the input text, providing a structured overview of the information. Community Reports are created using the LLM, offering an overview of the distinct information within each community, including key entities, relationships, and claims. They provide a scoped understanding of the graph, from high-level to low-level perspectives, depending on the community's level in the hierarchy. Overall, Community Reports serve as a valuable tool for understanding the dynamics and structure of specialized professional networks, enabling the identification of collaboration opportunities and knowledge gaps in the field.</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc,21cdf11c58927ae505d3d375d1b75c82,3e292d936b7efa377ba9530456cfd888,5b2968b8f1c891d47ecbe641c3391663,bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="INTERMEDIATE RESPONSE">
      <data key="d0">DATA, DOCUMENT</data>
      <data key="d1">An Intermediate Response is a document containing a list of points, each accompanied by a numerical rating indicating the importance of the point. These responses are produced during the map step of the map-reduce process and are used as input for the reduce step.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="FINAL RESPONSE">
      <data key="d0">DATA, DOCUMENT</data>
      <data key="d1">The Final Response is the aggregated output of the map-reduce process, containing a filtered set of the most important points from the intermediate responses. It is generated using the context provided by the community reports and the map-reduce process.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="GLOBALSEARCH CLASS">
      <data key="d0">CONCEPT, CLASS</data>
      <data key="d1">The GlobalSearch Class is a software class that implements the map-reduce process for generating responses. It includes key parameters such as the LLM model, context builder, map and reduce system prompts, response type, and settings for including general knowledge.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="MAP SYSTEM PROMPT">
      <data key="d0">CONCEPT, TEMPLATE</data>
      <data key="d1">The Map System Prompt is a template used in the map stage of the map-reduce process. It provides instructions for processing community reports and generating intermediate responses.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="REDUCE SYSTEM PROMPT">
      <data key="d0">CONCEPT, TEMPLATE</data>
      <data key="d1">The Reduce System Prompt is a template used in the reduce stage of the map-reduce process. It provides instructions for aggregating intermediate responses and generating the final response.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="RESPONSE TYPE">
      <data key="d0">CONCEPT, FORMAT</data>
      <data key="d1">The Response Type is a parameter of the GlobalSearch class that describes the desired format of the final response, such as multiple paragraphs or a multi-page report.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="ALLOW GENERAL KNOWLEDGE">
      <data key="d0">CONCEPT, SETTING</data>
      <data key="d1">Allow General Knowledge is a setting in the GlobalSearch class that, when enabled, includes additional instructions in the reduce_system_prompt to prompt the LLM to include general knowledge in the final response.&gt;</data>
      <data key="d2">1a4bca0786d529c91073997b63412adc</data>
    </node>
    <node id="MAP_SYSTEM_PROMPT">
      <data key="d0">PROPERTY, TEMPLATE</data>
      <data key="d1">The map_system_prompt is a template used in the map stage of data processing. It serves as a guideline for the initial processing of data, with a default template available for use.</data>
      <data key="d2">e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="REDUCE_SYSTEM_PROMPT">
      <data key="d0">PROPERTY, TEMPLATE</data>
      <data key="d1">The reduce_system_prompt is a template used in the reduce stage of data processing. It guides the consolidation and analysis of data, with a default template available for use.</data>
      <data key="d2">e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="RESPONSE_TYPE">
      <data key="d0">PROPERTY, FORMAT</data>
      <data key="d1">The response_type is a versatile parameter that specifies the desired format and structure of the final response or output. It serves as a free-form text field to describe the format and type of response expected from the query. The response_type can be tailored to various formats, including multiple paragraphs, a single paragraph, a single sentence, a list of 3-7 points, a single page, or a multi-page report. By default, the response_type is set to multiple paragraphs, providing flexibility to adapt the response to the specific needs of the request. This parameter enables customization of the output format, ensuring that the information is presented in a manner that best suits the context and requirements of the query.</data>
      <data key="d2">8c70a7321fb0e945054d226a8c69abee,e0cc1cf05b92456e09100790815186fe,e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="ALLOW_GENERAL_KNOWLEDGE">
      <data key="d0">PROPERTY, SETTING</data>
      <data key="d1">The allow_general_knowledge is a setting that, when enabled, allows the reduce stage to incorporate real-world knowledge outside of the dataset. This can enhance the response but may increase the risk of hallucinations.</data>
      <data key="d2">e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="GENERAL_KNOWLEDGE_INCLUSION_PROMPT">
      <data key="d0">PROPERTY, INSTRUCTION</data>
      <data key="d1">The general_knowledge_inclusion_prompt is an instruction added to the reduce_system_prompt when allow_general_knowledge is set to True. It guides the LLM to include relevant real-world knowledge in the response.</data>
      <data key="d2">e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="MAX_DATA_TOKENS">
      <data key="d0">PROPERTY, LIMIT</data>
      <data key="d1">The max_data_tokens is a token budget that limits the amount of context data that can be used in the processing stages.</data>
      <data key="d2">e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="MAP_LLM_PARAMS">
      <data key="d0">PROPERTY, PARAMETERS</data>
      <data key="d1">The map_llm_params is a set of additional parameters for the LLM call during the map stage, such as temperature and max_tokens, to customize the behavior of the LLM.</data>
      <data key="d2">e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="REDUCE_LLM_PARAMS">
      <data key="d0">PROPERTY, PARAMETERS</data>
      <data key="d1">The "REDUCE_LLM_PARAMS" is a dictionary designed to enhance the customization and fine-tuning of the LLM (Language Model) during the reduce stage. This set of additional parameters, which includes attributes like temperature and max_tokens, allows for a more precise control over the model's behavior, enabling users to adjust the output to better suit their specific needs and preferences. By passing these parameters to the LLM call, one can effectively influence aspects such as the randomness of the generated text (through temperature) and the length of the output (via max_tokens), thereby optimizing the performance of the model in various applications.</data>
      <data key="d2">e0cc1cf05b92456e09100790815186fe,e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="CONTEXT_BUILDER_PARAMS">
      <data key="d0">PROPERTY, PARAMETERS</data>
      <data key="d1">The context_builder_params is a pivotal set of additional parameters designed for the context_builder object. This dictionary of parameters is utilized to customize the construction of the context window during the map stage, thereby playing a crucial role in how the model interprets the context of the input data. By tweaking these parameters, users can fine-tune the model's understanding of the input data's context, enhancing its performance and accuracy in various applications.</data>
      <data key="d2">e0cc1cf05b92456e09100790815186fe,e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="CONCURRENT_COROUTINES">
      <data key="d0">PROPERTY, SETTING</data>
      <data key="d1">The "CONCURRENT_COROUTINES" is a parameter or setting that plays a crucial role in managing the degree of parallelism within the map stage of a computational process. This entity determines the number of tasks that can be processed or executed concurrently, thereby influencing the efficiency and performance of parallel computing operations. By adjusting the value of CONCURRENT_COROUTINES, users can optimize the system's ability to handle multiple tasks simultaneously, ensuring that resources are utilized effectively and that the processing speed is maximized. This setting is particularly important in environments where tasks can be divided and processed in parallel, such as in data processing, machine learning, and other high-performance computing scenarios.</data>
      <data key="d2">e0cc1cf05b92456e09100790815186fe,e442fbb7a67e97ebc4de131b25c639e1</data>
    </node>
    <node id="LLM CALL">
      <data key="d0">PROCESS, ACTION</data>
      <data key="d1">LLM call refers to the invocation of a Large Language Model, typically at the map stage of a data processing pipeline, to perform tasks such as text generation or analysis.</data>
      <data key="d2">e0cc1cf05b92456e09100790815186fe</data>
    </node>
    <node id="GRAPHRAG QUERY CLI">
      <data key="d0">TOOL, INTERFACE</data>
      <data key="d1">The GraphRAG query CLI is a command-line interface that enables no-code usage of the GraphRAG Query engine, allowing users to perform searches and generate responses based on the data indexed by the system.</data>
      <data key="d2">e0cc1cf05b92456e09100790815186fe</data>
    </node>
    <node id="DATA">
      <data key="d0">DATA, INPUT</data>
      <data key="d1">data refers to the input data, typically stored in .parquet files, that are used as the basis for queries and responses in the GraphRAG system.</data>
      <data key="d2">e0cc1cf05b92456e09100790815186fe</data>
    </node>
    <node id="COMMUNITY_LEVEL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The parameter "community_level" is a crucial element in the context of the Leiden community hierarchy. It determines the level from which community reports are loaded, with higher values signifying smaller, more specialized communities. This parameter enables users to delve into the intricacies of the hierarchy, allowing for a detailed exploration of various community sizes. The default setting for "community_level" is 2, providing a balanced view of the community structure. By adjusting this parameter, users can tailor their analysis to focus on broader or more niche communities, facilitating a comprehensive understanding of the network's dynamics and facilitating the identification of collaboration opportunities and knowledge gaps within the Motor Control and Drive Systems domain.</data>
      <data key="d2">8c70a7321fb0e945054d226a8c69abee,e0cc1cf05b92456e09100790815186fe</data>
    </node>
    <node id="METHOD">
      <data key="d0">PROPERTY, PARAMETER</data>
      <data key="d1">METHOD is a versatile command-line option and parameter that plays a crucial role in various functionalities within the software. Primarily, it determines the method to select documents, with options including "all," "random," or "top," and the default being "random." This feature is particularly significant for the auto-templating feature, where it serves as an optional property to specify the document selection method for template generation. Additionally, the method parameter influences the approach used to answer a query, which can be either "local" or "global." For a detailed understanding of its application, one should refer to the Overview section. The method property is also utilized in the configuration of the graphrag.prompt_tune command, further highlighting its importance in controlling the selection of text units for template generation.</data>
      <data key="d2">8c70a7321fb0e945054d226a8c69abee,9243633f55cccd0885ba553e14fa5e3f,ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="GRAPHRAG_API_KEY">
      <data key="d0">PROPERTY, ENV_VARIABLE</data>
      <data key="d1">GRAPHRAG_API_KEY is a critical configuration property and environment variable that serves as the API key for accessing both the LLM (Language Model) service and the GraphRAG service. It is a security measure that enables authentication and authorization for the system to interact with these services. The placeholder value "your_api_key" indicates where the actual API key should be inserted, which must be a valid key provided by the service provider. In the context of the GraphRAG pipeline, GRAPHRAG_API_KEY is defined in the .env file and is essential for authenticating requests made to the OpenAI API or Azure OpenAI endpoint. If GRAPHRAG_API_KEY is not provided, the system will fall back to using the OPENAI_API_KEY for authentication purposes. This setting is indispensable for any interaction with the GraphRAG API and ensures secure and authorized access to the required services.</data>
      <data key="d2">3da10b454f926a257b9fdf5d2487c0a5,5aaa26fbe97dc7573cd1a56d6fb11213,8ac79ce92be1254dfda9a10eb54ab703,8c70a7321fb0e945054d226a8c69abee</data>
    </node>
    <node id="GRAPHRAG_LLM_MODEL">
      <data key="d0">PROPERTY, ENV_VARIABLE</data>
      <data key="d1">GRAPHRAG_LLM_MODEL is a configuration property, also recognized as an environment variable, that determines the model to be utilized for the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG) system. Specifically, it is set to gpt-4-turbo-preview, which signifies that the GPT-4 Turbo Preview model has been chosen for Chat Completions within the HRAG framework. This selection enables advanced language processing capabilities, enhancing the system's performance in augmenting human requests with machine intelligence.</data>
      <data key="d2">8c70a7321fb0e945054d226a8c69abee,9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_MODEL">
      <data key="d0">PROPERTY, ENV_VARIABLE</data>
      <data key="d1">GRAPHRAG_EMBEDDING_MODEL is an environment variable that determines the model to use for Embeddings.</data>
      <data key="d2">8c70a7321fb0e945054d226a8c69abee</data>
    </node>
    <node id="GRAPHRAG_LLM_API_BASE">
      <data key="d0">PROPERTY, ENV_VARIABLE</data>
      <data key="d1">GRAPHRAG_LLM_API_BASE is a configuration property and an optional environment variable that plays a crucial role in specifying the base URL for the Azure OpenAI service API for LLM (Language Model) operations. By default, it is set to "http://&lt;domain&gt;.openai.azure.com", where "&lt;domain&gt;" is a placeholder for the actual domain name provided by Azure. However, it is also mentioned that the default value can be None, indicating that no specific base URL is predetermined. This flexibility allows users to customize the base URL according to their specific Azure OpenAI service setup.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,7b45dafa74553d3899e2291a3c9fb86e,8c70a7321fb0e945054d226a8c69abee</data>
    </node>
    <node id="GRAPHRAG_LLM_TYPE">
      <data key="d0">PROPERTY, ENV_VARIABLE</data>
      <data key="d1">GRAPHRAG_LLM_TYPE is an environment variable and configuration setting that plays a pivotal role in specifying the type of language model to be utilized for text generation tasks. It can be configured to either 'azure_openai_chat' or 'openai_chat', with 'openai_chat' being the default setting. This setting ensures that the chosen model, be it Azure's version of OpenAI's chat model or OpenAI's chat model directly, is employed for generating text based on the input data. The default value of GRAPHRAG_LLM_TYPE is set to 'openai_chat', signifying that OpenAI's chat model is the standard choice for LLM operations unless explicitly overridden.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,3da10b454f926a257b9fdf5d2487c0a5,8c70a7321fb0e945054d226a8c69abee</data>
    </node>
    <node id="GRAPHRAG_LLM_MAX_RETRIES">
      <data key="d0">PROPERTY, ENV_VARIABLE</data>
      <data key="d1">GRAPHRAG_LLM_MAX_RETRIES is a critical configuration property utilized in the Human-in-the-Loop Request Augmentation Gateway (HRAG) system. It serves as an environment variable that determines the maximum number of retries for a request to the Large Language Model (LLM) API in case of failure. Initially, it was mentioned that GRAPHRAG_LLM_MAX_RETRIES is set to 10, allowing for up to 10 retries. However, the default value provided for this variable is 20, indicating that the system is designed to retry a failed request up to 20 times before ceasing further attempts. This robust retry mechanism ensures that the HRAG system maintains a high level of reliability and resilience in its interactions with the LLM, optimizing the chances of successful request processing amidst potential transient errors.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,8c70a7321fb0e945054d226a8c69abee,9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_API_BASE">
      <data key="d0">ENVIRONMENT_VARIABLE, API_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_EMBEDDING_API_BASE is a pivotal configuration property utilized in the Human-in-the-Loop Request Augmentation Gateway (HRAG). This variable is specifically designed to define the base URL for the embedding service, which is crucial for Azure OpenAI users. It is set to "http://&lt;domain&gt;.openai.azure.com", providing a dedicated endpoint for embedding API requests. This setting is particularly relevant when GRAPHRAG_API_BASE is not configured, as it serves as the fallback base URL for embedding operations. By default, GRAPHRAG_EMBEDDING_API_BASE has a value of None, signifying that no predefined base URL is established for embedding tasks until explicitly set. This configuration ensures seamless integration and operation of embedding services within the HRAG framework for Azure OpenAI users.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,485c17007ccb3102887eaa47d6a6100f,9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_TYPE">
      <data key="d0">ENVIRONMENT_VARIABLE, API_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_EMBEDDING_TYPE is a pivotal configuration setting, functioning as an environment variable that specifies the type of embedding model to be utilized for text embedding tasks. This setting can be configured to either 'azure_openai_embedding' or 'openai_embedding', based on the preferred model. By default, GRAPHRAG_EMBEDDING_TYPE is set to 'openai_embedding', signifying that OpenAI's embedding model is the standard choice for generating embeddings from input data. This setting is critical for determining the model that will be employed for embedding operations, ensuring flexibility and adaptability in text processing workflows.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,3da10b454f926a257b9fdf5d2487c0a5</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_MAX_RETRIES">
      <data key="d0">ENVIRONMENT_VARIABLE, API_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_EMBEDDING_MAX_RETRIES is a critical configuration setting in the system, serving as an environment variable that determines the maximum number of retries for embedding operations. There seems to be a discrepancy in the default values mentioned; one source indicates it is set to 10, implying that the system will attempt up to 10 retries when a request to the embedding API fails. However, another source suggests a default value of 20, which would mean the system is configured to retry up to 20 times before abandoning a failed request. This setting is crucial for managing the resilience and reliability of embedding operations within the system.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_TEXT_UNIT_PROP">
      <data key="d0">ENVIRONMENT_VARIABLE, SEARCH_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_TEXT_UNIT_PROP is an environment variable that sets the proportion of the context window dedicated to related text units. The default value is 0.5, indicating that half of the context window is reserved for related text units.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_COMMUNITY_PROP">
      <data key="d0">ENVIRONMENT_VARIABLE, SEARCH_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_COMMUNITY_PROP is a configuration property, also functioning as an environment variable, that specifies the proportion of the context window allocated to community reports within the Motor Control and Drive Systems domain. This property is crucial for Social Network Analysis as it influences the visibility of community reports, enabling a better understanding of the structure and dynamics of specialized professional networks. With a default value of 0.1, it indicates that 10% of the context window is reserved for community reports, facilitating the identification of key influencers and collaboration opportunities within the field. This setting ensures that a significant portion of the interface is dedicated to showcasing community insights, which is essential for mapping complex relationships and identifying knowledge gaps in the Motor Control and Drive Systems domain.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_CONVERSATION_HISTORY_MAX_TURNS">
      <data key="d0">ENVIRONMENT_VARIABLE, SEARCH_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_CONVERSATION_HISTORY_MAX_TURNS is a configuration property, also functioning as an environment variable, that determines the maximum number of turns to be included in the conversation history for local search operations. This setting ensures that the system can reference up to 5 previous turns in the dialogue, aiding in context retention and more informed responses. The default value for GRAPHRAG_LOCAL_SEARCH_CONVERSATION_HISTORY_MAX_TURNS is set to 5, providing a balance between historical context and computational efficiency.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_TOP_K_ENTITIES">
      <data key="d0">ENVIRONMENT_VARIABLE, SEARCH_CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_TOP_K_ENTITIES is a configuration property, also functioning as an environment variable, that specifies the number of related entities to be fetched from the entity description embedding store. This setting has a default value of 10, implying that the system is configured to retrieve the top 10 related entities by default. This feature is crucial for optimizing the retrieval of relevant information and enhancing the efficiency of data processing within the system.</data>
      <data key="d2">1ef6439b7c457ba43993467ff734eedf,2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_TOP_K_RELATIONSHIPS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_TOP_K_RELATIONSHIPS is a configuration property that controls the number of out-of-network relationships to pull into the context window. It has a default value of 10.</data>
      <data key="d2">2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS is a configuration property that can be changed based on the token limit of the model being used. It has a default value of 12000.</data>
      <data key="d2">2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_LOCAL_SEARCH_LLM_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LOCAL_SEARCH_LLM_MAX_TOKENS is a configuration property that can be changed based on the token limit of the model being used. It has a default value of 2000.</data>
      <data key="d2">2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS is a configuration property that determines the maximum number of tokens for global search. It can be adjusted based on the token limit of the model being used, ensuring optimal performance. For instance, with an 8k limit model, a suitable setting could be 5000 tokens. The property has a default value of 12000 tokens, which serves as a baseline for models with higher token capacities.</data>
      <data key="d2">2049798d3000849f8bec3e88c0006807,2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS is a pivotal configuration property that governs the maximum number of tokens for data in global search. This setting is adaptable and should be fine-tuned in accordance with the token limit of the model being utilized. For instance, with an 8k limit model, a recommended setting is 5000 tokens. It is noteworthy that the default value for GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS is set at 12000 tokens, providing a baseline for users to adjust based on their specific model requirements.</data>
      <data key="d2">2049798d3000849f8bec3e88c0006807,2efb1fec56fe3b0543d395dd541295c3</data>
    </node>
    <node id="GRAPHRAG_GLOBAL_SEARCH_MAP_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_GLOBAL_SEARCH_MAP_MAX_TOKENS is a configuration property that determines the maximum number of tokens for mapping in global search. The default value is 500.</data>
      <data key="d2">2049798d3000849f8bec3e88c0006807</data>
    </node>
    <node id="GRAPHRAG_GLOBAL_SEARCH_REDUCE_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_GLOBAL_SEARCH_REDUCE_MAX_TOKENS is a configuration property that determines the maximum number of tokens for reduction in global search. It should be adjusted based on the token limit of the model being used. For an 8k limit model, a good setting could be 1000-1500. The default value is 2000.</data>
      <data key="d2">2049798d3000849f8bec3e88c0006807</data>
    </node>
    <node id="GRAPHRAG_GLOBAL_SEARCH_CONCURRENCY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_GLOBAL_SEARCH_CONCURRENCY is a pivotal configuration property within the GraphRag system, specifically designed to govern the degree of concurrency for global search operations. This property plays a crucial role in optimizing the performance of search functionalities, enabling adjustments based on the available system resources and specific requirements. By default, GRAPHRAG_GLOBAL_SEARCH_CONCURRENCY is set to a value of 32, providing a baseline for the system's search concurrency that can be fine-tuned for enhanced efficiency.</data>
      <data key="d2">2049798d3000849f8bec3e88c0006807,60df16c009594c15c4ead6125e1453ce</data>
    </node>
    <node id="GRAPHRAG KNOWLEDGE MODEL">
      <data key="d0">CONCEPT, DATA MODEL</data>
      <data key="d1">The GraphRAG Knowledge Model is a conceptual model designed to abstract over the underlying data storage technology, providing a common interface for the GraphRAG system to interact with. It is aligned with the outputs of the indexing engine in the Default Configuration Mode and is used to load data into a database system for the GraphRAG's Query Engine to interact with.</data>
      <data key="d2">25e04f0e9a961dcdc3f6eae6df7807b2</data>
    </node>
    <node id="GRAPHRAG INDEXER">
      <data key="d0">SOFTWARE, DATA PROCESSING</data>
      <data key="d1">The GraphRAG Indexer, a pivotal component of the GraphRAG system, specializes in indexing data for efficient retrieval and analysis. In its default configuration mode, the Indexer aligns its outputs with the GraphRAG Knowledge Model, preparing the data for seamless integration into a database system. This data is then readily accessible for further processing by the Query Engine, facilitating advanced querying capabilities. Moreover, the GraphRAG Indexer is equipped with a set of default prompts designed for knowledge discovery from text data. To enhance its adaptability across diverse contexts, the Indexer supports customization through a custom prompt file, enabling users to tailor its functionality to specific requirements. This feature makes the GraphRAG Indexer a versatile tool for knowledge discovery and data indexing in various professional networks and domains.</data>
      <data key="d2">25e04f0e9a961dcdc3f6eae6df7807b2,6a7157695d90d434b2625c3f05420916</data>
    </node>
    <node id="GRAPHRAG QUERY ENGINE">
      <data key="d0">SOFTWARE, QUERY PROCESSING</data>
      <data key="d1">The GraphRAG Query Engine is a component of the GraphRAG system that interacts with the database system using the knowledge model data-store types. It processes queries based on the data indexed by the GraphRAG Indexer and stored in the database.</data>
      <data key="d2">25e04f0e9a961dcdc3f6eae6df7807b2</data>
    </node>
    <node id="DATASHAPER">
      <data key="d0">SOFTWARE, DATA PROCESSING</data>
      <data key="d1">DataShaper is a versatile, open-source software tool and library that specializes in data transformation and manipulation through the use of workflows, known as verbs. These verbs represent relational concepts and are capable of modifying input data tables, which can then be passed through a pipeline for additional processing. Users can declaratively define data pipelines, schemas, and associated assets using well-established schemas. With implementations in both JavaScript and Python, DataShaper is designed to be extensible to other programming languages, making it a valuable resource for data processing across various platforms.</data>
      <data key="d2">25e04f0e9a961dcdc3f6eae6df7807b2,81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="WORKFLOW">
      <data key="d0">CONCEPT, DATA PROCESSING</data>
      <data key="d1">In the context of DataShaper, a Workflow is a pivotal resource type characterized by a series of steps, known as verbs, which collectively transform data. Each step within a Workflow is defined by a verb name and a configuration object, enabling the modeling of relational operations such as SELECT, DROP, JOIN, and more. Workflows in DataShaper are instrumental in processing data tables by passing them through a pipeline for successive transformations, ensuring that each verb modifies the input data table before it is handed over to the next step in the sequence. This structured approach facilitates efficient data manipulation and analysis within the DataShaper environment.</data>
      <data key="d2">25e04f0e9a961dcdc3f6eae6df7807b2,81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="GRAPHRAG INDEXING PIPELINE">
      <data key="d0">SOFTWARE, DATA PROCESSING</data>
      <data key="d1">The GraphRAG Indexing Pipeline is an integral component of the GraphRAG system, designed to facilitate the scheduling and processing of data indexing tasks. This sophisticated system enables workflows to establish dependencies on one another, forming a directed acyclic graph (DAG) that streamlines the data indexing process. Built upon the robust foundation of DataShaper, the GraphRAG Indexing Pipeline guides data through a sequence of steps or verbs, ensuring that it is appropriately indexed for the GraphRAG Knowledge Model. This pipeline plays a crucial role in maintaining the integrity and accessibility of data within the GraphRAG ecosystem.</data>
      <data key="d2">25e04f0e9a961dcdc3f6eae6df7807b2,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="VERB">
      <data key="d0">CONCEPT, PROCESS</data>
      <data key="d1">A Verb in DataShaper is a step within a workflow that models a specific data transformation action. Verbs can represent relational concepts like SELECT, JOIN, and BINARIZE, and each verb has a name and a configuration object that defines how it operates on an input data table.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="INPUT TABLE">
      <data key="d0">DATA, STRUCTURE</data>
      <data key="d1">An Input Table is the data structure that is fed into a verb within a DataShaper workflow. It is the starting point for data transformations and is passed through a series of verbs for processing.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="OUTPUT TABLE">
      <data key="d0">DATA, STRUCTURE</data>
      <data key="d1">An Output Table is the result of a verb's processing in a DataShaper workflow. It is the transformed data structure that is passed down the pipeline for further processing by subsequent verbs.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="GRAPHRAG'S INDEXING PIPELINE">
      <data key="d0">SOFTWARE, TOOL</data>
      <data key="d1">GraphRAG's Indexing Pipeline is a custom extension of DataShaper that implements additional verbs on top of the standard relational verbs. It is designed to augment text documents with rich, structured data using the power of LLMs like GPT-4, and it can be used to extract entities, relationships, claims, community structures, and community reports and summaries.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="LLM-BASED WORKFLOW STEPS">
      <data key="d0">CONCEPT, PROCESS</data>
      <data key="d1">LLM-based Workflow Steps are custom verbs in GraphRAG's Indexing Pipeline that utilize Large Language Models (LLMs) to perform data enrichment and extraction tasks. These steps can be customized and extended to support various AI-based data processing tasks.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="WORKFLOW GRAPHS">
      <data key="d0">CONCEPT, STRUCTURE</data>
      <data key="d1">Workflow Graphs, a crucial component of GraphRAG's Indexing Pipeline, serve to illustrate the intricate web of interdependencies that exist between various workflows. These graphs take the form of a directed acyclic graph (DAG), which is instrumental in scheduling processing tasks and managing the dependencies between different steps in the workflow. By representing data indexing tasks as a series of interconnected workflows, Workflow Graphs enable a comprehensive understanding of the complexity involved in data pipelines. This structure allows for the definition of dependencies among workflows, ensuring that the processing is carried out in an organized and efficient manner.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="DAG (DIRECTED ACYCLIC GRAPH)">
      <data key="d0">CONCEPT, STRUCTURE</data>
      <data key="d1">A Directed Acyclic Graph (DAG) is a structure used in GraphRAG's Indexing Pipeline to represent the dependencies between workflows. It is a graph that is directed (edges have a direction) and acyclic (no cycles exist), which allows for the scheduling and processing of workflows in a defined order.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd</data>
    </node>
    <node id="PREPARE">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">"PREPARE is a pivotal workflow step within the GraphRAG Indexing Pipeline, serving as a preparatory phase for data that is destined for further processing. This phase encompasses a range of tasks, including but not limited to, initial data cleaning, formatting, and setting up the data environment to optimize it for subsequent verbs in the pipeline. PREPARE ensures that the data is in an appropriate state, thereby facilitating smoother and more efficient processing downstream."</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="CHUNK">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">Chunk is a critical workflow step within the GraphRAG Indexing Pipeline, serving as a verb that signifies the division of data into smaller, more manageable pieces. This process is pivotal for handling large datasets by enabling more efficient and potentially parallel processing. Chunk refers to a segment of text or data that is processed as a discrete unit, a practice commonly employed in text analysis and information extraction workflows. By breaking down data into chunks, the system facilitates enhanced processing capabilities and optimizes resource utilization.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08,81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="EXTRACTGRAPH">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">ExtractGraph is a pivotal workflow step within the GraphRAG Indexing Pipeline, serving as a verb that signifies the process of extracting graph structures from data. This process entails meticulously identifying relationships, entities, and patterns within the data to construct a comprehensive graph representation. Through ExtractGraph, the pipeline is able to uncover and map the intricate connections and structures inherent in the data, facilitating a deeper understanding and analysis of the information at hand.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="EMBEDDOCUMENTS">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">EmbedDocuments is a critical workflow step within the GraphRAG Indexing Pipeline, serving as a verb that signifies the process of embedding documents into a vector space. This step is pivotal for converting documents into numerical representations, a transformation that facilitates advanced tasks such as document similarity analysis and information retrieval. By embedding documents, the system enables more sophisticated machine learning applications and enhances the efficiency of information retrieval processes.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="GENERATEREPORTS">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">GenerateReports is a pivotal workflow step within the GraphRAG Indexing Pipeline, serving as a verb that encapsulates the process of generating comprehensive reports based on the processed data. This critical phase involves a multifaceted approach to data analysis, encompassing the summarization of data, the identification of trends, and the provision of insightful observations. The reports created during the GenerateReports step are designed to consolidate findings, offer analytical insights, and meticulously document the outcomes of various data processing tasks, thereby facilitating a deeper understanding of the data's implications and enabling informed decision-making.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="EMBEDGRAPH">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">EmbedGraph is a critical workflow step within the GraphRAG Indexing Pipeline, serving as a verb that signifies the process of embedding graph structures into a vector space. This transformation is pivotal for converting graph data into numerical representations, thereby facilitating the application of machine learning algorithms to graph-based information. The ability to embed graphs in a vector space enhances the potential for tasks such as graph similarity analysis and graph-based information retrieval, making EmbedGraph a versatile and essential component in the processing and analysis of graph data.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="ENTITYRESOLUTION">
      <data key="d0">VERB, PROCESS</data>
      <data key="d1">EntityResolution is a critical workflow step within the GraphRAG Indexing Pipeline, designed to enhance data integrity and coherence. This process involves meticulously identifying and merging duplicate entities, ensuring that each unique entity is accurately represented within the system. Additionally, EntityResolution addresses ambiguities in data by disambiguating entities with similar names, thereby facilitating more precise and effective data analysis. Through these operations, EntityResolution plays a pivotal role in streamlining the indexing process and optimizing the overall performance of the GraphRAG system.</data>
      <data key="d2">81031e23c0b000ee60cd9b06950f96cd,d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="SAMPLE WORKFLOW DAG">
      <data key="d0">CONCEPT, DIAGRAM</data>
      <data key="d1">Sample Workflow DAG is a visual representation of a directed acyclic graph (DAG) that shows the dependencies between different workflows in the GraphRAG Indexing Pipeline.</data>
      <data key="d2">d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="DATAFRAME MESSAGE FORMAT">
      <data key="d0">CONCEPT, FORMAT</data>
      <data key="d1">Dataframe Message Format is the primary unit of communication between workflows and workflow steps in the GraphRAG Indexing Pipeline. It is an instance of pandas.DataFrame, which facilitates data-centric and table-centric data processing.</data>
      <data key="d2">d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="LLM CACHING">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">LLM Caching is a technique used in the GraphRAG library to improve the resilience of the indexer to network issues. It involves caching the results of Large Language Model (LLM) interactions to avoid reprocessing the same input set, thus saving resources and improving efficiency.</data>
      <data key="d2">d19a57bca2c14fc9c2bf5058958380fd</data>
    </node>
    <node id="GRAPHRAG LIBRARY">
      <data key="d0">SOFTWARE, TOOL</data>
      <data key="d1">The GraphRAG library is a software tool designed specifically for Large Language Model (LLM) interactions. It is equipped with features to handle common issues encountered when working with LLM APIs, such as network latency and throttling errors. The library includes a caching mechanism to improve the efficiency and reliability of LLM interactions.</data>
      <data key="d2">6335601c6ec22bd6f15c8b69c26f854b</data>
    </node>
    <node id="LLM INTERACTIONS">
      <data key="d0">PROCESS, COMMUNICATION</data>
      <data key="d1">LLM interactions refer to the communication and data exchange between a system and Large Language Models (LLMs). These interactions can be affected by network latency, throttling, and other errors, which can lead to decreased performance and reliability. Caching is used to mitigate these issues and improve the efficiency of the interactions.</data>
      <data key="d2">6335601c6ec22bd6f15c8b69c26f854b</data>
    </node>
    <node id="CACHING">
      <data key="d0">TECHNIQUE, STRATEGY</data>
      <data key="d1">Caching is a technique used in the GraphRAG library to store and reuse results from LLM interactions. When a completion request is made with the same input set (prompt and tuning parameters), the library checks if a cached result exists and returns it if available. This strategy enhances the system's resilience to network issues, ensures idempotency, and provides a more efficient end-user experience.</data>
      <data key="d2">6335601c6ec22bd6f15c8b69c26f854b</data>
    </node>
    <node id="LOCAL TO GLOBAL GRAPH RAG APPROACH">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">The Local to Global Graph RAG (Retrieval-Augmented Generation) Approach is a method designed for query-focused summarization tasks. It enables large language models to answer questions over private and/or previously unseen document collections by retrieving relevant information. The approach is particularly suited for global questions that require summarization of an entire text corpus, such as identifying main themes in a dataset. It combines the strengths of retrieval tasks and query-focused summarization methods, scaling to handle both the generality of user questions and the quantity of source text to be indexed.</data>
      <data key="d2">f76c18c7582167c3626f8741c2c9374f</data>
    </node>
    <node id="RETRIEVAL-AUGMENTED GENERATION (RAG)">
      <data key="d0">TECHNIQUE, TOOL</data>
      <data key="d1">Retrieval-Augmented Generation (RAG) is a sophisticated technique designed to empower large language models with the ability to answer questions by accessing and retrieving pertinent information from an external knowledge source. This method is particularly effective in addressing queries over private or previously unseen document collections, making it a valuable tool for information retrieval in specialized domains. RAG operates by identifying local regions of text within the dataset that are relevant to the question at hand, providing the necessary context for generating accurate responses. However, it is important to note that RAG may face challenges when confronted with global questions that demand a comprehensive summary of an entire text corpus, as these tasks are inherently query-focused and require a broader scope of analysis. Despite this limitation, RAG remains a well-established approach for answering user questions over extensive datasets by skillfully retrieving and leveraging the most pertinent information.</data>
      <data key="d2">c7669e6a1add9a2829b09196256b1492,f76c18c7582167c3626f8741c2c9374f</data>
    </node>
    <node id="QUERY-FOCUSED SUMMARIZATION (QFS)">
      <data key="d0">TECHNIQUE, TOOL</data>
      <data key="d1">Query-Focused Summarization (QFS) is a specialized summarization task introduced by Dang in 2006, designed to generate natural language summaries in direct response to user queries. Unlike generic summarization methods, QFS focuses on producing abstractive summaries that go beyond simple concatenations of text excerpts, aiming to provide relevant information tailored to specific queries. This technique is particularly useful for summarization tasks that require a comprehensive understanding of an entire text corpus, such as identifying main themes. However, QFS methods can struggle with scalability when dealing with the vast quantities of text indexed by typical Retrieval-Augmented Generation (RAG) systems. Despite this challenge, QFS remains a valuable tool for generating concise, query-specific summaries that enhance information retrieval and comprehension.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9,c7669e6a1add9a2829b09196256b1492,f76c18c7582167c3626f8741c2c9374f</data>
    </node>
    <node id="ENTITY KNOWLEDGE GRAPH">
      <data key="d0">DATA STRUCTURE, CONCEPT</data>
      <data key="d1">The ENTITY KNOWLEDGE GRAPH is a sophisticated data structure that encapsulates entities and their interconnections, extracted from source documents. This graph is a fundamental component in the initial phase of both the Graph RAG approach and the graph-based text indexing process. Its primary function is to facilitate the identification of closely-related entities, enabling the creation of comprehensive community summaries for clusters of these entities. Through this structure, insights into the relationships and dynamics within specialized professional networks, such as those in the Motor Control and Drive Systems domain, can be gleaned, aiding in the discovery of collaboration opportunities and knowledge gaps.</data>
      <data key="d2">b149708d0b4ac3ff417565739ea6b03b,f76c18c7582167c3626f8741c2c9374f</data>
    </node>
    <node id="GRAPH-BASED TEXT INDEX">
      <data key="d0">METHOD, TECHNOLOGY</data>
      <data key="d1">A graph-based text index is a method for organizing and querying text data that involves creating an entity knowledge graph from source documents and generating community summaries for groups of closely-related entities to improve the comprehensiveness and diversity of generated answers to questions. This method is particularly effective for global sensemaking questions over datasets in the 1 million token range.</data>
      <data key="d2">b149708d0b4ac3ff417565739ea6b03b</data>
    </node>
    <node id="GLOBAL SENSEMAKING QUESTIONS">
      <data key="d0">QUERY, INFORMATION NEED</data>
      <data key="d1">Global sensemaking questions are a class of questions that require understanding connections among people, places, and events in order to anticipate their trajectories and act effectively. They are typically asked over datasets in the 1 million token range and are used to test the effectiveness of the graph-based text index method.</data>
      <data key="d2">b149708d0b4ac3ff417565739ea6b03b</data>
    </node>
    <node id="GRAPH RAG">
      <data key="d0">METHOD, TECHNOLOGY</data>
      <data key="d1">Graph RAG (Retrieval-Augmented Generation) is a sophisticated AI model designed to enhance the generation of responses or summaries by leveraging a graph-based text index. This method significantly improves the comprehensiveness and diversity of answers to global sensemaking questions, outperforming a naive RAG baseline in both global and local approaches. Graph RAG's unique capability to use a self-generated graph index enables advanced retrieval and analysis of information, making it particularly effective for summarization tasks. It offers scalability advantages over traditional source text summarization, requiring fewer context tokens for low-level community summaries (C3) and root-level community summaries (C0). This efficiency is further demonstrated in iterative question answering, where Graph RAG boasts high win rates in terms of comprehensiveness and diversity. The model can be finely tuned to retain more details in the index, ensuring that the generated responses are rich in information. Graph RAG's graph-based approach to retrieval and generation allows for the handling of more complex and interconnected data, incorporating multiple concepts related to other systems. It is renowned for providing superior improvements in answer comprehensiveness and diversity when compared to source texts, especially in the context of community summaries.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96,7040ba36a7c09899a355d14a30d65375,71f14506a6b15dfabd93fd1606a67b73,7da3d8d244b67f09425a4a7783e4bb55,b149708d0b4ac3ff417565739ea6b03b,ed433e2f5d5387b47376eb0e45ca1c99</data>
    </node>
    <node id="LARGE LANGUAGE MODELS (LLMS)">
      <data key="d0">TECHNOLOGY, ARTIFICIAL INTELLIGENCE</data>
      <data key="d1">Large language models (LLMs) are artificial intelligence systems that are capable of processing and generating human-like language. They are used to automate human-like sensemaking in complex domains like scientific discovery and intelligence analysis.</data>
      <data key="d2">b149708d0b4ac3ff417565739ea6b03b</data>
    </node>
    <node id="SCIENTIFIC DISCOVERY">
      <data key="d0">DOMAIN, ACTIVITY</data>
      <data key="d1">Scientific discovery is a domain of human endeavor that involves the process of making new and significant contributions to scientific knowledge. It is an area where large language models are being used to automate human-like sensemaking.</data>
      <data key="d2">b149708d0b4ac3ff417565739ea6b03b</data>
    </node>
    <node id="INTELLIGENCE ANALYSIS">
      <data key="d0">DOMAIN, ACTIVITY</data>
      <data key="d1">Intelligence analysis is a domain of human endeavor that involves the process of gathering, processing, and analyzing information to support decision-making. It is an area where large language models are being used to automate human-like sensemaking.</data>
      <data key="d2">b149708d0b4ac3ff417565739ea6b03b</data>
    </node>
    <node id="AUTOMATED SENSEMAKING">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Automated sensemaking refers to the application of artificial intelligence and machine learning techniques to understand complex domains, such as scientific discovery and intelligence analysis, by identifying connections among people, places, and events to anticipate their trajectories and act effectively.</data>
      <data key="d2">c7669e6a1add9a2829b09196256b1492</data>
    </node>
    <node id="HUMAN-LED SENSEMAKING">
      <data key="d0">CONCEPT, PROCESS</data>
      <data key="d1">Human-led sensemaking is a process where humans apply and refine their mental model of data by asking questions of a global nature to understand connections among people, places, and events in order to anticipate their trajectories and act effectively.</data>
      <data key="d2">c7669e6a1add9a2829b09196256b1492</data>
    </node>
    <node id="ABSTRACTIVE SUMMARIZATION">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Abstractive Summarization is a sophisticated technique within the domain of text summarization. Unlike extractive summarization, which merely selects and concatenates existing sentences from the source material, Abstractive Summarization generates entirely new sentences to convey the essence and meaning of the original text. This process involves creating a summary that is not a direct copy of the source but rather a novel representation of its content, allowing for a more concise and rephrased version of the information. By generating new sentences, Abstractive Summarization offers a more flexible and comprehensive way to encapsulate the core ideas of a text, making it an essential tool in the field of information processing and analysis.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9,c7669e6a1add9a2829b09196256b1492</data>
    </node>
    <node id="EXTRACTIVE SUMMARIZATION">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Extractive Summarization is a technique within the field of text processing and natural language generation, where the system selects and concatenates existing sentences or phrases from the source text to create a concise summary. This method does not involve the generation of new sentences but focuses on extracting the most significant and relevant parts of the original text, ensuring that the summary retains the core information and meaning of the source material. By reordering and combining these selected sentences, Extractive Summarization provides a succinct representation of the text, making it easier for readers to grasp the essential points without having to read the entire document.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9,c7669e6a1add9a2829b09196256b1492</data>
    </node>
    <node id="TRANSFORMER ARCHITECTURE">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">The Transformer Architecture is a deep learning model introduced by Vaswani et al. in 2017, which has revolutionized natural language processing tasks, including summarization. It is based on the mechanism of self-attention, allowing the model to weigh the importance of different parts of the input data.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9</data>
    </node>
    <node id="LLMS (LARGE LANGUAGE MODELS)">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">LLMS, or Large Language Models (LLMs), are sophisticated AI models renowned for their ability to process and generate human-like text. These advanced AI systems excel in understanding and responding to complex queries, performing various language tasks, and have demonstrated their prowess in summarization tasks by leveraging in-context learning to condense content within their context window. LLMs are particularly adept at recognizing the common entity behind multiple name variations, making them invaluable in processing entity graphs and handling rich descriptive text in potentially noisy graph structures.

Moreover, LLMs are at the forefront of natural language generation, achieving state-of-the-art or competitive results compared to human judgments. They can generate reference-based metrics when gold standard answers are available and measure the qualities of generated texts in a reference-free style. LLMs also facilitate head-to-head comparisons of competing outputs and can evaluate the performance of conventional Retrieval-Augmented Generation (RAG) systems, assessing critical qualities such as context relevance, faithfulness, and answer relevance. In essence, LLMs are versatile tools that enhance the understanding and generation of natural language, making them essential in various applications within the AI domain.</data>
      <data key="d2">53455f8552b0787cb13c5a03eb550842,6dace8e490674ac8e031aed987a63789,85eff07c379a9dc24db0edb983acf3c9</data>
    </node>
    <node id="GPT (GENERATIVE PRE-TRAINED TRANSFORMER)">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">GPT (Generative Pre-trained Transformer) is a series of large language models developed by OpenAI. It is based on the transformer architecture and is known for its ability to generate coherent and contextually relevant text, making it highly effective for various natural language processing tasks, including summarization.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9</data>
    </node>
    <node id="LLAMA">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Llama is a large language model developed by Facebook AI Research (FAIR). It is designed to handle complex language tasks and can generate human-like text, making it suitable for summarization and other NLP tasks.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9</data>
    </node>
    <node id="GEMINI">
      <data key="d0">CONCEPT, TECHNOLOGY</data>
      <data key="d1">Gemini is a large language model developed by Alibaba Cloud. It is capable of understanding and generating text, making it useful for various NLP tasks, including summarization.</data>
      <data key="d2">85eff07c379a9dc24db0edb983acf3c9</data>
    </node>
    <node id="BROWN ET AL., 2020">
      <data key="d0">REFERENCE, ACADEMIC PAPER</data>
      <data key="d1">Brown et al., 2020, is an academic paper that discusses the use of in-context learning for summarizing content within the context window of large language models (LLMs).</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="LLAMA (TOUVRON ET AL., 2023)">
      <data key="d0">REFERENCE, ACADEMIC PAPER</data>
      <data key="d1">Llama (Touvron et al., 2023) is an academic paper that explores the application of in-context learning for summarization tasks, focusing on the Llama series which can summarize content within the context window of LLMs.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="GEMINI (ANIL ET AL., 2023)">
      <data key="d0">REFERENCE, ACADEMIC PAPER</data>
      <data key="d1">Gemini (Anil et al., 2023) is an academic paper that discusses the Gemini series, which utilizes in-context learning to summarize content within the context window of LLMs.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="IN-CONTEXT LEARNING">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">In-context learning is a technique used by LLMs to adapt their responses based on the context provided in their input, enabling them to summarize content within their context window.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Query-focused abstractive summarization is a technique that aims to generate a summary of a corpus in response to a specific query, focusing on the most relevant information.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="LLM CONTEXT WINDOW LIMITS">
      <data key="d0">PROPERTY, CONSTRAINT</data>
      <data key="d1">LLM context window limits refer to the maximum amount of context that a large language model can process at once, which can be a constraint for summarizing entire corpora.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="INFORMATION LOSS IN LONGER CONTEXTS">
      <data key="d0">PROPERTY, CONSTRAINT</data>
      <data key="d1">Information loss in longer contexts is a phenomenon where information can be overlooked or lost when processing very long texts, as the middle part of the context may not receive adequate attention.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="NAIVE RAG (RETRIEVAL-AUGMENTED GENERATION)">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Naive RAG (Retrieval-Augmented Generation) is a foundational technique in the domain of text generation and response creation. It operates by retrieving and augmenting information from a dataset to produce text, serving as a basic benchmark for evaluating the efficacy of more advanced methods. Naive RAG directly extracts text segments for summarization purposes; however, its capabilities might be limited when applied to query-focused summarization tasks across extensive document collections. This straightforward approach is often utilized to establish a baseline against which more sophisticated text generation models can be compared, highlighting its role in the initial stages of algorithm development and testing.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912,e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="GRAPH RAG APPROACH">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">The Graph RAG (Retrieval-Augmented Generation) approach is a sophisticated technique that integrates knowledge graph generation, retrieval-augmented generation, and query-focused summarization to support human sensemaking across extensive text corpora. This method enhances the process of generating text by leveraging a graph index to retrieve and augment information, specifically focusing on the modularity of graphs and the effectiveness of community detection algorithms in partitioning graphs into closely-related nodes. Designed to improve comprehensiveness, diversity, and token efficiency in summarization tasks, the Graph RAG approach refines and adapts current methodologies by operating in a more localized manner. It achieves this through embedding-based matching of user queries and graph annotations, and by implementing hybrid RAG schemes that combine embedding-based matching against community reports before employing map-reduce summarization mechanisms. This approach is particularly adept at processing text chunks extracted from source documents, making it a valuable tool for navigating and summarizing large volumes of information.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912,5e2933c9646c751e6a60c9de12a255f2,d2399fd0aae5bd200639806ca87184f8</data>
    </node>
    <node id="STRUCTURED RETRIEVAL AND TRAVERSAL">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Structured retrieval and traversal is a sophisticated technique that capitalizes on the inherent structure of graph indexes to facilitate efficient information retrieval and navigation. This method is specifically designed to access and traverse through structured data, such as graph indexes, enabling the precise location and extraction of specific information. By leveraging the organized nature of the data, structured retrieval and traversal ensures a streamlined and expedient process for finding and retrieving relevant details.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912,d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="GRAPH MODULARITY">
      <data key="d0">PROPERTY, CONCEPT</data>
      <data key="d1">Graph modularity is a property of graphs that refers to their inherent structure, allowing them to be partitioned into modular communities of closely-related nodes.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912</data>
    </node>
    <node id="COMMUNITY DETECTION ALGORITHMS">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Community Detection Algorithms are sophisticated computational methods employed to partition a graph into communities of nodes that exhibit stronger connections among themselves than with other nodes in the graph. These algorithms are pivotal in identifying the structure and semantics of large datasets by grouping similar nodes together, thereby aiding in the summarization of the entity graph into coherent communities. They operate by optimizing for modularity or other criteria that define the quality of the partitioning, aiming to identify groups of closely-related nodes within the graph. Essential for understanding the connectivity patterns in complex networks, Community Detection Algorithms facilitate the identification of collaboration opportunities and knowledge gaps in specialized professional networks, making them a valuable tool in the analysis of Motor Control and Drive Systems domain and beyond.</data>
      <data key="d2">3fc3718256cb7f614fcde622af2ed912,6dace8e490674ac8e031aed987a63789,a660289d2bf43f25d3524d35cd2d9a96,d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="LLM-DERIVED KNOWLEDGE GRAPH">
      <data key="d0">CONCEPT, INFORMATION STRUCTURE</data>
      <data key="d1">An LLM-derived knowledge graph is a structured representation of information, typically in the form of a graph, where nodes represent entities and edges represent relationships between those entities. This graph is used to store and retrieve knowledge in a more human-like manner, facilitating the understanding and analysis of complex information. The graph is characterized by its inherent modularity and the ability to be partitioned into modular communities of closely-related nodes using community detection algorithms.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="MODULARITY">
      <data key="d0">PROPERTY, CONCEPT</data>
      <data key="d1">Modularity is a property of graphs that describes the degree to which the graph can be partitioned into distinct subgraphs or communities. It is a measure of the inherent structure of the graph and its ability to be divided into meaningful, self-contained parts.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="LOUVAIN">
      <data key="d0">CONCEPT, ALGORITHM</data>
      <data key="d1">Louvain is a community detection algorithm that aims to optimize the modularity of a graph partition. It is an iterative process that starts by assigning each node to its own community and then iteratively merges the most similar communities until no further improvement in modularity can be achieved.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="LEIDEN">
      <data key="d0">CONCEPT, ALGORITHM</data>
      <data key="d1">Leiden is an extension of the Louvain algorithm for community detection. It aims to improve the resolution of the Louvain algorithm by adding a refinement step that allows for the detection of smaller communities. Leiden is designed to be more efficient and to produce more stable and reproducible results.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="QUERY-FOCUSED SUMMARIZATION">
      <data key="d0">CONCEPT, INFORMATION PROCESSING</data>
      <data key="d1">Query-focused summarization is a technique used to generate summaries that are specifically tailored to answer a particular query. It involves analyzing a corpus of documents or a graph index to extract information relevant to the query and then summarizing that information in a concise and coherent manner.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="MAP-REDUCE APPROACH">
      <data key="d0">CONCEPT, INFORMATION PROCESSING</data>
      <data key="d1">The map-reduce approach is a programming model for processing large data sets. It involves two main steps: the map step, where the data is divided into smaller chunks and processed independently, and the reduce step, where the results of the map step are combined to produce the final output. This approach is particularly useful for parallel and distributed processing of large datasets.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="ACTIVITY-CENTERED SENSEMAKING QUESTIONS">
      <data key="d0">CONCEPT, INFORMATION QUERY</data>
      <data key="d1">Activity-centered sensemaking questions are questions that are designed to help users understand complex information by focusing on specific activities or processes. These questions are often derived from short descriptions of datasets and are intended to guide the user in making sense of the underlying data and its implications.&gt;</data>
      <data key="d2">d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="PODCAST TRANSCRIPTS">
      <data key="d0">CONCEPT, INFORMATION TYPE</data>
      <data key="d1">The dataset known as Podcast Transcripts is a comprehensive collection of written records derived from podcast conversations, featuring discussions between Kevin Scott, the CTO of Microsoft, and other influential figures in the technology sector. This dataset is a significant component of the Behind the Tech series by Scott (2024), offering a rich resource for understanding the viewpoints of tech leaders on policy, regulation, and industry trends. The transcripts are meticulously structured, comprising 1669 segments of 600 tokens each, with 100-token overlaps between segments, aggregating to roughly 1 million tokens in total. This format facilitates in-depth analysis and is particularly valuable for tech journalists seeking insights and patterns within the tech community. Moreover, the transcripts enhance accessibility and searchability, making them a versatile tool for various analytical purposes.</data>
      <data key="d2">5d04129d46662571f635a4e63cb4d6b7,aed2ea39de8a027cc818c7f4557f0514,d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="NEWS ARTICLES">
      <data key="d0">CONCEPT, INFORMATION TYPE</data>
      <data key="d1">The News Articles dataset is a comprehensive benchmark collection that spans from September 2013 to December 2023, featuring a diverse range of categories such as entertainment, business, sports, technology, health, and science. This dataset, which is part of MultiHop-RAG (Tang and Yang, 2024), consists of 3197 text chunks, each containing 600 tokens, with 100-token overlaps between chunks, amounting to approximately 1.7 million tokens in total. News articles, as written pieces, serve to report on current events, issues, and trends, providing valuable information and insights on a multitude of topics. They are not only a source of information for the general public but also a valuable educational tool, particularly in the context of health and wellness, where educators incorporate current affairs into their curricula to enhance learning experiences. Typically published in newspapers, magazines, or online news platforms, these articles play a crucial role in informing and engaging the public on important events and developments across various domains.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc,5d04129d46662571f635a4e63cb4d6b7,aed2ea39de8a027cc818c7f4557f0514,d39abd5380fb3fe0468ea1e122512091</data>
    </node>
    <node id="DIVERSE ACTIVITY-CENTERED SENSE-MAKING QUESTIONS">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">Diverse activity-centered sense-making questions are generated from short descriptions of real-world datasets, specifically podcast transcripts and news articles, to foster understanding of broad issues and themes. These questions aim to empower and provide comprehensive and diverse insights. The development of these questions involves exploring the impact of varying hierarchical levels of community summaries used to answer queries and comparing them to naive RAG and global map-reduce summarization of source texts.</data>
      <data key="d2">d2399fd0aae5bd200639806ca87184f8</data>
    </node>
    <node id="REAL-WORLD DATASETS">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">Real-world datasets, including podcast transcripts and news articles, serve as the source material for generating diverse activity-centered sense-making questions. These datasets are used to develop understanding of broad issues and themes through the creation of questions that aim to be comprehensive, diverse, and empowering.</data>
      <data key="d2">d2399fd0aae5bd200639806ca87184f8</data>
    </node>
    <node id="COMPREHENSIVENESS, DIVERSITY, EMPOWERMENT">
      <data key="d0">CONCEPT, CRITERIA</data>
      <data key="d1">Comprehensiveness, diversity, and empowerment are the target qualities that guide the development of sense-making questions. These qualities are designed to foster a deeper understanding of broad issues and themes by ensuring that the questions cover a wide range of topics, offer varied perspectives, and empower the user to engage with the material in a meaningful way.</data>
      <data key="d2">d2399fd0aae5bd200639806ca87184f8</data>
    </node>
    <node id="HIERARCHICAL LEVEL OF COMMUNITY SUMMARIES">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">The hierarchical level of community summaries refers to the varying degrees of detail and abstraction in summarizing information from source texts. These summaries are used to answer queries and are compared to naive RAG and global map-reduce summarization techniques to assess their effectiveness in terms of comprehensiveness and diversity.</data>
      <data key="d2">d2399fd0aae5bd200639806ca87184f8</data>
    </node>
    <node id="NAIVE RAG">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Naive RAG, standing for Retrieval-Augmented Generation, is a foundational technique in the domain of text generation and summarization. This method simplifies the process by converting documents into text, dividing it into manageable segments, and embedding these segments into a vector space where proximity indicates semantic similarity. This embedded information serves as a context for queries, enabling Naive RAG to produce direct and succinct responses. In the context of question answering and summarization, Naive RAG is noted for its straightforwardness and efficiency in generating responses. It is contrasted with more complex global approaches and hierarchical community summaries, which evaluate comprehensiveness and diversity. While Naive RAG is less effective in providing specific examples, quotes, and citations that aid in informed understanding, it stands out for its simplicity and directness, making it a valuable tool in the arsenal of text generation techniques. Comparisons with Graph RAG and other global approaches highlight its role in the empowerment of text summarization methods.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97,71f14506a6b15dfabd93fd1606a67b73,d2399fd0aae5bd200639806ca87184f8,ed433e2f5d5387b47376eb0e45ca1c99</data>
    </node>
    <node id="GLOBAL MAP-REDUCE SUMMARIZATION">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Global map-reduce summarization is a technique that aggregates information from source texts to create summaries. It is compared to naive RAG and hierarchical community summaries to assess its effectiveness in providing comprehensive and diverse insights.</data>
      <data key="d2">d2399fd0aae5bd200639806ca87184f8</data>
    </node>
    <node id="TEXT CHUNKS">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">Text Chunks are pivotal components in the process of information extraction and graph index creation within the context of Large Language Model (LLM) prompts. These chunks are derived by dividing source documents into smaller, more manageable segments, which are then processed by LLMs. The size and granularity of these text chunks play a critical role in the efficiency and effectiveness of information extraction, impacting both the number of LLM calls needed for data extraction and the quality of the extracted information. Text Chunks serve as the fundamental units that enable the identification and extraction of graph nodes and edges, making them essential for meeting the baseline requirement of recognizing entities and their relationships within the text.</data>
      <data key="d2">d2399fd0aae5bd200639806ca87184f8,e50740c4332fdedb8739773592e2a402,e7caf4256ddea71533af1c4c50444146</data>
    </node>
    <node id="SOURCE DOCUMENTS">
      <data key="d0">DOCUMENT, INFORMATION</data>
      <data key="d1">Source Documents are the original texts or files from which information is extracted for processing. They serve as the primary input for the design and implementation of a graph index system.</data>
      <data key="d2">e7caf4256ddea71533af1c4c50444146</data>
    </node>
    <node id="LLM PROMPTS">
      <data key="d0">PROCEDURE, TOOL</data>
      <data key="d1">LLM (Language Model) Prompts are designed to extract various elements of a graph index from the Text Chunks. They are used to identify and extract instances of graph nodes and edges, as well as entities and their relationships, from each chunk of source text.</data>
      <data key="d2">e7caf4256ddea71533af1c4c50444146</data>
    </node>
    <node id="GRAPH INDEX">
      <data key="d0">DATA_STRUCTURE, INDEX</data>
      <data key="d1">The Graph Index is a sophisticated data structure that plays a pivotal role in the Graph RAG approach, designed for efficient querying and analysis of relationships between entities. It organizes information in a graph format, where nodes and edges represent entities and their relationships, respectively. The Graph Index is tailored for information retrieval and is specifically engineered to support conditions C0-C3. Its design and implementation are influenced by the granularity of text chunks and the effectiveness of LLM prompts. The indexing process involves the use of generic prompts for entity and relationship extraction, with entity types and few-shot examples customized to the domain of the data. For the Podcast dataset, a context window size of 600 tokens with 1 gleaning is utilized, while the News dataset employs 0 gleanings. This versatile data structure is crucial for understanding and navigating the complex relationships within specialized professional networks, enabling the identification of collaboration opportunities and knowledge gaps in various fields.</data>
      <data key="d2">53455f8552b0787cb13c5a03eb550842,7040ba36a7c09899a355d14a30d65375,e7caf4256ddea71533af1c4c50444146</data>
    </node>
    <node id="ENTITY REFERENCES">
      <data key="d0">REFERENCE, INFORMATION</data>
      <data key="d1">Entity References are mentions or instances of entities found within the Text Chunks. The number of entity references extracted is influenced by the size of the text chunks, with smaller chunks generally leading to higher recall but potentially lower precision.</data>
      <data key="d2">e7caf4256ddea71533af1c4c50444146</data>
    </node>
    <node id="GRAPH NODES">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">Graph Nodes represent entities or concepts that are identified within the text chunks. These nodes are the building blocks for constructing a graph representation of the information contained in the text.</data>
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="GRAPH EDGES">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">Graph Edges represent the relationships between graph nodes. They are identified and extracted from the text chunks to connect nodes in a graph, illustrating how entities are related to each other.</data>
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="LLM PROMPT">
      <data key="d0">CONCEPT, TOOL</data>
      <data key="d1">LLM (Language Model) Prompt is a multipart instruction used to guide the language model in identifying entities and relationships within the text chunks. It includes few-shot examples for in-context learning, tailored to the domain of the document corpus.</data>
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="FEW-SHOT EXAMPLES">
      <data key="d0">CONCEPT, TOOL</data>
      <data key="d1">Few-Shot Examples are crucial data points utilized for in-context learning, enabling the Large Language Model (LLM) to grasp domain-specific entities and relationships. These specialized instances are pivotal in enhancing the model's proficiency in recognizing and extracting information accurately. Particularly in niche fields such as science, medicine, and law, few-shot examples equip the LLM with the capability to understand and execute tasks effectively, even when the available data is limited. By providing these targeted samples, the model can be swiftly adapted to perform with precision in various specialized professional networks, facilitating a deeper understanding of the structure and dynamics within these domains.</data>
      <data key="d2">805a07a8f9c2ed5da2d9a61356aafa77,e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="NAMED ENTITIES">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">Named Entities are specific types of entities like people, places, and organizations that are generally applicable across various domains. They are identified by the default LLM prompt.</data>
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="SPECIALIZED KNOWLEDGE">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">Specialized Knowledge refers to domain-specific information that is relevant in fields such as science, medicine, and law. Few-shot examples specialized to these domains can improve the accuracy of entity and relationship extraction.</data>
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="COVARIATE PROMPT&lt;||COVARIATE PROMPT">
      <data key="d0">CONCEPT, TOOL</data>
      <data key="d1">Covariate Prompt is a secondary extraction prompt used to associate additional attributes or covariates with the detected node instances. It aims to extract claims linked to entities, including details such as the subject, object, type, description, source text span, and start and end dates.</data>
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="COVARIATE PROMPT">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">e50740c4332fdedb8739773592e2a402</data>
    </node>
    <node id="SECONDARY EXTRACTION PROMPT">
      <data key="d0">METHOD, PROCESS</data>
      <data key="d1">The secondary extraction prompt is a technique used to gather additional information or covariates associated with the extracted node instances. It aims to extract claims linked to detected entities, including details such as the subject, object, type, description, source text span, and start and end dates.</data>
      <data key="d2">805a07a8f9c2ed5da2d9a61356aafa77</data>
    </node>
    <node id="GLEANING PROCESS">
      <data key="d0">METHOD, PROCESS</data>
      <data key="d1">The gleaning process is a multi-stage method used to ensure that all entities are detected by the LLM. It involves multiple rounds of extraction, up to a specified maximum, to encourage the LLM to find any additional entities it may have missed in prior rounds. This process balances efficiency and quality by using larger chunk sizes without a drop in quality or the introduction of noise.</data>
      <data key="d2">805a07a8f9c2ed5da2d9a61356aafa77</data>
    </node>
    <node id="ELEMENT SUMMARIES">
      <data key="d0">OUTPUT, DATA</data>
      <data key="d1">Element Summaries are comprehensive, condensed representations of information distilled from source texts by a Large Language Model (LLM). These summaries serve as a crucial component for each graph element within the network, encompassing entity nodes, relationship edges, and claim covariates. They are instrumental in transforming instance-level details into cohesive, descriptive blocks of text. Element summaries are characterized by their abstractive nature, enabling the LLM to generate independently meaningful synopses of concepts that might be implied but not explicitly stated in the original texts. This process facilitates a deeper understanding of the relationships and dynamics within the network, enhancing the ability to identify key influencers and collaboration opportunities in specialized professional communities such as Motor Control and Drive Systems.</data>
      <data key="d2">805a07a8f9c2ed5da2d9a61356aafa77,a73d3e7b661743b7583d8a0fd412b6a7</data>
    </node>
    <node id="LLM (LARGE LANGUAGE MODEL)">
      <data key="d0">TECHNOLOGY, TOOL</data>
      <data key="d1">LLM, or Large Language Model, is a sophisticated tool used for generating summaries and extracting information from source texts. It is capable of creating meaningful summaries of concepts that may be implied but not explicitly stated in the text, including entities, relationships, and claims. The LLM can handle variations in entity references and maintain consistency across multiple instances. It is also used to summarize groups of instances into single blocks of descriptive text for each graph element, such as entity nodes, relationship edges, and claim covariates.</data>
      <data key="d2">a73d3e7b661743b7583d8a0fd412b6a7</data>
    </node>
    <node id="ELEMENT INSTANCES">
      <data key="d0">PROPERTY, INPUT</data>
      <data key="d1">Element Instances refer to the specific occurrences or representations of entities, relationships, and claims within source texts. These instances are the raw data that an LLM processes to generate element summaries.</data>
      <data key="d2">a73d3e7b661743b7583d8a0fd412b6a7</data>
    </node>
    <node id="GRAPH ELEMENTS">
      <data key="d0">CONCEPT, STRUCTURE</data>
      <data key="d1">Graph Elements are the components of a graph structure used to represent information extracted from texts. These elements include entity nodes, relationship edges, and claim covariates, which are summarized and described using rich descriptive text generated by an LLM.</data>
      <data key="d2">a73d3e7b661743b7583d8a0fd412b6a7</data>
    </node>
    <node id="ENTITY GRAPH">
      <data key="d0">CONCEPT, DATA STRUCTURE</data>
      <data key="d1">The Entity Graph is a representation of entities and their relationships, where entities are nodes and relationships are edges. It is used to detect closely-related communities of entities and summarize their connections. The graph is potentially noisy and includes homogeneous nodes with rich descriptive text, which is beneficial for LLMs and global, query-focused summarization.</data>
      <data key="d2">6dace8e490674ac8e031aed987a63789</data>
    </node>
    <node id="CONNECTIVITY">
      <data key="d0">PROPERTY, CONCEPT</data>
      <data key="d1">Connectivity refers to the links between entities in the entity graph. It is crucial for the resilience of the overall approach to variations in entity names, as long as there is sufficient connectivity from all variations to a shared set of closely-related entities.</data>
      <data key="d2">6dace8e490674ac8e031aed987a63789</data>
    </node>
    <node id="RICH DESCRIPTIVE TEXT">
      <data key="d0">PROPERTY, CONCEPT</data>
      <data key="d1">Rich Descriptive Text is detailed information associated with homogeneous nodes in the entity graph. It aligns with the capabilities of LLMs and is essential for global, query-focused summarization.</data>
      <data key="d2">6dace8e490674ac8e031aed987a63789</data>
    </node>
    <node id="KNOWLEDGE GRAPHS">
      <data key="d0">CONCEPT, DATA STRUCTURE</data>
      <data key="d1">Knowledge Graphs are structured representations of information, typically relying on concise and consistent knowledge triples (subject, predicate, object) for downstream reasoning tasks. They are contrasted with the entity graph, which uses rich descriptive text.</data>
      <data key="d2">6dace8e490674ac8e031aed987a63789</data>
    </node>
    <node id="LEIDEN ALGORITHM">
      <data key="d0">CONCEPT, ALGORITHM</data>
      <data key="d1">The Leiden Algorithm is a sophisticated community detection algorithm renowned for its efficiency in uncovering the hierarchical community structure within large-scale graphs. This algorithm is specifically chosen for its capability to manage extensive datasets, offering a hierarchy of community partitions that cover the nodes of the graph in a mutually-exclusive and collectively-exhaustive manner. The Leiden Algorithm's inclusion in the pipeline highlights its effectiveness in processing and analyzing complex networks, making it an indispensable tool in the field of social network analysis and beyond.</data>
      <data key="d2">6dace8e490674ac8e031aed987a63789,a660289d2bf43f25d3524d35cd2d9a96</data>
    </node>
    <node id="GRAPHCOMMUNITIES">
      <data key="d0">DATA STRUCTURE, CONCEPT</data>
      <data key="d1">GraphCommunities refers to the communities of nodes identified within a graph by community detection algorithms. These communities are groups of nodes that have stronger connections to each other than to the rest of the nodes in the graph.</data>
      <data key="d2">a660289d2bf43f25d3524d35cd2d9a96</data>
    </node>
    <node id="GLOBAL QUERIES">
      <data key="d0">CONCEPT, INFORMATION</data>
      <data key="d1">Global queries are questions that seek to understand the overall themes or information within a corpus. They can be answered using community summaries, which provide a structured way to access and interpret the information contained in the corpus.</data>
      <data key="d2">93d4d4effbf989e6ef1c4c3b4f42494e</data>
    </node>
    <node id="CHUNKS OF PRE-SPECIFIED TOKEN SIZE">
      <data key="d0">PROPERTY, PROCESS</data>
      <data key="d1">Chunks of pre-specified token size are created to ensure that relevant information is distributed across multiple chunks, preventing concentration and potential loss of information in a single context window. This process is essential for handling large datasets or documents effectively.</data>
      <data key="d2">aed2ea39de8a027cc818c7f4557f0514</data>
    </node>
    <node id="INTERMEDIATE ANSWERS">
      <data key="d0">PROPERTY, PROCESS</data>
      <data key="d1">Intermediate answers are generated in parallel, one for each chunk, by mapping community answers. The Large Language Model (LLM) also generates a score between 0-100 for each answer, indicating its helpfulness in answering the target question. Answers with a score of 0 are filtered out.</data>
      <data key="d2">aed2ea39de8a027cc818c7f4557f0514</data>
    </node>
    <node id="GLOBAL ANSWER">
      <data key="d0">PROPERTY, PROCESS</data>
      <data key="d1">Global answers are created by sorting intermediate community answers in descending order of helpfulness score and iteratively adding them into a new context window until the token limit is reached. This final context is used to generate the global answer that is returned to the user.</data>
      <data key="d2">aed2ea39de8a027cc818c7f4557f0514</data>
    </node>
    <node id="DATASET">
      <data key="d0">PROPERTY, DATA</data>
      <data key="d1">The "DATASET" is a comprehensive collection of data designed to be processed through a pipeline, typically represented as a DataFrame in the Python API. This dataset is versatile, serving multiple purposes such as facilitating the understanding of tech leaders' perspectives on policy and regulation, and providing educational content on health and wellness. It can encompass diverse data sources, including podcast transcripts and news articles. Specifically, the dataset is structured as a pandas DataFrame, featuring columns named col1 and col2, both populated with numerical values, making it a valuable resource for data analysis and processing tasks.</data>
      <data key="d2">aed2ea39de8a027cc818c7f4557f0514,b0505e11596cadd9890fef049c29473c,f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="TECH JOURNALIST">
      <data key="d0">PERSON, ROLE</data>
      <data key="d1">A tech journalist is a person who looks for insights and trends in the tech industry. They use podcast transcripts to understand how tech leaders view the role of policy and regulation.</data>
      <data key="d2">aed2ea39de8a027cc818c7f4557f0514</data>
    </node>
    <node id="EDUCATOR">
      <data key="d0">PERSON, ROLE</data>
      <data key="d1">An EDUCATOR is a dedicated professional who specializes in teaching and facilitating learning. They play a crucial role in the educational landscape by integrating current affairs and news articles into their curricula, particularly in the context of health education. This approach enables EDUCATORS to provide students with a comprehensive understanding of health and wellness, ensuring that they are well-informed about the latest developments and issues in the field. By incorporating real-world events and information, EDUCATORS enhance the relevance and impact of their lessons, making them more engaging and meaningful for students. This method of teaching not only broadens students' knowledge but also fosters critical thinking and encourages them to apply what they have learned to their own lives. As a result, EDUCATORS are instrumental in promoting health literacy and empowering the next generation to make informed decisions about their well-being.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc,aed2ea39de8a027cc818c7f4557f0514</data>
    </node>
    <node id="PRIVACY LAWS">
      <data key="d0">LEGAL, REGULATION</data>
      <data key="d1">Privacy laws are government regulations that govern the collection, use, and protection of personal data. They impact technology development by setting boundaries for data usage and influencing how tech companies design and implement their products and services.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="TECHNOLOGY DEVELOPMENT">
      <data key="d0">INDUSTRY, INNOVATION</data>
      <data key="d1">Technology development refers to the process of creating new technologies or improving existing ones. It is influenced by various factors, including legal frameworks, market demands, and ethical considerations.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="ETHICAL CONSIDERATIONS">
      <data key="d0">PHILOSOPHY, ETHICS</data>
      <data key="d1">Ethical considerations involve the principles and values that guide behavior and decision-making. In the context of technology, they ensure that innovations are developed and used responsibly, considering the impact on individuals and society.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="CURRENT POLICIES">
      <data key="d0">LEGAL, REGULATION</data>
      <data key="d1">Current policies refer to the existing laws, regulations, and guidelines that govern various aspects of society, including technology, health, and education. They are subject to change based on new insights, societal needs, and technological advancements.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="COLLABORATIONS">
      <data key="d0">RELATIONSHIP, PARTNERSHIP</data>
      <data key="d1">Collaborations refer to cooperative efforts between different entities, such as tech companies and governments. They can lead to the development of new technologies, policies, or solutions that benefit society.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="HEALTH EDUCATION CURRICULA">
      <data key="d0">EDUCATION, CURRICULUM</data>
      <data key="d1">Health education curricula are structured programs of study that focus on teaching students about health and wellness. They can be enriched by integrating current topics and news articles to provide real-world relevance.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="PREVENTIVE MEDICINE">
      <data key="d0">MEDICINE, HEALTH</data>
      <data key="d1">Preventive medicine is a branch of medicine that focuses on preventing diseases and promoting health. It includes practices such as vaccinations, screenings, and lifestyle modifications.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="WELLNESS">
      <data key="d0">HEALTH, LIFESTYLE</data>
      <data key="d1">Wellness refers to a state of being in good health, both physically and mentally. It encompasses a holistic approach to health, including nutrition, exercise, and mental well-being.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="HEALTH ARTICLES">
      <data key="d0">MEDIA, INFORMATION</data>
      <data key="d1">Health articles are written pieces that focus on health-related topics. They can provide insights into public health priorities, contradicting views on health issues, and the importance of health literacy.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="PUBLIC HEALTH PRIORITIES">
      <data key="d0">HEALTH, POLICY</data>
      <data key="d1">Public health priorities are the areas of focus for public health agencies and policymakers. They are determined by the prevalence of health issues, available resources, and societal needs.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="HEALTH LITERACY">
      <data key="d0">EDUCATION, HEALTH</data>
      <data key="d1">Health literacy is the ability to understand and use health information to make informed decisions. It is crucial for individuals to navigate the healthcare system and maintain their health.</data>
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="TECH COMPANIES">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </node>
    <node id="HOTPOTQA">
      <data key="d0">DATASET</data>
      <data key="d1">HotPotQA, introduced by Yang et al. in 2018, is a benchmark dataset designed for open-domain question answering. It emphasizes explicit fact retrieval over summarization, aiming to facilitate data sensemaking by enabling systems to accurately retrieve specific pieces of information rather than providing broad overviews. This dataset serves as a valuable resource for researchers and developers in the field of natural language processing, helping them to evaluate and improve the capabilities of question answering systems in handling complex, open-domain queries.</data>
      <data key="d2">5d04129d46662571f635a4e63cb4d6b7,8e69f04648f5fc24c299591365f1aa68</data>
    </node>
    <node id="MULTIHOP-RAG">
      <data key="d0">DATASET</data>
      <data key="d1">MultiHop-RAG, introduced by Tang and Yang in 2024, is a benchmark dataset specifically designed for open-domain question answering. This dataset emphasizes explicit fact retrieval over summarization, making it an invaluable tool for data sensemaking in complex information environments. By focusing on the retrieval of explicit facts, MultiHop-RAG facilitates a deeper understanding of the underlying data, enabling more accurate and comprehensive answers to open-domain questions.</data>
      <data key="d2">5d04129d46662571f635a4e63cb4d6b7,8e69f04648f5fc24c299591365f1aa68</data>
    </node>
    <node id="MT-BENCH">
      <data key="d0">DATASET</data>
      <data key="d1">MT-BENCH, also known as MT-Bench, is a benchmark dataset specifically designed for open-domain question answering. Introduced by Zheng et al. in 2024, this dataset emphasizes explicit fact retrieval over summarization, aiming to facilitate data sensemaking in a comprehensive manner. MT-BENCH serves as a valuable resource for researchers and practitioners in the field of information retrieval and natural language processing, enabling them to evaluate and improve their models' performance in answering questions that require the retrieval of specific facts from large, unstructured datasets.</data>
      <data key="d2">5d04129d46662571f635a4e63cb4d6b7,8e69f04648f5fc24c299591365f1aa68</data>
    </node>
    <node id="DATA SENSEMAKING">
      <data key="d0">CONCEPT, PROCESS</data>
      <data key="d1">Data sensemaking is the process through which people inspect, engage with, and contextualize data within the broader scope of real-world activities. It involves summarization for understanding data rather than explicit fact retrieval. Koesten et al. (2021) discuss this concept.</data>
      <data key="d2">8e69f04648f5fc24c299591365f1aa68</data>
    </node>
    <node id="SUMMARIZATION QUERIES">
      <data key="d0">PROPERTY, QUERY</data>
      <data key="d1">Summarization queries are questions or queries that aim to extract a high-level understanding of dataset contents, rather than details of specific texts. They are used for data sensemaking and global understanding of the corpus.</data>
      <data key="d2">8e69f04648f5fc24c299591365f1aa68</data>
    </node>
    <node id="RAG (RETRIEVAL-AUGMENTED GENERATION) SYSTEMS">
      <data key="d0">SYSTEM, TECHNOLOGY</data>
      <data key="d1">RAG systems are retrieval-augmented generation systems that combine information retrieval with text generation to answer questions. They are evaluated for their effectiveness in global sensemaking tasks using summarization queries.</data>
      <data key="d2">8e69f04648f5fc24c299591365f1aa68</data>
    </node>
    <node id="ACTIVITY-CENTERED APPROACH">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">The Activity-Centered Approach is a methodology used for automating the generation of questions. It involves identifying potential users and tasks per user, and then generating questions that require an understanding of the entire corpus for each (user, task) combination.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="EVALUATION DATASETS">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">Evaluation Datasets are collections of data used for testing and evaluating the effectiveness of the LLM in generating questions. Each dataset results in 125 test questions when N = 5.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="GRAPH RAG (RETRIEVAL-AUGMENTED GENERATION)">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">Graph RAG is a method that uses graph communities at different levels (C0, C1, C2, C3) to answer user queries. It leverages summaries from these communities to provide responses.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="TEXT SUMMARIZATION METHOD">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">The Text Summarization Method is an approach that applies a map-reduce technique directly to source texts (TS) to generate summaries. It is used as a condition for comparison in the evaluation.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="NAIVE SEMANTIC SEARCH RAG APPROACH">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">The Naive Semantic Search RAG Approach (SS) is a simple method for answering queries by searching for semantic matches in the text. It is used as a baseline for comparison in the evaluation.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="C0 (ROOT-LEVEL COMMUNITY SUMMARIES)">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">C0 refers to the root-level community summaries, which are the fewest in number. They are used in the Graph RAG approach to answer user queries.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="C1 (HIGH-LEVEL COMMUNITY SUMMARIES)">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">C1 represents high-level community summaries, which are sub-communities of C0 if present, or C0 communities projected down. They are used in the Graph RAG approach to answer queries.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="C2 (INTERMEDIATE-LEVEL COMMUNITY SUMMARIES)">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">C2 consists of intermediate-level community summaries, which are sub-communities of C1 if present, or C1 communities projected down. They are used in the Graph RAG approach to answer queries.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="C3 (LOW-LEVEL COMMUNITY SUMMARIES)">
      <data key="d0">DATA, INFORMATION</data>
      <data key="d1">C3 includes low-level community summaries, which are the greatest in number. They are sub-communities of C2 if present, or otherwise used directly in the Graph RAG approach to answer queries.</data>
      <data key="d2">a739018eb63cbb6c26b779bd37afc233</data>
    </node>
    <node id="C1">
      <data key="d0">COMMUNITY, INFORMATION SYSTEM</data>
      <data key="d1">C1 refers to a level of community summaries that are used to answer queries. These summaries are sub-communities of C0 if C0 is present, or C0 communities projected down if C0 is not present. C1 communities are at a higher level of detail than C0.</data>
      <data key="d2">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </node>
    <node id="C2">
      <data key="d0">COMMUNITY, INFORMATION SYSTEM</data>
      <data key="d1">C2 refers to a level of community summaries that are used to answer queries. These summaries are sub-communities of C1 if C1 is present, or C1 communities projected down if C1 is not present. C2 communities are at a higher level of detail than C1.</data>
      <data key="d2">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </node>
    <node id="C3">
      <data key="d0">COMMUNITY, INFORMATION SYSTEM</data>
      <data key="d1">C3 refers to a level of community summaries that are used to answer queries. These summaries are sub-communities of C2 if C2 is present, or C2 communities projected down if C2 is not present. C3 communities are the low-level community summaries, greatest in number, and are at a higher level of detail than C2.</data>
      <data key="d2">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </node>
    <node id="TS">
      <data key="d0">INFORMATION PROCESSING, SUMMARIZATION TECHNIQUE</data>
      <data key="d1">TS is a method that uses the same approach as described in subsection 2.6, with the exception that source texts are shuffled and chunked for the map-reduce summarization stages instead of using community summaries.</data>
      <data key="d2">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </node>
    <node id="SS">
      <data key="d0">INFORMATION PROCESSING, SUMMARIZATION TECHNIQUE</data>
      <data key="d1">SS is an implementation of a naive RAG (Retrieval-Augmented Generation) method in which text chunks are retrieved and added to the available context window until the specified token limit is reached.</data>
      <data key="d2">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </node>
    <node id="C0">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </node>
    <node id="RAGAS (RETRIEVAL-AUGMENTED GENERATION ASSESSMENT SYSTEM)">
      <data key="d0">TECHNOLOGY, INFORMATION RETRIEVAL</data>
      <data key="d1">RAGAS, or Retrieval-Augmented Generation Assessment System, is a system that uses LLMs to evaluate the performance of conventional RAG systems. It automatically assesses qualities such as context relevance, faithfulness, and answer relevance.</data>
      <data key="d2">53455f8552b0787cb13c5a03eb550842</data>
    </node>
    <node id="PODCAST DATASET">
      <data key="d0">DATA SET</data>
      <data key="d1">The Podcast Dataset is a comprehensive collection of audio content, typically featuring episodes or segments, utilized in various research contexts such as summarization, question answering, and language model performance evaluation. This dataset plays a pivotal role in the indexing process, culminating in a graph structure comprising 8564 nodes and 20691 edges. It serves as a critical resource for assessing language models under different context window sizes, with a specific focus on a context window of 600 tokens for gleaning insights. The Podcast Dataset's multifaceted use underscores its significance in advancing research methodologies and understanding in the field of language processing and analysis.</data>
      <data key="d2">3900d15a5f3ace358fc06038c34cdf79,53455f8552b0787cb13c5a03eb550842,71f14506a6b15dfabd93fd1606a67b73,d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="NEWS DATASET">
      <data key="d0">DATA SET</data>
      <data key="d1">The News Dataset is a comprehensive collection of data, primarily comprising news articles and reports, utilized in the indexing process and subsequent research. This dataset plays a pivotal role in evaluating the performance of language models across different context window sizes. With a total of 15,754 nodes and 19,520 edges, it forms a larger graph compared to the Podcast Dataset. The News Dataset is specifically employed in summarization and question answering experiments, offering a context window size of 600 tokens, albeit with 0 gleanings, indicating a potential absence of additional insights or summaries derived from the dataset.</data>
      <data key="d2">3900d15a5f3ace358fc06038c34cdf79,53455f8552b0787cb13c5a03eb550842,71f14506a6b15dfabd93fd1606a67b73,d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="RAGAS">
      <data key="d0" />
      <data key="d1" />
      <data key="d2">53455f8552b0787cb13c5a03eb550842</data>
    </node>
    <node id="WANG ET AL., 2023A">
      <data key="d0">AUTHOR, PUBLICATION</data>
      <data key="d1">Wang et al., 2023a, are authors of a publication that is referenced in the context of head-to-head comparison of competing outputs in the field of LLMs (Large Language Models).</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="LLM AS-A-JUDGE, ZHENG ET AL., 2024">
      <data key="d0">AUTHOR, PUBLICATION</data>
      <data key="d1">LLM as-a-judge, Zheng et al., 2024, are authors of a publication that discusses the use of LLMs as evaluators in head-to-head comparisons of competing outputs.</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="RAGAS, ES ET AL., 2023">
      <data key="d0">AUTHOR, PUBLICATION</data>
      <data key="d1">RAGAS, Es et al., 2023, are authors of a publication that focuses on the evaluation of RAG (Retrieval-Augmented Generation) systems, specifically looking at qualities like context relevance, faithfulness, and answer relevance.</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="GRAPH RAG MECHANISM">
      <data key="d0">METHOD, TECHNIQUE</data>
      <data key="d1">The Graph RAG mechanism is a multi-stage method used for sensemaking activities, which involves the retrieval and integration of information from various sources to answer complex questions.</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="HEAD-TO-HEAD COMPARISON APPROACH">
      <data key="d0">METHOD, TECHNIQUE</data>
      <data key="d1">The head-to-head comparison approach is a method used to evaluate and compare different outputs or methods by directly contrasting them against each other, often using an LLM evaluator.</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="LLM EVALUATOR">
      <data key="d0">TOOL, TECHNOLOGY</data>
      <data key="d1">The LLM Evaluator is a sophisticated Large Language Model designed to serve as a critical tool in the evaluation of various methods and outputs, particularly within the realm of sensemaking activities and the assessment of RAG (Relevance, Accuracy, and Grading) systems. This tool is instrumental in gauging the quality of answers by applying a set of predefined metrics. It meticulously compares answers to a given question, judiciously determining which answer is superior based on the established criteria. The LLM Evaluator not only selects the better answer but also provides a detailed rationale for its decision, offering valuable insights into the evaluation process.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f,cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="TARGET METRICS">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Target metrics are specific measures used to evaluate the performance of methods or outputs in sensemaking activities, focusing on qualities such as comprehensiveness, diversity, empowerment, and directness.</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="CONTROL METRIC (DIRECTNESS)">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">The control metric, directness, is a measure used to indicate the validity of the evaluation, specifically assessing how specifically and clearly an answer addresses a question. It is often used as a baseline for comparison.</data>
      <data key="d2">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </node>
    <node id="COMPREHENSIVENESS">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Comprehensiveness is a critical metric utilized in evaluating the quality of answers or responses, particularly those generated by language models. It serves as a measure of how much detail a response provides to cover all aspects and nuances of a question or topic, assessing the thoroughness of the answer in addressing the question's requirements. A higher comprehensiveness score indicates that the response more effectively covers the content of the datasets or the topic at hand, ensuring that all relevant details are included and considered. This metric is essential for determining the level of detail and completeness in responses, enabling a more accurate assessment of their quality and relevance.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f,3900d15a5f3ace358fc06038c34cdf79,cbfd4a09b266218f64dc6e6d80f8a77e,d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="DIVERSITY">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Diversity is a comprehensive metric utilized to evaluate the richness and variety of responses or answers in various contexts. It gauges the range of viewpoints, the depth of information, and the uniqueness of content presented, making it a crucial tool for assessing the breadth and quality of insights provided. In the context of language models, Diversity measures the range and uniqueness of generated content, with a higher score indicating greater diversity. Additionally, it is employed to measure the variety of approaches in handling different aspects of datasets, further emphasizing its versatility as a metric in evaluating multifaceted responses.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f,3900d15a5f3ace358fc06038c34cdf79,cbfd4a09b266218f64dc6e6d80f8a77e,d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="EMPOWERMENT">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Empowerment is a multifaceted concept that serves as a metric in evaluating the effectiveness of methods, systems, and language models in providing users with the necessary information to reach an informed understanding and make educated decisions about a topic. It measures the ability of an answer or generated content to facilitate clarity, understanding, and decision-making, enabling the reader to form educated opinions and take action. Comparisons have shown mixed results when assessing global approaches versus naive RAG and Graph RAG approaches against source text summarization, indicating that the concept of Empowerment can yield varying outcomes depending on the context and methodology employed. The metric of Empowerment is crucial in gauging the empowering potential of answers and content, with a higher win rate suggesting greater empowerment.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f,3900d15a5f3ace358fc06038c34cdf79,cbfd4a09b266218f64dc6e6d80f8a77e,ed433e2f5d5387b47376eb0e45ca1c99</data>
    </node>
    <node id="DIRECTNESS">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Directness, in the context of evaluating responses or answers, is a multifaceted validity test and measure. It assesses the degree to which an answer addresses the question in a clear, specific, and straightforward manner. This measure is crucial for determining the relevance and clarity of the response in relation to the query, ensuring that the information provided is not only accurate but also directly pertinent to the topic at hand. By quantifying directness, one can identify how well approaches or responses align with the intended subject, facilitating a more precise and effective communication process.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f,d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="COMPARISON METHOD">
      <data key="d0">PROCEDURE, EVALUATION METHOD</data>
      <data key="d1">The Comparison Method involves providing the LLM with a question, a target metric, and a pair of answers. The LLM then assesses which answer is better according to the metric and returns the winner, or a tie if the answers are fundamentally similar.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f</data>
    </node>
    <node id="STOCHASTICITY">
      <data key="d0">PROPERTY, BEHAVIOR</data>
      <data key="d1">Stochasticity refers to the inherent randomness or variability in the LLM's responses. To account for this, each comparison is run five times and mean scores are used to determine the outcome.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f</data>
    </node>
    <node id="TABLE 2">
      <data key="d0">DATA, EXAMPLE</data>
      <data key="d1">Table 2 is an example of LLM-generated assessment, showing the results of the head-to-head comparisons based on the metrics and conditions evaluated.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f</data>
    </node>
    <node id="FIGURE 4">
      <data key="d0">DATA, VISUALIZATION</data>
      <data key="d1">Figure 4 is a visual representation of head-to-head win rate percentages across two datasets, four metrics, and 125 questions per comparison. It shows the performance of different conditions in relation to the metrics.</data>
      <data key="d2">2a5e1212b351d63d059ba1a1dec2811f</data>
    </node>
    <node id="HEADWINRATEPERCENTAGES">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Headwin rate percentages represent the success rates of different conditions (row and column conditions) across two datasets, four metrics, and 125 questions per comparison. Each comparison is repeated five times and averaged. The overall winner per dataset and metric is shown in bold. Self-win rates were not computed but are shown as the expected 50% for reference.</data>
      <data key="d2">b83d819b03401fb8332316960610e5d6</data>
    </node>
    <node id="GRAPHRAGCONDITIONS">
      <data key="d0">PROPERTY, TECHNIQUE</data>
      <data key="d1">GraphRAG conditions refer to the conditions under which GraphRAG (a graph-based retrieval-augmented generation model) is applied. These conditions outperformed naive RAG (a retrieval-augmented generation model without a graph index) on comprehensiveness and diversity. Conditions C1-C3 also showed slight improvements in answer comprehensiveness and diversity over TS (global text summarization without a graph index).</data>
      <data key="d2">b83d819b03401fb8332316960610e5d6</data>
    </node>
    <node id="CONTEXTWINDOWSIZE">
      <data key="d0">PROPERTY, PARAMETER</data>
      <data key="d1">Context window size is a parameter that affects the performance of tasks, especially for models like GPT-4 Turbo, which have a large context size of 128k tokens. The effect of context window size on any particular task is unclear, especially for models with a large context size. The potential for information to be &#8220;lost in the middle&#8221; of longer contexts is a concern. The optimum context size for the baseline condition (SS) was determined and then used uniformly for all query-time LLM (large language model) use.</data>
      <data key="d2">b83d819b03401fb8332316960610e5d6</data>
    </node>
    <node id="VARYING CONTEXT WINDOW SIZE">
      <data key="d0">PROPERTY, RESEARCH</data>
      <data key="d1">Varying the context window size is a research variable that was explored to understand its effects on combinations of datasets, questions, and metrics in the context of language model (LLM) use. The goal was to determine the optimum context size for the baseline condition (SS) and apply it uniformly for all query-time LLM use.</data>
      <data key="d2">3900d15a5f3ace358fc06038c34cdf79</data>
    </node>
    <node id="OPTIMUM CONTEXT SIZE">
      <data key="d0">PROPERTY, RESEARCH</data>
      <data key="d1">The optimum context size is the ideal size of the context window that provides the best performance for a given set of conditions, such as datasets, questions, and metrics. In this case, the optimum context size was determined to be 8k tokens for the baseline condition (SS) and was used uniformly for all query-time LLM use.</data>
      <data key="d2">3900d15a5f3ace358fc06038c34cdf79</data>
    </node>
    <node id="WINDOWSIZE">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">The windowsize refers to the size of the window used in the indexing process for the final evaluation, in this case, it is 8k tokens.</data>
      <data key="d2">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="GLOBAL APPROACHES">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">Global Approaches refer to the methods used in the evaluation that consistently outperformed the naive RAG (SS) approach in both comprehensiveness and diversity metrics across datasets.</data>
      <data key="d2">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="NAIVE RAG (SS) APPROACH">
      <data key="d0">METHOD, APPROACH</data>
      <data key="d1">The Naive RAG (SS) Approach is a method used in the evaluation that was outperformed by global approaches in terms of comprehensiveness and diversity metrics.</data>
      <data key="d2">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </node>
    <node id="MAP-REDUCE SUMMARIZATION">
      <data key="d0">METHOD, ALGORITHM</data>
      <data key="d1">Map-Reduce Summarization is a computationally demanding technique utilized in the Graph RAG approach for summarizing extensive text corpora. This method, known for its resource-intensive nature, stands out due to its requirement for the highest number of context tokens among the discussed summarization methods, highlighting its significant computational demands. The process involves two primary stages: the data is first broken down into smaller, manageable parts (map) and then the results from these parts are combined (reduce) to generate a comprehensive summary. Map-Reduce Summarization is often employed in conjunction with other techniques to facilitate human sensemaking over large datasets, making it a crucial tool in the analysis of vast text collections.</data>
      <data key="d2">5e2933c9646c751e6a60c9de12a255f2,71f14506a6b15dfabd93fd1606a67b73</data>
    </node>
    <node id="SOURCE TEXTS">
      <data key="d0">DOCUMENT, ORIGINAL</data>
      <data key="d1">Source Texts are the original documents or data from which summaries are derived. They are the primary material that undergoes summarization or analysis.&gt;</data>
      <data key="d2">71f14506a6b15dfabd93fd1606a67b73</data>
    </node>
    <node id="SOURCE TEXT SUMMARIZATION">
      <data key="d0">METHOD, TECHNOLOGY</data>
      <data key="d1">Source Text Summarization is a method for creating summaries directly from the original text. It is compared to Graph RAG in terms of context tokens required and comprehensiveness and diversity of the summaries.</data>
      <data key="d2">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </node>
    <node id="LOW-LEVEL COMMUNITY SUMMARIES (C3)">
      <data key="d0">PROPERTY, CATEGORY</data>
      <data key="d1">Low-level Community Summaries (C3) are a category of summaries that are part of the analysis. Graph RAG requires 26-33% fewer context tokens for these summaries compared to source text summarization.</data>
      <data key="d2">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </node>
    <node id="ROOT-LEVEL COMMUNITY SUMMARIES (C0)">
      <data key="d0">PROPERTY, CATEGORY</data>
      <data key="d1">Root-level Community Summaries (C0) are a category of summaries that are part of the analysis. Graph RAG requires over 97% fewer context tokens for these summaries compared to source text summarization.</data>
      <data key="d2">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </node>
    <node id="INFORMED UNDERSTANDING">
      <data key="d0">CONCEPT, INFORMATION PROCESSING</data>
      <data key="d1">Informed Understanding refers to the process by which users reach a comprehensive and knowledgeable comprehension of a subject, often facilitated by specific examples, quotes, and citations.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97</data>
    </node>
    <node id="GRAPH RAG INDEX">
      <data key="d0">PROPERTY, INFORMATION SYSTEM</data>
      <data key="d1">Graph RAG Index is a component of a system designed to enhance the retrieval of relevant information by indexing and organizing data in a graph structure, potentially improving the efficiency and effectiveness of information retrieval.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97</data>
    </node>
    <node id="TUNING ELEMENT EXTRACTION PROMPTS">
      <data key="d0">PROPERTY, INFORMATION RETRIEVAL</data>
      <data key="d1">Tuning Element Extraction Prompts involves refining the prompts used to extract information from documents, which can help in retaining more details in the Graph RAG index, potentially improving the quality of information retrieval.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97</data>
    </node>
    <node id="RAG APPROACHES">
      <data key="d0">CONCEPT, INFORMATION RETRIEVAL</data>
      <data key="d1">RAG Approaches refer to the methods and systems used to retrieve relevant information from external data sources to augment the context window of Large Language Models (LLMs), aiding in answering queries with more comprehensive information.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97</data>
    </node>
    <node id="ADVANCED RAG&lt;||ADVANCED RAG">
      <data key="d0">CONCEPT, INFORMATION RETRIEVAL</data>
      <data key="d1">Advanced RAG encompasses more sophisticated variations of Retrieval-Augmented Generation (RAG) systems that include pre-retrieval, retrieval, and post-retrieval strategies designed to overcome the limitations of Naive RAG, often incorporating iterative and dynamic cycles of retrieval and generation.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97</data>
    </node>
    <node id="MODULAR RAG">
      <data key="d0">PROPERTY, INFORMATION RETRIEVAL</data>
      <data key="d1">Modular RAG is a type of RAG system that includes patterns for iterative and dynamic cycles of interleaved retrieval and generation, designed to enhance the flexibility and adaptability of information retrieval processes.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97</data>
    </node>
    <node id="ADVANCED RAG">
      <data key="d0" />
      <data key="d1">Advanced RAG (Retrieval-Augmented Generation) is a cutting-edge technology and research area where the index is a knowledge graph, as detailed by Gao et al. in 2023. This innovative approach leverages knowledge graphs to enhance the generation of text and facilitate advanced retrieval and analysis of information. By integrating a knowledge graph as the indexing mechanism, Advanced RAG enables more sophisticated and contextually rich text generation, making it a significant advancement in the field of information retrieval and natural language processing.</data>
      <data key="d2">38feec52b8bfbd3fd8e03635acdaec97,40f2d6a0270e54743e7ace239369da96,7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="LLM&#8217;S CONTEXT WINDOW">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">LLM&#8217;s context window refers to the limitation of a large language model in processing and understanding text, where the model can only consider a fixed amount of text at a time, typically a few thousand tokens. This context window can affect the model's ability to understand and generate text that is longer than the window size.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="ADVANCED RAG SYSTEMS">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Advanced RAG systems are retrieval-augmented generation systems that include pre-retrieval, retrieval, and post-retrieval strategies designed to overcome the drawbacks of Naive RAG. These systems are more sophisticated and can handle complex tasks by incorporating various strategies for retrieval and generation.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="MODULAR RAG SYSTEMS">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Modular RAG systems are retrieval-augmented generation systems that include patterns for iterative and dynamic cycles of interleaved retrieval and generation. These systems are designed to be flexible and can adapt to different retrieval and generation requirements.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="PARALLEL GENERATION OF COMMUNITY ANSWERS">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Parallel generation of community answers is a technique where community answers are generated simultaneously from community summaries. This approach can improve the efficiency and effectiveness of retrieval and generation by leveraging the power of parallel processing.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="ITERATIVE RETRIEVAL-GENERATION STRATEGY">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Iterative retrieval-generation strategy is a method where retrieval and generation are performed in a series of iterative cycles. This strategy allows for continuous refinement and improvement of the retrieval and generation process.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="FEDERATED RETRIEVAL-GENERATION STRATEGY">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Federated retrieval-generation strategy is a method where retrieval and generation are performed in a distributed and collaborative manner. This strategy allows for the sharing of resources and information across multiple systems or nodes.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="MULTI-DOCUMENT SUMMARIZATION">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Multi-document summarization is a technique for creating a concise and coherent summary of multiple documents. This technique is used to provide a high-level overview of the content of multiple documents, which can be useful for information retrieval and analysis.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="MULTI-HOP QUESTION ANSWERING">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Multi-hop question answering is a technique for answering questions that require reasoning over multiple pieces of information or documents. This technique is used to handle complex questions that cannot be answered by a single piece of information or document.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="HIERARCHICAL INDEX">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Hierarchical index is a data structure that organizes information in a hierarchical manner. This index is used to improve the efficiency and effectiveness of information retrieval by allowing for faster and more accurate search and retrieval of information.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="SUMMARIZATION">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Summarization is a technique for creating a concise and coherent summary of a document or set of documents. This technique is used to provide a high-level overview of the content of a document or set of documents, which can be useful for information retrieval and analysis.&gt;</data>
      <data key="d2">7da3d8d244b67f09425a4a7783e4bb55</data>
    </node>
    <node id="CAIRE-COVID">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">CAiRE-COVID is a research study conducted by Su et al. in 2020, focusing on the impact of COVID-19 on various aspects of society and health.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="ITRG">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">ITRG is a research study that involves multi-hop question answering, as described by Feng et al. in 2023.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="IR-COT">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">IR-CoT is a research study that deals with multi-hop question answering, as described by Trivedi et al. in 2022.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="DSP">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">DSP is a research study that involves multi-hop question answering, as described by Khattab et al. in 2022.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="RAPTOR">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">RAPTOR is a research study that involves generating a hierarchical index of text chunks by clustering the vectors of text embeddings, as described by Sarthi et al. in 2024.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="TREE OF CLARIFICATIONS">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">Tree of Clarifications is a research study that involves generating a &#8220;tree of clarifications&#8221; to answer multiple interpretations of ambiguous questions, as described by Kim et al. in 2023.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="KNOWLEDGE GRAPH CREATION">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">Knowledge Graph Creation is a research area that involves using LLMs to create knowledge graphs, as described by Trajanoska et al. in 2023.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="KNOWLEDGE GRAPH COMPLETION">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">Knowledge Graph Completion is a research area that involves using LLMs to complete knowledge graphs, as described by Yao et al. in 2023.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="CAUSAL GRAPH EXTRACTION">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">Causal Graph Extraction is a research area that involves using LLMs to extract causal graphs from source texts, as described by Ban et al. in 2023 and Zhang et al. in 2024.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96</data>
    </node>
    <node id="KAPING">
      <data key="d0">RESEARCH STUDY</data>
      <data key="d1">KAPING, a pioneering research study introduced by Baek et al. in 2023, delves into the realm of advanced RAG (Retrieval-Augmented Generation) techniques. Central to KAPING's methodology is the utilization of a knowledge graph as the index, a concept detailed by Baek et al. in their work. This system uniquely focuses on subsets of the graph structure as the primary objects of enquiry, enabling researchers to query specific parts of the knowledge graph for detailed information. Through this approach, KAPING aims to enhance understanding and facilitate the retrieval of intricate data within specialized professional networks, thereby identifying collaboration opportunities and knowledge gaps in the field.</data>
      <data key="d2">40f2d6a0270e54743e7ace239369da96,7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="LLMS FOR KNOWLEDGE GRAPH CREATION">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">LLMs (Large Language Models) are used for the creation of knowledge graphs, as demonstrated by the work of Trajanoska et al. in 2023. This involves the automatic generation of structured knowledge graphs from unstructured text data.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="LLMS FOR KNOWLEDGE GRAPH COMPLETION">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">LLMs are also applied for the completion of knowledge graphs, as shown by the research of Yao et al. in 2023. This involves enhancing existing knowledge graphs by filling in missing information or relationships.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="LLMS FOR CAUSAL GRAPH EXTRACTION">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">LLMs are used for the extraction of causal graphs from source texts, as evidenced by the work of Ban et al. in 2023 and Zhang et al. in 2024. This involves identifying causal relationships between entities in the text.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="G-RETRIEVER">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">G-Retriever, introduced by He et al. in 2024, is a system that retrieves specific parts of a graph structure. This involves querying a graph for specific information or relationships.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="GRAPHTOOLFORMER">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">GraphToolFormer, developed by Zhang in 2023, is a system that uses derived graph metrics as the objects of enquiry. This involves analyzing graph metrics to extract information.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="SURGE">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">SURGE, created by Kang et al. in 2023, is a system where narrative outputs are strongly grounded in the facts of retrieved subgraphs. This involves generating narratives based on the information retrieved from subgraphs.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="FABULA">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">FABULA, developed by Ranade and Joshi in 2023, is a system that serializes retrieved event-plot subgraphs using narrative templates. This involves creating narratives from event plots retrieved from graphs.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="SYSTEM FOR MULTI-HOP QUESTION ANSWERING">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">A system that supports both creation and traversal of text-relationship graphs for multi-hop question answering, as described by Wang et al. in 2023b. This involves answering complex questions that require understanding relationships across multiple pieces of text.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="LANGCHAIN">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">LangChain, as of 2024, is a versatile library that facilitates the creation and navigation of text-relationship graphs, specifically designed to enhance multi-hop question answering capabilities. It is compatible with a wide range of graph databases, making it an ideal choice for Retrieval-Augmented Generation (RAG) applications. LangChain offers a comprehensive suite of tools and interfaces for effectively managing and manipulating graph data, enabling users to leverage the power of graph databases in their projects.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7,e015335cdcae20e6546fe7cbdef56c1a</data>
    </node>
    <node id="LLAMAINDEX">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">LlamaIndex, as of 2024, is a sophisticated library designed to facilitate the creation and navigation of text-relationship graphs. This library is pivotal for multi-hop question answering, enabling complex queries that require understanding relationships between multiple pieces of information. It is notable for its compatibility with a wide range of graph databases, making it a versatile tool for Retrieval-Augmented Generation (RAG) applications. LlamaIndex offers a comprehensive suite of tools and interfaces for managing graph data, making it an essential resource for developers and researchers working in the field of graph-based information retrieval and analysis.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7,e015335cdcae20e6546fe7cbdef56c1a</data>
    </node>
    <node id="GRAPH-BASED RAG APPLICATIONS">
      <data key="d0">TECHNOLOGY, APPLICATION</data>
      <data key="d1">A more general class of graph-based RAG (Retrieval-Augmented Generation) applications is emerging. This involves the use of graph data in various applications for retrieval and generation.</data>
      <data key="d2">7383c69e93bb8c8648181f5355d2c9a7</data>
    </node>
    <node id="NEO4J">
      <data key="d0">SOFTWARE, DATABASE</data>
      <data key="d1">Neo4J is a graph database that can be used to create and reason over knowledge graphs, and is supported by LangChain and LlamaIndex libraries for RAG applications.</data>
      <data key="d2">e015335cdcae20e6546fe7cbdef56c1a</data>
    </node>
    <node id="NEBULAGRAPH">
      <data key="d0">SOFTWARE, DATABASE</data>
      <data key="d1">NebulaGraph is a graph database that can be used to create and reason over knowledge graphs, and is supported by LangChain and LlamaIndex libraries for RAG applications.</data>
      <data key="d2">e015335cdcae20e6546fe7cbdef56c1a</data>
    </node>
    <node id="EVALUATION APPROACH">
      <data key="d0">METHOD, PROCESS</data>
      <data key="d1">The evaluation approach has limitations, as it has only examined a certain class of sensemaking questions for two corpora in the region of 1 million tokens. More work is needed to understand how performance varies across different ranges of question types, data types, and dataset sizes.</data>
      <data key="d2">e015335cdcae20e6546fe7cbdef56c1a</data>
    </node>
    <node id="SELFCHECKGPT">
      <data key="d0">SOFTWARE, APPLICATION</data>
      <data key="d1">SelfCheckGPT, developed by Manakul et al. in 2023, is an advanced AI model designed to enhance the analysis of Graph RAG's performance. This innovative application specializes in comparing fabrication rates, offering a significant improvement over existing analysis methods. By leveraging the capabilities of SelfCheckGPT, users can gain deeper insights into the efficiency and effectiveness of fabrication processes, ultimately leading to optimized performance and better decision-making in the Motor Control and Drive Systems domain.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375,e015335cdcae20e6546fe7cbdef56c1a</data>
    </node>
    <node id="1 MILLION TOKENS">
      <data key="d0">QUANTITY, DATA_SIZE</data>
      <data key="d1">The region of 1 million tokens refers to a specific size of data or text, which is a significant amount for analyzing performance in various tasks and models.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="QUESTION TYPES">
      <data key="d0">PROPERTY, TASK</data>
      <data key="d1">Question types represent the various categories or formats of questions that can be asked or generated, which can affect the performance of models or systems in different ways.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="DATA TYPES">
      <data key="d0">PROPERTY, DATA_CLASSIFICATION</data>
      <data key="d1">Data types refer to the classification or categories of data, such as numerical, textual, categorical, etc., which can influence the performance of models or systems depending on their specific capabilities.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="DATASET SIZES">
      <data key="d0">QUANTITY, DATA_SIZE</data>
      <data key="d1">Dataset sizes refer to the volume or scale of datasets used for training or evaluating models, which can impact the performance of models due to the amount of data they have to process.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="END USERS">
      <data key="d0">PERSON, USER</data>
      <data key="d1">End users are the individuals or groups who will ultimately use or benefit from the models, systems, or services, and their feedback is crucial for validating the effectiveness of the sensemaking questions and target metrics.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="GLOBAL SUMMARIZATION">
      <data key="d0">PROPERTY, TEXT_PROCESSING</data>
      <data key="d1">Global summarization is a text processing technique that generates a concise summary of the entire source text, which can be used to provide an overview of the content and can be competitive with the graph index approach in some cases.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="COMPUTE BUDGET">
      <data key="d0">PROPERTY, COST</data>
      <data key="d1">Compute budget refers to the financial resources allocated for computing tasks, which can influence the decision to invest in building a graph index due to the potential costs associated with its creation and maintenance.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="LIFETIME QUERIES">
      <data key="d0">PROPERTY, QUERY_FREQUENCY</data>
      <data key="d1">Lifetime queries refer to the expected number of queries that will be made to a dataset over its lifetime, which can affect the decision to invest in building a graph index due to the potential benefits of faster querying.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="GRAPH-RELATED RAG APPROACHES">
      <data key="d0">PROPERTY, AI_MODEL</data>
      <data key="d1">Graph-related RAG approaches are variations of the Retrieval-Augmented Generation model that utilize graph structures for various tasks, such as summarization or question answering, and they can offer additional benefits beyond the basic graph index.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="FUTURE WORK">
      <data key="d0">PROPERTY, RESEARCH</data>
      <data key="d1">Future work refers to the potential research directions or improvements that can be pursued, which can include refining and adapting the current Graph RAG approach, as well as exploring hybrid RAG schemes that combine different techniques.</data>
      <data key="d2">7040ba36a7c09899a355d14a30d65375</data>
    </node>
    <node id="HIERARCHICAL COMMUNITY STRUCTURE">
      <data key="d0">CONCEPT, ORGANIZATION</data>
      <data key="d1">Hierarchical community structure refers to the organization of communities within a graph or text corpus at different levels. This structure supports the refinement and adaptation of the Graph RAG approach by allowing for "roll-up" operations across multiple levels and "drill down" mechanisms that follow the information scent contained in higher-level community summaries.</data>
      <data key="d2">5e2933c9646c751e6a60c9de12a255f2</data>
    </node>
    <node id="EMBEDDING-BASED MATCHING">
      <data key="d0">METHOD, TECHNIQUE</data>
      <data key="d1">Embedding-based matching is a technique used in the Graph RAG approach to match user queries and graph annotations in a more local manner. It involves converting textual information into numerical vectors (embeddings) to compare and find relevant information.</data>
      <data key="d2">5e2933c9646c751e6a60c9de12a255f2</data>
    </node>
    <node id="HYBRID RAG SCHEMES">
      <data key="d0">METHOD, TECHNIQUE</data>
      <data key="d1">Hybrid RAG schemes are a combination of different methods in the Graph RAG approach, which include embedding-based matching against community reports before employing map-reduce summarization mechanisms. These schemes aim to improve the comprehensiveness and diversity of answers by integrating various techniques.</data>
      <data key="d2">5e2933c9646c751e6a60c9de12a255f2</data>
    </node>
    <node id="ANSWER IIVENESS AND DIVERSITY">
      <data key="d0">PROPERTY, QUALITY</data>
      <data key="d1">Answer Iiveness and Diversity refers to the richness and variety of responses or solutions generated by a system or method. It is a measure of the system's capability to produce a wide range of answers or solutions to a given problem or query.</data>
      <data key="d2">e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="GLOBAL BUT GRAPH-FREE APPROACH">
      <data key="d0">METHOD, TECHNIQUE</data>
      <data key="d1">The Global but Graph-free Approach is a method that uses map-reduce source text summarization techniques to process and summarize large datasets without relying on graph structures. It is a computational approach that can handle global queries over large datasets.</data>
      <data key="d2">e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="ENTITY-BASED GRAPH INDEX">
      <data key="d0">DATA STRUCTURE, INDEX</data>
      <data key="d1">The Entity-based Graph Index is a data structure that organizes and indexes information based on entities and their relationships in a graph format. It provides a superior data index for root-level communities, offering better performance and efficiency compared to naive RAG (Retrieval-Augmented Generation) and other global methods.</data>
      <data key="d2">e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="ROOT-LEVEL COMMUNITIES">
      <data key="d0">GROUP, COMMUNITY</data>
      <data key="d1">Root-level Communities are the fundamental or top-level groups within a dataset or graph structure. They represent the highest-level divisions or categories in the data, which can be further analyzed or queried for detailed information.</data>
      <data key="d2">e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="GLOBAL METHODS">
      <data key="d0">METHOD, TECHNIQUE</data>
      <data key="d1">Global Methods refer to computational techniques or algorithms that can handle global queries or operations over large datasets. These methods are designed to process and analyze data at a global scale, often requiring significant computational resources and token costs.</data>
      <data key="d2">e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="TOKEN COST">
      <data key="d0">PROPERTY, COST</data>
      <data key="d1">Token Cost is a measure of the computational or resource cost associated with processing or analyzing data in a graph or dataset. It represents the cost of operations or queries in terms of the number of tokens or computational units required.</data>
      <data key="d2">e31d2d134cf501c93f9445914d7350f9</data>
    </node>
    <node id="GRAPHRAG INDEXING">
      <data key="d0">SOFTWARE, TECHNOLOGY</data>
      <data key="d1">GraphRAG Indexing is a sophisticated suite of data pipeline and transformation tools, designed to extract structured data from unstructured text by leveraging Large Language Models (LLMs). This system is highly configurable, offering both default and custom configuration modes to optimize the performance and integration of the Indexing Engine pipelines. GraphRAG Indexing includes workflows, standard and custom steps, prompt templates, and input/output adapters, enabling entity extraction, relationship detection, community detection, and data embedding into vector spaces. The suite supports configuration through the init command, environment variables, and JSON or YAML files for deeper control, ensuring flexibility and adaptability. Outputs from GraphRAG Indexing can be stored in JSON, Parquet, or accessed via the Python API, facilitating seamless integration into various data processing workflows.</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0,ccd2de9e2219521fbca779843c65af58</data>
    </node>
    <node id="INDEXING PIPELINES">
      <data key="d0">PROCESS, TECHNOLOGY</data>
      <data key="d1">Indexing Pipelines are configurable workflows within the GraphRAG indexing package. They consist of standard and custom steps, prompt templates, and input/output adapters. These pipelines are designed to extract entities, relationships, and claims from raw text, perform community detection, generate summaries and reports, and embed data into vector spaces. Outputs can be stored in various formats or accessed through the Python API.&gt;</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0</data>
    </node>
    <node id="ENTITY EXTRACTION">
      <data key="d0">FUNCTION, TECHNOLOGY</data>
      <data key="d1">Entity Extraction is a pivotal function within the GraphRAG indexing suite, serving as a configuration section that specializes in settings for extracting entities from text. This process is essential for deciphering the components of unstructured text, acting as a cornerstone in the indexing pipeline. The section manages various parameters such as the Language Model (LLM), parallelization, async mode, prompt file, entity types, max gleanings, and strategy. The entities identified and extracted through Entity Extraction are further analyzed to uncover relationships and detect communities, contributing significantly to the structured data produced by the indexing process.</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0,abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="RELATIONSHIP DETECTION">
      <data key="d0">FUNCTION, TECHNOLOGY</data>
      <data key="d1">Relationship Detection is a function within the GraphRAG indexing suite that identifies relationships between entities extracted from unstructured text. This process helps in understanding the context and connections within the text, contributing to the generation of structured data. The detected relationships are part of the outputs that can be stored in various formats or accessed through the Python API.&gt;</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0</data>
    </node>
    <node id="COMMUNITY DETECTION">
      <data key="d0">FUNCTION, TECHNOLOGY</data>
      <data key="d1">Community Detection is a sophisticated function embedded within the GraphRAG indexing suite, specifically operating as a subprocess during Phase 3: Graph Augmentation. This process leverages the Hierarchical Leiden Algorithm to meticulously identify and generate a hierarchy of entity communities, thereby facilitating a deeper understanding of the community structure within the graph. By analyzing the connectivity patterns of nodes, Community Detection effectively discerns groups or clusters of related entities, enabling the summarization and reporting of text data at various levels of granularity. The outcomes of this algorithmic process significantly enrich the structured data outputs of the indexing procedure, providing a valuable tool for navigating and summarizing the graph data in a more organized and insightful manner.</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0,493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08,a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </node>
    <node id="DATA EMBEDDING">
      <data key="d0">FUNCTION, TECHNOLOGY</data>
      <data key="d1">Data Embedding is a function within the GraphRAG indexing suite that transforms extracted entities and text chunks into vector representations. This process is used to embed entities into a graph vector space and text chunks into a textual vector space, facilitating further analysis and storage of the structured data. Outputs can be stored in various formats or accessed through the Python API.&gt;</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0</data>
    </node>
    <node id="OUTPUT FORMATS">
      <data key="d0">PROPERTY, TECHNOLOGY</data>
      <data key="d1">Output Formats refer to the various ways in which the structured data generated by the GraphRAG indexing suite can be stored. These include JSON and Parquet formats, which are commonly used for data storage and analysis. Additionally, the outputs can be handled manually via the Python API, providing flexibility in how the data is accessed and used.&gt;</data>
      <data key="d2">251e8d332b451d900df961cbe215bca0</data>
    </node>
    <node id="CONFIG FILE">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">A Config File is a pivotal document utilized in the setup and execution of software applications or systems, particularly the GraphRAG system. This file contains essential settings, parameters, and configurations that dictate how a pipeline should be executed, ensuring the system functions correctly. It can be utilized with the Command Line Interface (CLI) or the Python API, offering flexibility in specifying execution details. The Config File supports two modes of operation: Default Configuration mode and Custom Configuration mode. By default, it operates in Default Configuration mode, but users can opt out and execute a custom configuration by specifying the Config File using the --config flag, followed by the path to the configuration file. This feature is crucial for setting up the environment and pipeline for the GraphRAG system, as it allows for the precise specification of required environment variables and settings.</data>
      <data key="d2">919cb44d9688a14bf48fa7c98163ed81,9f2cd3d789fd49f220d4cda6b9e8048c,b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="PYTHON API">
      <data key="d0">PROGRAMMING INTERFACE, TOOL</data>
      <data key="d1">The Python API is a programming interface that allows users to run the pipeline using Python code. It provides functions and methods for executing the pipeline and can be used to specify the workflow and parameters for the pipeline.&gt;</data>
      <data key="d2">b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="NODE">
      <data key="d0">EXECUTION ENVIRONMENT, TOOL</data>
      <data key="d1">NODE serves multiple roles within the system. Primarily, it contains layout information for rendered graph-views of the Entities and Documents, which have been embedded and clustered for effective visualization. This makes NODE an integral part of the table that organizes and presents data in a comprehensible format. Additionally, NODE functions as an execution environment, enabling users to run the Command Line Interface (CLI) in a JavaScript environment. This capability provides the necessary runtime and dependencies for executing pipeline commands, making NODE a versatile component that supports both data visualization and execution tasks within the system.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,85e50a4d70697a2c4420e7a9fc82f22d,b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="PIPELINE WORKFLOW REFERENCE">
      <data key="d0">DATA STRUCTURE, CONFIGURATION</data>
      <data key="d1">Pipeline Workflow Reference is a data structure used in the Python API to define the workflow of the pipeline. It contains a list of steps that specify the operations to be performed on the data. Each step can include a verb, arguments, and an optional input.&gt;</data>
      <data key="d2">b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="DERIVE VERB">
      <data key="d0">FUNCTION, CONFIGURATION</data>
      <data key="d1">The Derive Verb is a built-in function used in the Pipeline Workflow Reference to perform operations on the data. It can be used to create new columns by applying an operator to existing columns. The Derive Verb requires arguments such as column names, the operator to be used, and the name of the new column.&gt;</data>
      <data key="d2">b0505e11596cadd9890fef049c29473c</data>
    </node>
    <node id="COL1">
      <data key="d0">PROPERTY, COLUMN</data>
      <data key="d1">COL1 is a significant data column within the dataset, characterized by its numerical values. It plays a crucial role as an input in the operational workflow, specifically in the derive step of workflow2, demonstrating its importance in data processing activities. COL1's numerical nature and its utilization in workflow operations highlight its relevance and necessity in the analytical processes defined within the workflow framework.</data>
      <data key="d2">76d9dcb9a27c2caea1f46bb5050851c6,f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="COL2">
      <data key="d0">PROPERTY, COLUMN</data>
      <data key="d1">"COL2 is a significant data column within the dataset, characterized by its numerical values. It holds the position of the second input in the operational workflow, demonstrating its integral role in the data processing sequence. Specifically, COL2 is utilized as an input in the derive step of workflow2, highlighting its functional importance in generating new data attributes or modifying existing ones through the workflow's operations."</data>
      <data key="d2">76d9dcb9a27c2caea1f46bb5050851c6,f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="COL_MULTIPLIED">
      <data key="d0">PROPERTY, COLUMN</data>
      <data key="d1">col_multiplied is a new column in the dataset that is the result of multiplying the values of col1 and col2.</data>
      <data key="d2">f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="RUN_PIPELINE">
      <data key="d0">FUNCTION, PROCESS</data>
      <data key="d1">run_pipeline is a function that takes a dataset and workflows as input, and asynchronously processes the data according to the workflows. It returns the result of the operations as outputs.</data>
      <data key="d2">f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="WORKFLOWS">
      <data key="d0">PROPERTY, PROCESS</data>
      <data key="d1">The Workflows section is a critical component that meticulously outlines the Directed Acyclic Graph (DAG) for the data processing pipeline. This section encompasses an array of workflows, each detailing a series of operations to be executed on the dataset. Each operation within the workflows specifies the source columns, the target column, and the operator to be utilized, thereby defining the transformation steps and inter-dependencies between the workflows. This structured approach ensures a clear and systematic processing sequence, enabling efficient data manipulation and analysis.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484,f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="OUTPUTS">
      <data key="d0">PROPERTY, DATA</data>
      <data key="d1">outputs is a list that collects the results of the run_pipeline function. Each output is the result of processing the dataset according to the workflows.</data>
      <data key="d2">f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="PIPELINE_RESULT">
      <data key="d0">PROPERTY, DATA</data>
      <data key="d1">pipeline_result is the last element in the outputs list, representing the final result of the pipeline execution.</data>
      <data key="d2">f3a07680cbe8ab1f6055369da05f4f38</data>
    </node>
    <node id="DEFAULT PROMPTS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Default Prompts are the simplest way to start using the GraphRAG system. They are designed to work with minimal configuration and are suitable for out-of-the-box use. These prompts cover various aspects such as entity and relationship extraction, description summarization, claim extraction, and community reports.</data>
      <data key="d2">bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="ENTITY/RELATIONSHIP EXTRACTION">
      <data key="d0">PROPERTY, FUNCTION</data>
      <data key="d1">Entity/Relationship Extraction, a crucial function within the GraphRAG system, specializes in identifying and extracting entities along with their relationships from input data. This process is pivotal for the construction of a knowledge graph. It operates by processing text data, using prompts and token-replacements to analyze the input text. The outcome is the generation of tuples, which accurately represent individual entities or the relationships between them. This procedure is foundational in the domain of Social Network Analysis, enabling a deeper understanding of complex relationships and facilitating the identification of key influencers within specialized professional networks such as Motor Control and Drive Systems. By leveraging Entity/Relationship Extraction, professionals can uncover collaboration opportunities and pinpoint knowledge gaps, enhancing their ability to navigate and contribute to the field.</data>
      <data key="d2">6a7157695d90d434b2625c3f05420916,bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="ENTITY/RELATIONSHIP DESCRIPTION SUMMARIZATION">
      <data key="d0">PROPERTY, FUNCTION</data>
      <data key="d1">Entity/Relationship Description Summarization is a function that provides concise summaries of the descriptions of entities and their relationships. This can be useful for understanding the context and significance of the extracted information.</data>
      <data key="d2">bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="CLAIM EXTRACTION">
      <data key="d0">PROPERTY, FUNCTION</data>
      <data key="d1">Claim Extraction is a critical function and process within the domain of text analysis and information retrieval. It specializes in identifying and extracting claims or assertions made within text data, focusing on the validity or truthfulness of statements. This process is particularly useful in scenarios where the accuracy of information is paramount. Claim Extraction uses prompts and token-replacements to process text and pinpoint claims based on the values provided by the extractor. It is a specific type of information extraction that zeroes in on identifying assertions or statements made within the text, often about entities and their attributes. The process is designed to identify positive factual statements with an evaluated status and time-bounds, emitting these claims as a primary artifact known as Covariates. This capability is essential in the pipeline of text analysis, enabling the extraction of meaningful and accurate information from large volumes of textual data.</data>
      <data key="d2">10d01d36390b307a63fd5bc97d8682c0,493f38f41b89e767fc23d84e1fa5ba20,6a7157695d90d434b2625c3f05420916,6f92ce3fcd05dd5697ded83586f7bc08,bdb8f9e797229f596744d9636ab857b0,d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </node>
    <node id="AUTO TEMPLATING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Auto Templating is a feature that leverages input data and LLM interactions to create domain adaptive templates for the generation of the knowledge graph. It is highly recommended for better results in an Index Run. The process involves analyzing the input data and automatically generating templates that are tailored to the specific domain or use case.</data>
      <data key="d2">bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="MANUAL CONFIGURATION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Manual Configuration is an advanced use-case feature of the GraphRAG system. It allows users to customize the prompts and templates used in the generation of the knowledge graph manually. This is typically used when the Auto Templating feature does not meet specific requirements or when more control over the process is desired.</data>
      <data key="d2">bdb8f9e797229f596744d9636ab857b0</data>
    </node>
    <node id="TEMPLATE GENERATION ALGORITHM">
      <data key="d0">ALGORITHM, PROCESS</data>
      <data key="d1">The Template Generation Algorithm is a process within GraphRAG that is responsible for creating domain adaptive templates for knowledge graph generation. It involves loading inputs, splitting them into chunks (text units), and then running a series of LLM invocations and template substitutions to generate the final prompts. The algorithm can be customized by adjusting various parameters such as the method, limit, language, max tokens, and chunk size. The default values provided by the script are suggested for use, but users can explore and tweak the algorithm for better results. The algorithm is executed as part of the automatic template generation process, which is optional but highly recommended for better Index Run results.</data>
      <data key="d2">9b364093aeecfc789c70fc5bd9503487</data>
    </node>
    <node id="INITIALIZATION PROCESS">
      <data key="d0">PROCESS, TASK</data>
      <data key="d1">The Initialization Process is a task that must be completed before running the automatic template generation in GraphRAG. It involves using the graphrag.index --init command to create the necessary configuration files and default prompts. This process is crucial for setting up the workspace and ensuring that the tool can function properly. For more detailed information about the initialization process, refer to the Init Documentation.</data>
      <data key="d2">9b364093aeecfc789c70fc5bd9503487</data>
    </node>
    <node id="ROOT">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">ROOT is a pivotal command-line option that designates the base directory or starting point for all relative paths and configurations within the data project. This directory is essential as it houses the config files in formats such as YML, JSON, or .env, an input directory containing the input data, and an .env file with environment variables. ROOT defaults to the current directory if not explicitly specified. It is a required property utilized in the configuration of commands like graphrag.prompt_tune, serving as a guide to locate the input data and other resources necessary for prompt generation. The presence of ROOT ensures that the system can accurately identify and access the project's critical components, making it a fundamental aspect of the project's setup and operation.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f,abac77a5673e907cf8d65161c2612784,ce9cc3ed2e5f890d02e867ed0b0f8ff9,f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="DOMAIN">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">The "DOMAIN" is a versatile command-line option and an optional property that plays a crucial role in specifying the subject area or topic of the input data. It enables users to explicitly define the domain related to the data, such as 'space science', 'microbiology', or 'environmental news', which is particularly useful in the configuration of the graphrag.prompt_tune command. This command is designed to customize the prompt generation process to better suit a specific field of interest. If the "DOMAIN" is left unspecified, it intelligently infers the domain from the input data, ensuring that the analysis and processing are contextually accurate and relevant. This feature is pivotal in enhancing the precision and effectiveness of data processing and analysis within specialized professional networks.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f,ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="LIMIT">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">LIMIT is a versatile command-line option and optional property utilized in the context of text unit selection. It serves a dual purpose: first, as a command-line parameter, it establishes the maximum number of text units to load when employing random or top selection methods, with a default setting of 15. Second, within the configuration of the graphrag.prompt_tune command, LIMIT acts as a property to regulate the sample size for prompt generation, thereby influencing the template creation process. This feature provides users with the flexibility to control the scope and scale of text data used in various operations, ensuring optimal outcomes in text analysis and processing tasks.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f,ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="LANGUAGE">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">The entity "LANGUAGE" serves a dual purpose within the context of input processing and command configuration. Firstly, as a command-line option, LANGUAGE designates the language to be used for input processing. If the specified language differs from that of the inputs, the Language Model (LLM) facilitates translation. By default, if no language is explicitly set, the system automatically detects the language from the inputs. Secondly, "language" is also an optional property utilized in the configuration of the graphrag.prompt_tune command. This property ensures that the prompt generation process is customized to match the language of the input documents, thereby enhancing the accuracy and relevance of the generated prompts.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f,ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="MAX_TOKENS">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">MAX_TOKENS is a crucial command-line option utilized in the context of Language Models (LLM). It serves the dual purpose of setting the maximum token count for prompt generation as well as determining the maximum number of tokens allowed in the output of the LLM. By default, MAX_TOKENS is set to 2000, providing a limit to the length of the generated text to ensure efficient and manageable responses. This parameter is integral for controlling the verbosity and scope of the model's output, making it a key attribute for users to adjust according to their specific needs and constraints.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2,9243633f55cccd0885ba553e14fa5e3f</data>
    </node>
    <node id="CHUNK_SIZE">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">CHUNK_SIZE is a command-line option that determines the size in tokens to use for generating text units from input documents. The default is 200.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f</data>
    </node>
    <node id="NO_ENTITY_TYPES">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">NO_ENTITY_TYPES is a command-line option that enables untyped entity extraction generation. It is recommended for data covering a lot of topics or highly randomized data.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f</data>
    </node>
    <node id="OUTPUT">
      <data key="d0">COMMAND, OPTION</data>
      <data key="d1">OUTPUT is a command-line option and an optional property used in the configuration of the graphrag.prompt_tune command. It specifies the folder where the generated prompts will be saved, with the default location being "prompts". This feature enables users to customize the storage location for output files, enhancing flexibility and organization within their projects.</data>
      <data key="d2">9243633f55cccd0885ba553e14fa5e3f,ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="MAX-TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">max-tokens is an optional property that specifies the maximum token count for prompt generation. The default value is 2000. It is used in the configuration of the graphrag.prompt_tune command to control the size of the generated prompts.</data>
      <data key="d2">ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="CHUNK-SIZE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">chunk-size is an optional property that defines the size in tokens to use for generating text units from input documents. The default value is 200. It is used in the configuration of the graphrag.prompt_tune command to determine how input documents are segmented for processing.</data>
      <data key="d2">ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="NO-ENTITY-TYPES">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">no-entity-types is an optional property that, when used, enables untyped entity extraction generation. It is recommended for data that covers a wide range of topics or is highly randomized. It is used in the configuration of the graphrag.prompt_tune command to control the type of entity extraction.</data>
      <data key="d2">ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </node>
    <node id="CHUNK SIZE PARAMETER">
      <data key="d0">PROPERTY, PARAMETER</data>
      <data key="d1">The Chunk Size Parameter determines the size of text units into which the input data is divided for processing in template generation. It is a crucial setting for managing the granularity of text analysis.</data>
      <data key="d2">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </node>
    <node id="RANDOM SELECTION METHOD">
      <data key="d0">PROPERTY, STRATEGY</data>
      <data key="d1">The Random Selection Method is a strategy used in template generation to select text units randomly. It is the default and recommended option for most use cases, ensuring a diverse and unbiased sample of text units.</data>
      <data key="d2">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </node>
    <node id="TOP SELECTION METHOD">
      <data key="d0">PROPERTY, STRATEGY</data>
      <data key="d1">The Top Selection Method is a strategy used in template generation to select the head n text units. This method is useful for focusing on the most significant or relevant parts of the input data.</data>
      <data key="d2">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </node>
    <node id="ALL SELECTION METHOD">
      <data key="d0">PROPERTY, STRATEGY</data>
      <data key="d1">The All Selection Method is a strategy used in template generation to use all text units for the generation. It is recommended for small datasets where comprehensive analysis is desired, but not usually recommended for larger datasets due to computational constraints.</data>
      <data key="d2">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </node>
    <node id="GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE serves as both a configuration setting and an environment variable, playing a crucial role in the entity extraction process. This entity is responsible for specifying the file used for entity extraction prompts during the index run. When set to None, it indicates the absence of a designated file for this purpose. However, GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE defaults to the path "prompts/entity_extraction.txt" when a specific file is not defined, ensuring that the entity extraction process has a fallback resource to rely on.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73,4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </node>
    <node id="GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE is a configuration property, also serving as an environment variable, that designates the file to be utilized for generating community report prompts. This dual-purpose entity is pivotal for the index run, as it specifies the path to the prompt file. Currently, it is set to None, but its default path is "prompts/community_report.txt" when not explicitly defined. This file plays a crucial role in the Motor Control and Drive Systems domain by facilitating the creation of detailed reports that help in understanding the structure and dynamics of specialized professional networks, identifying key influencers, and mapping complex relationships within the community.</data>
      <data key="d2">4f37c0e9c3c9bac4e5c1c6821eea442e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE is a configuration setting, represented as an environment variable, that designates the file to be utilized for summarizing descriptions during the index run. Currently, it is set to None, which signifies the absence of a specified file for this purpose. The default path for this file, when set, is "prompts/summarize_descriptions.txt". This configuration property plays a crucial role in the process of consolidating and streamlining descriptions within the system.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73,4f37c0e9c3c9bac4e5c1c6821eea442e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="CUSTOM PROMPT FILE">
      <data key="d0">CONCEPT, DOCUMENT</data>
      <data key="d1">A Custom Prompt File is a plaintext document that can be created to override the default prompts used by the GraphRAG indexer. It enables users to specify their own prompts, using token-replacements, to better align with their specific use cases in knowledge discovery.</data>
      <data key="d2">6a7157695d90d434b2625c3f05420916</data>
    </node>
    <node id="TOKEN-REPLACEMENTS">
      <data key="d0">CONCEPT, TECHNIQUE</data>
      <data key="d1">Token-Replacements is a technique used in customizing prompts for the GraphRAG indexer. It involves replacing placeholders in the form of {token_name} with actual values provided by the extractor, such as input text, entity types, and delimiters, to tailor the prompts to specific needs.</data>
      <data key="d2">6a7157695d90d434b2625c3f05420916</data>
    </node>
    <node id="SUMMARIZE ENTITY/RELATIONSHIP DESCRIPTIONS">
      <data key="d0">CONCEPT, PROCEDURE</data>
      <data key="d1">Summarize Entity/Relationship Descriptions is a procedure that involves summarizing the descriptions of entities or relationships. It uses prompts and token-replacements to process a list of descriptions for an entity or relationship.</data>
      <data key="d2">6a7157695d90d434b2625c3f05420916</data>
    </node>
    <node id="RECORD_DELIMITER">
      <data key="d0">PROPERTY, DELIMITER</data>
      <data key="d1">The record_delimiter is a special character or string used to separate different tuple instances in a text. It helps in identifying the boundaries of individual records or entities within a larger text or data stream.</data>
      <data key="d2">853bfe9a74a916130a20f81506bcaf09</data>
    </node>
    <node id="COMPLETION_DELIMITER">
      <data key="d0">PROPERTY, DELIMITER</data>
      <data key="d1">The completion_delimiter is a marker or indicator used to signal the end of a series of generated data or text. It is particularly useful in scenarios where data is generated in a stream or sequence, and there is a need to determine when the generation process has been completed.</data>
      <data key="d2">853bfe9a74a916130a20f81506bcaf09</data>
    </node>
    <node id="ENTITY_NAME">
      <data key="d0">PROPERTY, IDENTIFIER</data>
      <data key="d1">The entity_name is a unique identifier or label given to an entity or object in a data set or text. It is used to distinguish one entity from another and is crucial for referencing and linking entities in the context of relationships and descriptions.</data>
      <data key="d2">853bfe9a74a916130a20f81506bcaf09</data>
    </node>
    <node id="DESCRIPTION_LIST">
      <data key="d0">PROPERTY, LIST</data>
      <data key="d1">The description_list is a collection of descriptions or attributes associated with an entity or relationship. It provides detailed information about the entity's characteristics, functions, or the nature of the relationship between entities.</data>
      <data key="d2">853bfe9a74a916130a20f81506bcaf09</data>
    </node>
    <node id="INPUT_TEXT">
      <data key="d0">PROPERTY, TEXT</data>
      <data key="d1">The input_text is the text or data provided as input for processing. It can contain information about entities, relationships, or other data points that need to be analyzed, summarized, or extracted.</data>
      <data key="d2">853bfe9a74a916130a20f81506bcaf09</data>
    </node>
    <node id="TUPLE_DELIMITER">
      <data key="d0">PROPERTY, DELIMITER</data>
      <data key="d1">The tuple_delimiter is a character or string used to separate values within a single tuple. A tuple is a collection of related values that represent an individual entity or a relationship between entities.</data>
      <data key="d2">853bfe9a74a916130a20f81506bcaf09</data>
    </node>
    <node id="CONFIGURATION DOCUMENTATION">
      <data key="d0">DOCUMENT, INSTRUCTION</data>
      <data key="d1">The Configuration Documentation is a comprehensive guide designed to assist users in understanding and modifying the settings of GraphRAG, a tool for information discovery and processing. It offers detailed instructions on various configuration aspects, including the use of the init command, .env files, and config files, as well as other configuration-related commands. This documentation serves as a valuable resource for users looking to optimize their information extraction and processing capabilities by customizing GraphRAG's settings.</data>
      <data key="d2">12294feb07a1d202b27241eaaf64718b,21cdf11c58927ae505d3d375d1b75c82,32e96c66a531ecd0a8edc7414aec0803,d0f7c236538005bc3056b7daed2401d8</data>
    </node>
    <node id="PROMPT SOURCE">
      <data key="d0">SOURCE, DATA</data>
      <data key="d1">Prompt Source refers to the origin or provider of the input text used to generate the report. It could be a database, a file, or any other data source that contains the information needed for the report generation.&gt;</data>
      <data key="d2">21cdf11c58927ae505d3d375d1b75c82</data>
    </node>
    <node id="TOKENS">
      <data key="d0">PROPERTY, DATA</data>
      <data key="d1">Tokens represent values provided by the extractor, such as the input text. They are key components in the process of generating reports, as they contain the actual data that will be analyzed and presented in the report.&gt;</data>
      <data key="d2">21cdf11c58927ae505d3d375d1b75c82</data>
    </node>
    <node id="GRAPHRAG SYSTEM">
      <data key="d0">SOFTWARE, SYSTEM</data>
      <data key="d1">The GraphRAG system is a software solution designed for indexing and querying text data. It offers various ways to get started, including using the GraphRAG Accelerator solution, installing from PyPI, or using it from source. The system is compatible with Python versions 3.10 to 3.12.</data>
      <data key="d2">84d24b5db902baca7217b5e3bb6ec462</data>
    </node>
    <node id="GRAPHRAG ACCELERATOR SOLUTION">
      <data key="d0">SOFTWARE, SOLUTION</data>
      <data key="d1">The GraphRAG Accelerator solution is a user-friendly package that provides an end-to-end experience with Azure resources for getting started with the GraphRAG system. It is recommended for a quick and easy setup.</data>
      <data key="d2">84d24b5db902baca7217b5e3bb6ec462</data>
    </node>
    <node id="INDEXING PIPELINE OVERVIEW">
      <data key="d0">PROCESS, SYSTEM COMPONENT</data>
      <data key="d1">The Indexing Pipeline Overview is a component of the GraphRAG system that describes the process of indexing text data. It involves setting up a data project, initial configuration, and using the system to index text for later querying.</data>
      <data key="d2">84d24b5db902baca7217b5e3bb6ec462</data>
    </node>
    <node id="QUERY ENGINE OVERVIEW">
      <data key="d0">PROCESS, SYSTEM COMPONENT</data>
      <data key="d1">The Query Engine Overview is a component of the GraphRAG system that describes the process of querying indexed data. It allows users to ask questions about the documents that have been indexed using the system.</data>
      <data key="d2">84d24b5db902baca7217b5e3bb6ec462</data>
    </node>
    <node id="ENVIRONMENT VARIABLES">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Environment variables are variables that are set in the operating system's environment and can be accessed by any program running on the system. They are used to configure the behavior of software applications and systems. In the context of the GraphRAG system, environment variables are necessary for setting up the workspace and specifying the API key for the OpenAI API or Azure OpenAI endpoint.&gt;</data>
      <data key="d2">9f2cd3d789fd49f220d4cda6b9e8048c</data>
    </node>
    <node id="SAMPLE DATASET">
      <data key="d0">DATA, FILE</data>
      <data key="d1">A sample dataset is a collection of data used for testing and demonstration purposes. In this case, the sample dataset is a copy of "A Christmas Carol" by Charles Dickens, obtained from a trusted source and saved as a text file named book.txt in the ./ragtest/input directory.&gt;</data>
      <data key="d2">9f2cd3d789fd49f220d4cda6b9e8048c</data>
    </node>
    <node id="A CHRISTMAS CAROL">
      <data key="d0">LITERATURE, BOOK</data>
      <data key="d1">"A Christmas Carol" is a novella by Charles Dickens, first published in 1843. It is a classic Christmas story that tells the tale of Ebenezer Scrooge, an elderly miser who is visited by the ghost of his former business partner Jacob Marley and the Ghosts of Christmas Past, Present, and Yet to Come. The story has been adapted into numerous films, plays, and other media.&gt;</data>
      <data key="d2">9f2cd3d789fd49f220d4cda6b9e8048c</data>
    </node>
    <node id="GRAPHRAG PIPELINE">
      <data key="d0">SOFTWARE, PROCESS</data>
      <data key="d1">The GraphRAG Pipeline is a sophisticated software tool specifically engineered for the efficient indexing and processing of graph data, as well as large text datasets. This dual-purpose capability makes it a versatile solution for a wide range of data management needs. The pipeline supports seamless integration with OpenAI and Azure OpenAI APIs, enhancing its functionality by enabling advanced features and services. To ensure secure and authorized access, the GraphRAG Pipeline necessitates an API key for authentication. Configuration of the pipeline is facilitated through the settings.yaml file, where users can customize various parameters to suit their specific requirements.

The pipeline's workflow includes setting up a workspace, configuring essential environment variables, and executing the indexing command. Once the indexing process is complete, the system generates a comprehensive index of the text dataset. This index serves as a powerful resource for answering complex questions and retrieving pertinent information, making the GraphRAG Pipeline an indispensable tool for data analysis and retrieval tasks.</data>
      <data key="d2">5aaa26fbe97dc7573cd1a56d6fb11213,9f2cd3d789fd49f220d4cda6b9e8048c</data>
    </node>
    <node id="OPENAI API">
      <data key="d0">API, SERVICE</data>
      <data key="d1">The OpenAI API, a service offered by OpenAI, provides developers with access to a range of AI capabilities, including language models, embeddings, text generation, translation, and summarization. To utilize these features, an API key is required for authentication. The OpenAI API can be seamlessly integrated with the GraphRAG pipeline, enhancing its functionality. Developers can activate OpenAI mode by updating the value of the GRAPHRAG_API_KEY environment variable, enabling them to leverage the full potential of the OpenAI models through the API.</data>
      <data key="d2">5aaa26fbe97dc7573cd1a56d6fb11213,9f2cd3d789fd49f220d4cda6b9e8048c</data>
    </node>
    <node id="AZURE OPENAI">
      <data key="d0">API, SERVICE</data>
      <data key="d1">Azure OpenAI, a service offered by Microsoft Azure, is a platform designed for enterprise and business use cases, providing access to OpenAI's models and services. It enables users to customize and deploy AI models in a secure and scalable environment, supporting various API versions and requiring an API key for authentication. Azure OpenAI can be integrated with the GraphRAG pipeline by configuring specific settings in the settings.yaml file, including the API base URL, API version, and deployment name. As an alternative to the OpenAI API, Azure OpenAI can be utilized by updating the value of the GRAPHRAG_API_KEY environment variable, offering similar AI capabilities.</data>
      <data key="d2">5aaa26fbe97dc7573cd1a56d6fb11213,7b45dafa74553d3899e2291a3c9fb86e,9f2cd3d789fd49f220d4cda6b9e8048c</data>
    </node>
    <node id="SETTINGS.YAML">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">The settings.yaml file is a pivotal configuration settings file for the GraphRAG system, created by the init command. This file is central to the operation and customization of the GraphRAG pipeline, as it contains essential configuration settings. It references environment variables defined in the .env file, enabling the setup of the GraphRAG system. The settings.yaml file includes adjustable parameters that can be modified to tailor the pipeline's behavior according to specific needs. Notably, it features settings for Azure OpenAI users, such as the API key, facilitating integration and additional functionalities for those utilizing Azure's services.</data>
      <data key="d2">12294feb07a1d202b27241eaaf64718b,32e96c66a531ecd0a8edc7414aec0803,5aaa26fbe97dc7573cd1a56d6fb11213</data>
    </node>
    <node id="API_BASE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The API_BASE is a crucial configuration property utilized in the context of accessing Azure OpenAI API services. It serves as a string attribute that designates the base URL for the API, specifically for the Language Model (LLM) service. This property is mandatory for initiating API calls to the designated Azure instance. To ensure proper functionality, the placeholder URL within API_BASE should be substituted with the accurate instance name of the Azure service. This configuration enables seamless interaction with the Azure OpenAI API, facilitating the use of advanced language model capabilities.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2,7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </node>
    <node id="API_VERSION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The API_VERSION is a crucial configuration property utilized in the Azure OpenAI API, serving as a string attribute that specifies the particular version of the API to be employed by the LLM (Language Model) service. This feature enables users to customize their applications for different API versions, facilitating compatibility and optimization. An example of a version that might be used is "2024-02-15-preview", highlighting the ability to work with preview versions for early access to new features and improvements.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2,7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </node>
    <node id="DEPLOYMENT_NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The deployment_name is a crucial string attribute within the configuration properties of Azure's cognitive services, specifically designed to identify and access the desired model deployment. This parameter is essential for utilizing Azure-based Language Model (LLM) services, as it specifies the exact name of the deployment to be used, enabling seamless interaction with the particular model instance hosted on Azure.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2,7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </node>
    <node id="GRAPHRAG CONFIGURATION DOCUMENTATION">
      <data key="d0">DOCUMENTATION</data>
      <data key="d1">The GraphRAG configuration documentation provides detailed information about how to configure GraphRAG, including settings for the API base, version, and deployment name.&gt;</data>
      <data key="d2">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </node>
    <node id="INITIALIZATION DOCUMENTATION">
      <data key="d0">DOCUMENTATION</data>
      <data key="d1">The Initialization documentation offers guidance on how to initialize GraphRAG, which is necessary for setting up the environment and preparing for data indexing and querying.&gt;</data>
      <data key="d2">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </node>
    <node id="CLI DOCUMENTATION">
      <data key="d0">DOCUMENTATION</data>
      <data key="d1">The CLI documentation explains how to use the command-line interface (CLI) for GraphRAG, including commands for indexing data and running the query engine.&gt;</data>
      <data key="d2">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </node>
    <node id="QUERY ENGINE DOCS">
      <data key="d0">DOCUMENTATION, REFERENCE MATERIAL</data>
      <data key="d1">Query Engine Docs are reference materials that provide detailed information on how to use the Local and Global search mechanisms effectively. They offer guidance on leveraging these search methods to extract meaningful insights from data after the Indexer has completed its processing.</data>
      <data key="d2">ae6e91a8cc5773dbd4789773c9ef5a30</data>
    </node>
    <node id="SCROOGE">
      <data key="d0">CHARACTER, PERSON</data>
      <data key="d1">Scrooge is a character within the dataset. Specific questions can be asked about Scrooge to understand his main relationships and characteristics within the context of the story or data being analyzed.</data>
      <data key="d2">ae6e91a8cc5773dbd4789773c9ef5a30</data>
    </node>
    <node id="DEFAULT CONFIGURATION MODE">
      <data key="d0">CONFIGURATION MODE</data>
      <data key="d1">Default Configuration Mode is the simplest way to start using the GraphRAG system. It is designed to work with minimal configuration and is suitable for most users. The mode supports configuration through the init command, environment variables, and JSON or YAML files for more control. It creates a .env and settings.yaml files with necessary configuration settings when initialized.</data>
      <data key="d2">ccd2de9e2219521fbca779843c65af58</data>
    </node>
    <node id="CUSTOM CONFIGURATION MODE">
      <data key="d0">CONFIGURATION MODE</data>
      <data key="d1">Custom Configuration Mode is an advanced feature designed for experienced users of the GraphRAG system, enabling them to exert deeper control over the system's configuration. This mode is particularly useful for those who need to fine-tune the system for specific needs, as it allows users to define their own configuration for the Indexing Engine pipelines, rather than relying on the default settings. Custom Configuration Mode supports configuration through JSON or YAML files, providing flexibility in how users can input their custom settings. Detailed guidance on how to utilize this mode can be found in the Custom Configuration Mode documentation, making it accessible to those with the requisite expertise. It is important to note that this mode is not recommended for most users due to its complexity and the level of technical knowledge required to effectively utilize it.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc,ccd2de9e2219521fbca779843c65af58</data>
    </node>
    <node id="INIT COMMAND">
      <data key="d0">COMMAND</data>
      <data key="d1">The INIT COMMAND is a pivotal tool designed to streamline the setup process for the GraphRAG system. It facilitates the initialization of the system in the Default Configuration Mode, making it the go-to option for the majority of users looking to quickly and efficiently get started with GraphRAG. Upon execution, the INIT COMMAND adeptly generates the essential configuration files, namely .env and settings.yaml, within the specified directory. Additionally, it outputs the default LLM (Language Model) prompts that are integral to the operation of GraphRAG, ensuring that users have all the necessary components to begin utilizing the system with ease.</data>
      <data key="d2">ccd2de9e2219521fbca779843c65af58,d0f7c236538005bc3056b7daed2401d8</data>
    </node>
    <node id=".ENV">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">The .env file, a crucial component in the configuration process of GraphRAG, serves as the environment variables file. It is automatically generated by the init command of GraphRAG and contains settings that are referenced in the settings.yaml file. These variables play a significant role in the configuration document, as they can be used for token replacements using the ${ENV_VAR} syntax. This enables dynamic and flexible configuration of GraphRAG, allowing for the customization and adaptation of settings based on the specific environment in which it is being used.</data>
      <data key="d2">12294feb07a1d202b27241eaaf64718b,32e96c66a531ecd0a8edc7414aec0803</data>
    </node>
    <node id="PROMPTS/">
      <data key="d0">DIRECTORY, CONFIGURATION</data>
      <data key="d1">The "prompts/" directory is a crucial component within the GraphRAG system, serving as the repository for default Language Model (LLM) prompts. These prompts are instrumental in facilitating the system's functionality. Users have the flexibility to customize these prompts to better suit their specific data requirements. Additionally, the Auto Prompt Tuning command offers an advanced feature, enabling the generation of new prompts that are finely tuned to the nuances of the user's data, ensuring optimal performance and adaptability within the Motor Control and Drive Systems domain or any other specialized professional networks being analyzed.</data>
      <data key="d2">12294feb07a1d202b27241eaaf64718b,32e96c66a531ecd0a8edc7414aec0803</data>
    </node>
    <node id="PYTHON -M GRAPHRAG.INDEX">
      <data key="d0">COMMAND, INITIALIZATION</data>
      <data key="d1">The python -m graphrag.index command is used to initialize GraphRAG in a specified directory. It can create necessary configuration files and output default prompts used by GraphRAG.</data>
      <data key="d2">12294feb07a1d202b27241eaaf64718b</data>
    </node>
    <node id="PROMPT TUNING COMMAND">
      <data key="d0">COMMAND, ACTIVITY</data>
      <data key="d1">The Prompt Tuning command is a valuable feature designed to enable users to customize and optimize the default prompts according to their specific data requirements. This command plays a crucial role in enhancing the performance of GraphRAG by fine-tuning the language model to better suit the user's unique data set and use case, thereby improving the accuracy and relevance of the generated insights.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,d0f7c236538005bc3056b7daed2401d8</data>
    </node>
    <node id=".ENV FILE">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">The .env file contains environment variables referenced in the settings.yaml file, providing configuration settings for GraphRAG.</data>
      <data key="d2">d0f7c236538005bc3056b7daed2401d8</data>
    </node>
    <node id="SETTINGS.YAML FILE">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">The settings.yaml file contains configuration settings for GraphRAG, including parameters for indexing and other system operations.</data>
      <data key="d2">d0f7c236538005bc3056b7daed2401d8</data>
    </node>
    <node id="PROMPTS FOLDER">
      <data key="d0">DIRECTORY, FILE</data>
      <data key="d1">The prompts folder contains the default prompts used by GraphRAG, which can be modified or adapted through the Auto Prompt Tuning command.</data>
      <data key="d2">d0f7c236538005bc3056b7daed2401d8</data>
    </node>
    <node id="CONFIG.JSON">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">Config.json is a versatile JSON configuration file that serves dual purposes within the technological framework. Primarily, it functions as a settings file for the LLM (Language Model) service, detailing operational parameters such as input type, file encoding, and API key within its structured JSON object. Additionally, Config.json is utilized for configuring GraphRAG, setting up the default configuration mode and facilitating token replacements through environment variables when used in conjunction with .env files. This dual role underscores the file's adaptability and importance in managing settings for distinct yet interconnected services.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,f135654a3c057c66b9e5f97a960d302f</data>
    </node>
    <node id="CONFIG.YML">
      <data key="d0">FILE, CONFIGURATION</data>
      <data key="d1">config.yml is a YAML configuration file used to set up the default configuration mode for GraphRAG. It can be used alongside .env for environment variable token replacements.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803</data>
    </node>
    <node id="API_KEY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The entity known as API_KEY is an environment variable that holds a string attribute, specifically the OpenAI API key. This key is essential for authentication purposes when utilizing services provided by OpenAI. The API_KEY is defined in the .env file and is referenced in the configuration files, such as config.json or config.yml, to ensure secure and proper access to the Language Model (LLM) configuration.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="INPUT">
      <data key="d0">SECTION, CONFIGURATION</data>
      <data key="d1">The "input" entity refers to the data or file that is being read and processed within the workflow2 context. It is configured as a 'file' type with a specific 'file_type' of 'csv'. The base directory for these CSV files is set at '../data/csv'. A sophisticated file pattern, utilizing a regex, is employed to match the CSV files, extracting critical metadata such as the source, year, month, day, and author from the file names. This input section in the configuration file not only specifies the type and file type but also includes details on encoding and the file pattern, ensuring a seamless data input process for GraphRAG. The input property comprehensively describes the configuration for reading data, integrating all necessary parameters for efficient data processing.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,3900b87693f02c43b4294e38647eb7cd,6839baed839d7a5e837af1da93e462e5,76d9dcb9a27c2caea1f46bb5050851c6</data>
    </node>
    <node id="TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The "type" entity serves multiple roles within various contexts. Primarily, it is a string attribute that designates the type of Language Model (LLM) to be utilized, with options such as openai_chat, azure_openai_chat, openai_embedding, and azure_openai_embedding. Additionally, "type" is a configuration property that determines the storage or reporting mechanism to be employed, with choices including file, memory, or blob storage. In the context of data input, "type" is a property that defines the input method for loading data, with file being the default option. Furthermore, "type" acts as a setting that specifies the cache or storage type to be used, which influences how data is stored and accessed, again with options such as file, memory, or blob.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,647be47c939b4d72f1c0b29a2e0d2cb2,abac77a5673e907cf8d65161c2612784,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="FILE_TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The file_type is a property situated under the input section, primarily serving to designate the kind of input data to be loaded. By default, the file_type is set to "text," however, it can be explicitly defined as "csv" when dealing with comma-separated values files. This attribute is crucial for ensuring that the system correctly interprets and processes the incoming data format.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,3900b87693f02c43b4294e38647eb7cd</data>
    </node>
    <node id="FILE_ENCODING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">file_encoding is a property under the input section that sets the encoding of the input file. The default is utf-8.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803</data>
    </node>
    <node id="FILE_PATTERN">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The "file_pattern" is a crucial property situated under the input section, designed to contain a regex pattern. This pattern is specifically utilized to match CSV files within the system. The detailed pattern is expressed as '.*[\\/](?P&lt;source&gt;[^\\/]+)[\\/](?P&lt;year&gt;\\d{4})-(?P&lt;month&gt;\\d{2})-(?P&lt;day&gt;\\d{2})_(?P&lt;author&gt;[^_]+)_\\d+\\.csv$'. This intricate regex formulation allows for the identification and matching of files based on their source, date (year, month, day), author, and a numerical identifier, all encapsulated within the filename. Although a default pattern is not explicitly mentioned, the provided pattern serves as a comprehensive guide for matching input files in the system.</data>
      <data key="d2">32e96c66a531ecd0a8edc7414aec0803,3900b87693f02c43b4294e38647eb7cd</data>
    </node>
    <node id="API KEY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The API Key is a configuration property used to authenticate and access the LLM (Language Model) service, specifically the OpenAI API. It is stored in the config.json file under the llm section. The value of the API Key is represented by the placeholder ${API_KEY}.</data>
      <data key="d2">f135654a3c057c66b9e5f97a960d302f</data>
    </node>
    <node id="INPUT CONFIGURATION">
      <data key="d0">SECTION, CONFIGURATION</data>
      <data key="d1">The Input Configuration is a section within the config.json file that specifies how input data should be handled. It includes fields for input type, file type, file encoding, file pattern, and additional parameters specific to CSV mode, such as source column, timestamp column, timestamp format, text column, title column, and document attribute columns. It also includes settings for file input from Azure Storage, such as connection string, container name, base directory, and storage account blob URL.</data>
      <data key="d2">f135654a3c057c66b9e5f97a960d302f</data>
    </node>
    <node id="LLM CONFIGURATION">
      <data key="d0">SECTION, CONFIGURATION</data>
      <data key="d1">The LLM Configuration is a section within the config.json file that specifies settings for the LLM (Language Model) service. It includes the API Key field, which is used to authenticate and access the service. Other steps in the system may override this base configuration with their own LLM configuration.</data>
      <data key="d2">f135654a3c057c66b9e5f97a960d302f</data>
    </node>
    <node id="CONNECTION_STRING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The CONNECTION_STRING is a crucial string attribute designed for Azure Blob storage, enabling access to the storage service by supplying essential connection details. This connection_string setting is pivotal for utilizing blob storage for various purposes, including caching and storage, as it furnishes the Azure Storage connection string required for these functionalities.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="CONTAINER_NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The CONTAINER_NAME is a string attribute pivotal in the context of Azure Blob storage. It specifically denotes the name of the container within Azure Storage where data is housed. This setting is crucial for organizing and accessing data in blob storage, enabling efficient management and retrieval of information. The CONTAINER_NAME plays a significant role in structuring data within Azure's cloud storage environment, facilitating streamlined operations and enhanced data accessibility.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="BASE_DIR">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The base_dir, a string attribute in the configuration, plays a multifaceted role in managing the file structure for storage. It primarily specifies the base directory for accessing CSV files, with the default being "../data/csv", relative to the config file. Additionally, this setting is utilized to determine the base directory from which input data should be read, again relative to the root directory of the storage. Furthermore, base_dir is also employed to define the base directory for writing cache or reports to, facilitating efficient storage management by maintaining a clear and organized file structure. This setting is crucial for ensuring that data, cache, and reports are correctly located and accessed within the storage system.</data>
      <data key="d2">3900b87693f02c43b4294e38647eb7cd,647be47c939b4d72f1c0b29a2e0d2cb2,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="STORAGE_ACCOUNT_BLOB_URL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The STORAGE_ACCOUNT_BLOB_URL is a string attribute that plays a crucial role in accessing and managing data within Azure's blob storage service. This setting specifically provides the URL for the Azure storage account blob, enabling direct access to the storage service. It is essential for various operations involving blob storage, ensuring seamless interaction with the data stored.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="MODEL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The model is a string attribute that identifies the name of the model to be used for the LLM (Language Model) configuration.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="REQUEST_TIMEOUT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The request_timeout is a float attribute that defines the timeout duration for each request made to the LLM (Language Model) service.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="ORGANIZATION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The organization is a string attribute that identifies the client organization using the LLM (Language Model) service.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="PROXY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The proxy is a string attribute that specifies the URL of the proxy server to be used when making requests to the LLM (Language Model) service.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="COGNITIVE_SERVICES_ENDPOINT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The cognitive_services_endpoint is a string attribute that provides the URL endpoint for cognitive services, used for accessing additional AI services.</data>
      <data key="d2">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="MODEL_SUPPORTS_JSON">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The MODEL_SUPPORTS_JSON is a boolean attribute serving as a configuration parameter for the Language Model (LLM). This flag determines whether the model is capable of generating responses in JSON format. When enabled, it facilitates JSON-mode output, accommodating structured data requirements for various applications and integrations.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="TOKENS_PER_MINUTE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The "tokens_per_minute" is an integral configuration parameter that governs the rate at which tokens can be consumed by the LLM (Language Model) service. It acts as a leaky-bucket throttle, setting a limit on the number of tokens processed per minute, thereby controlling the pace of token usage in requests. This attribute is crucial for managing the service's token consumption rate, ensuring efficient and controlled processing.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="REQUESTS_PER_MINUTE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The requests_per_minute is an integer attribute that serves as a critical configuration parameter for managing the rate of incoming requests to the LLM (Language Model) service. It sets a throttle limit, implementing a leaky-bucket algorithm to control the number of requests processed per minute. This mechanism ensures that the service can handle requests at a sustainable rate, preventing overload and maintaining optimal performance.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="MAX_RETRIES">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The "MAX_RETRIES" is an integer attribute that plays a crucial role in the configuration of request handling in the LLM (Language Model) service. It specifies the maximum number of retries to be attempted when a request fails, ensuring that the system has multiple opportunities to successfully process the request before giving up. This parameter is essential for managing the reliability and resilience of the service, as it determines how persistent the system will be in the face of temporary failures or network issues. By setting the "MAX_RETRIES" value, users can balance between the need for robustness and the risk of wasting resources on repeatedly attempting doomed requests.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="MAX_RETRY_WAIT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The "MAX_RETRY_WAIT" is a float attribute that plays a crucial role in the configuration of retry mechanisms for failed requests to the LLM (Language Model) service. It specifies the maximum backoff time before a failed request is retried, effectively determining the upper limit for the wait time before initiating a retry. This parameter is essential for managing the retry strategy in the context of service requests, ensuring that there is an optimal balance between persistence and resource management.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="SLEEP_ON_RATE_LIMIT_RECOMMENDATION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The sleep_on_rate_limit_recommendation is a boolean configuration parameter that governs the system's behavior when encountering rate limit recommendations from the LLM (Language Model) service. When set to true, the system will pause and adhere to sleep recommendations upon reaching rate limits, ensuring compliance with service limitations and optimizing resource usage. This flag enables control over how the system responds to rate limit encounters, providing a mechanism to manage system performance and service adherence.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </node>
    <node id="URL ENDPOINT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The url endpoint is the specific address used to access cognitive services, such as Azure's deployment for machine learning models. It is a configuration parameter that determines where requests are sent.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="CONCURRENT_REQUESTS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The concurrent_requests is an integer that specifies the number of open requests to allow at once. It is a configuration parameter that manages the concurrency of requests.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="TEMPERATURE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The temperature is a float that specifies the temperature to use in generating completions. It is a configuration parameter that influences the randomness of model outputs.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="TOP_P">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The top_p is a float that specifies the top-p value to use in generating completions. It is a configuration parameter that influences the diversity of model outputs.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="N">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The n is an integer that specifies the number of completions to generate. It is a configuration parameter that determines the quantity of completions returned by the model.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="STAGGER">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The stagger is a float that specifies the threading stagger value for parallelization. It is a configuration parameter that influences the scheduling of threads.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="NUM_THREADS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The num_threads is an integer that specifies the maximum number of work threads for parallelization. It is a configuration parameter that determines the number of threads used for processing.&gt;</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa</data>
    </node>
    <node id="ASYNC_MODE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">ASYNC_MODE is a pivotal configuration property within the system, serving as a top-level setting that dictates the asynchronous processing mode. It is set to either asyncio or threaded, with asyncio indicating that the system is configured to use asyncio for asynchronous operations. This setting is found in the Async Mode top-level configuration and is referenced across various sections, including entity extraction, summarization, and claim extraction strategies. ASYNC_MODE is crucial for defining how tasks are handled concurrently, enabling the system to operate in asynchronous mode and facilitate concurrent processing of tasks. It is a configuration field within community_reports, highlighting its significance in specifying settings related to asynchronous processing in the system.</data>
      <data key="d2">1b24101de07b1c195448240237b84b37,3c66b7e86b3675fce14fe0047ae731aa,53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,9cbd4e21339eeed5e22a638e52a094cb,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="BATCH_SIZE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The entity "BATCH_SIZE" refers to an integer configuration parameter known as the batch size, which is crucial in determining the maximum number of items to be processed in a single batch for embeddings. This setting significantly influences the efficiency and resource usage of the system by specifying the size of batches for processing embeddings, ensuring optimal performance and resource management.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="BATCH_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The entity "BATCH_MAX_TOKENS" is an integer configuration parameter that plays a crucial role in managing computational resources and processing time within the context of embeddings. It specifies the maximum number of tokens that can be processed in a single batch, thereby controlling the batch size for embeddings. This setting ensures efficient use of resources and helps in optimizing the processing time by limiting the batch size to a manageable number of tokens.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="TARGET">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The TARGET is a pivotal configuration parameter within the text-embedding process. It functions as a setting that dictates which specific set of embeddings should be emitted, either limiting the output to those deemed required or encompassing all available embeddings. This versatile parameter plays a crucial role in shaping the output of the embeddings, thereby impacting the overall text-embedding procedure. By adjusting the TARGET, users can tailor the output to meet their specific needs, whether it's a focused set of embeddings or a comprehensive collection.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="SKIP">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The "skip" is a configuration parameter utilized in the text-embedding process. It manifests as a list of strings, each representing an embedding that should be excluded from processing. This feature enables customization and optimization of the output by allowing certain embeddings to be bypassed, catering to specific requirements or preferences in the data processing pipeline.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="STRATEGY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The entity "STRATEGY" is a versatile and critical configuration property utilized in various text processing tasks. It serves as a dictionary setting that enables the full override of default strategies in entity extraction, summarization, claim extraction, and text embedding. This setting is pivotal for customizing the algorithms and rules applied to these processes, offering flexibility in how text is processed and embedded. By fully overriding the entity extraction process and text-embedding strategy, the "STRATEGY" entity empowers users to tailor the text processing methods to their specific needs, enhancing the precision and effectiveness of the analysis.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,9cbd4e21339eeed5e22a638e52a094cb,abac77a5673e907cf8d65161c2612784,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="SIZE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The entity "SIZE" refers to an integer configuration parameter that plays a crucial role in the processing of data. Specifically, it defines the maximum chunk size in tokens, which is instrumental in managing the size of text segments for processing. This setting ensures that the data is divided into manageable chunks, optimizing the efficiency and effectiveness of the processing tasks.</data>
      <data key="d2">3c66b7e86b3675fce14fe0047ae731aa,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="PARALLELIZATION">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">Parallelization is a pivotal configuration property found in the top-level settings of various systems, particularly in entity extraction and community_reports. It serves as a critical parameter that dictates how tasks are processed in parallel, enabling the distribution of tasks across multiple processors or threads. This distribution is aimed at enhancing performance and efficiency by optimizing the concurrent processing of tasks. Parallelization is referenced in multiple sections of the configuration, underscoring its importance in managing and improving the system's processing capabilities.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,9cbd4e21339eeed5e22a638e52a094cb,abac77a5673e907cf8d65161c2612784,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="OVERLAP">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">overlap is an integer setting that specifies the chunk overlap in tokens, which helps in maintaining context across adjacent chunks during the chunking process.</data>
      <data key="d2">d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="GROUP_BY_COLUMNS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">group_by_columns is a list of strings that specifies fields to group documents by before chunking, which can help in organizing and processing related documents together.</data>
      <data key="d2">d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="CACHE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">"CACHE" is a pivotal configuration section designed to outline the caching strategy for a pipeline. It enables the specification of the cache type, a critical factor in performance optimization and resource management within the pipeline. The cache types that can be defined under "CACHE" include file, memory, and blob, each catering to different storage and access requirements. Additionally, "CACHE" encompasses further parameters for detailed cache configuration, such as connection strings for accessing external storage and directories for specifying cache locations. This comprehensive control over caching aspects facilitates efficient data handling and retrieval, enhancing the overall performance and reliability of the pipeline.</data>
      <data key="d2">d27237468a1b9e89110eeeca8080f63c,e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="STORAGE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">The STORAGE entity represents a critical configuration section within a system, specifically designed to manage and define the output strategy for data pipelines. This section encompasses various storage settings, enabling the selection of storage types, including file, memory, or blob. For blob storage, STORAGE facilitates the input of a connection string and container name, essential for establishing a connection to the blob storage service. When file storage is chosen, STORAGE allows for the specification of a base directory, relative to the root, for writing reports. This comprehensive configuration flexibility ensures that STORAGE can be tailored to meet the specific needs of different environments and use cases, optimizing data handling and output strategies.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784,d27237468a1b9e89110eeeca8080f63c,e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="REPORTING">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">Reporting is a critical configuration section that manages settings associated with the generation, storage, and access of reports within the Motor Control and Drive Systems domain. This section encompasses various reporting types, such as file, console, or blob, and provides detailed instructions for each. For instance, when the blob type is selected, the reporting settings include a connection string and container name for proper data management. Additionally, the base directory for writing reports is specified relative to the root, ensuring organized and accessible report storage. Through these comprehensive settings, the reporting section facilitates efficient and tailored reporting functionalities, enabling users to customize and manage reports according to their specific needs.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784,d27237468a1b9e89110eeeca8080f63c</data>
    </node>
    <node id="STORAGE ACCOUNT BLOB URL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The storage account blob URL is a configuration property used to specify the URL of the Azure Storage account blob. It is used in blob storage type configurations.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="FIELDS">
      <data key="d0">CONCEPT, CONFIGURATION</data>
      <data key="d1">Fields represent the specific configuration options or parameters within a given section, such as storage or reporting.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="CONNECTION STRING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The Connection String, a crucial configuration property specific to blob storage type, is employed to designate the Azure Storage connection string. This property facilitates the establishment of a link to blob storage, enabling seamless interaction and management of data within the Azure environment.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484,abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="BASE DIR">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Base Dir is a configuration property that specifies the base directory to write reports to, relative to the root.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="ASYNC MODE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Async Mode is a configuration property used in entity extraction, specifying whether to process text asynchronously.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="PROMPT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Prompt is a configuration property used in entity extraction, specifying the prompt file to use for guiding the text analysis.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="ENTITY TYPES">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Entity Types is a configuration property used in entity extraction, specifying the types of entities to identify.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="MAX GLEANINGS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Max Gleanings is a configuration property used in entity extraction, specifying the maximum number of gleaning cycles to use.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="SUMMARIZE DESCRIPTIONS">
      <data key="d0">CONCEPT, CONFIGURATION</data>
      <data key="d1">Summarize Descriptions is a configuration section that deals with settings for summarizing descriptions, including the LLM (Language Model), parallelization, async mode, and prompt.</data>
      <data key="d2">abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="CONTAINER NAME">
      <data key="d0" />
      <data key="d1">The CONTAINER NAME is a crucial configuration property, specifically designed for blob storage in the Motor Control and Drive Systems domain. This property, applicable exclusively to blob type storage, determines the container that will be utilized for storing blob data. By defining the CONTAINER NAME, users can effectively manage and access their blob storage resources, ensuring optimal data handling and retrieval within their systems.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484,abac77a5673e907cf8d65161c2612784</data>
    </node>
    <node id="PROMPT STR">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">Prompt STR is a pivotal configuration field within the community_reports framework, serving a dual role in specifying the prompt file for both report generation and a range of AI-driven processes. As a string value, it points to a file that is crucial for entity extraction, summarization, and claim extraction tasks. This setting is fundamental in defining the initial input or guidance for the AI or machine learning models engaged in these activities, ensuring that the models are accurately directed and that the reports generated are comprehensive and relevant. By configuring Prompt STR, users can tailor the input parameters to meet specific requirements, enhancing the precision and effectiveness of the AI models in processing and analyzing data.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="ENTITY_TYPES">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">entity_types is a list of strings that defines the types of entities to identify during the entity extraction process. This setting is crucial for specifying the categories of information to be recognized and extracted from the text.&gt;</data>
      <data key="d2">9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="MAX_GLEANINGS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">max_gleanings is an integer setting that determines the maximum number of gleaning cycles to use in the entity extraction process. This setting is important for controlling the depth and thoroughness of the information extraction from the text.&gt;</data>
      <data key="d2">9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="SUMMARIZE_DESCRIPTIONS">
      <data key="d0">PROCESS, FUNCTION</data>
      <data key="d1">summarize_descriptions is a process or function that involves summarizing the descriptions of entities. It utilizes the llm (language model), parallelization, and async_mode settings, and has its own strategy for summarization. This process is essential for generating concise and informative summaries of entity descriptions.&gt;</data>
      <data key="d2">9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="MAX_LENGTH">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">MAX_LENGTH is a crucial configuration field found within the community_reports settings. It serves as an integer-based limit, specifying the maximum number of output tokens per summarization or report. This setting is essential for controlling the length of the generated text, ensuring that the report output remains within acceptable and manageable limits. By defining MAX_LENGTH, users can effectively manage the size of their reports, making the information more digestible and easier to handle.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="CLAIM_EXTRACTION">
      <data key="d0">PROCESS, FUNCTION</data>
      <data key="d1">claim_extraction is a process or function that involves identifying and extracting claims from text. It has a boolean setting 'enabled' to toggle its functionality, and utilizes the llm, parallelization, and async_mode settings. This process is essential for identifying assertions or statements of fact within the text.&gt;</data>
      <data key="d2">9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="COMMUNITY_REPORTS">
      <data key="d0">PROCESS, FUNCTION</data>
      <data key="d1">The "community_reports" is a pivotal configuration section and process within the system, designed to manage and generate comprehensive reports based on data from various community sources. This dual-function entity includes several customizable fields for optimization, such as LLM (likely referring to Large Language Model), parallelization, async_mode, prompt, max_length, max_input_length, and strategy dict settings. These settings enable the process to operate efficiently and effectively, tailoring the report generation to specific needs. The "community_reports" process itself leverages the aforementioned settings, particularly LLM, parallelization, and async_mode, to compile and analyze data. Its unique strategy for report generation ensures that the output is insightful and valuable for understanding community dynamics and identifying potential collaboration opportunities or knowledge gaps.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,9cbd4e21339eeed5e22a638e52a094cb</data>
    </node>
    <node id="GLEANINGS INT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">gleanings int is a configuration parameter that specifies the maximum number of gleaning cycles to use in the system. It is an integer value.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="STRATEGY DICT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The "STRATEGY DICT" is a versatile configuration setting within the system, designed to provide comprehensive control over various strategies, including graph embedding and claim extraction. This dictionary enables users to fully customize and override the default strategies, accommodating specific requirements or preferences. By containing a range of settings and rules, the STRATEGY DICT empowers users to tailor the system's behavior to better suit their needs, enhancing flexibility and adaptability in different contexts.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="MAX_INPUT_LENGTH">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">max_input_length is a configuration field within community_reports that specifies the maximum number of input tokens to use when generating reports. It is an integer value that limits the size of the input for report generation.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="CLUSTER_GRAPH">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">cluster_graph is a configuration section that includes various fields for managing cluster graphs, such as max_cluster_size, strategy dict, and other settings related to graph clustering.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="MAX_CLUSTER_SIZE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">max_cluster_size is a configuration field within cluster_graph that specifies the maximum cluster size to emit. It is an integer value that limits the size of clusters in the graph.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="EMBED_GRAPH">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">embed_graph is a configuration section that includes various fields for managing graph embeddings, such as enabled, num_walks, walk_length, window_size, iterations, random_seed, and strategy dict settings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="ENABLED BOOL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The "ENABLED BOOL" is a critical configuration property within the system, specifically within the embed_graph feature. This boolean value acts as a toggle, determining the activation or deactivation of certain functionalities. When set to true, it enables features such as UMAP layouts and graph embeddings, facilitating the visualization and analysis of complex networks. Conversely, when set to false, these features are disabled, affecting the system's capability to perform detailed network analysis and visualization tasks. The "ENABLED BOOL" plays a pivotal role in managing the system's feature set, allowing users to customize their experience and optimize resource usage based on their specific needs.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="NUM_WALKS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">num_walks is a configuration field within embed_graph that specifies the node2vec number of walks. It is an integer value that determines the number of walks performed in the graph for node2vec embeddings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="WALK_LENGTH">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">walk_length is a configuration field within embed_graph that specifies the node2vec walk length. It is an integer value that determines the length of each walk in the graph for node2vec embeddings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="WINDOW_SIZE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">window_size is a configuration field within embed_graph that specifies the node2vec window size. It is an integer value that determines the size of the window for context in node2vec embeddings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="ITERATIONS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">iterations is a configuration field within embed_graph that specifies the node2vec number of iterations. It is an integer value that determines the number of iterations for node2vec embeddings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="RANDOM_SEED">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">random_seed is a configuration field within embed_graph that specifies the node2vec random seed. It is an integer value that determines the seed for the random number generator used in node2vec embeddings.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="UMAP">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">UMAP, or Uniform Manifold Approximation and Projection, is an advanced algorithm utilized for dimension reduction and data visualization. In the specific context provided, UMAP is referenced as a versatile configuration option within a software or system. This option allows users to enable or disable UMAP layouts for data visualization, offering greater control over how complex data sets are represented. The "umap" configuration section encompasses various fields, including an enabled boolean flag and additional settings, all of which are geared towards managing and customizing UMAP visualization to suit specific analytical needs. This feature is particularly valuable for professionals in fields such as Social Network Analysis and Motor Control and Drive Systems, where understanding the structure and dynamics of data is crucial for identifying key influencers, collaboration opportunities, and knowledge gaps.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="SNAPSHOTS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Snapshots is a configuration section within the Motor Control and Drive Systems domain that encompasses a variety of settings for managing and generating different types of snapshots. These snapshots include graphml, raw entity, and top-level-node snapshots, which serve as crucial tools for data preservation and analysis. The entity, SNAPSHOTS, is central to the management of these configuration options, enabling users to control the creation and maintenance of snapshots for various purposes within the specialized professional network. By leveraging the SNAPSHOTS configuration, users can identify collaboration opportunities, address knowledge gaps, and better understand the structure and dynamics of the Motor Control and Drive Systems domain.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="GRAPHML">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">graphml is a configuration field within snapshots that specifies whether to emit graphml snapshots. It is a boolean value that determines if graphml snapshots are generated.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="RAW_ENTITIES">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">raw_entities is a configuration field within snapshots that specifies whether to emit raw entity snapshots. It is a boolean value that determines if raw entity snapshots are generated.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="TOP_LEVEL_NODES">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">top_level_nodes is a configuration field within snapshots that specifies whether to emit top-level-node snapshots. It is a boolean value that determines if top-level-node snapshots are generated.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </node>
    <node id="ENCODING_MODEL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The encoding_model is a configuration setting within the Motor Control and Drive Systems domain, serving as a crucial parameter for text encoding processes. This entity is a string value that specifies the text encoding model to be utilized. By default, the encoding_model is set to cl100k_base; however, it can be adjusted to accommodate various models based on the specific requirements of the text encoding task. This flexibility allows for optimized encoding methods tailored to different applications within the field, enhancing the efficiency and accuracy of text processing tasks.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="SKIP_WORKFLOWS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The "skip_workflows" is a crucial configuration property utilized in the execution of pipelines within systems that manage workflows. This property enables the specification of a list of workflow names that are to be skipped during pipeline execution. It serves as a mechanism for enhancing efficiency by bypassing unnecessary or problematic workflows, thereby optimizing the overall process flow. The skip_workflows field is implemented as a list of strings, each string representing the name of a workflow that should be excluded from the system's operational sequence. This feature is particularly beneficial in scenarios where certain workflows are redundant or require troubleshooting, allowing for a streamlined and more effective pipeline execution.</data>
      <data key="d2">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="NODE2VEC RANDOM SEED">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The node2vec random seed is a configuration parameter that determines the initial conditions for the node2vec algorithm, which is used for generating embeddings in graph structures. It is crucial for reproducibility and can affect the resulting embeddings.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="GRAPHML BOOL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The graphml bool is a configuration property that controls whether graphml snapshots are emitted. Graphml is a file format for graphs and networks, used for data exchange and visualization.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="RAW_ENTITIES BOOL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The raw_entities bool is a configuration property that determines if raw entity snapshots are emitted. Raw entity snapshots capture the state of entities in their raw form for later analysis.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="TOP_LEVEL_NODES BOOL">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The top_level_nodes bool is a configuration property that specifies whether top-level-node snapshots are emitted. These snapshots capture the state of the top-level nodes in the system.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="INDEXING ENGINE EXAMPLES">
      <data key="d0">PROPERTY, EXAMPLES</data>
      <data key="d1">Indexing Engine Examples are a set of examples provided in the examples directory that demonstrate how to use the Indexing Engine with custom configuration. These examples are useful for learning and adapting the engine to various use-cases.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </node>
    <node id="EXTENDS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The "EXTENDS" property, also referred to as a configuration directive, is a versatile feature designed to enhance the modularity and flexibility of configuration management. It enables users to extend or override existing configurations, making it possible to include or modify settings from other sections or files. This directive is particularly useful in pipeline configuration files, where it facilitates the inheritance of settings from one or more base configurations, streamlining the process of managing complex configurations and ensuring consistency across different sections or files. By leveraging the "EXTENDS" directive, users can efficiently update and customize configurations while maintaining the integrity of the base settings.</data>
      <data key="d2">b70cb2eda62c6afad9e8d22daafe61cc,e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="RUN.PY">
      <data key="d0">FILE, CODE</data>
      <data key="d1">run.py is a Python script file that is used to run examples, typically found in the examples directory of a project. It can be executed using the Python interpreter with the PYTHONPATH environment variable set to the project's root directory.</data>
      <data key="d2">e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="POETRY SHELL">
      <data key="d0">COMMAND, ENVIRONMENT</data>
      <data key="d1">poetry shell is a command used to activate a virtual environment with the required dependencies for a project. It is often used in conjunction with the Python API and pipeline configuration files to ensure that the correct environment is set up for running examples.</data>
      <data key="d2">e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="PYTHONPATH">
      <data key="d0">ENVIRONMENT VARIABLE, PATH</data>
      <data key="d1">PYTHONPATH is an environment variable that specifies the directory where Python looks for modules and packages. Setting PYTHONPATH to the project's root directory ensures that the Python interpreter can find and import the necessary modules and scripts when running examples.</data>
      <data key="d2">e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="ROOT_DIR">
      <data key="d0">CONFIGURATION, PATH</data>
      <data key="d1">root_dir is a configuration directive that sets the root directory for the pipeline. All data inputs and outputs are assumed to be relative to this path, which is crucial for specifying the correct location of data files and output directories.</data>
      <data key="d2">e01c546120a27319dcbdf7a6b89bab26</data>
    </node>
    <node id="STORAGE TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The Storage Type is a configuration property that determines the type of storage to use for data. The options are file, memory, and blob. This property is essential for specifying how data will be stored and accessed within the system.&gt;("entity"</data>
      <data key="d2">763b51b68ecc9b69bc8014cf6f59fd33</data>
    </node>
    <node id="REPORTING TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The Reporting Type is a configuration property that specifies the type of reporting to use, with options including file, memory, and blob.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </node>
    <node id="BASE DIRECTORY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The Base Directory is a configuration property (type: file only) that defines the base directory to store the reports in, relative to the config root.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </node>
    <node id="WORKFLOW NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The Workflow Name is a property used to reference a specific workflow in other parts of the config, establishing dependencies between workflows.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </node>
    <node id="STEPS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">Steps are the DataShaper operations that a workflow comprises. Steps can define inputs in the form of workflow:&lt;workflow_name&gt; to establish dependencies on the output of other workflows.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </node>
    <node id="INPUT TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The Input Type is a configuration property that specifies the type of input to use, with options including file or blob.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </node>
    <node id="FILE TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The File Type is a field that discriminates between different input types, with options including csv and text.</data>
      <data key="d2">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </node>
    <node id="WORKFLOW2">
      <data key="d0">WORKFLOW, PROCESS</data>
      <data key="d1">workflow2 is a process that includes steps to derive data, with dependencies on other workflows, specifically workflow1. It operates on columns such as col1 and col2.</data>
      <data key="d2">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </node>
    <node id="WORKFLOW1">
      <data key="d0">WORKFLOW, PROCESS</data>
      <data key="d1">workflow1 is a process that serves as a source for workflow2, establishing a dependency through the derive step.</data>
      <data key="d2">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </node>
    <node id="DERIVE">
      <data key="d0">ACTION, PROCESS</data>
      <data key="d1">derive is an action or process step within workflow2 that uses col1 and col2 as inputs and has a dependency on workflow1.</data>
      <data key="d2">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </node>
    <node id="TIMESTAMP_COLUMN">
      <data key="d0">PROPERTY, DATA_COLUMN</data>
      <data key="d1">The timestamp_column is a property in the CSV data that contains the timestamp of the data entries. It is formatted according to the timestamp_format specified in the configuration. The format used is "%Y%m%d%H%M%S".</data>
      <data key="d2">3900b87693f02c43b4294e38647eb7cd</data>
    </node>
    <node id="TIMESTAMP_FORMAT">
      <data key="d0">PROPERTY, DATA_FORMAT</data>
      <data key="d1">The timestamp_format is a property that defines the format of the timestamps in the timestamp_column. It is specified as "%Y%m%d%H%M%S", which represents the year, month, day, hour, minute, and second in the timestamp.</data>
      <data key="d2">3900b87693f02c43b4294e38647eb7cd</data>
    </node>
    <node id="SOURCE_COLUMN">
      <data key="d0">PROPERTY, DATA_COLUMN</data>
      <data key="d1">The source_column is a property that indicates the column containing the source or author of the data. It is specified as "author".</data>
      <data key="d2">3900b87693f02c43b4294e38647eb7cd</data>
    </node>
    <node id="TEXT_COLUMN">
      <data key="d0">PROPERTY, DATA_COLUMN</data>
      <data key="d1">The text_column is a property that indicates the column containing the text of the data. It is specified as "message".</data>
      <data key="d2">3900b87693f02c43b4294e38647eb7cd</data>
    </node>
    <node id="AUTHOR">
      <data key="d0">PERSON, ROLE</data>
      <data key="d1">The Author is the person or entity responsible for creating or originating the data. The author's name is stored in the 'author' column of the CSV file. The author's role could be a writer, a researcher, or any other individual or organization contributing to the data. The specific author for this data is not directly provided in the text but is referenced through the 'author' column in the CSV file.</data>
      <data key="d2">6839baed839d7a5e837af1da93e462e5</data>
    </node>
    <node id="MESSAGE">
      <data key="d0">TEXT, COMMUNICATION</data>
      <data key="d1">The Message is the content or text of the data that is being communicated. It is stored in the 'message' column of the CSV file. The message could be a written document, a note, or any form of textual communication. The specific content of the message is not provided in the text but is referenced through the 'message' column in the CSV file.</data>
      <data key="d2">6839baed839d7a5e837af1da93e462e5</data>
    </node>
    <node id="DATE">
      <data key="d0">TIME, DATE</data>
      <data key="d1">The Date is the timestamp associated with the data, indicating when the data was created or recorded. It is stored in the 'date(yyyyMMddHHmmss)' column of the CSV file. The date format is specified as '%Y%m%d%H%M%S', which represents the year, month, day, hour, minute, and second. The specific date for this data is not directly provided in the text but is referenced through the 'date(yyyyMMddHHmmss)' column in the CSV file.</data>
      <data key="d2">6839baed839d7a5e837af1da93e462e5</data>
    </node>
    <node id="POST PROCESS">
      <data key="d0">PROCESS, DATA_MANIPULATION</data>
      <data key="d1">The entity "POST PROCESS" refers to a series of operations that are executed on data after it has been extracted from a CSV file and prior to its utilization in a workflow. These post-process steps are crucial for data preparation and can encompass various actions such as filtering, transformation, and other forms of data manipulation to ensure the data is suitable for the workflow requirements. Specifically, one of the post-process steps involves filtering the data based on the 'title' column, where only records with the value 'My document' are selected for further processing. This filter is implemented using a filter verb that precisely targets the 'title' column with the specified value, ensuring that only relevant data is processed in the subsequent workflow stages.</data>
      <data key="d2">6839baed839d7a5e837af1da93e462e5,765d8a78606fe81a03a0da4f7ff231fa</data>
    </node>
    <node id="CSV FILE PATTERN">
      <data key="d0">PROPERTY, REGEX</data>
      <data key="d1">The CSV File Pattern is a regular expression used to match CSV files, specifically looking for the format that includes day and author information. The pattern is: {2})-(?P&lt;day&gt;\d{2})_(?P&lt;author&gt;[^_]+)_\d+\.csv$.</data>
      <data key="d2">765d8a78606fe81a03a0da4f7ff231fa</data>
    </node>
    <node id="FILE FILTER">
      <data key="d0">PROPERTY, FILTER</data>
      <data key="d1">The File Filter is a set of criteria used to further filter files based on named groups from the CSV File Pattern. It includes filters for year, month, and potentially day. Currently, it filters files for the year 2023 and month 06.</data>
      <data key="d2">765d8a78606fe81a03a0da4f7ff231fa</data>
    </node>
    <node id="CONFIGURATION TEMPLATE">
      <data key="d0">PROPERTY, TEMPLATE</data>
      <data key="d1">The Configuration Template is a template for a .env file used in the Indexing Pipeline execution. It includes settings for LLM (Language Model), API key, API base, API version, and text generation settings. The LLM type is set to "azure_openai_chat".</data>
      <data key="d2">765d8a78606fe81a03a0da4f7ff231fa</data>
    </node>
    <node id="GRAPHRAG_API_BASE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_API_BASE is a critical configuration property that serves a dual purpose within the system. Primarily, it designates the base URL for the LLM (Language Model) service API, guiding the system to the endpoint where requests to access the LLM service are sent. The placeholder value "http://&lt;domain&gt;.openai.azure.com" is indicative of the actual domain that should be specified, making this property indispensable for directing the system to the correct LLM service endpoint. Additionally, GRAPHRAG_API_BASE is a configuration setting that specifically points to the base URL for the GraphRAG API, a detail of particular significance for Azure OpenAI users. It must be set to the domain of the Azure OpenAI service to ensure that all API requests are correctly directed to the intended endpoint. This setting is mandatory for Azure OpenAI users, as it facilitates the seamless integration and operation of the system with the Azure OpenAI service.</data>
      <data key="d2">3da10b454f926a257b9fdf5d2487c0a5,8ac79ce92be1254dfda9a10eb54ab703</data>
    </node>
    <node id="GRAPHRAG_API_VERSION">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_API_VERSION is a critical configuration property that specifies the API version to be used when interacting with the Azure OpenAI service, particularly when accessing the LLM (Language Model) service API. By default, it is set to "api_version," which serves as a placeholder for the actual API version. This setting is essential for Azure OpenAI users as it ensures compatibility and correct functionality when making API calls to the GraphRAG service. GRAPHRAG_API_VERSION determines which version of the API the system will use to access the LLM service, thereby playing a pivotal role in maintaining compatibility between the system and the LLM service.</data>
      <data key="d2">3da10b454f926a257b9fdf5d2487c0a5,7b45dafa74553d3899e2291a3c9fb86e,8ac79ce92be1254dfda9a10eb54ab703</data>
    </node>
    <node id="GRAPHRAG_LLM_DEPLOYMENT_NAME">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_LLM_DEPLOYMENT_NAME is a configuration setting that specifies the name of the deployment for the language model being used. It is set to 'gpt-4-turbo-preview' in the provided configuration. This setting is crucial for identifying the specific model deployment to be used for text generation.</data>
      <data key="d2">3da10b454f926a257b9fdf5d2487c0a5</data>
    </node>
    <node id="GRAPHRAG_LLM_MODEL_SUPPORTS_JSON">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_LLM_MODEL_SUPPORTS_JSON is a configuration property that plays a crucial role in determining the compatibility of the language model with JSON data. This setting, which is set to True by default in the provided configuration, signifies that the model is capable of processing and generating JSON input and output. The capability to handle JSON data is essential for defining the format of the input data that the model can effectively process, thereby enabling a wider range of applications and data interoperability.</data>
      <data key="d2">3da10b454f926a257b9fdf5d2487c0a5,7b45dafa74553d3899e2291a3c9fb86e</data>
    </node>
    <node id="GRAPHRAG_INPUT_TYPE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_INPUT_TYPE is a pivotal configuration property that dictates the nature of input data to be processed by the system. It can be configured to "file" for file-based input, indicating that the system expects data in a file format, or set to "text" for processing textual data. This setting is essential for defining the format of the input data, which is critical for text generation or embedding tasks. The GRAPHRAG_INPUT_TYPE property plays a significant role in configuring the input data source, ensuring that the system processes data in the correct format.</data>
      <data key="d2">3da10b454f926a257b9fdf5d2487c0a5,8ac79ce92be1254dfda9a10eb54ab703</data>
    </node>
    <node id="GRAPHRAG_INPUT_FILE_TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_INPUT_FILE_TYPE is a configuration property that specifies the type of files to be processed, set to "text" for text files. It determines the format of the input files that the system will handle. The value "text" indicates that the system is configured to process text files. This property is essential for specifying the file type that the system should expect and process.&gt;</data>
      <data key="d2">8ac79ce92be1254dfda9a10eb54ab703</data>
    </node>
    <node id="GRAPHRAG_INPUT_FILE_PATTERN">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_INPUT_FILE_PATTERN is a configuration property that specifies the pattern for matching input files, set to ".*\.txt$". It determines which files will be selected for processing based on their names. The value ".*\.txt$" indicates that the system is configured to process files with a .txt extension. This property is crucial for filtering the input files based on their names.&gt;</data>
      <data key="d2">8ac79ce92be1254dfda9a10eb54ab703</data>
    </node>
    <node id="GRAPHRAG_INPUT_SOURCE_COLUMN">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_INPUT_SOURCE_COLUMN is a configuration property that specifies the column name in the input data that contains the source information. It determines which column in the input data should be used for source information. The value "source" indicates that the system is configured to use the "source" column for source information. This property is essential for identifying the source of the input data.&gt;</data>
      <data key="d2">8ac79ce92be1254dfda9a10eb54ab703</data>
    </node>
    <node id="GRAPHRAG_LLM_API_KEY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_API_KEY is a configuration property that holds the API key required for authentication when using the Azure OpenAI service. It is set to "your_api_key" as a placeholder, indicating that a valid API key should be provided by the user.&gt;</data>
      <data key="d2">7b45dafa74553d3899e2291a3c9fb86e</data>
    </node>
    <node id="GRAPHRAG_LLM_API_VERSION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_API_VERSION is a configuration property that specifies the API version to be used when interacting with the Azure OpenAI service for language model requests. It is set to "api_version" by default and is crucial for compatibility and functionality.&gt;</data>
      <data key="d2">7b45dafa74553d3899e2291a3c9fb86e</data>
    </node>
    <node id="HRAG_LLM_DEPLOYMENT_NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">HRAG_LLM_DEPLOYMENT_NAME is a configuration property that specifies the name of the deployment for the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is currently set to None, indicating that no specific deployment name is provided.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_MAX_TOKENS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_MAX_TOKENS is a configuration property that specifies the maximum number of tokens allowed for a request to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 4000, indicating that the maximum number of tokens for a request is 4000.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_REQUEST_TIMEOUT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_REQUEST_TIMEOUT is a configuration property that specifies the timeout duration for a request to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 180, indicating that the request will time out after 180 seconds.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_THREAD_STAGGER">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_THREAD_STAGGER is a configuration property that specifies the stagger time between threads for processing requests to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 0.3, indicating that there will be a 0.3-second stagger between threads.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_CONCURRENT_REQUESTS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_CONCURRENT_REQUESTS is a configuration property that specifies the number of concurrent requests allowed to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 25, indicating that 25 concurrent requests are allowed.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_TPM">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_TPM is a configuration property that specifies the target tokens per minute for requests to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 0, indicating that no specific target tokens per minute is set.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_RPM">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_RPM is a configuration property that specifies the target requests per minute for requests to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 0, indicating that no specific target requests per minute is set.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_MAX_RETRY_WAIT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_MAX_RETRY_WAIT is a configuration property that specifies the maximum wait time between retries for a request to the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to 10, indicating that the maximum wait time between retries is 10 seconds.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION is a configuration property that specifies whether the system should sleep when it receives a rate limit recommendation from the Large Language Model (LLM) in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to True, indicating that the system will sleep when it receives a rate limit recommendation.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_API_KEY">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_EMBEDDING_API_KEY is a configuration property that specifies the API key for the embedding service in the Human-in-the-Loop Request Augmentation Gateway (HRAG). It is set to "your_api_key", indicating that a specific API key is provided for the embedding service.&gt;</data>
      <data key="d2">9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_API_VERSION">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_EMBEDDING_API_VERSION is a configuration property utilized in the Human-in-the-Loop Request Augmentation Gateway (HRAG), specifically designed for Azure OpenAI users. This variable is set to "api_version", which denotes the designated API version for the embedding service. It comes into play when GRAPHRAG_API_VERSION is not defined, ensuring that embedding requests are processed with a specified API version, thereby maintaining consistency and compatibility in the system.</data>
      <data key="d2">485c17007ccb3102887eaa47d6a6100f,9aff9243c57cabca574b35438bf31a50</data>
    </node>
    <node id="APHRAG_API_KEY">
      <data key="d0">VARIABLE, CONFIGURATION</data>
      <data key="d1">APHRAG_API_KEY is a configuration variable that is not set. It is likely used for authentication or access to certain services or APIs.&gt;</data>
      <data key="d2">485c17007ccb3102887eaa47d6a6100f</data>
    </node>
    <node id="HRAG_EMBEDDING_THREAD_COUNT">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">HRAG_EMBEDDING_THREAD_COUNT is a configuration setting that specifies the number of threads used for embedding operations. It is set to None, indicating that the default or system-determined number of threads will be used.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_THREAD_STAGGER">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_EMBEDDING_THREAD_STAGGER is a configuration setting that determines the stagger or delay between embedding threads. It is set to 50, indicating a 50 millisecond delay between starting threads.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS is a configuration setting that specifies the number of concurrent requests for embedding operations. It is set to 25, indicating that up to 25 requests can be processed simultaneously.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_TPM">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_EMBEDDING_TPM is a configuration setting that represents the transactions per minute for embedding operations. It is set to 0, indicating that there is no specific limit.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_RPM">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_EMBEDDING_RPM is a configuration setting that represents the requests per minute for embedding operations. It is set to 0, indicating that there is no specific limit.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT is a configuration setting that determines the maximum wait time between retries for embedding operations. It is set to 10, indicating a 10-second maximum wait.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION is a configuration setting that indicates whether the system should sleep or pause when rate limits are reached during embedding operations. It is set to True, indicating that the system will sleep.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_INPUT_ENCODING">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_INPUT_ENCODING is a configuration setting that specifies the encoding used for input data. It is set to utf-8, indicating that the system uses UTF-8 encoding for input data.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_CHUNK_SIZE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_CHUNK_SIZE is a configuration setting that determines the size of data chunks for processing. It is set to 1200, indicating that data is processed in chunks of 1200 units.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_CHUNK_OVERLAP">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_CHUNK_OVERLAP is a configuration setting that specifies the overlap between data chunks. It is set to 100, indicating that there is a 100-unit overlap between chunks.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_CHUNK_BY_COLUMNS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_CHUNK_BY_COLUMNS is a configuration setting that indicates the column(s) used for chunking data. It is set to id, indicating that data is chunked based on the 'id' column.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS is a configuration setting that determines the maximum number of gleanings or entities extracted. It is set to 1, indicating that only one entity will be extracted.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </node>
    <node id="GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES is a pivotal configuration property designed to delineate and specify the categories of entities to be extracted within a given context. This setting ensures the identification and extraction of entities falling under the categories of organization, person, event, and geo, thereby facilitating a comprehensive analysis and understanding of the relationships and dynamics among these entities in various domains and scenarios. By setting GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES to include organization, person, event, and geo, the system is optimized to recognize and categorize these specific types of entities, enhancing the precision and relevance of the extracted information.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH is a configuration property that determines the maximum length of summarized descriptions. Currently set to 500, this setting ensures that all descriptions are summarized to a concise limit of 500 characters, facilitating clear and succinct communication within the system.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION is a configuration property pivotal in the domain of threat analysis. This setting specifically delineates the criteria for the type of claims or facts to be extracted, ensuring that any information that could potentially be relevant to threat analysis is captured. The description for claim extraction, as defined by GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION, is set to "Any claims or facts that could be relevant to threat analysis," highlighting its comprehensive approach to identifying pertinent data points in the analysis process.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">The GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE is a configuration property within the system, designed to specify the file that should be utilized for claim extraction prompts. Currently, this setting is configured as None, which signifies the absence of a designated file for this purpose. This configuration property plays a crucial role in the claim extraction process, enabling flexibility in choosing the appropriate prompts file, although at present, no specific file is being employed.</data>
      <data key="d2">2b777e3d591ce1511a03abd1a6d8dc73,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GLEANINGS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GLEANINGS is a configuration property with a value of 1, indicating a setting or a state in the system related to data processing or analysis.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS is a configuration property that sets the maximum number of gleanings for claim extraction, currently set to 1.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_COMMUNITY_REPORT_MAX_LENGTH">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_COMMUNITY_REPORT_MAX_LENGTH is a configuration property that sets the maximum length for community reports, currently set to 1500.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_STORAGE_TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_STORAGE_TYPE is a configuration property that specifies the type of storage system to use, currently set to file.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_STORAGE_CONNECTION_STRING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_STORAGE_CONNECTION_STRING is a configuration property that provides the connection string for the storage system, currently set to None.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_STORAGE_CONTAINER_NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_STORAGE_CONTAINER_NAME is a configuration property that specifies the name of the storage container, currently set to None.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_STORAGE_BASE_DIR">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_STORAGE_BASE_DIR is a configuration property that indicates the base directory for the storage system, currently set to None.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CACHE_TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_CACHE_TYPE is a configuration property that specifies the type of cache system to use, currently set to file.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CACHE_CONNECTION_STRING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_CACHE_CONNECTION_STRING is a configuration property that provides the connection string for the cache system, currently set to None.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CACHE_CONTAINER_NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_CACHE_CONTAINER_NAME is a configuration property that specifies the name of the cache container, currently set to None.&gt;</data>
      <data key="d2">cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_CACHE_BASE_DIR">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_CACHE_BASE_DIR is a configuration property that specifies the base directory for the cache system. It is pertinent to note that this setting is currently configured as None, which indicates the absence of a specifically defined base directory for caching purposes. This configuration property plays a crucial role in managing the caching mechanism, allowing for the potential designation of a dedicated directory to optimize cache operations. However, with its current setting of None, no particular directory is being utilized as the base for caching, suggesting that default system behaviors or other fallback mechanisms might be in place for cache management.</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_REPORTING_TYPE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_REPORTING_TYPE is a configuration property within the system, designed to determine the specific reporting mechanism that will be utilized. Currently, GRAPHRAG_REPORTING_TYPE is set to 'file', which signifies that all reporting activities will be executed through the creation and management of files. This setting ensures that data and reports are stored and accessed in a structured format, facilitating ease of use and management for users interacting with the system's reporting functionalities.</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_REPORTING_CONNECTION_STRING">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_REPORTING_CONNECTION_STRING is a configuration property utilized to specify the connection string for the reporting system. This setting is pivotal for establishing the reporting functionality within the system. Currently, GRAPHRAG_REPORTING_CONNECTION_STRING is set to None, which signifies that no particular connection string has been designated for reporting purposes. This configuration setting remains undefined, suggesting that the reporting system may not be fully operational or integrated until a valid connection string is provided.</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_REPORTING_CONTAINER_NAME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_REPORTING_CONTAINER_NAME is a configuration property utilized for specifying the name of the reporting container within a system. This setting is pivotal for defining the container that will be used for reporting purposes. Currently, GRAPHRAG_REPORTING_CONTAINER_NAME is set to None, which indicates that no specific container name has been designated for reporting. This configuration setting allows for flexibility in managing and organizing reporting functionalities within the system, ensuring that reports can be directed to a specified container when needed.</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_REPORTING_BASE_DIR">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_REPORTING_BASE_DIR is a configuration property utilized within the system, which designates the base directory for the reporting mechanism. Currently, this setting is configured as None, suggesting that no particular base directory has been explicitly defined for the reporting functionalities. This configuration property plays a crucial role in determining the location where reports are generated and stored, and its current setting to None implies that the system may either use a default location or require a directory to be specified on a case-by-case basis for report generation.</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_NODE2VEC_ENABLED">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_NODE2VEC_ENABLED is a configuration property that specifies the status of the Node2Vec algorithm within the system. Currently, GRAPHRAG_NODE2VEC_ENABLED is set to False, indicating that the Node2Vec algorithm is not activated. This setting is crucial for controlling the application of the Node2Vec algorithm in social network analysis, particularly in mapping complex relationships and identifying key influencers within communities related to Motor Control and Drive Systems. When GRAPHRAG_NODE2VEC_ENABLED is set to True, it enables the system to utilize Node2Vec for analyzing the structure and dynamics of specialized professional networks, facilitating the identification of collaboration opportunities and knowledge gaps in the field. However, with the current setting of False, the system is not employing the Node2Vec algorithm for these purposes.</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="GRAPHRAG_NODE2VEC_NUM_WALKS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">GRAPHRAG_NODE2VEC_NUM_WALKS is a configuration property pivotal in the application of the Node2Vec algorithm within the context of Social Network Analysis. This setting determines the number of walks, or traversals, the algorithm will undertake through the network to learn the structure and relationships. Currently, GRAPHRAG_NODE2VEC_NUM_WALKS is set to 10, signifying that the algorithm will perform 10 walks to capture the network's dynamics effectively. This parameter is crucial for optimizing the learning process and ensuring that the algorithm can adequately model the complex relationships within specialized professional networks, such as those found in the Motor Control and Drive Systems domain. By adjusting GRAPHRAG_NODE2VEC_NUM_WALKS, one can influence the depth and breadth of the network analysis, facilitating the identification of key influencers, collaboration opportunities, and knowledge gaps in the field.</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e,cde833db73c46ca28f08e35195134441</data>
    </node>
    <node id="APHRAG_CACHE_CONTAINER_NAME">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">APHRAG_CACHE_CONTAINER_NAME is a configuration setting that specifies the name of the container for caching purposes. It is currently set to None, indicating that no specific container name has been defined.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_NODE2VEC_WALK_LENGTH">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_NODE2VEC_WALK_LENGTH is a configuration setting that specifies the length of each walk for the Node2Vec algorithm. It is currently set to 40, indicating that each walk will have a length of 40.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_NODE2VEC_WINDOW_SIZE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_NODE2VEC_WINDOW_SIZE is a configuration setting that specifies the window size for the Node2Vec algorithm. It is currently set to 2, indicating that the window size is 2.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_NODE2VEC_ITERATIONS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_NODE2VEC_ITERATIONS is a configuration setting that specifies the number of iterations for the Node2Vec algorithm. It is currently set to 3, indicating that the algorithm will be run for 3 iterations.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_NODE2VEC_RANDOM_SEED">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_NODE2VEC_RANDOM_SEED is a configuration setting that specifies the random seed for the Node2Vec algorithm. It is currently set to 597832, indicating that this number will be used as the random seed.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_SNAPSHOT_GRAPHML">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_SNAPSHOT_GRAPHML is a configuration setting that specifies whether to take a snapshot in GraphML format. It is currently set to False, indicating that no GraphML snapshot will be taken.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_SNAPSHOT_RAW_ENTITIES">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_SNAPSHOT_RAW_ENTITIES is a configuration setting that specifies whether to take a snapshot of raw entities. It is currently set to False, indicating that no snapshot of raw entities will be taken.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES is a configuration setting that specifies whether to take a snapshot of top-level nodes. It is currently set to False, indicating that no snapshot of top-level nodes will be taken.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_ASYNC_MODE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_ASYNC_MODE is a configuration setting that specifies the asynchronous mode to be used. It is currently set to 'asyncio', indicating that asyncio will be used for asynchronous operations.&gt;</data>
      <data key="d2">79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_ENCODING_MODEL">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_ENCODING_MODEL is a configuration property within the domain of Social Network Analysis and Motor Control and Drive Systems, specifically designed to specify the encoding model for graph representation. Currently, it is set to 'cl100k_base', a particular model that will be utilized for encoding purposes. This setting ensures that the structure and dynamics of specialized professional networks can be accurately captured, facilitating the identification of collaboration opportunities and knowledge gaps in the field. The use of 'cl100k_base' as the encoding model under GRAPHRAG_ENCODING_MODEL highlights a preference for a specific level of detail and complexity in graph representation, which is crucial for understanding the intricate relationships within the Motor Control and Drive Systems community.</data>
      <data key="d2">1b24101de07b1c195448240237b84b37,79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_MAX_CLUSTER_SIZE">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_MAX_CLUSTER_SIZE is a configuration property within the system, specifically set to 10. This setting dictates the maximum size of a cluster in the graph representation, ensuring that no cluster exceeds 10 elements. This limitation is crucial for managing the complexity and scale of clusters within the graph, maintaining an optimal structure for analysis and processing.</data>
      <data key="d2">1b24101de07b1c195448240237b84b37,79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_ENTITY_RESOLUTION_ENABLED">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_ENTITY_RESOLUTION_ENABLED is a configuration property within the system, currently set to False. This setting indicates that entity resolution is disabled in the graph representation, meaning that the system is not actively resolving or merging entities that may represent the same underlying object or concept. With entity resolution disabled, the graph may contain duplicate nodes or edges that could potentially be merged if this feature were enabled. This configuration setting is crucial for controlling the complexity and accuracy of the graph representation in the context of social network analysis or other graph-based data processing tasks.</data>
      <data key="d2">1b24101de07b1c195448240237b84b37,79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_SKIP_WORKFLOWS">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_SKIP_WORKFLOWS is a configuration property within the system, currently set to None. This setting indicates that no workflows are being skipped in the graph representation, as no specific workflows have been defined to be excluded from the processing. This configuration allows for the control and customization of which workflows are considered in the graph generation process, ensuring comprehensive and tailored graph representations based on the needs of the user or system requirements.</data>
      <data key="d2">1b24101de07b1c195448240237b84b37,79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="GRAPHRAG_UMAP_ENABLED">
      <data key="d0">CONFIGURATION, SETTING</data>
      <data key="d1">GRAPHRAG_UMAP_ENABLED is a configuration property within the system, currently set to False. This setting indicates that UMAP (Uniform Manifold Approximation and Projection), a dimension reduction technique used for visualizing high-dimensional data, is disabled in the graph representation. As a result, alternative methods might be employed for data visualization and analysis in the context of Motor Control and Drive Systems domain, potentially impacting the way complex relationships and network structures are understood and interpreted.</data>
      <data key="d2">1b24101de07b1c195448240237b84b37,79d4b8574baf3734b60b969b66326b2e</data>
    </node>
    <node id="DOCUMENT">
      <data key="d0">DOCUMENT, DATA</data>
      <data key="d1">A Document, in the context of the text processing pipeline, serves as the primary container for information, typically representing individual rows in a CSV or standalone .txt files. These documents are crucial as they form the basis for further analysis and processing. They have a versatile relationship with TextUnits, which are smaller, analyzable segments of text. In a standard scenario, a one-to-many relationship exists, where a single Document can be broken down into multiple TextUnits. However, the relationship can also be many-to-many in specific instances, such as when analyzing short documents like Tweets or chat logs, where information from multiple documents might be combined to create a single, more comprehensive TextUnit for analysis. This flexibility in relationship structure allows for a nuanced and detailed examination of text data, catering to various types of documents and analytical needs.</data>
      <data key="d2">81f57cf867ea246ad9a6e794ed613375,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </node>
    <node id="TEXTUNIT">
      <data key="d0">TEXT, DATA</data>
      <data key="d1">A TextUnit represents a segment of text that serves as the fundamental unit for analysis and processing within the pipeline. These chunks, which can be configured in terms of size, overlap, and adherence to data boundaries, are derived from Documents. A common configuration involves setting the CHUNK_BY_COLUMNS parameter to 'id', establishing a 1-to-many relationship between documents and TextUnits, as opposed to a many-to-many relationship. TextUnits are processed individually for various tasks, including graph extraction, entity and relationship extraction, and other analytical operations. Each TextUnit undergoes text-embedding before being passed to the subsequent phase of the pipeline for deeper analysis. This approach ensures that the text is broken down into manageable pieces, facilitating more detailed and efficient processing.</data>
      <data key="d2">10d01d36390b307a63fd5bc97d8682c0,81f57cf867ea246ad9a6e794ed613375,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </node>
    <node id="ENTITY">
      <data key="d0">ENTITY, DATA</data>
      <data key="d1">ENTITY is a significant concept, object, or subject extracted from a text unit during the Graph Extraction phase. These entities represent people, places, events, or other relevant subjects within the text and are characterized by a name, type, and description. They are information elements that hold substantial meaning and are crucial for understanding the structure and dynamics of the text. ENTITY serves as a representation of the significant aspects of the text, enabling a deeper analysis and comprehension of the content.</data>
      <data key="d2">81f57cf867ea246ad9a6e794ed613375,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </node>
    <node id="RELATIONSHIP">
      <data key="d0">RELATIONSHIP, DATA</data>
      <data key="d1">In the context of Social Network Analysis, a Relationship is a significant connection that exists between two entities. These relationships are meticulously generated from the covariates, reflecting the underlying variables that influence the nature of the connections. They serve as the links that bind entities together, representing the interactions and associations that are present within the network. Relationships are a crucial component extracted during the Graph Extraction phase, playing a pivotal role in shaping the structure of the graph that is being constructed from the analyzed text. This process ensures that the graph accurately reflects the complex dynamics and relationships within the Motor Control and Drive Systems domain, enabling a deeper understanding of the network's structure and facilitating the identification of collaboration opportunities and knowledge gaps.</data>
      <data key="d2">81f57cf867ea246ad9a6e794ed613375,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </node>
    <node id="COVARIATE">
      <data key="d0">PROPERTY, DATA</data>
      <data key="d1">Covariate, a key component in the analysis of textual data, specifically within the realm of claim information, is described as a type of extracted data that encapsulates statements about various entities. These entities might be associated with specific timeframes, adding a temporal dimension to the information. Covariates serve a crucial role in furnishing context and additional details about the entities mentioned in the text, thereby enriching the understanding of the relationships and dynamics within the data. This information is pivotal for anyone seeking to analyze or interpret claim-related documents, as it helps in identifying the nuances and specifics tied to the entities in question.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </node>
    <node id="COMMUNITY REPORT">
      <data key="d0">REPORT, DATA</data>
      <data key="d1">The "Community Report" is a comprehensive document that emerges following the creation of entities within the Motor Control and Drive Systems domain. This report is generated through a sophisticated process of hierarchical community detection, which meticulously analyzes and categorizes the entities based on their relationships and influence within the community. Each "Community Report" offers detailed insights and summaries tailored to every community within the hierarchical structure, providing a deep understanding of the dynamics, collaboration opportunities, and knowledge gaps present in the specialized professional network. This enables stakeholders to identify key influencers, potential collaborators, and areas for further exploration, enhancing the overall connectivity and productivity within the Motor Control and Drive Systems domain.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </node>
    <node id="DEFAULT CONFIGURATION WORKFLOW">
      <data key="d0">PROCESS, WORKFLOW</data>
      <data key="d1">The Default Configuration Workflow is a series of steps that transform text documents into the GraphRAG Knowledge Model. It includes major phases such as Network Visualization, Document Processing, Community Summarization, Graph Augmentation, Graph Extraction, and Compose TextUnits.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20</data>
    </node>
    <node id="NETWORK VISUALIZATION">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Network Visualization is a phase in the workflow where the network of entities and their relationships are visualized in a graph format.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20</data>
    </node>
    <node id="DOCUMENT PROCESSING">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Document Processing is a critical phase in the workflow dedicated to transforming and analyzing documents to fit into the GraphRAG Knowledge Model. This phase is responsible for creating the Documents table for the knowledge model through a series of steps. These steps include augmenting documents to enrich their content, linking them to TextUnits for detailed analysis, and calculating the average embeddings to represent the documents numerically. This comprehensive process ensures that documents are thoroughly processed and prepared for further analysis within the GraphRAG Knowledge Model.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,493f38f41b89e767fc23d84e1fa5ba20,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="COMMUNITY SUMMARIZATION">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Community Summarization is a comprehensive process that plays a crucial role in understanding the structure and dynamics of specialized professional networks, such as those found in the Motor Control and Drive Systems domain. This process involves the generation and summarization of community reports, embedding communities, and emitting community tables, all of which aim to provide a high-level understanding of the graph at various levels of granularity. By summarizing the characteristics and insights of a community, often derived from social network analysis or community detection algorithms, Community Summarization enables the identification of key influencers, collaboration opportunities, and knowledge gaps within the field. This phase in the workflow ensures that the insights and reports generated are insightful and useful for further analysis and decision-making.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,5b2968b8f1c891d47ecbe641c3391663,6f92ce3fcd05dd5697ded83586f7bc08</data>
      <data key="d3">PROPERTY, INFORMATION</data>
    </node>
    <node id="GRAPH AUGMENTATION">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Graph Augmentation is a critical phase in the workflow of Social Network Analysis, particularly within specialized professional networks such as the Motor Control and Drive Systems domain. This process follows the extraction of entities and relationships and involves enhancing the graph with additional information and relationships, thereby enriching the understanding of the community structure. Through Graph Augmentation, the graph is not only expanded but also deepened, allowing for a more comprehensive analysis of the dynamics and connections within the network. This phase is essential for identifying key influencers, mapping complex relationships, and uncovering collaboration opportunities and knowledge gaps in the field.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </node>
    <node id="GRAPH EXTRACTION">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Graph Extraction is a critical phase in the pipeline of text analysis, where each TextUnit undergoes a meticulous process to extract graph primitives. This phase employs sophisticated methods, including the use of entity_extract and claim_extract verbs, to identify and isolate key components such as Entities, Relationships, and Claims from the raw text. The objective is to transform the textual data into a structured format, resulting in the creation of a subgraph for every TextUnit. These subgraphs comprehensively list the entities and relationships discovered within the text, facilitating a deeper understanding of the text's structure and dynamics. Graph Extraction is an essential step in the workflow, enabling the analysis of complex relationships and the identification of key influencers within the text's context.</data>
      <data key="d2">10d01d36390b307a63fd5bc97d8682c0,493f38f41b89e767fc23d84e1fa5ba20,81f57cf867ea246ad9a6e794ed613375</data>
    </node>
    <node id="COMPOSE TEXTUNITS">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Compose TextUnits is the first phase in the workflow where input documents are transformed into TextUnits, which are chunks of text used for graph analysis.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20</data>
    </node>
    <node id="UMAP DOCUMENTS">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Umap Documents is a sophisticated process that specializes in mapping documents into a lower-dimensional space, primarily a 2D representation, for the purpose of visualization and analysis. This process leverages UMAP (Uniform Manifold Approximation and Projection) dimensionality reduction technique to generate a 2D representation that effectively visualizes the intricate relationships between documents. Umap Documents enables users to discern patterns, clusters, and connections that might not be apparent in high-dimensional data, making it an invaluable tool for understanding the structure and dynamics of document sets in various domains.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,56506e2d064c0732efa3cf418057edfd</data>
    </node>
    <node id="UMAP ENTITIES">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Umap Entities is a sophisticated process that specializes in mapping complex entities into a lower-dimensional space, specifically a 2D representation, to facilitate both visualization and analysis. This method employs UMAP (Uniform Manifold Approximation and Projection) dimensionality reduction, a cutting-edge technique that generates a concise yet comprehensive view of the relationships between entities. By transforming high-dimensional data into a 2D format, Umap Entities enables users to easily identify patterns, clusters, and connections that might not be apparent in the original data set. This process is invaluable for understanding the structure and dynamics of specialized professional networks, identifying collaboration opportunities, and pinpointing knowledge gaps in fields such as Motor Control and Drive Systems. Through Umap Entities, entities can be visualized in a way that highlights their interconnections, making it an essential tool for Social Network Analysis.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,56506e2d064c0732efa3cf418057edfd</data>
    </node>
    <node id="NODES TABLE">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">The Nodes Table contains information about the nodes in the graph, including their layout and relationships.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20</data>
    </node>
    <node id="LINK TO TEXTUNITS">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Link to TextUnits is a pivotal process in Document Processing that facilitates the connection between documents and their constituent text units. This process is instrumental in establishing a clear relationship between documents and text-units, enabling a detailed analysis and deeper understanding of document structure. As a property in the workflow, Link to TextUnits plays a crucial role in graph representation, ensuring that nodes are accurately connected to their corresponding TextUnits. This process is executed after the creation of text-units in the initial phase of the workflow, ensuring that each document is seamlessly linked to its relevant text components. Through this process, the intricacies of document composition and structure can be thoroughly explored, making it an essential component in the analysis and processing of textual data.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,493f38f41b89e767fc23d84e1fa5ba20,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="DOCUMENT EMBEDDING">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Document Embedding is a sophisticated process and property that transforms documents into numerical vectors for analysis and comparison. This transformation is achieved by averaging the embeddings of document slices, with weights determined by the token-count. The resulting vector representation facilitates a deeper understanding of the relationships between documents, making it an invaluable tool for network visualization and analysis. This method enables the identification of patterns, similarities, and connections within a set of documents, enhancing the ability to uncover insights and knowledge gaps in specialized professional networks such as the Motor Control and Drive Systems domain. By leveraging Document Embedding, experts can more effectively identify collaboration opportunities and key influencers within their community, thereby fostering innovation and knowledge sharing.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="DOCUMENT GRAPH CREATION">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Document Graph Creation is a process that creates a graph representation of the documents and their relationships.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20</data>
    </node>
    <node id="DOCUMENT TABLES">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Document Tables contain information about the documents, including metadata and analysis results.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20</data>
    </node>
    <node id="COMMUNITY EMBEDDING">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Community Embedding is a sophisticated process that transforms the intricate structure of communities into a lower-dimensional vector space, enabling both visualization and in-depth analysis. This representation is achieved by generating text embeddings from various components of community data, including the community report, its summary, and the title, to create a comprehensive vector representation. This vector space representation is not only useful for visualizing community dynamics but also serves as a powerful tool for machine learning tasks, such as clustering and classification, allowing for the identification of patterns and similarities among communities. Community Embedding, therefore, acts as a bridge between the complex social network data and the analytical capabilities of machine learning algorithms, facilitating a deeper understanding of community relationships and dynamics.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="COMMUNITY TABLES">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Community Tables are comprehensive data structures designed to systematically organize and present information about various communities. These tables encompass a wide range of details, including the identities of community members, the relationships that exist among them, and the distinct attributes that define each community. By consolidating this information, Community Tables serve as a valuable resource for understanding the structure, dynamics, and characteristics of different communities, thereby facilitating the identification of collaboration opportunities and knowledge gaps within the field.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="GRAPH EMBEDDING">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Graph Embedding is a sophisticated technique pivotal in the domain of graph analysis and visualization. It serves as a critical step in the process, particularly as a subprocess of Phase 3: Graph Augmentation. The method involves the conversion of a graph into a vector space, where nodes are transformed into points and edges into distances between these points. This vector representation is generated using the Node2Vec algorithm, enabling a deeper understanding of the graph's implicit structure. The vector-space created not only facilitates various graph analysis tasks but also provides an additional dimension for searching related concepts during the query phase, enhancing the overall efficiency and effectiveness of graph-related operations.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,5b2968b8f1c891d47ecbe641c3391663,6f92ce3fcd05dd5697ded83586f7bc08,a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </node>
    <node id="AUGMENTED GRAPH TABLES">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Augmented Graph Tables are sophisticated data structures designed to store comprehensive information about graphs. These tables not only hold the basic graph data but also incorporate enhanced details such as additional metadata and analytical results post augmentation. This augmentation process enriches the tables with extra entities and relationships, making them a valuable resource for in-depth graph analysis and understanding.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="ENTITY &amp; RELATIONSHIP EXTRACTION">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Entity &amp; Relationship Extraction is a sophisticated process primarily utilized in the domains of natural language processing and information extraction. This process excels at identifying and mapping out entities, along with the relationships that exist between them, from various text-based or data sources. By doing so, it enables a deeper understanding of the structure and dynamics within specialized professional networks, such as those found in the Motor Control and Drive Systems domain. This understanding is crucial for identifying key influencers, collaboration opportunities, and knowledge gaps within the field. The process of Entity &amp; Relationship Extraction is not limited to text analysis but can be applied to a wide range of data sources, making it a versatile tool for data analysis and information retrieval.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="ENTITY &amp; RELATIONSHIP SUMMARIZATION">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Entity &amp; Relationship Summarization is a process that consolidates and provides a concise overview of the key entities and their interactions identified within a dataset or text. This method is crucial for understanding the structure and dynamics of specialized professional networks, enabling the identification of collaboration opportunities and knowledge gaps in fields such as Motor Control and Drive Systems. By mapping complex relationships and pinpointing key influencers, Entity &amp; Relationship Summarization facilitates a deeper comprehension of the network's composition and facilitates informed decision-making.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="ENTITY RESOLUTION">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Entity Resolution is a critical process in data management that identifies and merges duplicate or similar entities in a dataset, enhancing data quality and consistency. This process is designed to resolve entities that represent the same real-world object, concept, or entity but may have different names. Initially, the implementation of Entity Resolution was destructive, merging entities into a single entity and updating their relationships. However, future implementations aim to be non-destructive, allowing end-users the flexibility to undo indexing-side resolutions and add their own resolutions. This optional process in the pipeline takes a conservative, non-destructive approach to ensure that no valuable information is lost, making it a robust solution for maintaining data integrity.</data>
      <data key="d2">10d01d36390b307a63fd5bc97d8682c0,493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08,d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </node>
    <node id="GRAPH TABLES">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Graph Tables are sophisticated data structures designed to represent graph data comprehensively. They encapsulate a wealth of information, including nodes, edges, and various attributes, making them invaluable tools for graph analysis and visualization. By storing details about the graph's composition and properties, Graph Tables enable a deeper understanding of the relationships and dynamics within the graph, facilitating more effective analysis and interpretation of complex networks.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="DOCUMENTS">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Documents, in the context of the workflow and knowledge model, are individual units of text or data that undergo transformation and analysis. These documents serve as the primary input in the workflow, where they are processed and structured to extract valuable information. The Documents table, a critical data structure created during the Document Processing phase, encapsulates metadata and content details about the documents, facilitating their management and analysis within the knowledge model. This table is essential for maintaining the integrity and accessibility of the documents throughout the workflow, ensuring that all relevant data is captured and utilized effectively.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="TEXT UNITS">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Text Units represent the fundamental segments of text that are utilized for analysis and processing within text analysis workflows. These units are typically extracted from documents and serve as the foundational elements for subsequent analysis or information extraction processes. As the core components in text analysis, Text Units play a crucial role in enabling detailed examination and manipulation of textual data.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="EMBED">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">Embed is a versatile process primarily utilized in the domains of machine learning and natural language processing. It transforms information or data into a numerical or vector representation, facilitating analysis and comparison. This conversion enables the handling of complex data types, making it an essential tool for extracting meaningful insights from various data sources.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="DATAFLOW OVERVIEW">
      <data key="d0">PROPERTY, INFORMATION</data>
      <data key="d1">The Dataflow Overview is a comprehensive depiction of the flow of data and processes within a system or workflow, spanning from the initial input to the final output. It serves as a valuable tool for understanding the sequence of operations and transformations that data undergoes as it moves through the system. This overview is crucial for gaining insights into the dynamics of data processing, identifying potential bottlenecks, and optimizing the efficiency of the workflow. By visualizing the dataflow, stakeholders can better comprehend how data is manipulated and how various processes interact, enabling them to make informed decisions regarding system improvements and data management strategies.</data>
      <data key="d2">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="PHASE 1: COMPOSE TEXTUNITS">
      <data key="d0">PROCESS, ANALYSIS</data>
      <data key="d1">Phase 1: Compose TextUnits is the initial step in a workflow where input documents are transformed into TextUnits, which are chunks of text used for graph extraction techniques and as source references for extracted knowledge items.</data>
      <data key="d2">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </node>
    <node id="CLAIM">
      <data key="d0">CLAIM, INFORMATION_STATEMENT</data>
      <data key="d1">Claims are statements or assertions extracted from the text during the Graph Extraction phase. They represent the assertions made within the text and are a component of the graph being built.</data>
      <data key="d2">81f57cf867ea246ad9a6e794ed613375</data>
    </node>
    <node id="GRAPH SUMMARIZATION">
      <data key="d0">PROCESS, INFORMATION SUMMARIZATION</data>
      <data key="d1">Graph Summarization is a process that consolidates multiple subgraphs into a single graph by merging entities and relationships with the same name and type, creating arrays of their descriptions for a comprehensive view.</data>
      <data key="d2">10d01d36390b307a63fd5bc97d8682c0</data>
    </node>
    <node id="COVARIATES">
      <data key="d0">ARTIFACT, DATA</data>
      <data key="d1">Covariates, the primary artifacts emitted from the Claim Extraction &amp; Emission process, are significant components within the domain of analysis. These entities represent positive factual statements that are meticulously evaluated and come with defined time-bounds, ensuring their relevance and accuracy within specific periods. Despite a slight variation in the descriptions provided, it is clear that Covariates are the outcome of a specialized extraction process, highlighting their importance in maintaining the integrity and temporal relevance of claims within the field.</data>
      <data key="d2">a6bcb4514cb6de67e3d74ad0ea62452d,d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </node>
    <node id="CLAIM EXTRACTION &amp; EMISSION">
      <data key="d0">PROCESS, WORKFLOW</data>
      <data key="d1">Claim Extraction &amp; Emission is a process where claims are extracted from the source TextUnits. These claims are positive factual statements with an evaluated status and time-bounds, and they are emitted as a primary artifact called Covariates.</data>
      <data key="d2">a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </node>
    <node id="PHASE 3: GRAPH AUGMENTATION">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Phase 3: Graph Augmentation is a process where the graph of entities and relationships is augmented with additional information to understand their community structure. This is done through Community Detection and Graph Embedding.</data>
      <data key="d2">a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </node>
    <node id="GRAPH TABLES EMISSION">
      <data key="d0">SUBPROCESS, DATA EMISSION</data>
      <data key="d1">Graph Tables Emission is a critical subprocess that falls under Phase 3: Graph Augmentation. This process is characterized by the emission of the final entities and relationships in a tabular format. It is initiated once the text fields of these entities and relationships have been text-embedded, marking the culmination of the graph augmentation steps. This subprocess ensures that the enriched data, post-augmentation, is systematically organized and presented in a structured manner, facilitating easier analysis and understanding of the network's dynamics.</data>
      <data key="d2">5b2968b8f1c891d47ecbe641c3391663,a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </node>
    <node id="NODE2VEC ALGORITHM">
      <data key="d0">ALGORITHM, TOOL</data>
      <data key="d1">Node2Vec is an algorithm used for generating vector representations of nodes in a graph, facilitating the understanding of the graph's structure and enabling vector-space search for related concepts.</data>
      <data key="d2">5b2968b8f1c891d47ecbe641c3391663</data>
    </node>
    <node id="COMMUNITY TABLES EMISSION">
      <data key="d0">ACTION, PROCESS</data>
      <data key="d1">Community Tables Emission is the process of generating and outputting the Communities and CommunityReports tables as part of the workflow.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888</data>
    </node>
    <node id="AUGMENT">
      <data key="d0">ACTION, PROCESS</data>
      <data key="d1">Augment, a crucial process in the Document Processing domain, specializes in enhancing documents by incorporating supplementary information. This augmentation enriches the content and context of the documents, making them more informative and valuable. Specifically, when the workflow is processing CSV data, Augment adds extra fields to the Documents output. These additional fields are designed to align with the existing columns on the incoming CSV tables, thereby expanding the dataset without compromising its integrity. Through this process, Augment facilitates a more comprehensive understanding of the information contained within the documents, enabling better analysis and decision-making.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="AVG. EMBEDDING">
      <data key="d0">PROPERTY, MEASUREMENT</data>
      <data key="d1">Avg. Embedding is a critical property calculated during Document Processing, which encapsulates the essence of a document through a vector representation. This representation is derived by averaging the embeddings of various slices within the document, providing a comprehensive summary of its content. The Avg. Embedding algorithm facilitates the analysis of implicit relationships between documents, making it an invaluable tool for network visualization and other analytical purposes. By generating a vector that reflects the document's characteristics, Avg. Embedding enables a deeper understanding of document interconnectivity and supports the identification of patterns and trends within document networks.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="DOCUMENT TABLE EMISSION">
      <data key="d0">ACTION, PROCESS</data>
      <data key="d1">Document Table Emission represents a critical phase in the document processing workflow. It involves the emission of the Documents table into the knowledge model, enabling the document data to be readily accessible for subsequent processing or analysis. This process is pivotal as it facilitates the generation and output of the Documents table, ensuring that the information contained within is prepared and structured for further utilization. Through Document Table Emission, the intricacies of document data are transformed into a format that supports advanced analysis and integration into broader knowledge systems.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="AUGMENT WITH COLUMNS (CSV ONLY)">
      <data key="d0">ACTION, PROCESS</data>
      <data key="d1">Augment with Columns (CSV Only) is a specialized function designed for enhancing the Document Processing workflow, particularly when handling CSV data. This feature enables the addition of extra fields to the Documents output, leveraging the information present in the incoming CSV tables. Configuration is necessary to utilize this function effectively, ensuring that the added fields are relevant and useful. The process is exclusive to CSV data, making it a valuable tool for enriching document outputs with additional context and detail.</data>
      <data key="d2">3e292d936b7efa377ba9530456cfd888,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="COMMUNITIES">
      <data key="d0">TABLE, DATA STRUCTURE</data>
      <data key="d1">The Communities table is a data structure that holds information about various communities, likely in the context of a knowledge model or database.</data>
      <data key="d2">827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="COMMUNITYREPORTS">
      <data key="d0">TABLE, DATA STRUCTURE</data>
      <data key="d1">The CommunityReports table is a data structure that contains reports or data related to communities, possibly including statistics, summaries, or other relevant information.</data>
      <data key="d2">827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="DOCUMENTS TABLE EMISSION">
      <data key="d0">FUNCTION, PROCESS</data>
      <data key="d1">Documents Table Emission is a critical function or process that involves integrating and emitting the Documents table into the knowledge model. This process makes the document data accessible for subsequent processing or analysis, enabling the representation of documents within the model. By facilitating the incorporation of document data, Documents Table Emission plays a pivotal role in ensuring that the knowledge model is comprehensive and can support various analytical tasks.</data>
      <data key="d2">56506e2d064c0732efa3cf418057edfd,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="PHASE 6: NETWORK VISUALIZATION">
      <data key="d0">PHASE, PROCESS</data>
      <data key="d1">Phase 6: Network Visualization is a critical phase in the workflow dedicated to the visualization of high-dimensional vector spaces within the context of existing graphs. This phase focuses on transforming complex relationships between documents and entities into a comprehensible network format, thereby facilitating a deeper understanding of the connections and dynamics within the data. Through this phase, the visualization of intricate networks becomes accessible, enabling users to discern patterns, clusters, and key influencers that might not be evident in raw data. This process is essential for identifying collaboration opportunities, knowledge gaps, and the structure of specialized professional networks, particularly in domains like Motor Control and Drive Systems, where mapping complex relationships is crucial for innovation and advancement.</data>
      <data key="d2">56506e2d064c0732efa3cf418057edfd,827fd80da359cf05b091c24e465dd05d</data>
    </node>
    <node id="ENTITY-RELATIONSHIP GRAPH">
      <data key="d0">CONCEPT, GRAPH</data>
      <data key="d1">The Entity-Relationship graph is a logical graph that represents the relationships between entities and their attributes, providing a structured view of the data.</data>
      <data key="d2">56506e2d064c0732efa3cf418057edfd</data>
    </node>
    <node id="DOCUMENT GRAPH">
      <data key="d0">CONCEPT, GRAPH</data>
      <data key="d1">The Document graph is a logical graph that represents the relationships between documents, providing a structured view of the document space.</data>
      <data key="d2">56506e2d064c0732efa3cf418057edfd</data>
    </node>
    <node id="NODES TABLE EMISSION">
      <data key="d0">ACTION, PROCESS</data>
      <data key="d1">Nodes Table Emission is the process of emitting a table of nodes, which includes information about whether the node is a document or an entity, and the UMAP coordinates for visualization.</data>
      <data key="d2">56506e2d064c0732efa3cf418057edfd</data>
    </node>
    <node id="NETWORK VISUALIZATION WORKFLOWS">
      <data key="d0">CONCEPT, PROCESS</data>
      <data key="d1">Network Visualization Workflows are the processes and steps taken to visualize high-dimensional vector spaces within graphs, enabling a 2D representation of the graph and the understanding of relationships between nodes.</data>
      <data key="d2">56506e2d064c0732efa3cf418057edfd</data>
    </node>
    <node id="DISCRIMINATOR">
      <data key="d0">PROPERTY, CLASSIFICATION</data>
      <data key="d1">The Discriminator is a property that classifies whether a node represents a document or an entity in a data structure. It is a binary indicator that helps in distinguishing between different types of nodes within a system.</data>
      <data key="d2">2011f03f21e526cf9277c27bf3e68242</data>
    </node>
    <node id="UMAP COORDINATES">
      <data key="d0">PROPERTY, LOCATION</data>
      <data key="d1">UMAP (Uniform Manifold Approximation and Projection) Coordinates are a set of values that represent the location of a node in a reduced dimensional space. They are used for visualizing high-dimensional data in a lower-dimensional format, typically for the purpose of data exploration and machine learning tasks.</data>
      <data key="d2">2011f03f21e526cf9277c27bf3e68242</data>
    </node>
    <node id="GRAPHRAG INDEXER CLI">
      <data key="d0">SOFTWARE, TOOL</data>
      <data key="d1">The GraphRAG Indexer CLI is a command-line interface tool that enables no-code usage of the GraphRAG Indexer, facilitating the indexing process for data projects without the need for programming. It provides various options for configuring the indexing process.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="CLI ARGUMENTS">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">CLI Arguments are parameters that can be passed to the GraphRAG Indexer CLI to customize its behavior. These include options for verbosity, specifying the root directory, initializing the project, resuming a previous run, custom configuration, progress reporting, and output formats.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="VERBOSE">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The verbose argument, when added, increases the amount of logging information during the execution of the GraphRAG Indexer CLI.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="INIT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The init argument initializes the data project directory at the specified root with bootstrap configuration and prompt-overrides.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="RESUME">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The resume argument, when specified with a timestamp, allows the pipeline to resume a prior run. The parquet files from the prior run are loaded as inputs, and the workflows that generated those files are skipped.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="CONFIG">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The config argument allows opting out of the Default Configuration mode and executing a custom configuration specified in a config file.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="REPORTER">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The reporter argument specifies the progress reporter to use during the execution of the GraphRAG Indexer CLI. The default is rich, and valid values include rich, print, and none.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="EMIT">
      <data key="d0">PROPERTY, CONFIGURATION</data>
      <data key="d1">The emit argument specifies the table output formats for the GraphRAG Indexer CLI. It can include types such as json, csv, and parquet.&gt;</data>
      <data key="d2">f239de6498e0f471bf418974c00f1e36</data>
    </node>
    <node id="TIMESTAMPED OUTPUT FOLDER">
      <data key="d0">PROPERTY, FOLDER</data>
      <data key="d1">The Timestamped Output Folder is a property that specifies the folder where the output of the process is stored, named with a timestamp, for example, "20240105-143721".</data>
      <data key="d2">919cb44d9688a14bf48fa7c98163ed81</data>
    </node>
    <node id="PROGRESS REPORTER">
      <data key="d0">PROPERTY, REPORTER</data>
      <data key="d1">The Progress Reporter is a property that specifies the type of progress reporter to use during the process. The default is 'rich', but it can be changed to 'print' or 'none' using the --reporter flag.</data>
      <data key="d2">919cb44d9688a14bf48fa7c98163ed81</data>
    </node>
    <node id="TABLE OUTPUT FORMATS">
      <data key="d0">PROPERTY, FORMATS</data>
      <data key="d1">The Table Output Formats is a property that specifies the formats in which the pipeline should emit the table output. The default is 'parquet', but it can be changed to 'csv' or 'json' using the --emit flag, with formats separated by commas.</data>
      <data key="d2">919cb44d9688a14bf48fa7c98163ed81</data>
    </node>
    <node id="CACHING MECHANISM">
      <data key="d0">PROPERTY, MECHANISM</data>
      <data key="d1">The Caching Mechanism is a feature that can be disabled using the --nocache flag. This is useful for debugging and development but should not be used in production as it can affect performance.</data>
      <data key="d2">919cb44d9688a14bf48fa7c98163ed81</data>
    </node>
    <edge source="PYTHON 3.10-3.12" target="POETRY">
      <data key="d4">1.0</data>
      <data key="d5">Python 3.10-3.12 is the environment in which Poetry operates, providing the necessary runtime for executing Poetry commands and managing dependencies for GraphRAG</data>
      <data key="d6">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </edge>
    <edge source="PYTHON 3.10-3.12" target="GRAPHRAG SYSTEM">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG system is compatible with Python versions 3.10 to 3.12, which are the supported versions for running the system</data>
      <data key="d6">84d24b5db902baca7217b5e3bb6ec462</data>
    </edge>
    <edge source="POETRY" target="INDEXING ENGINE">
      <data key="d4">1.0</data>
      <data key="d5">Poetry is used to execute the Indexing Engine, which is a part of GraphRAG, by running the 'poe index' command</data>
      <data key="d6">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </edge>
    <edge source="POETRY" target="QUERY ENGINE">
      <data key="d4">1.0</data>
      <data key="d5">Poetry is used to execute the Query Engine, which is a part of GraphRAG, by running the 'poe query' command</data>
      <data key="d6">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </edge>
    <edge source="POETRY" target="LIFECYCLE SCRIPTS">
      <data key="d4">1.0</data>
      <data key="d5">Poetry is used to manage and execute the Lifecycle Scripts, which are essential for building, testing, and executing the GraphRAG package</data>
      <data key="d6">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </edge>
    <edge source="AZURITE" target="UNIT AND SMOKE TESTS">
      <data key="d4">1.0</data>
      <data key="d5">Azurite is used in unit and smoke tests to emulate Azure resources, providing a testing environment for GraphRAG</data>
      <data key="d6">4cf772ca8a1ffad729902e9b630e1ec0</data>
    </edge>
    <edge source="QUERY ENGINE" target="INDEXING PIPELINE">
      <data key="d4">1.0</data>
      <data key="d5">The Query Engine and the Indexing Pipeline are related as they are the two main components of the Graph RAG library, working together to process and retrieve information. The Indexing Pipeline prepares data for the Query Engine to perform searches and generate answers</data>
      <data key="d6">f8cf53ce98a8bc52581f7907ad98ef70</data>
    </edge>
    <edge source="QUERY ENGINE" target="LOCAL SEARCH">
      <data key="d4">1.0</data>
      <data key="d5">The Query Engine uses the Local Search method to generate answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents</data>
      <data key="d6">f8cf53ce98a8bc52581f7907ad98ef70</data>
    </edge>
    <edge source="QUERY ENGINE" target="GLOBAL SEARCH">
      <data key="d4">1.0</data>
      <data key="d5">The Query Engine uses the Global Search method to generate answers by searching over all AI-generated community reports in a map-reduce fashion. This method is resource-intensive but gives good responses for questions that require an understanding of the dataset as a whole</data>
      <data key="d6">f8cf53ce98a8bc52581f7907ad98ef70</data>
    </edge>
    <edge source="QUERY ENGINE" target="QUESTION GENERATION">
      <data key="d4">1.0</data>
      <data key="d5">The Query Engine uses the Question Generation functionality to take a list of user queries and generate the next candidate questions. This is useful for generating follow-up questions in a conversation or for generating a list of questions for the investigator to dive deeper into the dataset</data>
      <data key="d6">f8cf53ce98a8bc52581f7907ad98ef70</data>
    </edge>
    <edge source="QUERY ENGINE" target="CLI DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The CLI documentation is related to the Query Engine, as it provides guidance on how to use the command-line interface to run queries against the indexed data, including examples of different query methods.</data>
      <data key="d6">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE QUERY">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe query command, which runs the Query CLI for executing queries through the command line interface.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY BUILD">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry build command, which builds a wheel file and other distributable artifacts for the package.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE TEST">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe test command, which runs all tests for the package to ensure its functionality and integrity.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE TEST_UNIT">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe test_unit command, which runs unit tests to verify the correctness of individual units of code.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE TEST_INTEGRATION">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe test_integration command, which runs integration tests to verify the interaction between different parts of the system.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE TEST_SMOKE">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe test_smoke command, which runs smoke tests to ensure the system is in a stable state before more extensive testing.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE CHECK">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe check command, which performs a suite of static checks across the package, including formatting, documentation formatting, linting, security patterns, and type-checking.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE FIX">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe fix command, which applies any available auto-fixes to the package, typically limited to formatting fixes.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE FIX_UNSAFE">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe fix_unsafe command, which applies any available auto-fixes to the package, including those that may be unsafe.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="POETRY RUN POE FORMAT">
      <data key="d4">1.0</data>
      <data key="d5">The CLI is used to execute the poetry run poe format command, which explicitly runs the formatter across the package to ensure consistent code style and formatting.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="CLI" target="TROUBLESHOOTING">
      <data key="d4">1.0</data>
      <data key="d5">The CLI may be used in conjunction with the Troubleshooting section, which provides solutions to common problems encountered when using the software or executing commands.</data>
      <data key="d6">563caa38fe33c495449888d62950b959</data>
    </edge>
    <edge source="RUNTIMEERROR" target="LLVM_CONFIG">
      <data key="d4">1.0</data>
      <data key="d5">The RuntimeError encountered during the 'poetry install' process is related to the LLVM_CONFIG environment variable, as the error message suggests setting this variable to the path of the llvm-config executable. This relationship indicates that the error is due to a missing or incorrect configuration of the LLVM environment.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="RUNTIMEERROR" target="LLVM-9">
      <data key="d4">1.0</data>
      <data key="d5">The RuntimeError encountered during the 'poetry install' process is related to the llvm-9 software package, as the error message suggests installing llvm-9 and llvm-9-dev. This relationship indicates that the error is due to the absence of the required LLVM components.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="RUNTIMEERROR" target="LLVM-9-DEV">
      <data key="d4">1.0</data>
      <data key="d5">The RuntimeError encountered during the 'poetry install' process is related to the llvm-9-dev software package, as the error message suggests installing llvm-9 and llvm-9-dev. This relationship indicates that the error is due to the absence of the required LLVM development components.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="RUNTIMEERROR" target="POETRY INSTALL">
      <data key="d4">1.0</data>
      <data key="d5">The RuntimeError encountered during the 'poetry install' process is directly related to the execution of the 'poetry install' command. This relationship indicates that the error occurs as a result of running this command, which is intended to install project dependencies.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="RUNTIMEERROR" target="PYTHON.H">
      <data key="d4">1.0</data>
      <data key="d5">The RuntimeError encountered during the 'poetry install' process is related to the missing Python.h file, as indicated by the error message "numba/_pymodule.h:6:10: fatal error: Python.h: No such file or directory". This relationship indicates that the error is due to the absence of the required Python header file.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="RUNTIMEERROR" target="PYTHON3.10-DEV">
      <data key="d4">1.0</data>
      <data key="d5">The RuntimeError encountered during the 'poetry install' process is related to the python3.10-dev software package, as the error message suggests installing python3.10-dev to resolve the issue with the missing Python.h file. This relationship indicates that the error is due to the absence of the required Python development components.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="GRAPHRAG_LLM_THREAD_COUNT" target="GRAPHRAG_EMBEDDING_THREAD_COUNT">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_LLM_THREAD_COUNT and GRAPHRAG_EMBEDDING_THREAD_COUNT environment variables are related to each other as they both determine the level of concurrency in the GraphRAG system. Modifying these values can help reduce the concurrency and improve system stability, as they are both set to 50 by default, which can lead to high concurrency and potential performance issues.</data>
      <data key="d6">0dc1f5e4f8fb5903f12acf8e141fb205</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_THREAD_COUNT" target="GRAPHRAG_EMBEDDING_THREAD_STAGGER">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_EMBEDDING_THREAD_STAGGER setting is related to the GRAPHRAG_EMBEDDING_THREAD_COUNT as it determines the delay between starting threads, which can affect the efficiency and resource usage when multiple threads are specified</data>
      <data key="d6">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_THREAD_COUNT" target="GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS setting is related to the GRAPHRAG_EMBEDDING_THREAD_COUNT as it specifies the number of concurrent requests that can be processed, which can be influenced by the number of threads available for processing</data>
      <data key="d6">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </edge>
    <edge source="GRAPHRAG" target="GPT-4 TURBO">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG uses GPT-4 Turbo to generate knowledge graphs from text data. The model's ability to create detailed and structured representations of information is essential for the GraphRAG process.</data>
      <data key="d6">e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="MICROSOFT RESEARCH BLOG POST">
      <data key="d4">1.0</data>
      <data key="d5">The Microsoft Research Blog Post provides information about GraphRAG and its applications. It is a resource for users who want to learn more about how GraphRAG can enhance the ability of LLMs to reason about private data.</data>
      <data key="d6">e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="GITHUB REPOSITORY">
      <data key="d4">1.0</data>
      <data key="d5">The GitHub Repository is a resource for developers who want to access, contribute to, and collaborate on the GraphRAG project. It contains the source code and documentation for GraphRAG.</data>
      <data key="d6">e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="GRAPHRAG ARXIV">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG Arxiv is a repository for research papers and documentation related to GraphRAG. It is a resource for users who want to learn more about the theory, implementation, and applications of GraphRAG.</data>
      <data key="d6">e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="SOLUTION ACCELERATOR">
      <data key="d4">1.0</data>
      <data key="d5">The Solution Accelerator is a package that provides a user-friendly end-to-end experience with Azure resources for quickstarting the GraphRAG system. It is recommended for users who want to start using GraphRAG with ease.</data>
      <data key="d6">e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="GET STARTED GUIDE">
      <data key="d4">2.0</data>
      <data key="d5">The Get Started Guide, a comprehensive resource meticulously crafted by GraphRAG, serves as an invaluable starting point for users looking to familiarize themselves with the software's rich array of features and functionalities. This guide is specifically designed to cater to newcomers, offering clear, step-by-step instructions that facilitate a smooth onboarding process and enable users to harness the full potential of GraphRAG from the get-go. By providing detailed guidance on how to begin using the system effectively, the Get Started Guide ensures that users can quickly and confidently navigate through GraphRAG's sophisticated interface, maximizing their productivity and leveraging the software's capabilities to meet their specific needs.</data>
      <data key="d6">32603b739bed06b4695b0cc3915b2c4b,e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="INDEXER">
      <data key="d4">2.0</data>
      <data key="d5">The Indexer, a critical sub-system of GraphRAG, specializes in processing raw text to construct a knowledge graph, thereby facilitating the creation of a structured information representation. This subsystem is pivotal in enhancing GraphRAG's efficiency in data organization and retrieval, working in tandem with the main software to optimize question-and-answer capabilities. Through its sophisticated mechanisms, the Indexer significantly contributes to GraphRAG's overall performance, making it an indispensable component in the software's functionality.</data>
      <data key="d6">32603b739bed06b4695b0cc3915b2c4b,e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="QUERY PACKAGE">
      <data key="d4">2.0</data>
      <data key="d5">The Query package, a vital subsystem of GraphRAG, serves as a powerful tool for users to interact with the indexed data and conduct searches within the knowledge graph. This feature facilitates the retrieval of relevant information, making it an essential component for the software's functionality and user engagement. As part of GraphRAG, the Query package ensures that users can efficiently and effectively access the data they need, enhancing the overall utility of the system.</data>
      <data key="d6">32603b739bed06b4695b0cc3915b2c4b,e6fa3bdaf65c92df6b3430f02804321a</data>
    </edge>
    <edge source="GRAPHRAG" target="SOLUTION ACCELERATOR PACKAGE">
      <data key="d4">1.0</data>
      <data key="d5">The Solution Accelerator package includes GraphRAG, which is a component that enhances the user experience by providing advanced RAG capabilities for better handling of complex information and reasoning about private datasets.</data>
      <data key="d6">32603b739bed06b4695b0cc3915b2c4b</data>
    </edge>
    <edge source="GRAPHRAG" target="BASELINE RAG">
      <data key="d4">2.0</data>
      <data key="d5">GraphRAG, an evolved iteration of Baseline RAG, is meticulously designed to surmount the constraints of its predecessor, especially in managing intricate data and reasoning over private datasets. This advanced system showcases significant enhancements in question-and-answer efficiency and reasoning skills, excelling in connecting disjointed information fragments and grasping summarized semantic ideas across extensive data repositories. GraphRAG's superior performance and mastery in these domains clearly outshine those of Baseline RAG, positioning it as a more capable and intelligent solution for handling complex information landscapes.</data>
      <data key="d6">32603b739bed06b4695b0cc3915b2c4b,d441b136505c273cf3577b6867e872e4</data>
    </edge>
    <edge source="GRAPHRAG" target="LLMS">
      <data key="d4">1.0</data>
      <data key="d5">LLMs are used in GraphRAG to create a knowledge graph from an input corpus. The LLMs extract entities, relationships, and key claims from the TextUnits, which are then used to build the graph and enhance the query responses in GraphRAG.</data>
      <data key="d6">d441b136505c273cf3577b6867e872e4</data>
    </edge>
    <edge source="GRAPHRAG" target="LEIDEN TECHNIQUE">
      <data key="d4">2.0</data>
      <data key="d5">GraphRAG, a sophisticated tool for social network analysis, utilizes the Leiden technique for hierarchical clustering of the graph. This method significantly enhances the visualization and analysis of entities and their communities within the graph. By employing the Leiden technique, GraphRAG is able to effectively represent the clustering of entities and relationships, which is a critical step in the process of understanding the structure and dynamics of specialized professional networks. This approach enables users to identify key influencers, collaboration opportunities, and knowledge gaps in the field, particularly in the Motor Control and Drive Systems domain.</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7,d441b136505c273cf3577b6867e872e4</data>
    </edge>
    <edge source="GRAPHRAG" target="TEXTUNITS">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG process involves slicing the input corpus into TextUnits, which are used as analyzable units for entity and relationship extraction.</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7</data>
    </edge>
    <edge source="GRAPHRAG" target="LLM">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG utilizes LLM for entity, relationship, and key claim extraction from the TextUnits.</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7</data>
    </edge>
    <edge source="GRAPHRAG" target="COMMUNITY SUMMARIES">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG generates Community Summaries from the bottom-up to provide a holistic understanding of each community and its constituents.</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7</data>
    </edge>
    <edge source="GRAPHRAG" target="GLOBAL SEARCH">
      <data key="d4">2.0</data>
      <data key="d5">Global Search is a sophisticated query mode integrated within the GraphRAG process, designed to facilitate reasoning and answer holistic questions concerning the entire corpus. By leveraging the community summaries and the structure of the LLM-generated knowledge graph, Global Search enables the identification of themes and semantic clusters within the dataset. This capability significantly enhances the aggregation of information, making the process more effective and efficient. GraphRAG, in this context, serves as the backbone, creating a knowledge graph that Global Search utilizes to uncover deeper insights and connections within the data.</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7,3e143a60e2aeb57eb418a68d1484bbb3</data>
    </edge>
    <edge source="GRAPHRAG" target="LOCAL SEARCH">
      <data key="d4">2.0</data>
      <data key="d5">Local Search, a feature within the GraphRAG (Graph Representation and Analysis of Graphs) process, is a query mode designed to facilitate reasoning about specific entities. This is achieved by exploring the immediate network around the entity, fanning out to its neighbors and associated concepts within the graph. GraphRAG, as a tool, enables users to analyze and understand the relationships and connections between entities in a graph database, making Local Search a powerful technique for uncovering insights and knowledge hidden within the data. By leveraging Local Search, users can efficiently investigate the context and connections of particular entities, enhancing their understanding of the graph's structure and dynamics.</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7,849698743b07680402ff8572b1c6c469</data>
    </edge>
    <edge source="GRAPHRAG" target="PROMPT TUNING">
      <data key="d4">2.0</data>
      <data key="d5">Prompt Tuning is a specialized technique recommended for optimizing the performance of GraphRAG, particularly when working with specific data sets. This method involves fine-tuning prompts in accordance with the documentation to better suit the data and tasks at hand, ensuring the best possible results are achieved. By leveraging Prompt Tuning, users can enhance the effectiveness of GraphRAG in various applications, making it a valuable strategy for maximizing the system's potential.</data>
      <data key="d6">369b39fdfd649d6df32a5d7b4cc559b7,849698743b07680402ff8572b1c6c469</data>
    </edge>
    <edge source="GRAPHRAG" target="LLM-GENERATED KNOWLEDGE GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG utilizes the LLM-generated knowledge graph to organize data into semantic clusters and summarize themes, enabling it to respond to user queries with relevant information</data>
      <data key="d6">812b3414c467da0b62f7932d2adcbad4</data>
    </edge>
    <edge source="GRAPHRAG" target="GLOBAL SEARCH METHOD">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG employs the global search method to process user queries, using LLM-generated community reports as context data to generate intermediate and final responses</data>
      <data key="d6">812b3414c467da0b62f7932d2adcbad4</data>
    </edge>
    <edge source="GRAPHRAG" target="EVALUATION APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG's performance has been evaluated using an approach that has limitations, as it has only examined a certain class of sensemaking questions for two corpora in the region of 1 million tokens</data>
      <data key="d6">e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="GRAPHRAG" target="TEMPLATE GENERATION ALGORITHM">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG utilizes the Template Generation Algorithm to create domain adaptive templates for the generation of knowledge graphs. The algorithm is a core component of GraphRAG, enabling it to adapt to different domains and improve the results of Index Runs by generating customized prompts based on the input data and specified parameters.</data>
      <data key="d6">9b364093aeecfc789c70fc5bd9503487</data>
    </edge>
    <edge source="GRAPHRAG" target="INITIALIZATION PROCESS">
      <data key="d4">1.0</data>
      <data key="d5">The Initialization Process is a prerequisite for using GraphRAG effectively. Before running the automatic template generation, the workspace must be initialized using the graphrag.index --init command. This process creates the necessary configuration files and default prompts, allowing GraphRAG to function properly and ensuring that the tool is ready for use.</data>
      <data key="d6">9b364093aeecfc789c70fc5bd9503487</data>
    </edge>
    <edge source="GRAPHRAG" target=".ENV">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG uses the .env file to store environment variables that are referenced in the settings.yaml file, essential for the proper configuration and operation of the software.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="GRAPHRAG" target="SETTINGS.YAML">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG uses the settings.yaml file to store configuration settings necessary for its operation. This file is created during the initialization process and is referenced by the software.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="GRAPHRAG" target="PROMPTS/">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG uses the prompts/ directory to store default prompts that can be modified or new ones generated through the Auto Prompt Tuning command. These prompts are essential for the software's operations.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="INDEXER" target="COMMUNITY_LEVEL">
      <data key="d4">1.0</data>
      <data key="d5">The Indexer uses the community_level parameter to determine which level of community reports to load from the Leiden community hierarchy, affecting the granularity of the data processed.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="RESPONSE_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The Indexer's output format and type are influenced by the response_type parameter, which dictates the structure of the generated reports.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The method parameter influences how the Indexer answers queries, either using a local or global approach, impacting the scope and methodology of the analysis.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GRAPHRAG_API_KEY">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_API_KEY environment variable is required for the Indexer to execute, providing the necessary API Key for model execution.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GRAPHRAG_LLM_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_LLM_MODEL environment variable specifies the model the Indexer uses for Chat Completions, affecting the conversational capabilities of the tool.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GRAPHRAG_EMBEDDING_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_EMBEDDING_MODEL environment variable determines the model the Indexer uses for Embeddings, impacting the representation and processing of data.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GRAPHRAG_LLM_API_BASE">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_LLM_API_BASE environment variable can be set to customize the API Base URL for the Indexer's LLM operation, allowing for flexibility in deployment environments.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GRAPHRAG_LLM_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_LLM_TYPE environment variable defines the LLM operation type used by the Indexer, influencing the method of interaction with the language model.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GRAPHRAG_LLM_MAX_RETRIES">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_LLM_MAX_RETRIES environment variable sets the maximum number of retries the Indexer will attempt when a request fails, affecting the tool's resilience and reliability.</data>
      <data key="d6">8c70a7321fb0e945054d226a8c69abee</data>
    </edge>
    <edge source="INDEXER" target="GLOBAL SEARCH">
      <data key="d4">1.0</data>
      <data key="d5">The Indexer is a prerequisite for using the Global Search method, as it processes the data to make it searchable and ready for high-level questions</data>
      <data key="d6">ae6e91a8cc5773dbd4789773c9ef5a30</data>
    </edge>
    <edge source="INDEXER" target="LOCAL SEARCH">
      <data key="d4">1.0</data>
      <data key="d5">The Indexer is a prerequisite for using the Local Search method, as it processes the data to make it searchable and ready for specific questions about particular aspects or entities within the dataset</data>
      <data key="d6">ae6e91a8cc5773dbd4789773c9ef5a30</data>
    </edge>
    <edge source="LLMS" target="ENTITY GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">LLMs are capable of understanding the rich descriptive text associated with homogeneous nodes in the entity graph, which is crucial for global, query-focused summarization</data>
      <data key="d6">6dace8e490674ac8e031aed987a63789</data>
    </edge>
    <edge source="LLMS" target="RAGAS">
      <data key="d4">1.0</data>
      <data key="d5">LLMs are used by RAGAS to evaluate the performance of conventional RAG systems, automatically assessing qualities such as context relevance, faithfulness, and answer relevance.</data>
      <data key="d6">53455f8552b0787cb13c5a03eb550842</data>
    </edge>
    <edge source="LLMS" target="GRAPH RAG">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG and LLMs are connected through the use of large language models in the creation and analysis of graph indexes for advanced information retrieval.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="LLMS" target="KNOWLEDGE GRAPH CREATION">
      <data key="d4">1.0</data>
      <data key="d5">LLMs are used in the creation of knowledge graphs, as described in the research study by Trajanoska et al., indicating a direct connection between LLMs and knowledge graph creation techniques.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="LLM" target="GLOBAL SEARCH">
      <data key="d4">1.0</data>
      <data key="d5">The LLM is used in Global Search to summarize themes identified in the dataset's semantic clusters, responding to user queries that require understanding of the entire dataset</data>
      <data key="d6">3e143a60e2aeb57eb418a68d1484bbb3</data>
    </edge>
    <edge source="LLM" target="GLOBALSEARCH CLASS">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class uses the LLM for response generation, following the map-reduce process.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="LLM" target="PARALLELIZATION">
      <data key="d4">1.0</data>
      <data key="d5">llm and parallelization are related because the language model's configuration can influence how tasks are distributed across multiple processors or threads, affecting the overall performance and efficiency of the system.</data>
      <data key="d6">d27237468a1b9e89110eeeca8080f63c</data>
    </edge>
    <edge source="LLM" target="ASYNC_MODE">
      <data key="d4">1.0</data>
      <data key="d5">llm and async_mode are related because the language model's operation in asynchronous mode can impact how the model processes tasks concurrently, potentially affecting the throughput and response time of the system.</data>
      <data key="d6">d27237468a1b9e89110eeeca8080f63c</data>
    </edge>
    <edge source="LLM" target="ENTITY EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the LLM property, specifying the language model to use for text analysis</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="LLM" target="SUMMARIZE_DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The llm setting is used in the summarize_descriptions process, specifying the language model to be used for generating summaries</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="LLM" target="CLAIM_EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The llm setting is used in the claim_extraction process, specifying the language model to be used for identifying claims</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="LLM" target="COMMUNITY_REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The llm setting is used in the community_reports process, specifying the language model to be used for generating reports</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="LLM" target="ENTITY RESOLUTION">
      <data key="d4">1.0</data>
      <data key="d5">LLM is used in the process of Entity Resolution to determine which entities should be merged based on their representation of the same real-world entity but with different names. The current implementation is destructive, but future implementations aim to be non-destructive.</data>
      <data key="d6">d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </edge>
    <edge source="LLM" target="CLAIM EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">LLM is used in the Claim Extraction process to identify positive factual statements with an evaluated status and time-bounds from source TextUnits. These claims are emitted as Covariates.</data>
      <data key="d6">d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="CORPUS">
      <data key="d4">1.0</data>
      <data key="d5">The corpus can be summarized or analyzed to create community summaries, which provide condensed versions of the information contained in the corpus</data>
      <data key="d6">849698743b07680402ff8572b1c6c469</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="LOCAL TO GLOBAL GRAPH RAG APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">The Local to Global Graph RAG Approach utilizes Community Summaries to generate partial responses to a given question. These summaries are pre-generated for groups of closely-related entities and are combined into a final response.</data>
      <data key="d6">f76c18c7582167c3626f8741c2c9374f</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="ENTITY KNOWLEDGE GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The entity knowledge graph is used to generate community summaries for groups of closely-related entities, which are then used to generate partial responses to questions</data>
      <data key="d6">b149708d0b4ac3ff417565739ea6b03b</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="GRAPHCOMMUNITIES">
      <data key="d4">1.0</data>
      <data key="d5">Community Summaries are created for each community in the Leiden hierarchy, providing a detailed report of the structure and semantics of the dataset</data>
      <data key="d6">a660289d2bf43f25d3524d35cd2d9a96</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="GLOBAL QUERIES">
      <data key="d4">1.0</data>
      <data key="d5">Community summaries are used to answer global queries by providing structured information that can be searched or analyzed to address the broader themes or questions posed by the user</data>
      <data key="d6">93d4d4effbf989e6ef1c4c3b4f42494e</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="USER QUERY">
      <data key="d4">1.0</data>
      <data key="d5">A user query can be answered using community summaries, as the summaries provide relevant information that can be matched to the query to provide detailed or thematic responses</data>
      <data key="d6">93d4d4effbf989e6ef1c4c3b4f42494e</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="PODCAST DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The Podcast Dataset's graph structure influences the number of community summaries at different levels of the graph community hierarchy</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="NEWS DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The News Dataset's graph structure influences the number of community summaries at different levels of the graph community hierarchy</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="SOURCE TEXTS">
      <data key="d4">1.0</data>
      <data key="d5">Community Summaries are derived from Source Texts, representing a condensed version of the original content. The relationship indicates that summaries are created to provide a more concise representation of the source material.</data>
      <data key="d6">71f14506a6b15dfabd93fd1606a67b73</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="GRAPH RAG">
      <data key="d4">2.0</data>
      <data key="d5">Graph RAG, a sophisticated retrieval-augmented generation model, integrates community summaries as a critical component of its self-memory system. This integration allows Graph RAG to leverage the rich context and information contained within community summaries, significantly enhancing its retrieval capabilities. The relationship between Graph RAG and community summaries is characterized by a high strength, as the summaries are directly utilized in the question answering process. This direct use of community summaries in Graph RAG's methodology leads to notable improvements in the comprehensiveness and diversity of answers generated, making community summaries an indispensable element in the model's operation.</data>
      <data key="d6">71f14506a6b15dfabd93fd1606a67b73,7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="GRAPH INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The decision to invest in building a graph index depends on the value obtained from other aspects of the graph index, including the generic community summaries</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="COMMUNITY SUMMARIES" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the consideration of the generic community summaries</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GLOBAL SEARCH" target="QUERY ENGINE DOCS">
      <data key="d4">1.0</data>
      <data key="d5">The Global Search method is described and explained in the Query Engine Docs, which provide detailed information on how to use this search technique effectively to ask high-level questions about the dataset</data>
      <data key="d6">ae6e91a8cc5773dbd4789773c9ef5a30</data>
    </edge>
    <edge source="LOCAL SEARCH" target="QUESTION GENERATION">
      <data key="d4">2.0</data>
      <data key="d5">Question Generation and Local Search are closely intertwined functionalities within the realm of information retrieval and processing. Question Generation leverages the context-building approach synonymous with Local Search to sift through and prioritize pertinent data. Both methodologies are instrumental in generating and addressing queries, utilizing input documents and structured data derived from a knowledge graph. This synergy enables a comprehensive understanding and response to inquiries, making these entities indispensable in the domain of data analysis and information extraction.</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4,364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="LOCAL SEARCH" target="ENTITY-BASED REASONING">
      <data key="d4">1.0</data>
      <data key="d5">Local Search is related to Entity-based Reasoning as Entity-based Reasoning is an approach used within the Local Search method to reason about information based on entities and their relationships</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="LOCAL SEARCH" target="QUESTION GENERATION FUNCTION">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search method uses the Question Generation Function to generate context for the search, which is then used to find semantically similar content within the dataset</data>
      <data key="d6">3e143a60e2aeb57eb418a68d1484bbb3</data>
    </edge>
    <edge source="LOCAL SEARCH" target="QUERY ENGINE DOCS">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search method is described and explained in the Query Engine Docs, which provide detailed information on how to use this search technique effectively to ask specific questions about particular aspects or entities within the dataset</data>
      <data key="d6">ae6e91a8cc5773dbd4789773c9ef5a30</data>
    </edge>
    <edge source="PROMPT TUNING" target="AUTO TEMPLATING">
      <data key="d4">1.0</data>
      <data key="d5">Prompt Tuning includes the Auto Templating feature, which is a method for creating domain adaptive templates for the generation of the knowledge graph. This relationship indicates that Auto Templating is a part of the broader Prompt Tuning capabilities</data>
      <data key="d6">bdb8f9e797229f596744d9636ab857b0</data>
    </edge>
    <edge source="PROMPT TUNING" target="MANUAL CONFIGURATION">
      <data key="d4">1.0</data>
      <data key="d5">Prompt Tuning includes the option for Manual Configuration, which allows for advanced customization of the prompts and templates used in the generation of the knowledge graph. This relationship indicates that Manual Configuration is an alternative method within the Prompt Tuning feature set</data>
      <data key="d6">bdb8f9e797229f596744d9636ab857b0</data>
    </edge>
    <edge source="PROMPT TUNING" target="GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE is a configuration variable that can be modified during Prompt Tuning to customize the entity extraction process</data>
      <data key="d6">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </edge>
    <edge source="PROMPT TUNING" target="GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE is a configuration variable that can be modified during Prompt Tuning to customize the community report generation process</data>
      <data key="d6">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </edge>
    <edge source="PROMPT TUNING" target="GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE is a configuration variable that can be modified during Prompt Tuning to customize the summarization of descriptions process</data>
      <data key="d6">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </edge>
    <edge source="PROMPT TUNING" target="GRAPHRAG INDEXER">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Indexer allows for the customization of its default prompts through the process of Prompt Tuning, enabling better alignment with specific use cases in knowledge discovery</data>
      <data key="d6">6a7157695d90d434b2625c3f05420916</data>
    </edge>
    <edge source="PROMPT TUNING" target="CUSTOM PROMPT FILE">
      <data key="d4">1.0</data>
      <data key="d5">A Custom Prompt File is created as part of the Prompt Tuning process to override default prompts, allowing for the customization of the GraphRAG indexer's behavior</data>
      <data key="d6">6a7157695d90d434b2625c3f05420916</data>
    </edge>
    <edge source="INDEXING PIPELINE" target="CLI DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The CLI documentation is related to the Indexing pipeline, as it explains how to use the command-line interface to run the indexing process, including the specific Python script and parameters required.</data>
      <data key="d6">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </edge>
    <edge source="INDEXING PIPELINE" target="PROMPT TUNING COMMAND">
      <data key="d4">1.0</data>
      <data key="d5">The Prompt Tuning command can be run before or after the Indexing Pipeline, as it adapts the prompts to better fit the data being indexed</data>
      <data key="d6">d0f7c236538005bc3056b7daed2401d8</data>
    </edge>
    <edge source="INDEXING PIPELINE" target="CONFIGURATION DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The Indexing Pipeline is a process that can be run after configuring GraphRAG as detailed in the Configuration documentation, allowing for the indexing of data for use with the system.</data>
      <data key="d6">32e96c66a531ecd0a8edc7414aec0803</data>
    </edge>
    <edge source="QUESTION GENERATION" target="GLOBAL SEARCH DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The Global Search documentation is related to Question Generation as it provides more information about the functionality that Question Generation is a part of, which is searching and retrieving information across various sources</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="QUESTION GENERATION" target="STRUCTURED DATA">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation combines structured data from a knowledge graph to generate candidate questions related to specific entities</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="UNSTRUCTURED DATA">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation combines unstructured data from input documents to generate candidate questions related to specific entities</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="KNOWLEDGE GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation uses structured data from the knowledge graph to generate candidate questions</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="OPENAI MODEL">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation uses an OpenAI model for response generation</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="CONTEXT BUILDER">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation uses a context builder to prepare context data from collections of knowledge model objects</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="SYSTEM PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation uses a system prompt template to generate candidate questions</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="LLM PARAMETERS">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation uses LLM parameters to customize the behavior of the LLM call</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="QUESTION GENERATION" target="CONTEXT BUILDER PARAMETERS">
      <data key="d4">1.0</data>
      <data key="d5">Question Generation uses context builder parameters to customize the context-building process</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="ENTITY-BASED REASONING" target="EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Entity-based Reasoning is related to Embedding as Embedding is a technique used in the Entity-based Reasoning approach to represent entities and text in a numerical format that can be processed by machine learning models</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="ENTITY-TEXT UNIT MAPPING" target="RANKING + FILTERING">
      <data key="d4">1.0</data>
      <data key="d5">Entity-Text Unit Mapping is related to Ranking + Filtering as Ranking + Filtering is a technique used after Entity-Text Unit Mapping to prioritize and select the most relevant text units based on their relevance to a user query</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="RANKING + FILTERING" target="ENTITY-REPORT MAPPING">
      <data key="d4">1.0</data>
      <data key="d5">Entity-Report Mapping is related to Ranking + Filtering as Ranking + Filtering is a technique used after Entity-Report Mapping to prioritize and select the most relevant community reports based on their relevance to a user query</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="RANKING + FILTERING" target="ENTITY-ENTITY RELATIONSHIPS">
      <data key="d4">1.0</data>
      <data key="d5">Entity-Entity Relationships are related to Ranking + Filtering as Ranking + Filtering is a technique used to prioritize and select the most relevant entity relationships based on their relevance to a user query</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="RANKING + FILTERING" target="ENTITY-COVARIATE MAPPINGS">
      <data key="d4">1.0</data>
      <data key="d5">Entity-Covariate Mappings are related to Ranking + Filtering as Ranking + Filtering is a technique used to prioritize and select the most relevant covariates based on their relevance to a user query</data>
      <data key="d6">364624242a84e1859e758069d914d8c8</data>
    </edge>
    <edge source="USER QUERY" target="LOCAL SEARCH DATAFLOW">
      <data key="d4">1.0</data>
      <data key="d5">The User Query initiates the Local Search Dataflow, guiding the search process and influencing the prioritization of entities, relationships, and covariates</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="USER QUERY" target="GLOBAL SEARCH METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The global search method is triggered by a user query, which serves as the input for generating responses based on the LLM-generated knowledge graph and semantic clusters</data>
      <data key="d6">812b3414c467da0b62f7932d2adcbad4</data>
    </edge>
    <edge source="CONVERSATION HISTORY" target="LOCAL SEARCH DATAFLOW">
      <data key="d4">1.0</data>
      <data key="d5">The Conversation History provides context for the Local Search Dataflow, influencing the prioritization of entities, relationships, and covariates based on previous interactions</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="CONVERSATION HISTORY" target="GLOBAL SEARCH METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The global search method can take conversation history as additional input to provide more context for generating informed and relevant responses to user queries</data>
      <data key="d6">812b3414c467da0b62f7932d2adcbad4</data>
    </edge>
    <edge source="LOCAL SEARCH DATAFLOW" target="PRIORITIZED TEXT UNITS">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search Dataflow extracts and prioritizes the Prioritized Text Units, selecting relevant text chunks from the input documents</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="LOCAL SEARCH DATAFLOW" target="PRIORITIZED COMMUNITY REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search Dataflow extracts and prioritizes the Prioritized Community Reports, selecting relevant reports from the knowledge graph</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="LOCAL SEARCH DATAFLOW" target="PRIORITIZED ENTITIES">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search Dataflow identifies and prioritizes the Prioritized Entities, serving as access points into the knowledge graph</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="LOCAL SEARCH DATAFLOW" target="PRIORITIZED RELATIONSHIPS">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search Dataflow identifies and prioritizes the Prioritized Relationships, providing context about the connections between entities</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="LOCAL SEARCH DATAFLOW" target="PRIORITIZED COVARIATES">
      <data key="d4">1.0</data>
      <data key="d5">The Local Search Dataflow identifies and prioritizes the Prioritized Covariates, providing additional details about the entities and their attributes</data>
      <data key="d6">3a0742c280217fe600b9af2d06b58eea</data>
    </edge>
    <edge source="LOCALSEARCH CLASS" target="QUESTION GENERATION METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The LocalSearch class is related to the Question Generation method as it uses a similar context-building approach to extract and prioritize relevant information for generating questions related to specific entities</data>
      <data key="d6">25797740f434cc2bf16365fc498791f6</data>
    </edge>
    <edge source="LOCALSEARCH CLASS" target="CONFIGURATION PARAMETERS">
      <data key="d4">1.0</data>
      <data key="d5">The Configuration Parameters are essential for the LocalSearch class as they define how the class operates, including the model, context builder, and various settings that affect the search and response generation process</data>
      <data key="d6">25797740f434cc2bf16365fc498791f6</data>
    </edge>
    <edge source="LLM COMPLETION STREAMING EVENTS" target="CALLBACKS">
      <data key="d4">1.0</data>
      <data key="d5">Callbacks can be used to handle LLM Completion Streaming Events, providing custom processing or reactions to the information generated by the LLM during the completion of tasks or queries</data>
      <data key="d6">1415949832ba3fee570ea961998a8ac4</data>
    </edge>
    <edge source="CONTEXT BUILDER" target="GLOBALSEARCH CLASS">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class utilizes the Context Builder to prepare context data from community reports for the map-reduce process.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="CALLBACKS" target="LLM CALL">
      <data key="d4">1.0</data>
      <data key="d5">The LLM call can trigger callbacks, which are optional functions that handle custom events, such as completion streaming events, for real-time processing or logging.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="FEW-SHOT EXAMPLES">
      <data key="d4">1.0</data>
      <data key="d5">The LLM is trained using few-shot examples specialized to various domains, which helps it to understand and perform tasks in those specific fields</data>
      <data key="d6">805a07a8f9c2ed5da2d9a61356aafa77</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="SECONDARY EXTRACTION PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The LLM uses the secondary extraction prompt to gather additional information or covariates associated with the extracted node instances, enhancing the detail and context of the extracted data</data>
      <data key="d6">805a07a8f9c2ed5da2d9a61356aafa77</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="GLEANING PROCESS">
      <data key="d4">1.0</data>
      <data key="d5">The LLM employs the gleaning process to ensure that all entities are detected, improving the completeness and accuracy of the data extraction</data>
      <data key="d6">805a07a8f9c2ed5da2d9a61356aafa77</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="ELEMENT SUMMARIES">
      <data key="d4">1.0</data>
      <data key="d5">The LLM generates element summaries, which are descriptions of entities, relationships, and claims represented in source texts, providing meaningful and concise information</data>
      <data key="d6">805a07a8f9c2ed5da2d9a61356aafa77</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="SUMMARIZATION QUERIES">
      <data key="d4">1.0</data>
      <data key="d5">LLM is used to generate summarization queries that require understanding of the entire corpus, aiding in the evaluation of RAG systems for data sensemaking tasks</data>
      <data key="d6">8e69f04648f5fc24c299591365f1aa68</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="ACTIVITY-CENTERED APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">The Activity-Centered Approach utilizes the LLM to identify users, tasks, and generate questions for evaluation</data>
      <data key="d6">a739018eb63cbb6c26b779bd37afc233</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="EVALUATION DATASETS">
      <data key="d4">1.0</data>
      <data key="d5">The LLM generates questions for the Evaluation Datasets, resulting in 125 test questions per dataset when N = 5</data>
      <data key="d6">a739018eb63cbb6c26b779bd37afc233</data>
    </edge>
    <edge source="LLM (LANGUAGE MODEL)" target="COMMUNITY REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">Community Reports are generated using the LLM (Language Model) to summarize the distinct information within each community, offering insights from high-level to low-level perspectives</data>
      <data key="d6">5b2968b8f1c891d47ecbe641c3391663</data>
    </edge>
    <edge source="GLOBAL SEARCH METHOD" target="AGGREGATED INTERMEDIATE RESPONSES">
      <data key="d4">1.0</data>
      <data key="d5">The global search method produces aggregated intermediate responses by filtering and selecting the most important points from intermediate responses, which are then used to generate the final response</data>
      <data key="d6">812b3414c467da0b62f7932d2adcbad4</data>
    </edge>
    <edge source="GRAPH'S COMMUNITY HIERARCHY" target="COMMUNITY REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The Graph's Community Hierarchy influences the level of detail in Community Reports, which are segmented into text chunks during the map step of the map-reduce process.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="MAP-REDUCE PROCESS" target="GLOBALSEARCH CLASS">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class implements the Map-Reduce Process for generating responses from community reports.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="COMMUNITY REPORTS" target="INTERMEDIATE RESPONSE">
      <data key="d4">1.0</data>
      <data key="d5">Community Reports are segmented into text chunks, which are used to produce an Intermediate Response containing points rated for importance.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="COMMUNITY REPORTS" target="DEFAULT PROMPTS">
      <data key="d4">1.0</data>
      <data key="d5">Default Prompts include the function of generating Community Reports, which provide insights and summaries based on the input data. This relationship indicates that Community Reports are one of the outputs provided by the Default Prompts</data>
      <data key="d6">bdb8f9e797229f596744d9636ab857b0</data>
    </edge>
    <edge source="COMMUNITY REPORTS" target="CONFIGURATION DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The Configuration Documentation is related to Community Reports as it provides guidelines on how to configure the settings that affect the generation of these reports</data>
      <data key="d6">21cdf11c58927ae505d3d375d1b75c82</data>
    </edge>
    <edge source="COMMUNITY REPORTS" target="PROMPT SOURCE">
      <data key="d4">1.0</data>
      <data key="d5">The Prompt Source is related to Community Reports as it is the origin of the input text that is used to generate the reports</data>
      <data key="d6">21cdf11c58927ae505d3d375d1b75c82</data>
    </edge>
    <edge source="COMMUNITY REPORTS" target="TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">Tokens are related to Community Reports as they contain the data, such as the input text, that is analyzed and presented in the reports</data>
      <data key="d6">21cdf11c58927ae505d3d375d1b75c82</data>
    </edge>
    <edge source="COMMUNITY REPORTS" target="DOCUMENT PROCESSING">
      <data key="d4">1.0</data>
      <data key="d5">Community Reports are processed in the Document Processing phase, where they are augmented, linked to TextUnits, and their embeddings are averaged to create the Documents table</data>
      <data key="d6">3e292d936b7efa377ba9530456cfd888</data>
    </edge>
    <edge source="INTERMEDIATE RESPONSE" target="FINAL RESPONSE">
      <data key="d4">1.0</data>
      <data key="d5">The Intermediate Response, containing points rated for importance, is aggregated during the reduce step to generate the Final Response.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="GLOBALSEARCH CLASS" target="MAP SYSTEM PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class uses the Map System Prompt as a template for the map step of the map-reduce process.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="GLOBALSEARCH CLASS" target="REDUCE SYSTEM PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class uses the Reduce System Prompt as a template for the reduce step of the map-reduce process.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="GLOBALSEARCH CLASS" target="RESPONSE TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class includes the Response Type as a parameter to specify the format of the final response.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="GLOBALSEARCH CLASS" target="ALLOW GENERAL KNOWLEDGE">
      <data key="d4">1.0</data>
      <data key="d5">The GlobalSearch Class includes the Allow General Knowledge setting to control whether general knowledge is included in the final response.</data>
      <data key="d6">1a4bca0786d529c91073997b63412adc</data>
    </edge>
    <edge source="MAP_SYSTEM_PROMPT" target="REDUCE_SYSTEM_PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The map_system_prompt and reduce_system_prompt are related as they both serve as templates guiding the processing of data in the map and reduce stages respectively</data>
      <data key="d6">e442fbb7a67e97ebc4de131b25c639e1</data>
    </edge>
    <edge source="REDUCE_SYSTEM_PROMPT" target="RESPONSE_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The response_type is related to the reduce_system_prompt as it influences the structure and format of the output generated during the reduce stage</data>
      <data key="d6">e442fbb7a67e97ebc4de131b25c639e1</data>
    </edge>
    <edge source="RESPONSE_TYPE" target="GRAPHRAG QUERY CLI">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG query CLI's output is customized by response_type, which dictates the format and type of response generated by the query engine.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="ALLOW_GENERAL_KNOWLEDGE" target="GENERAL_KNOWLEDGE_INCLUSION_PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The allow_general_knowledge setting is related to the general_knowledge_inclusion_prompt as enabling the former triggers the inclusion of the latter in the reduce_system_prompt</data>
      <data key="d6">e442fbb7a67e97ebc4de131b25c639e1</data>
    </edge>
    <edge source="MAX_DATA_TOKENS" target="CONTEXT_BUILDER_PARAMS">
      <data key="d4">1.0</data>
      <data key="d5">The max_data_tokens property is related to the context_builder_params as it sets a limit on the amount of data that can be processed, which the context_builder must adhere to when building the context window</data>
      <data key="d6">e442fbb7a67e97ebc4de131b25c639e1</data>
    </edge>
    <edge source="REDUCE_LLM_PARAMS" target="LLM CALL">
      <data key="d4">1.0</data>
      <data key="d5">The LLM call is configured by reduce_llm_params, which specifies additional parameters for the reduce stage, influencing the output of the LLM call.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="CONTEXT_BUILDER_PARAMS" target="LLM CALL">
      <data key="d4">1.0</data>
      <data key="d5">The LLM call's effectiveness in the map stage is enhanced by context_builder_params, which customize the context window for better understanding of the input data.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="CONCURRENT_COROUTINES" target="LLM CALL">
      <data key="d4">1.0</data>
      <data key="d5">The performance of the LLM call in the map stage is influenced by concurrent_coroutines, which determines the level of parallelism in processing tasks.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="GRAPHRAG QUERY CLI" target="DATA">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG query CLI uses data, typically in .parquet files, as input for queries and responses, enabling searches and generation of outputs based on indexed data.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="GRAPHRAG QUERY CLI" target="COMMUNITY_LEVEL">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG query CLI's functionality is influenced by community_level, which specifies the level of detail in community reports, affecting the granularity of the search results.</data>
      <data key="d6">e0cc1cf05b92456e09100790815186fe</data>
    </edge>
    <edge source="METHOD" target="ROOT">
      <data key="d4">2.0</data>
      <data key="d5">ROOT and METHOD are integral components in the document selection and template generation process within a project. ROOT specifies the project's root directory, which serves as the primary location for input data and may house configuration files that impact the document selection method. The method property, in conjunction with ROOT, determines the approach for selecting text units from the input data stored in the project directory, facilitating the creation of templates. This interplay between ROOT and METHOD ensures a structured and customizable document selection process, enabling efficient template generation based on the project's specific requirements.</data>
      <data key="d6">9243633f55cccd0885ba553e14fa5e3f,ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </edge>
    <edge source="METHOD" target="LIMIT">
      <data key="d4">1.0</data>
      <data key="d5">METHOD and LIMIT are related as the method chosen for selecting documents (all, random, or top) can affect how the limit of text units is applied</data>
      <data key="d6">9243633f55cccd0885ba553e14fa5e3f</data>
    </edge>
    <edge source="GRAPHRAG_API_KEY" target="GRAPHRAG PIPELINE">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG pipeline uses the GRAPHRAG_API_KEY for authentication when making requests to the OpenAI API or Azure OpenAI endpoint. This key is essential for the pipeline to access and utilize the AI services provided by these APIs.</data>
      <data key="d6">5aaa26fbe97dc7573cd1a56d6fb11213</data>
    </edge>
    <edge source="GRAPHRAG_API_KEY" target="SETTINGS.YAML">
      <data key="d4">1.0</data>
      <data key="d5">The settings.yaml file includes the GRAPHRAG_API_KEY, which is an environment variable that holds the API key for the OpenAI API or Azure OpenAI endpoint. This key is essential for the GraphRAG pipeline to authenticate and access the respective API services.</data>
      <data key="d6">5aaa26fbe97dc7573cd1a56d6fb11213</data>
    </edge>
    <edge source="GRAPHRAG_API_KEY" target="GRAPHRAG_API_BASE">
      <data key="d4">2.0</data>
      <data key="d5">GRAPHRAG_API_KEY and GRAPHRAG_API_BASE are integral components for accessing the LLM service API. GRAPHRAG_API_KEY serves as the authentication mechanism, ensuring secure and authorized access to the service. Meanwhile, GRAPHRAG_API_BASE specifies the foundational URL for the LLM service API, determining the endpoint to which requests are directed. The relationship between these two entities is robust and essential, as both are required for successful interaction with the API. Without the API key for authentication and the base URL for endpoint identification, accessing the services provided by the LLM service would not be possible.</data>
      <data key="d6">3da10b454f926a257b9fdf5d2487c0a5,8ac79ce92be1254dfda9a10eb54ab703</data>
    </edge>
    <edge source="GRAPHRAG_API_KEY" target="GRAPHRAG_API_VERSION">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_API_KEY and GRAPHRAG_API_VERSION are related because GRAPHRAG_API_KEY is used for authentication to access the LLM service, and GRAPHRAG_API_VERSION specifies the version of the LLM service API. Together, they ensure compatibility between the system and the LLM service by specifying the correct version of the API to use.</data>
      <data key="d6">8ac79ce92be1254dfda9a10eb54ab703</data>
    </edge>
    <edge source="GRAPHRAG_LLM_API_BASE" target="GRAPHRAG_LLM_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LLM_API_BASE and GRAPHRAG_LLM_TYPE are related as they both configure the LLM operations, with the API Base URL and the type of operation, respectively</data>
      <data key="d6">1ef6439b7c457ba43993467ff734eedf</data>
    </edge>
    <edge source="GRAPHRAG_LLM_API_BASE" target="GRAPHRAG_LLM_MAX_RETRIES">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LLM_API_BASE and GRAPHRAG_LLM_MAX_RETRIES are related as they both pertain to the configuration of LLM operations, with the API Base URL and the maximum number of retries for failed requests, respectively</data>
      <data key="d6">1ef6439b7c457ba43993467ff734eedf</data>
    </edge>
    <edge source="GRAPHRAG_LLM_API_BASE" target="AZURE OPENAI">
      <data key="d4">1.0</data>
      <data key="d5">The Azure OpenAI service requires the GRAPHRAG_LLM_API_BASE configuration to determine the base URL for API requests. This relationship is necessary for establishing a connection to the service and sending requests to the correct endpoint.</data>
      <data key="d6">7b45dafa74553d3899e2291a3c9fb86e</data>
    </edge>
    <edge source="GRAPHRAG_LLM_TYPE" target="GRAPHRAG_LLM_DEPLOYMENT_NAME">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LLM_TYPE and GRAPHRAG_LLM_DEPLOYMENT_NAME are related because the language model type and the deployment name together determine the specific language model to be used for text generation. The deployment name is crucial for identifying the correct model within the specified type. The strength of this relationship is high because both settings are required for successful text generation.</data>
      <data key="d6">3da10b454f926a257b9fdf5d2487c0a5</data>
    </edge>
    <edge source="GRAPHRAG_LLM_TYPE" target="GRAPHRAG_LLM_MODEL_SUPPORTS_JSON">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LLM_TYPE and GRAPHRAG_LLM_MODEL_SUPPORTS_JSON are related because the language model type and the model's support for JSON input together determine the format of the input data that can be processed by the model. The model's support for JSON is crucial for ensuring that the input data is in a compatible format. The strength of this relationship is high because both settings are required for successful text generation.</data>
      <data key="d6">3da10b454f926a257b9fdf5d2487c0a5</data>
    </edge>
    <edge source="GRAPHRAG_LLM_TYPE" target="GRAPHRAG_INPUT_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_INPUT_TYPE and GRAPHRAG_LLM_TYPE are related because the input data type and the language model type together determine the compatibility of the input data with the model being used for text generation. The input data type is crucial for ensuring that the data is in a format that can be processed by the specified model. The strength of this relationship is high because both settings are required for successful text generation.</data>
      <data key="d6">3da10b454f926a257b9fdf5d2487c0a5</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_API_BASE" target="GRAPHRAG_EMBEDDING_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_EMBEDDING_API_BASE and GRAPHRAG_EMBEDDING_TYPE are related as they both configure the embedding operations, with the API Base URL and the type of embedding client, respectively</data>
      <data key="d6">1ef6439b7c457ba43993467ff734eedf</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_API_BASE" target="GRAPHRAG_EMBEDDING_MAX_RETRIES">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_EMBEDDING_API_BASE and GRAPHRAG_EMBEDDING_MAX_RETRIES are related as they both pertain to the configuration of embedding operations, with the API Base URL and the maximum number of retries for failed requests, respectively</data>
      <data key="d6">1ef6439b7c457ba43993467ff734eedf</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_API_BASE" target="APHRAG_API_KEY">
      <data key="d4">1.0</data>
      <data key="d5">APHRAG_API_KEY is related to GRAPHRAG_EMBEDDING_API_BASE as both are configuration variables that may be required for accessing and using the embedding API. The absence of APHRAG_API_KEY might affect the functionality or access to the API specified by GRAPHRAG_EMBEDDING_API_BASE.</data>
      <data key="d6">485c17007ccb3102887eaa47d6a6100f</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_TYPE" target="GRAPHRAG_LLM_DEPLOYMENT_NAME">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_EMBEDDING_TYPE and GRAPHRAG_LLM_DEPLOYMENT_NAME are related because the embedding model type and the deployment name together determine the specific embedding model to be used for text embedding. The deployment name is crucial for identifying the correct model within the specified type. The strength of this relationship is high because both settings are required for successful text embedding.</data>
      <data key="d6">3da10b454f926a257b9fdf5d2487c0a5</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_MAX_RETRIES" target="GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_EMBEDDING_MAX_RETRIES setting is related to the GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT as they both determine the retry mechanism for embedding operations, with the number of retries and the wait time between retries</data>
      <data key="d6">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </edge>
    <edge source="GRAPHRAG_LOCAL_SEARCH_TEXT_UNIT_PROP" target="GRAPHRAG_LOCAL_SEARCH_COMMUNITY_PROP">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LOCAL_SEARCH_TEXT_UNIT_PROP and GRAPHRAG_LOCAL_SEARCH_COMMUNITY_PROP are related as they both configure the proportions of the context window dedicated to related text units and community reports, respectively</data>
      <data key="d6">1ef6439b7c457ba43993467ff734eedf</data>
    </edge>
    <edge source="GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS" target="GRAPHRAG_LOCAL_SEARCH_LLM_MAX_TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS and GRAPHRAG_LOCAL_SEARCH_LLM_MAX_TOKENS are related as they both are configuration properties that can be adjusted based on the token limit of the model being used, affecting the local search context window and language model token limits respectively</data>
      <data key="d6">2efb1fec56fe3b0543d395dd541295c3</data>
    </edge>
    <edge source="GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS" target="GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS and GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS are related as they both are configuration properties that can be adjusted based on the token limit of the model being used, affecting the local and global search context window token limits respectively</data>
      <data key="d6">2efb1fec56fe3b0543d395dd541295c3</data>
    </edge>
    <edge source="GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS" target="GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">Both GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS and GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS are related as they both pertain to the token limits for global search and should be adjusted based on the model's token limit</data>
      <data key="d6">2049798d3000849f8bec3e88c0006807</data>
    </edge>
    <edge source="GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS" target="GRAPHRAG_GLOBAL_SEARCH_REDUCE_MAX_TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS and GRAPHRAG_GLOBAL_SEARCH_REDUCE_MAX_TOKENS are related as they both pertain to the token limits for global search and should be adjusted based on the model's token limit</data>
      <data key="d6">2049798d3000849f8bec3e88c0006807</data>
    </edge>
    <edge source="GRAPHRAG KNOWLEDGE MODEL" target="GRAPHRAG INDEXER">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Knowledge Model is aligned with the outputs of the GraphRAG Indexer, which means the data processed by the Indexer is structured according to the Knowledge Model for compatibility with the GraphRAG system</data>
      <data key="d6">25e04f0e9a961dcdc3f6eae6df7807b2</data>
    </edge>
    <edge source="GRAPHRAG INDEXER" target="GRAPHRAG QUERY ENGINE">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Indexer's outputs are loaded into a database system, which is then accessed by the GraphRAG Query Engine to process queries based on the indexed data</data>
      <data key="d6">25e04f0e9a961dcdc3f6eae6df7807b2</data>
    </edge>
    <edge source="DATASHAPER" target="GRAPHRAG INDEXING PIPELINE">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Indexing Pipeline is built on top of DataShaper, utilizing its data processing capabilities to transform and index data for the GraphRAG system</data>
      <data key="d6">25e04f0e9a961dcdc3f6eae6df7807b2</data>
    </edge>
    <edge source="DATASHAPER" target="WORKFLOW">
      <data key="d4">1.0</data>
      <data key="d5">DataShaper provides the framework for defining and executing Workflows, which are sequences of steps (verbs) that transform data tables through a pipeline</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="WORKFLOW" target="VERB">
      <data key="d4">1.0</data>
      <data key="d5">A Workflow in DataShaper is composed of Verbs, which are steps that model relational concepts and transform input data tables</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="WORKFLOW GRAPHS">
      <data key="d4">1.0</data>
      <data key="d5">Workflow Graphs are a fundamental concept in the GraphRAG Indexing Pipeline, where they are used to represent the series of interdependent workflows that form the data indexing process.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="PREPARE">
      <data key="d4">1.0</data>
      <data key="d5">The Prepare step is part of the GraphRAG Indexing Pipeline, where it is used to prepare data for subsequent processing tasks.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="CHUNK">
      <data key="d4">1.0</data>
      <data key="d5">The Chunk step is part of the GraphRAG Indexing Pipeline, where it is used to break down data into smaller, manageable pieces for more efficient processing.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="EXTRACTGRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The ExtractGraph step is part of the GraphRAG Indexing Pipeline, where it is used to extract graph structures from data.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="EMBEDDOCUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">The EmbedDocuments step is part of the GraphRAG Indexing Pipeline, where it is used to convert documents into numerical representations for machine learning or information retrieval.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="GENERATEREPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The GenerateReports step is part of the GraphRAG Indexing Pipeline, where it is used to create reports based on the processed data.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="EMBEDGRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The EmbedGraph step is part of the GraphRAG Indexing Pipeline, where it is used to convert graph structures into numerical representations for machine learning algorithms.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="ENTITYRESOLUTION">
      <data key="d4">1.0</data>
      <data key="d5">The EntityResolution step is part of the GraphRAG Indexing Pipeline, where it is used to resolve ambiguities in data, such as identifying and merging duplicate entities.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="SAMPLE WORKFLOW DAG">
      <data key="d4">1.0</data>
      <data key="d5">The Sample Workflow DAG is a visual representation of the directed acyclic graph (DAG) that is part of the GraphRAG Indexing Pipeline, showing the dependencies between different workflows.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="DATAFRAME MESSAGE FORMAT">
      <data key="d4">1.0</data>
      <data key="d5">The Dataframe Message Format is used in the GraphRAG Indexing Pipeline as the primary unit of communication between workflows and workflow steps, facilitating data-centric and table-centric data processing.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="GRAPHRAG INDEXING PIPELINE" target="LLM CACHING">
      <data key="d4">1.0</data>
      <data key="d5">LLM Caching is a technique used in the GraphRAG Indexing Pipeline to improve the resilience of the indexer to network issues by caching the results of Large Language Model (LLM) interactions.</data>
      <data key="d6">d19a57bca2c14fc9c2bf5058958380fd</data>
    </edge>
    <edge source="VERB" target="INPUT TABLE">
      <data key="d4">1.0</data>
      <data key="d5">A Verb in DataShaper operates on an Input Table, transforming it according to the verb's specific function</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="VERB" target="OUTPUT TABLE">
      <data key="d4">1.0</data>
      <data key="d5">A Verb in DataShaper produces an Output Table, which is the result of the transformation applied to the Input Table</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="GRAPHRAG'S INDEXING PIPELINE" target="LLM-BASED WORKFLOW STEPS">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG's Indexing Pipeline implements custom LLM-based Workflow Steps that utilize Large Language Models to perform data enrichment and extraction tasks</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="GRAPHRAG'S INDEXING PIPELINE" target="WORKFLOW GRAPHS">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG's Indexing Pipeline uses Workflow Graphs to represent the complex interdependencies between multiple workflows, forming a DAG for scheduling processing</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="WORKFLOW GRAPHS" target="DAG (DIRECTED ACYCLIC GRAPH)">
      <data key="d4">1.0</data>
      <data key="d5">Workflow Graphs in GraphRAG's Indexing Pipeline are represented as a DAG (Directed Acyclic Graph) to manage the dependencies between different workflow steps</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="PREPARE" target="CHUNK">
      <data key="d4">1.0</data>
      <data key="d5">Prepare may be followed by Chunk in a workflow, where data is prepared and then divided into smaller, manageable pieces</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="CHUNK" target="EXTRACTGRAPH">
      <data key="d4">1.0</data>
      <data key="d5">Chunk may be followed by ExtractGraph in a workflow, where data chunks are analyzed to extract graph structures</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="CHUNK" target="DOCUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">Chunk is related to Documents as chunks are often derived from documents, with a user-configurable size, and there is a strict 1-to-many relationship between Documents and TextUnits by default</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="EXTRACTGRAPH" target="EMBEDDOCUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">ExtractGraph may be followed by EmbedDocuments in a workflow, where graph structures are used to inform the embedding of documents into a vector space</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="EMBEDDOCUMENTS" target="GENERATEREPORTS">
      <data key="d4">1.0</data>
      <data key="d5">EmbedDocuments may be followed by GenerateReports in a workflow, where embedded documents are used to generate reports based on the processed data</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="GENERATEREPORTS" target="EMBEDGRAPH">
      <data key="d4">1.0</data>
      <data key="d5">GenerateReports may be followed by EmbedGraph in a workflow, where reports are used to inform the embedding of graph structures into a vector space</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="EMBEDGRAPH" target="ENTITYRESOLUTION">
      <data key="d4">1.0</data>
      <data key="d5">EmbedGraph may be followed by EntityResolution in a workflow, where embedded graph structures are used to resolve entities within the data</data>
      <data key="d6">81031e23c0b000ee60cd9b06950f96cd</data>
    </edge>
    <edge source="GRAPHRAG LIBRARY" target="LLM INTERACTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG library is designed to facilitate and optimize LLM interactions by addressing common issues such as network latency and throttling errors through the implementation of a caching mechanism.</data>
      <data key="d6">6335601c6ec22bd6f15c8b69c26f854b</data>
    </edge>
    <edge source="GRAPHRAG LIBRARY" target="CACHING">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG library incorporates caching as a core feature to improve the efficiency and reliability of LLM interactions by storing and reusing results when the same input set is used.</data>
      <data key="d6">6335601c6ec22bd6f15c8b69c26f854b</data>
    </edge>
    <edge source="LLM INTERACTIONS" target="CACHING">
      <data key="d4">1.0</data>
      <data key="d5">Caching is a strategy used in LLM interactions to enhance system performance by storing and reusing results, which helps in mitigating network issues and improving the efficiency of the interactions.</data>
      <data key="d6">6335601c6ec22bd6f15c8b69c26f854b</data>
    </edge>
    <edge source="LOCAL TO GLOBAL GRAPH RAG APPROACH" target="RETRIEVAL-AUGMENTED GENERATION (RAG)">
      <data key="d4">1.0</data>
      <data key="d5">The Local to Global Graph RAG Approach builds upon the concept of Retrieval-Augmented Generation (RAG) by addressing its limitations in handling global questions. It extends RAG's capabilities to include query-focused summarization tasks over large text corpora.</data>
      <data key="d6">f76c18c7582167c3626f8741c2c9374f</data>
    </edge>
    <edge source="LOCAL TO GLOBAL GRAPH RAG APPROACH" target="QUERY-FOCUSED SUMMARIZATION (QFS)">
      <data key="d4">1.0</data>
      <data key="d5">The Local to Global Graph RAG Approach incorporates elements of Query-Focused Summarization (QFS) to scale to the quantities of text indexed by typical RAG systems. It aims to combine the strengths of QFS and RAG to handle both the generality of user questions and the quantity of source text.</data>
      <data key="d6">f76c18c7582167c3626f8741c2c9374f</data>
    </edge>
    <edge source="LOCAL TO GLOBAL GRAPH RAG APPROACH" target="ENTITY KNOWLEDGE GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The Local to Global Graph RAG Approach uses an Entity Knowledge Graph as a foundational component. The graph is derived from source documents and is used to identify closely-related entities for generating community summaries.</data>
      <data key="d6">f76c18c7582167c3626f8741c2c9374f</data>
    </edge>
    <edge source="RETRIEVAL-AUGMENTED GENERATION (RAG)" target="QUERY-FOCUSED SUMMARIZATION (QFS)">
      <data key="d4">1.0</data>
      <data key="d5">Retrieval-Augmented Generation (RAG) is a specific approach within the broader task of query-focused summarization, but it is designed for situations where answers are contained locally within regions of text, whereas QFS focuses on generating summaries in response to user queries</data>
      <data key="d6">c7669e6a1add9a2829b09196256b1492</data>
    </edge>
    <edge source="QUERY-FOCUSED SUMMARIZATION (QFS)" target="ABSTRACTIVE SUMMARIZATION">
      <data key="d4">2.0</data>
      <data key="d5">Query-Focused Summarization (QFS) is a specialized subset of Abstractive Summarization, a technique that generates concise, natural language summaries. Unlike generic summarization methods, QFS is tailored to create summaries that are directly relevant to specific queries. This process involves the creation of new sentences that accurately capture the information sought by the query, rather than simply extracting and concatenating existing sentences from the source text. QFS's focus on abstractive summarization ensures that the generated summaries are not merely a collection of excerpts, but rather a coherent and meaningful representation of the original content, specifically addressing the information needs indicated by the query.</data>
      <data key="d6">85eff07c379a9dc24db0edb983acf3c9,c7669e6a1add9a2829b09196256b1492</data>
    </edge>
    <edge source="QUERY-FOCUSED SUMMARIZATION (QFS)" target="EXTRACTIVE SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Query-Focused Summarization (QFS) contrasts with extractive summarization, as it focuses on generating new sentences in response to user queries, rather than selecting and concatenating existing sentences</data>
      <data key="d6">c7669e6a1add9a2829b09196256b1492</data>
    </edge>
    <edge source="ENTITY KNOWLEDGE GRAPH" target="GRAPH-BASED TEXT INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The graph-based text index method involves the creation of an entity knowledge graph from source documents, which is used to generate community summaries for groups of closely-related entities</data>
      <data key="d6">b149708d0b4ac3ff417565739ea6b03b</data>
    </edge>
    <edge source="GRAPH-BASED TEXT INDEX" target="GLOBAL SENSEMAKING QUESTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The graph-based text index method is particularly effective for answering global sensemaking questions over datasets in the 1 million token range, as it improves the comprehensiveness and diversity of generated answers</data>
      <data key="d6">b149708d0b4ac3ff417565739ea6b03b</data>
    </edge>
    <edge source="GLOBAL SENSEMAKING QUESTIONS" target="GRAPH RAG">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG leads to substantial improvements over a naive RAG baseline for answering global sensemaking questions, as it uses a graph-based text index to improve the comprehensiveness and diversity of generated answers</data>
      <data key="d6">b149708d0b4ac3ff417565739ea6b03b</data>
    </edge>
    <edge source="GRAPH RAG" target="SOURCE TEXTS">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG is compared against Source Texts in terms of summarization and question answering effectiveness. Graph RAG generally provides a small but consistent improvement in answer comprehensiveness and diversity over source texts, especially with community summaries.</data>
      <data key="d6">71f14506a6b15dfabd93fd1606a67b73</data>
    </edge>
    <edge source="GRAPH RAG" target="SOURCE TEXT SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG requires fewer context tokens for both low-level (C3) and root-level (C0) community summaries compared to source text summarization, indicating a scalability advantage</data>
      <data key="d6">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </edge>
    <edge source="GRAPH RAG" target="LOW-LEVEL COMMUNITY SUMMARIES (C3)">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG requires 26-33% fewer context tokens for low-level community summaries (C3) compared to source text summarization, showing a scalability advantage</data>
      <data key="d6">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </edge>
    <edge source="GRAPH RAG" target="ROOT-LEVEL COMMUNITY SUMMARIES (C0)">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG requires over 97% fewer context tokens for root-level community summaries (C0) compared to source text summarization, demonstrating a significant scalability advantage</data>
      <data key="d6">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </edge>
    <edge source="GRAPH RAG" target="EMPOWERMENT">
      <data key="d4">1.0</data>
      <data key="d5">Empowerment comparisons showed mixed results for Graph RAG versus naive RAG and source text summarization, indicating that Graph RAG may need tuning to retain more details in the index to better empower users</data>
      <data key="d6">ed433e2f5d5387b47376eb0e45ca1c99</data>
    </edge>
    <edge source="GRAPH RAG" target="PARALLEL GENERATION OF COMMUNITY ANSWERS">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG uses parallel generation of community answers as a technique for improving the efficiency and effectiveness of retrieval and generation. This relationship indicates that parallel generation of community answers is a key feature of Graph RAG, enabling faster and more accurate retrieval and generation.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="ITERATIVE RETRIEVAL-GENERATION STRATEGY">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG incorporates iterative retrieval-generation strategy as a method for continuous refinement and improvement of the retrieval and generation process. This relationship indicates that iterative retrieval-generation strategy is a core component of Graph RAG, allowing for iterative cycles of retrieval and generation.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="FEDERATED RETRIEVAL-GENERATION STRATEGY">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG uses federated retrieval-generation strategy as a method for sharing resources and information across multiple systems or nodes. This relationship indicates that federated retrieval-generation strategy is a key feature of Graph RAG, enabling distributed and collaborative retrieval and generation.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="MULTI-DOCUMENT SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG bears resemblance to multi-document summarization techniques, which are used for creating a concise and coherent summary of multiple documents. This relationship indicates that multi-document summarization is a related concept to Graph RAG, sharing similarities in the handling of multiple documents or data sources.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="MULTI-HOP QUESTION ANSWERING">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG is related to multi-hop question answering techniques, which are used for answering questions that require reasoning over multiple pieces of information or documents. This relationship indicates that multi-hop question answering is a related concept to Graph RAG, sharing similarities in the handling of complex questions and information retrieval.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="HIERARCHICAL INDEX">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG uses a hierarchical index as a data structure for organizing information in a hierarchical manner. This relationship indicates that hierarchical index is a key component of Graph RAG, improving the efficiency and effectiveness of information retrieval.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG incorporates summarization techniques for creating a concise and coherent summary of a document or set of documents. This relationship indicates that summarization is a related concept to Graph RAG, sharing similarities in the handling of document summarization and information retrieval.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="GRAPH RAG" target="TREE OF CLARIFICATIONS">
      <data key="d4">1.0</data>
      <data key="d5">Tree of Clarifications and Graph RAG are both research studies that involve advanced information retrieval and analysis, with Tree of Clarifications focusing on ambiguous questions and Graph RAG on self-generated graph indexes.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="GRAPH RAG" target="SELFCHECKGPT">
      <data key="d4">1.0</data>
      <data key="d5">Comparing fabrication rates using approaches like SelfCheckGPT can improve the current analysis and potentially enhance the performance of Graph RAG</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH RAG" target="GRAPH INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The graph index is a key component in the Graph RAG approach, which consistently achieves the best head-to-head results against other methods</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH RAG" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the Graph RAG model</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="LARGE LANGUAGE MODELS (LLMS)" target="SCIENTIFIC DISCOVERY">
      <data key="d4">1.0</data>
      <data key="d5">Large language models are being used to automate human-like sensemaking in the domain of scientific discovery, where the process of making new and significant contributions to scientific knowledge is supported by the use of artificial intelligence</data>
      <data key="d6">b149708d0b4ac3ff417565739ea6b03b</data>
    </edge>
    <edge source="LARGE LANGUAGE MODELS (LLMS)" target="INTELLIGENCE ANALYSIS">
      <data key="d4">1.0</data>
      <data key="d5">Large language models are being used to automate human-like sensemaking in the domain of intelligence analysis, where the process of gathering, processing, and analyzing information to support decision-making is supported by the use of artificial intelligence</data>
      <data key="d6">b149708d0b4ac3ff417565739ea6b03b</data>
    </edge>
    <edge source="AUTOMATED SENSEMAKING" target="HUMAN-LED SENSEMAKING">
      <data key="d4">1.0</data>
      <data key="d5">Automated sensemaking complements human-led sensemaking by providing tools and techniques to support humans in understanding complex domains and refining their mental model of the data</data>
      <data key="d6">c7669e6a1add9a2829b09196256b1492</data>
    </edge>
    <edge source="ABSTRACTIVE SUMMARIZATION" target="EXTRACTIVE SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Abstractive Summarization is distinct from Extractive Summarization in that it generates new sentences to convey the meaning of the original text, whereas Extractive Summarization selects and reorders existing sentences from the source text. Abstractive Summarization is more flexible and can provide a more coherent summary, while Extractive Summarization is more literal and may not capture the essence of the text as effectively.</data>
      <data key="d6">85eff07c379a9dc24db0edb983acf3c9</data>
    </edge>
    <edge source="TRANSFORMER ARCHITECTURE" target="LLMS (LARGE LANGUAGE MODELS)">
      <data key="d4">1.0</data>
      <data key="d5">The Transformer Architecture is foundational to the development of LLMs (Large Language Models), which have shown substantial improvements in various natural language processing tasks, including summarization. The transformer's self-attention mechanism allows LLMs to process and generate text more effectively, making them highly capable in summarization tasks.</data>
      <data key="d6">85eff07c379a9dc24db0edb983acf3c9</data>
    </edge>
    <edge source="LLMS (LARGE LANGUAGE MODELS)" target="GPT (GENERATIVE PRE-TRAINED TRANSFORMER)">
      <data key="d4">1.0</data>
      <data key="d5">GPT (Generative Pre-trained Transformer) is a type of LLM (Large Language Model) that has been particularly effective in summarization tasks. It uses the transformer architecture to generate coherent and contextually relevant text, making it a powerful tool for summarization and other NLP tasks.</data>
      <data key="d6">85eff07c379a9dc24db0edb983acf3c9</data>
    </edge>
    <edge source="LLMS (LARGE LANGUAGE MODELS)" target="LLAMA">
      <data key="d4">1.0</data>
      <data key="d5">Llama is a type of LLM (Large Language Model) that is designed to handle complex language tasks, including summarization. It is based on the transformer architecture and can generate human-like text, making it a valuable resource for summarization and other NLP tasks.</data>
      <data key="d6">85eff07c379a9dc24db0edb983acf3c9</data>
    </edge>
    <edge source="LLMS (LARGE LANGUAGE MODELS)" target="GEMINI">
      <data key="d4">1.0</data>
      <data key="d5">Gemini is a type of LLM (Large Language Model) that is capable of understanding and generating text, making it suitable for various NLP tasks, including summarization. It is based on the transformer architecture and can process and generate text effectively, making it a powerful tool for summarization tasks.</data>
      <data key="d6">85eff07c379a9dc24db0edb983acf3c9</data>
    </edge>
    <edge source="BROWN ET AL., 2020" target="LLAMA (TOUVRON ET AL., 2023)">
      <data key="d4">1.0</data>
      <data key="d5">Brown et al., 2020, and Llama (Touvron et al., 2023) are related in that they both discuss the use of in-context learning for summarization tasks within the context window of LLMs</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="LLAMA (TOUVRON ET AL., 2023)" target="GEMINI (ANIL ET AL., 2023)">
      <data key="d4">1.0</data>
      <data key="d5">Llama (Touvron et al., 2023) and Gemini (Anil et al., 2023) are related in that they both explore the application of in-context learning for summarization tasks, focusing on different series that can summarize content within the context window of LLMs</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="IN-CONTEXT LEARNING" target="QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">In-context learning is related to query-focused abstractive summarization in that it is a technique used by LLMs to adapt their responses based on the context provided, which is essential for generating summaries in response to specific queries</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION" target="LLM CONTEXT WINDOW LIMITS">
      <data key="d4">1.0</data>
      <data key="d5">Query-focused abstractive summarization is constrained by LLM context window limits, as the volumes of text in entire corpora can greatly exceed the processing capacity of LLMs</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION" target="NAIVE RAG (RETRIEVAL-AUGMENTED GENERATION)">
      <data key="d4">1.0</data>
      <data key="d5">Naive RAG (Retrieval-Augmented Generation) is related to query-focused abstractive summarization in that it is a technique that directly retrieves text chunks for summarization, which may not be sufficient for summarization tasks over large corpora</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION" target="GRAPH RAG APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">The Graph RAG approach is related to query-focused abstractive summarization in that it is a technique based on global summarization of an LLM-derived knowledge graph, which can address the limitations of naive RAG for summarization tasks over large corpora</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="LLM CONTEXT WINDOW LIMITS" target="INFORMATION LOSS IN LONGER CONTEXTS">
      <data key="d4">1.0</data>
      <data key="d5">LLM context window limits are related to information loss in longer contexts, as the middle part of very long texts may not receive adequate attention, leading to potential loss of information</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="NAIVE RAG (RETRIEVAL-AUGMENTED GENERATION)" target="ENTITY-BASED GRAPH INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The Entity-based Graph Index achieves superior performance compared to Naive RAG (Retrieval-Augmented Generation), offering a more efficient and effective data index for root-level communities and global queries.</data>
      <data key="d6">e31d2d134cf501c93f9445914d7350f9</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="STRUCTURED RETRIEVAL AND TRAVERSAL">
      <data key="d4">1.0</data>
      <data key="d5">Structured retrieval and traversal is related to the Graph RAG approach in that it exploits the structured nature of graph indexes for efficient information retrieval and navigation, which is a key component of the Graph RAG approach</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="GRAPH MODULARITY">
      <data key="d4">1.0</data>
      <data key="d5">Graph modularity is related to the Graph RAG approach in that it is a property of graphs that allows them to be partitioned into modular communities of closely-related nodes, which is a key aspect of the Graph RAG approach</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="COMMUNITY DETECTION ALGORITHMS">
      <data key="d4">1.0</data>
      <data key="d5">Community detection algorithms are related to the Graph RAG approach in that they are techniques used to identify groups of closely-related nodes in a graph, which can be useful for summarization tasks within the Graph RAG approach</data>
      <data key="d6">3fc3718256cb7f614fcde622af2ed912</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="TEXT CHUNKS">
      <data key="d4">1.0</data>
      <data key="d5">The Graph RAG approach involves processing text chunks extracted from source documents. The granularity of these chunks affects the efficiency and quality of the information retrieval and augmentation process in the Graph RAG technique.</data>
      <data key="d6">d2399fd0aae5bd200639806ca87184f8</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="EMBEDDING-BASED MATCHING">
      <data key="d4">1.0</data>
      <data key="d5">The Graph RAG approach utilizes embedding-based matching as one of its methods to refine and adapt the current approach by operating in a more local manner, matching user queries and graph annotations</data>
      <data key="d6">5e2933c9646c751e6a60c9de12a255f2</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="HYBRID RAG SCHEMES">
      <data key="d4">1.0</data>
      <data key="d5">The Graph RAG approach can be enhanced by implementing hybrid RAG schemes that combine embedding-based matching against community reports before employing map-reduce summarization mechanisms, improving the comprehensiveness and diversity of answers</data>
      <data key="d6">5e2933c9646c751e6a60c9de12a255f2</data>
    </edge>
    <edge source="GRAPH RAG APPROACH" target="MAP-REDUCE SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The Graph RAG approach uses map-reduce summarization as a mechanism to summarize large amounts of data, which is particularly useful for global queries over the same dataset, providing summaries of root-level communities in the entity-based graph index</data>
      <data key="d6">5e2933c9646c751e6a60c9de12a255f2</data>
    </edge>
    <edge source="STRUCTURED RETRIEVAL AND TRAVERSAL" target="LLM-DERIVED KNOWLEDGE GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The LLM-derived knowledge graph is related to structured retrieval and traversal because the graph structure facilitates efficient and effective information retrieval and navigation. The inherent modularity of the graph and the use of community detection algorithms enable the identification of relevant information for specific queries.</data>
      <data key="d6">d39abd5380fb3fe0468ea1e122512091</data>
    </edge>
    <edge source="COMMUNITY DETECTION ALGORITHMS" target="MODULARITY">
      <data key="d4">1.0</data>
      <data key="d5">Modularity is related to community detection algorithms because these algorithms are designed to identify and optimize the modularity of a graph by partitioning it into communities of closely-related nodes. The algorithms aim to maximize the modularity score, which measures the quality of the partitioning.</data>
      <data key="d6">d39abd5380fb3fe0468ea1e122512091</data>
    </edge>
    <edge source="COMMUNITY DETECTION ALGORITHMS" target="ENTITY GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">Community Detection Algorithms are applied to the entity graph to partition it into communities of closely-related entities</data>
      <data key="d6">6dace8e490674ac8e031aed987a63789</data>
    </edge>
    <edge source="COMMUNITY DETECTION ALGORITHMS" target="LEIDEN ALGORITHM">
      <data key="d4">1.0</data>
      <data key="d5">The Leiden Algorithm is a type of Community Detection Algorithm that is specifically used in the text for its ability to efficiently recover the hierarchical community structure of large-scale graphs</data>
      <data key="d6">a660289d2bf43f25d3524d35cd2d9a96</data>
    </edge>
    <edge source="LOUVAIN" target="LEIDEN">
      <data key="d4">1.0</data>
      <data key="d5">Louvain is related to Leiden because Leiden is an extension of the Louvain algorithm for community detection. It builds upon the Louvain algorithm by adding a refinement step that allows for the detection of smaller communities, improving the resolution and stability of the partitioning.</data>
      <data key="d6">d39abd5380fb3fe0468ea1e122512091</data>
    </edge>
    <edge source="QUERY-FOCUSED SUMMARIZATION" target="MAP-REDUCE APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">Query-focused summarization is related to the map-reduce approach because the map-reduce approach can be used to implement query-focused summarization on large datasets. The map step involves using each community summary to answer the query independently and in parallel, while the reduce step summarizes all relevant partial answers into a final global answer.</data>
      <data key="d6">d39abd5380fb3fe0468ea1e122512091</data>
    </edge>
    <edge source="ACTIVITY-CENTERED SENSEMAKING QUESTIONS" target="PODCAST TRANSCRIPTS">
      <data key="d4">1.0</data>
      <data key="d5">Activity-centered sensemaking questions are related to podcast transcripts because the questions can be derived from short descriptions of podcast transcripts to help users understand the content and context of the podcasts. The questions are intended to guide the user in making sense of the underlying data and its implications.</data>
      <data key="d6">d39abd5380fb3fe0468ea1e122512091</data>
    </edge>
    <edge source="ACTIVITY-CENTERED SENSEMAKING QUESTIONS" target="NEWS ARTICLES">
      <data key="d4">1.0</data>
      <data key="d5">Activity-centered sensemaking questions are related to news articles because the questions can be derived from short descriptions of news articles to help users understand the events and issues reported in the news. The questions are intended to guide the user in making sense of the underlying data and its implications.</data>
      <data key="d6">d39abd5380fb3fe0468ea1e122512091</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The dataset is related to podcast transcripts as they are a specific type of data within the dataset. Podcast transcripts are used for understanding tech leaders' views on policy and regulation.</data>
      <data key="d6">aed2ea39de8a027cc818c7f4557f0514</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="TECH JOURNALIST">
      <data key="d4">1.0</data>
      <data key="d5">A tech journalist is related to podcast transcripts as they use these transcripts to find insights and trends in the tech industry, specifically focusing on tech leaders' views on policy and regulation.</data>
      <data key="d6">aed2ea39de8a027cc818c7f4557f0514</data>
    </edge>
    <edge source="PODCAST TRANSCRIPTS" target="NEWS ARTICLES">
      <data key="d4">1.0</data>
      <data key="d5">Both Podcast Transcripts and News Articles are datasets selected for evaluation in the one million token range, representative of the kind of corpora that users may encounter in their real-world activities</data>
      <data key="d6">5d04129d46662571f635a4e63cb4d6b7</data>
    </edge>
    <edge source="NEWS ARTICLES" target="DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The dataset is related to news articles as they are a specific type of data within the dataset. News articles are used for teaching about health and wellness.</data>
      <data key="d6">aed2ea39de8a027cc818c7f4557f0514</data>
    </edge>
    <edge source="NEWS ARTICLES" target="EDUCATOR">
      <data key="d4">2.0</data>
      <data key="d5">The educator, a pivotal figure in the academic landscape, leverages news articles as a valuable resource to enrich the learning experience. By integrating current affairs into the curricula, particularly focusing on health and wellness, educators ensure that students are not only well-informed about contemporary issues but also develop a deeper understanding of various topics. News articles serve as a bridge between the classroom and the real world, enabling educators to provide relevant and up-to-date information that enhances the educational process. This approach not only broadens students' knowledge but also fosters critical thinking and awareness of global events.</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc,aed2ea39de8a027cc818c7f4557f0514</data>
    </edge>
    <edge source="NEWS ARTICLES" target="CURRENT POLICIES">
      <data key="d4">1.0</data>
      <data key="d5">News articles often discuss current policies and their implications, providing insights into how policies are perceived and how they might evolve</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="DIVERSE ACTIVITY-CENTERED SENSE-MAKING QUESTIONS" target="REAL-WORLD DATASETS">
      <data key="d4">1.0</data>
      <data key="d5">Diverse activity-centered sense-making questions are generated from real-world datasets, which include podcast transcripts and news articles. These datasets provide the context and content necessary for developing questions that aim to be comprehensive, diverse, and empowering.</data>
      <data key="d6">d2399fd0aae5bd200639806ca87184f8</data>
    </edge>
    <edge source="REAL-WORLD DATASETS" target="COMPREHENSIVENESS, DIVERSITY, EMPOWERMENT">
      <data key="d4">1.0</data>
      <data key="d5">Real-world datasets are used to create sense-making questions that aim to be comprehensive, diverse, and empowering. The datasets serve as the foundation for developing questions that foster understanding of broad issues and themes.</data>
      <data key="d6">d2399fd0aae5bd200639806ca87184f8</data>
    </edge>
    <edge source="COMPREHENSIVENESS, DIVERSITY, EMPOWERMENT" target="HIERARCHICAL LEVEL OF COMMUNITY SUMMARIES">
      <data key="d4">1.0</data>
      <data key="d5">The target qualities of comprehensiveness, diversity, and empowerment are assessed by exploring the impact of varying the hierarchical level of community summaries used to answer queries. This relationship helps in evaluating the effectiveness of different summarization techniques in meeting these criteria.</data>
      <data key="d6">d2399fd0aae5bd200639806ca87184f8</data>
    </edge>
    <edge source="HIERARCHICAL LEVEL OF COMMUNITY SUMMARIES" target="NAIVE RAG">
      <data key="d4">1.0</data>
      <data key="d5">The hierarchical level of community summaries is compared to naive RAG to evaluate its performance in terms of comprehensiveness and diversity. This comparison helps in understanding the advantages and limitations of each approach in summarization tasks.</data>
      <data key="d6">d2399fd0aae5bd200639806ca87184f8</data>
    </edge>
    <edge source="HIERARCHICAL LEVEL OF COMMUNITY SUMMARIES" target="GLOBAL MAP-REDUCE SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The hierarchical level of community summaries is compared to global map-reduce summarization to assess its effectiveness in providing comprehensive and diverse insights. This comparison is crucial for determining the best summarization technique based on the target qualities.</data>
      <data key="d6">d2399fd0aae5bd200639806ca87184f8</data>
    </edge>
    <edge source="NAIVE RAG" target="MAP-REDUCE SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Naive RAG and Map-Reduce Summarization are related in the context of summarization and question answering, as both are methods used to process and generate responses from source texts. However, Naive RAG is noted for its directness and simplicity, whereas Map-Reduce Summarization is more resource-intensive.</data>
      <data key="d6">71f14506a6b15dfabd93fd1606a67b73</data>
    </edge>
    <edge source="NAIVE RAG" target="ADVANCED RAG">
      <data key="d4">1.0</data>
      <data key="d5">Advanced RAG builds upon the foundational principles of Naive RAG, addressing its limitations by incorporating more sophisticated retrieval and generation strategies, leading to improved information retrieval and context augmentation</data>
      <data key="d6">38feec52b8bfbd3fd8e03635acdaec97</data>
    </edge>
    <edge source="TEXT CHUNKS" target="SOURCE DOCUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">Source Documents are split into Text Chunks for processing by LLM prompts. The granularity of this split affects the efficiency and effectiveness of information extraction</data>
      <data key="d6">e7caf4256ddea71533af1c4c50444146</data>
    </edge>
    <edge source="TEXT CHUNKS" target="LLM PROMPTS">
      <data key="d4">1.0</data>
      <data key="d5">Text Chunks are processed by LLM Prompts to extract graph index elements. The size of the text chunks impacts the number of LLM calls and the recall of the extraction process</data>
      <data key="d6">e7caf4256ddea71533af1c4c50444146</data>
    </edge>
    <edge source="TEXT CHUNKS" target="ENTITY REFERENCES">
      <data key="d4">1.0</data>
      <data key="d5">The number of Entity References extracted is influenced by the size of the Text Chunks. Smaller chunks tend to extract more references, but the process needs to balance recall and precision</data>
      <data key="d6">e7caf4256ddea71533af1c4c50444146</data>
    </edge>
    <edge source="TEXT CHUNKS" target="GRAPH NODES">
      <data key="d4">1.0</data>
      <data key="d5">Text Chunks are the source from which Graph Nodes are identified and extracted. The relationship indicates that the processing of text chunks leads to the identification of entities represented as graph nodes.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="TEXT CHUNKS" target="GRAPH EDGES">
      <data key="d4">1.0</data>
      <data key="d5">Text Chunks are the source from which Graph Edges are identified and extracted. The relationship indicates that the processing of text chunks leads to the identification of relationships between entities, represented as graph edges.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="TEXT CHUNKS" target="LLM PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Prompt is used to process Text Chunks for the identification of entities and relationships. The relationship indicates that the prompt guides the extraction of information from text chunks.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="LLM PROMPTS" target="GRAPH INDEX">
      <data key="d4">1.0</data>
      <data key="d5">LLM Prompts are used to construct the Graph Index by identifying and extracting instances of graph nodes and edges from the Text Chunks</data>
      <data key="d6">e7caf4256ddea71533af1c4c50444146</data>
    </edge>
    <edge source="GRAPH INDEX" target="PODCAST DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The Graph Index was created using a context window size of 600 tokens with 1 gleaning for the Podcast dataset, indicating that the Podcast dataset was used in the indexing process.</data>
      <data key="d6">53455f8552b0787cb13c5a03eb550842</data>
    </edge>
    <edge source="GRAPH INDEX" target="NEWS DATASET">
      <data key="d4">1.0</data>
      <data key="d5">The Graph Index was created using a context window size of 600 tokens with 0 gleanings for the News dataset, indicating that the News dataset was used in the indexing process.</data>
      <data key="d6">53455f8552b0787cb13c5a03eb550842</data>
    </edge>
    <edge source="GRAPH INDEX" target="GLOBAL SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The decision to invest in building a graph index depends on multiple factors, including the performance of global summarization of source texts, which can be competitive in many cases</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH INDEX" target="COMPUTE BUDGET">
      <data key="d4">1.0</data>
      <data key="d5">The decision to invest in building a graph index depends on the compute budget, as it can influence the feasibility of creating and maintaining the index</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH INDEX" target="LIFETIME QUERIES">
      <data key="d4">1.0</data>
      <data key="d5">The decision to invest in building a graph index depends on the expected number of lifetime queries per dataset, as it can affect the value obtained from the index</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH INDEX" target="GRAPH-RELATED RAG APPROACHES">
      <data key="d4">1.0</data>
      <data key="d5">The decision to invest in building a graph index depends on the value obtained from other aspects of the graph index, including the use of other graph-related RAG approaches</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH INDEX" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the graph index</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH NODES" target="COVARIATE PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The Covariate Prompt is used to associate additional attributes with Graph Nodes. The relationship indicates that the prompt helps in enriching the information associated with the nodes extracted from the text.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="LLM PROMPT" target="FEW-SHOT EXAMPLES">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Prompt utilizes Few-Shot Examples for in-context learning, enhancing its ability to identify entities and relationships specific to the domain of the document corpus.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="LLM PROMPT" target="NAMED ENTITIES">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Prompt is used to identify Named Entities within the text chunks. The relationship indicates that the prompt facilitates the extraction of named entities from the text.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="LLM PROMPT" target="SPECIALIZED KNOWLEDGE">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Prompt can be tailored to incorporate Specialized Knowledge by using domain-specific few-shot examples. The relationship indicates that the prompt can be adapted to better handle specialized knowledge.</data>
      <data key="d6">e50740c4332fdedb8739773592e2a402</data>
    </edge>
    <edge source="ELEMENT SUMMARIES" target="LLM (LARGE LANGUAGE MODEL)">
      <data key="d4">1.0</data>
      <data key="d5">The LLM generates Element Summaries by processing Element Instances, creating condensed representations of information for each graph element, such as entity nodes, relationship edges, and claim covariates</data>
      <data key="d6">a73d3e7b661743b7583d8a0fd412b6a7</data>
    </edge>
    <edge source="ELEMENT SUMMARIES" target="ELEMENT INSTANCES">
      <data key="d4">1.0</data>
      <data key="d5">Element Summaries are derived from Element Instances, as the LLM processes these instances to create condensed representations of information for each graph element</data>
      <data key="d6">a73d3e7b661743b7583d8a0fd412b6a7</data>
    </edge>
    <edge source="ENTITY GRAPH" target="CONNECTIVITY">
      <data key="d4">1.0</data>
      <data key="d5">Connectivity is essential for the resilience of the entity graph approach to variations in entity names, ensuring that all variations are connected to a shared set of closely-related entities</data>
      <data key="d6">6dace8e490674ac8e031aed987a63789</data>
    </edge>
    <edge source="ENTITY GRAPH" target="RICH DESCRIPTIVE TEXT">
      <data key="d4">1.0</data>
      <data key="d5">Rich Descriptive Text is used in the entity graph to provide detailed information about homogeneous nodes, aligning with the capabilities of LLMs</data>
      <data key="d6">6dace8e490674ac8e031aed987a63789</data>
    </edge>
    <edge source="ENTITY GRAPH" target="KNOWLEDGE GRAPHS">
      <data key="d4">1.0</data>
      <data key="d5">The entity graph is differentiated from typical knowledge graphs by its use of rich descriptive text and its focus on global, query-focused summarization</data>
      <data key="d6">6dace8e490674ac8e031aed987a63789</data>
    </edge>
    <edge source="ENTITY GRAPH" target="LEIDEN ALGORITHM">
      <data key="d4">1.0</data>
      <data key="d5">The Leiden Algorithm is used in the entity graph pipeline for community detection, due to its efficiency in handling large-scale graphs</data>
      <data key="d6">6dace8e490674ac8e031aed987a63789</data>
    </edge>
    <edge source="LEIDEN ALGORITHM" target="GRAPHCOMMUNITIES">
      <data key="d4">1.0</data>
      <data key="d5">The Leiden Algorithm is used to generate GraphCommunities by partitioning the graph into communities of nodes with stronger connections to one another than to the other nodes in the graph</data>
      <data key="d6">a660289d2bf43f25d3524d35cd2d9a96</data>
    </edge>
    <edge source="CHUNKS OF PRE-SPECIFIED TOKEN SIZE" target="INTERMEDIATE ANSWERS">
      <data key="d4">1.0</data>
      <data key="d5">Chunks of pre-specified token size are related to intermediate answers as they are the input for generating these answers. The process ensures that each chunk is analyzed independently to produce relevant intermediate answers.</data>
      <data key="d6">aed2ea39de8a027cc818c7f4557f0514</data>
    </edge>
    <edge source="INTERMEDIATE ANSWERS" target="GLOBAL ANSWER">
      <data key="d4">1.0</data>
      <data key="d5">Intermediate answers are related to the global answer as they are sorted and combined to create the final context used for generating the global answer. The helpfulness score of each intermediate answer determines its inclusion in the global answer.</data>
      <data key="d6">aed2ea39de8a027cc818c7f4557f0514</data>
    </edge>
    <edge source="DATASET" target="RUN_PIPELINE">
      <data key="d4">1.0</data>
      <data key="d5">run_pipeline function takes the dataset as input and processes it according to the workflows specified</data>
      <data key="d6">f3a07680cbe8ab1f6055369da05f4f38</data>
    </edge>
    <edge source="EDUCATOR" target="HEALTH LITERACY">
      <data key="d4">1.0</data>
      <data key="d5">Educators can use health articles and current affairs to highlight the importance of health literacy, teaching students how to understand and use health information effectively</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="PRIVACY LAWS" target="TECHNOLOGY DEVELOPMENT">
      <data key="d4">1.0</data>
      <data key="d5">Privacy laws can impact technology development by setting restrictions on data usage and influencing the design and implementation of products and services</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="TECHNOLOGY DEVELOPMENT" target="ETHICAL CONSIDERATIONS">
      <data key="d4">1.0</data>
      <data key="d5">Technology development is influenced by ethical considerations, which ensure that innovations are developed and used responsibly, considering the impact on individuals and society</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="COLLABORATIONS" target="TECH COMPANIES">
      <data key="d4">1.0</data>
      <data key="d5">Collaborations between tech companies and governments can lead to the development of new technologies, policies, or solutions that benefit society</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="HEALTH EDUCATION CURRICULA" target="PREVENTIVE MEDICINE">
      <data key="d4">1.0</data>
      <data key="d5">Health education curricula can include topics on preventive medicine, teaching students about practices that prevent diseases and promote health</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="HEALTH ARTICLES" target="PUBLIC HEALTH PRIORITIES">
      <data key="d4">1.0</data>
      <data key="d5">Health articles can provide insights into public health priorities based on the topics they cover and the emphasis they place on certain health issues</data>
      <data key="d6">5a5a94f85dfc4d119ebb87f3037fd1cc</data>
    </edge>
    <edge source="HOTPOTQA" target="MULTIHOP-RAG">
      <data key="d4">1.0</data>
      <data key="d5">HotPotQA, MultiHop-RAG, and MT-Bench are all benchmark datasets for open-domain question answering, with a focus on explicit fact retrieval rather than summarization for data sensemaking</data>
      <data key="d6">5d04129d46662571f635a4e63cb4d6b7</data>
    </edge>
    <edge source="HOTPOTQA" target="DATA SENSEMAKING">
      <data key="d4">1.0</data>
      <data key="d5">HotPotQA, as a benchmark dataset for open-domain question answering, does not target data sensemaking as its questions focus on explicit fact retrieval rather than summarization for understanding data</data>
      <data key="d6">8e69f04648f5fc24c299591365f1aa68</data>
    </edge>
    <edge source="MULTIHOP-RAG" target="DATA SENSEMAKING">
      <data key="d4">1.0</data>
      <data key="d5">MultiHop-RAG, as a benchmark dataset for open-domain question answering, does not target data sensemaking as its questions focus on explicit fact retrieval rather than summarization for understanding data</data>
      <data key="d6">8e69f04648f5fc24c299591365f1aa68</data>
    </edge>
    <edge source="MT-BENCH" target="DATA SENSEMAKING">
      <data key="d4">1.0</data>
      <data key="d5">MT-Bench, as a benchmark dataset for open-domain question answering, does not target data sensemaking as its questions focus on explicit fact retrieval rather than summarization for understanding data</data>
      <data key="d6">8e69f04648f5fc24c299591365f1aa68</data>
    </edge>
    <edge source="DATA SENSEMAKING" target="SUMMARIZATION QUERIES">
      <data key="d4">1.0</data>
      <data key="d5">Summarization queries are essential for data sensemaking as they aim to extract a high-level understanding of dataset contents, facilitating the process of inspecting, engaging with, and contextualizing data within real-world activities</data>
      <data key="d6">8e69f04648f5fc24c299591365f1aa68</data>
    </edge>
    <edge source="SUMMARIZATION QUERIES" target="RAG (RETRIEVAL-AUGMENTED GENERATION) SYSTEMS">
      <data key="d4">1.0</data>
      <data key="d5">RAG systems are evaluated using summarization queries to assess their effectiveness in global sensemaking tasks, where a high-level understanding of dataset contents is required rather than details of specific texts</data>
      <data key="d6">8e69f04648f5fc24c299591365f1aa68</data>
    </edge>
    <edge source="EVALUATION DATASETS" target="GRAPH RAG (RETRIEVAL-AUGMENTED GENERATION)">
      <data key="d4">1.0</data>
      <data key="d5">Graph RAG uses summaries from different levels of graph communities to answer queries in the Evaluation Datasets</data>
      <data key="d6">a739018eb63cbb6c26b779bd37afc233</data>
    </edge>
    <edge source="EVALUATION DATASETS" target="TEXT SUMMARIZATION METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The Text Summarization Method is applied to the Evaluation Datasets to generate summaries for comparison</data>
      <data key="d6">a739018eb63cbb6c26b779bd37afc233</data>
    </edge>
    <edge source="EVALUATION DATASETS" target="NAIVE SEMANTIC SEARCH RAG APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">The Naive Semantic Search RAG Approach is used on the Evaluation Datasets as a baseline for comparison</data>
      <data key="d6">a739018eb63cbb6c26b779bd37afc233</data>
    </edge>
    <edge source="C1" target="C2">
      <data key="d4">1.0</data>
      <data key="d5">C1 communities are higher-level summaries that can be projected down to form C2 communities, which are sub-communities of C1 if C1 is present. This relationship indicates a hierarchical structure in the community summaries.</data>
      <data key="d6">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </edge>
    <edge source="C1" target="C0">
      <data key="d4">1.0</data>
      <data key="d5">C0 communities can be projected down to form C1 communities if C0 is present. This relationship indicates a hierarchical structure in the community summaries, with C1 being a sub-community of C0.</data>
      <data key="d6">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </edge>
    <edge source="C2" target="C3">
      <data key="d4">1.0</data>
      <data key="d5">C2 communities are higher-level summaries that can be projected down to form C3 communities, which are sub-communities of C2 if C2 is present. This relationship indicates a hierarchical structure in the community summaries.</data>
      <data key="d6">88847c4d3e6c5a64a5b44d9d99d06237</data>
    </edge>
    <edge source="PODCAST DATASET" target="NEWS DATASET">
      <data key="d4">2.0</data>
      <data key="d5">The Podcast Dataset and the News Dataset are integral components in the domain of language model evaluation and natural language processing research. These datasets serve dual purposes, contributing to the assessment of language models' performance under different context window sizes and facilitating experiments in summarization and question answering. The News Dataset, larger in scale with a higher density of nodes and edges in the graph generated through indexing, complements the Podcast Dataset in size and scope. Both datasets are pivotal in gauging the effectiveness of summarization methods, where certain levels of community summaries have demonstrated remarkable outcomes in terms of comprehensiveness and diversity. Through their utilization in various research contexts, these datasets offer valuable insights into the capabilities and limitations of language models and summarization techniques.</data>
      <data key="d6">3900d15a5f3ace358fc06038c34cdf79,71f14506a6b15dfabd93fd1606a67b73</data>
    </edge>
    <edge source="PODCAST DATASET" target="WINDOWSIZE">
      <data key="d4">1.0</data>
      <data key="d5">The windowsize of 8k tokens is used in the indexing process for the Podcast Dataset, affecting the resulting graph's structure and size</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="NEWS DATASET" target="WINDOWSIZE">
      <data key="d4">1.0</data>
      <data key="d5">The windowsize of 8k tokens is used in the indexing process for the News Dataset, affecting the resulting graph's structure and size</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="WANG ET AL., 2023A" target="LLM AS-A-JUDGE, ZHENG ET AL., 2024">
      <data key="d4">1.0</data>
      <data key="d5">Wang et al., 2023a, and LLM as-a-judge, Zheng et al., 2024, are related through their contributions to the field of LLMs, particularly in the context of head-to-head comparisons of competing outputs</data>
      <data key="d6">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </edge>
    <edge source="RAGAS, ES ET AL., 2023" target="GRAPH RAG MECHANISM">
      <data key="d4">1.0</data>
      <data key="d5">RAGAS, Es et al., 2023, and the Graph RAG mechanism are related in the context of evaluating RAG systems, where the Graph RAG mechanism is a multi-stage method that could potentially be assessed using the evaluation criteria discussed in RAGAS, Es et al., 2023</data>
      <data key="d6">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </edge>
    <edge source="HEAD-TO-HEAD COMPARISON APPROACH" target="LLM EVALUATOR">
      <data key="d4">1.0</data>
      <data key="d5">The head-to-head comparison approach is related to the LLM evaluator, as the LLM evaluator is used as a tool to implement the head-to-head comparison approach in evaluating different methods or outputs</data>
      <data key="d6">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </edge>
    <edge source="LLM EVALUATOR" target="COMPREHENSIVENESS">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Evaluator uses Comprehensiveness as one of the metrics to assess the quality of answers. It evaluates how well the answers cover all aspects and details of the question.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="LLM EVALUATOR" target="DIVERSITY">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Evaluator uses Diversity as one of the metrics to assess the quality of answers. It evaluates how varied and rich the answers are in providing different perspectives and insights on the question.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="LLM EVALUATOR" target="EMPOWERMENT">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Evaluator uses Empowerment as one of the metrics to assess the quality of answers. It evaluates how well the answers help the reader understand and make informed judgements about the topic.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="LLM EVALUATOR" target="DIRECTNESS">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Evaluator uses Directness as one of the metrics to assess the quality of answers. It evaluates how specifically and clearly the answers address the question.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="LLM EVALUATOR" target="TABLE 2">
      <data key="d4">1.0</data>
      <data key="d5">Table 2 is an output of the LLM Evaluator, showing an example of the assessment generated by the tool based on the metrics and conditions evaluated.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="TARGET METRICS" target="CONTROL METRIC (DIRECTNESS)">
      <data key="d4">1.0</data>
      <data key="d5">The target metrics are related to the control metric (directness) in the context of evaluation, as the control metric serves as a baseline for assessing the validity of the target metrics in evaluating the performance of methods or outputs</data>
      <data key="d6">cbfd4a09b266218f64dc6e6d80f8a77e</data>
    </edge>
    <edge source="COMPREHENSIVENESS" target="OPTIMUM CONTEXT SIZE">
      <data key="d4">1.0</data>
      <data key="d5">The optimum context size (8k) was found to have the best performance on the comprehensiveness metric, with an average win rate of 58.1%. This indicates that the 8k context window size provides the most comprehensive answers or responses among the tested sizes.</data>
      <data key="d6">3900d15a5f3ace358fc06038c34cdf79</data>
    </edge>
    <edge source="COMPREHENSIVENESS" target="GLOBAL APPROACHES">
      <data key="d4">1.0</data>
      <data key="d5">Global Approaches achieved higher comprehensiveness win rates for both Podcast transcripts and News articles</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="DIVERSITY" target="OPTIMUM CONTEXT SIZE">
      <data key="d4">1.0</data>
      <data key="d5">The optimum context size (8k) was found to have comparable performance on the diversity metric, with an average win rate of 52.4%. This indicates that the 8k context window size provides answers or responses with a range and uniqueness similar to those generated with larger context sizes.</data>
      <data key="d6">3900d15a5f3ace358fc06038c34cdf79</data>
    </edge>
    <edge source="DIVERSITY" target="GLOBAL APPROACHES">
      <data key="d4">1.0</data>
      <data key="d5">Global Approaches achieved higher diversity win rates for both Podcast transcripts and News articles</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="EMPOWERMENT" target="OPTIMUM CONTEXT SIZE">
      <data key="d4">1.0</data>
      <data key="d5">The optimum context size (8k) was found to have comparable performance on the empowerment metric, with an average win rate of 51.3%. This indicates that the 8k context window size provides answers or responses that are as empowering as those generated with larger context sizes.</data>
      <data key="d6">3900d15a5f3ace358fc06038c34cdf79</data>
    </edge>
    <edge source="DIRECTNESS" target="NAIVE RAG (SS) APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">The Naive RAG (SS) Approach produced the most direct responses across all comparisons, as measured by the directness validity test</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="COMPARISON METHOD" target="STOCHASTICITY">
      <data key="d4">1.0</data>
      <data key="d5">The Comparison Method accounts for the stochasticity of LLMs by running each comparison five times and using mean scores to determine the outcome.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="COMPARISON METHOD" target="FIGURE 4">
      <data key="d4">1.0</data>
      <data key="d5">Figure 4 visualizes the results of the head-to-head comparisons conducted using the Comparison Method, showing the win rate percentages across different conditions and metrics.</data>
      <data key="d6">2a5e1212b351d63d059ba1a1dec2811f</data>
    </edge>
    <edge source="HEADWINRATEPERCENTAGES" target="GRAPHRAGCONDITIONS">
      <data key="d4">1.0</data>
      <data key="d5">The headwin rate percentages are related to the performance of GraphRAG conditions, as they measure the success rates of these conditions across datasets, metrics, and questions. GraphRAG conditions outperformed naive RAG on comprehensiveness and diversity.</data>
      <data key="d6">b83d819b03401fb8332316960610e5d6</data>
    </edge>
    <edge source="GRAPHRAGCONDITIONS" target="CONTEXTWINDOWSIZE">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG conditions are related to the context window size, as varying the context window size can affect the performance of GraphRAG conditions. The optimum context size for the baseline condition (SS) was determined and then used for all query-time LLM use, which can impact the performance of GraphRAG conditions.</data>
      <data key="d6">b83d819b03401fb8332316960610e5d6</data>
    </edge>
    <edge source="VARYING CONTEXT WINDOW SIZE" target="OPTIMUM CONTEXT SIZE">
      <data key="d4">1.0</data>
      <data key="d5">The relationship between varying the context window size and determining the optimum context size is that the optimum context size was found through testing different context window sizes (8k, 16k, 32k, 64k) and evaluating their performance on comprehensiveness, diversity, and empowerment metrics. The smallest context window size (8k) was found to be universally better for comprehensiveness and comparable for diversity and empowerment, leading to its selection as the optimum context size.</data>
      <data key="d6">3900d15a5f3ace358fc06038c34cdf79</data>
    </edge>
    <edge source="GLOBAL APPROACHES" target="NAIVE RAG (SS) APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">Global Approaches outperformed the Naive RAG (SS) Approach in terms of comprehensiveness and diversity metrics across datasets</data>
      <data key="d6">d08fc91bbfe9749abab38a99a1a88dc6</data>
    </edge>
    <edge source="INFORMED UNDERSTANDING" target="TUNING ELEMENT EXTRACTION PROMPTS">
      <data key="d4">1.0</data>
      <data key="d5">Tuning Element Extraction Prompts can contribute to a higher level of Informed Understanding by ensuring that more detailed information is retained in the Graph RAG index, thus providing a richer context for comprehension</data>
      <data key="d6">38feec52b8bfbd3fd8e03635acdaec97</data>
    </edge>
    <edge source="GRAPH RAG INDEX" target="RAG APPROACHES">
      <data key="d4">1.0</data>
      <data key="d5">RAG Approaches, including Naive RAG and Advanced RAG, are integral to the functionality of the Graph RAG Index, as they determine how information is retrieved and integrated into the index to support informed understanding</data>
      <data key="d6">38feec52b8bfbd3fd8e03635acdaec97</data>
    </edge>
    <edge source="ADVANCED RAG" target="CAUSAL GRAPH EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Causal Graph Extraction and Advanced RAG are both research studies that involve the use of LLMs in the context of advanced information retrieval and analysis, suggesting a shared interest in graph-based techniques.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="ADVANCED RAG" target="KAPING">
      <data key="d4">1.0</data>
      <data key="d5">Advanced RAG and KAPING are both research studies that involve the use of knowledge graphs as an index for advanced retrieval and analysis of information, indicating a connection in their research focus and methodology.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="ADVANCED RAG" target="LLMS FOR CAUSAL GRAPH EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">LLMs for causal graph extraction and Advanced RAG are related as they both involve the use of LLMs for extracting information from text, with causal graph extraction focusing on causal relationships and Advanced RAG focusing on retrieval-augmented generation</data>
      <data key="d6">7383c69e93bb8c8648181f5355d2c9a7</data>
    </edge>
    <edge source="LLM&#8217;S CONTEXT WINDOW" target="ADVANCED RAG SYSTEMS">
      <data key="d4">1.0</data>
      <data key="d5">Advanced RAG systems are designed to overcome the limitations of LLM&#8217;s context window by incorporating pre-retrieval, retrieval, and post-retrieval strategies. This relationship indicates that Advanced RAG systems are an evolution of LLM&#8217;s context window, addressing its drawbacks and enhancing its capabilities.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="ADVANCED RAG SYSTEMS" target="MODULAR RAG SYSTEMS">
      <data key="d4">1.0</data>
      <data key="d5">Modular RAG systems are a type of Advanced RAG systems that include patterns for iterative and dynamic cycles of interleaved retrieval and generation. This relationship indicates that Modular RAG systems are a specific implementation of Advanced RAG systems, focusing on modularity and flexibility.</data>
      <data key="d6">7da3d8d244b67f09425a4a7783e4bb55</data>
    </edge>
    <edge source="CAIRE-COVID" target="ITRG">
      <data key="d4">1.0</data>
      <data key="d5">Both CAiRE-COVID and ITRG are research studies that involve the use of LLMs and RAG, but CAiRE-COVID focuses on the impact of COVID-19, while ITRG deals with multi-hop question answering.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="ITRG" target="IR-COT">
      <data key="d4">1.0</data>
      <data key="d5">ITRG and IR-CoT are both research studies that deal with multi-hop question answering, indicating a connection in their research focus and methodology.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="IR-COT" target="DSP">
      <data key="d4">1.0</data>
      <data key="d5">IR-CoT and DSP are both research studies that deal with multi-hop question answering, suggesting a shared interest in advanced question answering techniques.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="RAPTOR" target="TREE OF CLARIFICATIONS">
      <data key="d4">1.0</data>
      <data key="d5">RAPTOR and Tree of Clarifications are both research studies that involve advanced text processing techniques, with RAPTOR focusing on text embeddings and Tree of Clarifications on answering ambiguous questions.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="KNOWLEDGE GRAPH CREATION" target="KNOWLEDGE GRAPH COMPLETION">
      <data key="d4">1.0</data>
      <data key="d5">Knowledge Graph Creation and Knowledge Graph Completion are both research studies that involve the use of LLMs in the context of knowledge graphs, suggesting a connection in their research focus and methodology.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="KNOWLEDGE GRAPH COMPLETION" target="CAUSAL GRAPH EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Knowledge Graph Completion and Causal Graph Extraction are both research studies that involve the use of LLMs in the context of graph analysis, indicating a connection in their research focus and methodology.</data>
      <data key="d6">40f2d6a0270e54743e7ace239369da96</data>
    </edge>
    <edge source="KAPING" target="G-RETRIEVER">
      <data key="d4">1.0</data>
      <data key="d5">KAPING and G-Retriever are related as they both involve querying specific parts of a graph structure, with KAPING focusing on subsets of the graph and G-Retriever focusing on retrieving specific parts of the graph</data>
      <data key="d6">7383c69e93bb8c8648181f5355d2c9a7</data>
    </edge>
    <edge source="LLMS FOR KNOWLEDGE GRAPH CREATION" target="LLMS FOR KNOWLEDGE GRAPH COMPLETION">
      <data key="d4">1.0</data>
      <data key="d5">LLMs for knowledge graph creation and LLMs for knowledge graph completion are related as they both involve the use of LLMs in the context of knowledge graphs, with creation focusing on the initial generation and completion focusing on enhancing existing graphs</data>
      <data key="d6">7383c69e93bb8c8648181f5355d2c9a7</data>
    </edge>
    <edge source="GRAPHTOOLFORMER" target="SURGE">
      <data key="d4">1.0</data>
      <data key="d5">GraphToolFormer and SURGE are related as they both involve the use of graph data, with GraphToolFormer focusing on derived graph metrics and SURGE focusing on generating narratives based on retrieved subgraphs</data>
      <data key="d6">7383c69e93bb8c8648181f5355d2c9a7</data>
    </edge>
    <edge source="FABULA" target="SYSTEM FOR MULTI-HOP QUESTION ANSWERING">
      <data key="d4">1.0</data>
      <data key="d5">FABULA and the system for multi-hop question answering are related as they both involve the use of graph data for generating narratives or answering questions, with FABULA focusing on event plots and the system for multi-hop question answering focusing on complex question answering</data>
      <data key="d6">7383c69e93bb8c8648181f5355d2c9a7</data>
    </edge>
    <edge source="LANGCHAIN" target="LLAMAINDEX">
      <data key="d4">2.0</data>
      <data key="d5">LangChain and LlamaIndex are two sophisticated libraries that share a common purpose: they facilitate the creation and navigation of text-relationship graphs, which are essential for multi-hop question answering. These libraries are designed to be versatile, as they are compatible with a wide range of graph databases, making them valuable assets for RAG (Retrieval-Augmented Generation) applications. Both LangChain and LlamaIndex offer tools and interfaces that enable users to effectively work with graph data, enhancing the capabilities of systems that rely on graph databases for information retrieval and analysis.</data>
      <data key="d6">7383c69e93bb8c8648181f5355d2c9a7,e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="LANGCHAIN" target="NEO4J">
      <data key="d4">1.0</data>
      <data key="d5">LangChain supports the use of Neo4J, a graph database that can be used to create and reason over knowledge graphs</data>
      <data key="d6">e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="LANGCHAIN" target="NEBULAGRAPH">
      <data key="d4">1.0</data>
      <data key="d5">LangChain supports the use of NebulaGraph, a graph database that can be used to create and reason over knowledge graphs</data>
      <data key="d6">e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="LLAMAINDEX" target="NEO4J">
      <data key="d4">1.0</data>
      <data key="d5">LlamaIndex supports the use of Neo4J, a graph database that can be used to create and reason over knowledge graphs</data>
      <data key="d6">e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="LLAMAINDEX" target="NEBULAGRAPH">
      <data key="d4">1.0</data>
      <data key="d5">LlamaIndex supports the use of NebulaGraph, a graph database that can be used to create and reason over knowledge graphs</data>
      <data key="d6">e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="EVALUATION APPROACH" target="SELFCHECKGPT">
      <data key="d4">1.0</data>
      <data key="d5">Using SelfCheckGPT to compare fabrication rates would improve the current analysis of Graph RAG's performance</data>
      <data key="d6">e015335cdcae20e6546fe7cbdef56c1a</data>
    </edge>
    <edge source="1 MILLION TOKENS" target="QUESTION TYPES">
      <data key="d4">1.0</data>
      <data key="d5">Understanding how performance varies across different ranges of question types is crucial for analyzing the effectiveness of models or systems when dealing with a region of 1 million tokens</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="1 MILLION TOKENS" target="DATA TYPES">
      <data key="d4">1.0</data>
      <data key="d5">Understanding how performance varies across different data types is crucial for analyzing the effectiveness of models or systems when dealing with a region of 1 million tokens</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="1 MILLION TOKENS" target="DATASET SIZES">
      <data key="d4">1.0</data>
      <data key="d5">Understanding how performance varies across different dataset sizes is crucial for analyzing the effectiveness of models or systems when dealing with a region of 1 million tokens</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="QUESTION TYPES" target="DATA TYPES">
      <data key="d4">1.0</data>
      <data key="d5">Understanding how performance varies across different data types is crucial for analyzing the effectiveness of models or systems when dealing with various question types</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="QUESTION TYPES" target="DATASET SIZES">
      <data key="d4">1.0</data>
      <data key="d5">Understanding how performance varies across different dataset sizes is crucial for analyzing the effectiveness of models or systems when dealing with various question types</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="QUESTION TYPES" target="END USERS">
      <data key="d4">1.0</data>
      <data key="d5">Validating sensemaking questions and target metrics with end users is crucial for understanding how performance varies across different question types</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="DATA TYPES" target="DATASET SIZES">
      <data key="d4">1.0</data>
      <data key="d5">Understanding how performance varies across different dataset sizes is crucial for analyzing the effectiveness of models or systems when dealing with various data types</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="DATA TYPES" target="END USERS">
      <data key="d4">1.0</data>
      <data key="d5">Validating sensemaking questions and target metrics with end users is crucial for understanding how performance varies across different data types</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="DATASET SIZES" target="END USERS">
      <data key="d4">1.0</data>
      <data key="d5">Validating sensemaking questions and target metrics with end users is crucial for understanding how performance varies across different dataset sizes</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GLOBAL SUMMARIZATION" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the global summarization technique</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="COMPUTE BUDGET" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the consideration of the compute budget</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="LIFETIME QUERIES" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the consideration of the expected number of lifetime queries per dataset</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="GRAPH-RELATED RAG APPROACHES" target="FUTURE WORK">
      <data key="d4">1.0</data>
      <data key="d5">Future work includes the possibility of refining and adapting the current Graph RAG approach, which can involve the consideration of other graph-related RAG approaches</data>
      <data key="d6">7040ba36a7c09899a355d14a30d65375</data>
    </edge>
    <edge source="ANSWER IIVENESS AND DIVERSITY" target="GLOBAL BUT GRAPH-FREE APPROACH">
      <data key="d4">1.0</data>
      <data key="d5">The Answer Iiveness and Diversity can be compared to the results obtained from a Global but Graph-free Approach, which uses map-reduce source text summarization. The comparison helps to evaluate the effectiveness and efficiency of different methods in generating diverse and rich answers or solutions.</data>
      <data key="d6">e31d2d134cf501c93f9445914d7350f9</data>
    </edge>
    <edge source="ENTITY-BASED GRAPH INDEX" target="ROOT-LEVEL COMMUNITIES">
      <data key="d4">1.0</data>
      <data key="d5">The Entity-based Graph Index provides summaries of root-level communities, which are the fundamental or top-level groups within a dataset. The summaries help to improve the data index and facilitate global queries over the same dataset.</data>
      <data key="d6">e31d2d134cf501c93f9445914d7350f9</data>
    </edge>
    <edge source="ENTITY-BASED GRAPH INDEX" target="GLOBAL METHODS">
      <data key="d4">1.0</data>
      <data key="d5">The Entity-based Graph Index achieves competitive performance to other Global Methods at a fraction of the token cost. This indicates that the Entity-based Graph Index is a more cost-effective and efficient method for handling global queries over large datasets.</data>
      <data key="d6">e31d2d134cf501c93f9445914d7350f9</data>
    </edge>
    <edge source="GRAPHRAG INDEXING" target="INDEXING PIPELINES">
      <data key="d4">1.0</data>
      <data key="d5">GraphRAG Indexing includes Indexing Pipelines as a core component of its suite, which are responsible for the extraction and transformation of data from unstructured text into structured formats. The pipelines are configurable and can be customized to meet specific needs, making them integral to the overall functionality of GraphRAG Indexing.</data>
      <data key="d6">251e8d332b451d900df961cbe215bca0</data>
    </edge>
    <edge source="GRAPHRAG INDEXING" target="DEFAULT CONFIGURATION MODE">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Indexing system includes the Default Configuration Mode as one of its configuration options, which is designed for simplicity and ease of use for most users</data>
      <data key="d6">ccd2de9e2219521fbca779843c65af58</data>
    </edge>
    <edge source="GRAPHRAG INDEXING" target="CUSTOM CONFIGURATION MODE">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Indexing system includes the Custom Configuration Mode as an advanced configuration option, which is designed for users who require deeper control over the system's configuration</data>
      <data key="d6">ccd2de9e2219521fbca779843c65af58</data>
    </edge>
    <edge source="INDEXING PIPELINES" target="ENTITY EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Indexing Pipelines utilize Entity Extraction as one of their functions to identify and extract meaningful entities from unstructured text. This step is essential for the subsequent analysis and transformation of data within the pipelines.</data>
      <data key="d6">251e8d332b451d900df961cbe215bca0</data>
    </edge>
    <edge source="INDEXING PIPELINES" target="RELATIONSHIP DETECTION">
      <data key="d4">1.0</data>
      <data key="d5">Indexing Pipelines incorporate Relationship Detection to identify relationships between entities extracted from text. This function is crucial for understanding the context and connections within the data, enhancing the structured outputs of the pipelines.</data>
      <data key="d6">251e8d332b451d900df961cbe215bca0</data>
    </edge>
    <edge source="INDEXING PIPELINES" target="COMMUNITY DETECTION">
      <data key="d4">1.0</data>
      <data key="d5">Indexing Pipelines use Community Detection to identify groups of related entities within the extracted data. This function aids in summarizing and reporting on the text at multiple levels of granularity, contributing to the structured data outputs of the pipelines.</data>
      <data key="d6">251e8d332b451d900df961cbe215bca0</data>
    </edge>
    <edge source="INDEXING PIPELINES" target="DATA EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Indexing Pipelines employ Data Embedding to transform extracted entities and text chunks into vector representations. This process facilitates further analysis and storage of the structured data, making it an integral part of the pipelines' functionality.</data>
      <data key="d6">251e8d332b451d900df961cbe215bca0</data>
    </edge>
    <edge source="INDEXING PIPELINES" target="OUTPUT FORMATS">
      <data key="d4">1.0</data>
      <data key="d5">Indexing Pipelines generate structured data that can be stored in various Output Formats, including JSON and Parquet. These formats, along with the option to access outputs via the Python API, provide flexibility in how the data is handled and used.</data>
      <data key="d6">251e8d332b451d900df961cbe215bca0</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="FIELDS">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section contains the Fields, which are specific configuration options for entity extraction settings</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="PARALLELIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the Parallelization property, specifying the parallelization settings for processing text</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="ASYNC MODE">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the Async Mode property, specifying whether to process text asynchronously</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="PROMPT">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the Prompt property, specifying the prompt file to use for guiding the text analysis</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="ENTITY TYPES">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the Entity Types property, specifying the types of entities to identify</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="MAX GLEANINGS">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the Max Gleanings property, specifying the maximum number of gleaning cycles to use</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ENTITY EXTRACTION" target="STRATEGY">
      <data key="d4">1.0</data>
      <data key="d5">The Entity Extraction section includes the Strategy property, specifying the strategy to fully override the entity extraction process</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="COMMUNITY SUMMARIZATION">
      <data key="d4">2.0</data>
      <data key="d5">Community Detection and Community Summarization are intricately linked processes within the domain of Social Network Analysis. Community Detection involves the identification of groups of related entities within a graph, which is a crucial step in understanding the structure of specialized professional networks. This process enables the pinpointing of key influencers and mapping of complex relationships, thereby facilitating the identification of collaboration opportunities and knowledge gaps in fields such as Motor Control and Drive Systems.

Community Summarization, on the other hand, relies heavily on the outcomes of Community Detection. It involves the aggregation and analysis of the characteristics of communities identified through detection, providing a concise and comprehensive overview of each group's dynamics, composition, and influence. This summarization is essential for gaining insights into the collective behavior and trends within communities, further enriching the understanding of the network's overall structure and facilitating strategic decision-making and network navigation.</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="COMMUNITY TABLES">
      <data key="d4">1.0</data>
      <data key="d5">Community Tables are related to Community Detection as they are often used to store and organize the results of community detection algorithms, including details about the communities and their members</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="COMMUNITY DETECTION" target="PHASE 3: GRAPH AUGMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">Community Detection is a subprocess of Phase 3: Graph Augmentation, aimed at understanding the community structure of the graph</data>
      <data key="d6">a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </edge>
    <edge source="CONFIG FILE" target="TIMESTAMPED OUTPUT FOLDER">
      <data key="d4">1.0</data>
      <data key="d5">The Timestamped Output Folder is related to the Config File as the Config File may contain settings that determine the naming and location of the Timestamped Output Folder</data>
      <data key="d6">919cb44d9688a14bf48fa7c98163ed81</data>
    </edge>
    <edge source="CONFIG FILE" target="PROGRESS REPORTER">
      <data key="d4">1.0</data>
      <data key="d5">The Config File is related to the Progress Reporter as it may contain settings that determine the type of Progress Reporter to be used during the process</data>
      <data key="d6">919cb44d9688a14bf48fa7c98163ed81</data>
    </edge>
    <edge source="CONFIG FILE" target="TABLE OUTPUT FORMATS">
      <data key="d4">1.0</data>
      <data key="d5">The Config File is related to the Table Output Formats as it may contain settings that determine the formats in which the pipeline should emit the table output</data>
      <data key="d6">919cb44d9688a14bf48fa7c98163ed81</data>
    </edge>
    <edge source="CONFIG FILE" target="CACHING MECHANISM">
      <data key="d4">1.0</data>
      <data key="d5">The Config File is related to the Caching Mechanism as it may contain settings that determine whether the Caching Mechanism is enabled or disabled</data>
      <data key="d6">919cb44d9688a14bf48fa7c98163ed81</data>
    </edge>
    <edge source="NODE" target="ENTITY">
      <data key="d4">1.0</data>
      <data key="d5">Nodes contain layout information for rendered graph-views of the entities, showing how entities are embedded and clustered</data>
      <data key="d6">85e50a4d70697a2c4420e7a9fc82f22d</data>
    </edge>
    <edge source="NODE" target="COMMUNITY REPORT">
      <data key="d4">1.0</data>
      <data key="d5">Community Reports are related to the Nodes in the graph, as they provide insights and summaries for the communities represented by the nodes</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="NODE" target="DEFAULT CONFIGURATION WORKFLOW">
      <data key="d4">1.0</data>
      <data key="d5">Nodes are part of the Default Configuration Workflow, as they are used in the visualization and analysis of the graph created by the workflow</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="COL1" target="COL_MULTIPLIED">
      <data key="d4">1.0</data>
      <data key="d5">The values in col1 are used as one of the inputs to calculate the values in col_multiplied, which is the result of multiplying col1 and col2</data>
      <data key="d6">f3a07680cbe8ab1f6055369da05f4f38</data>
    </edge>
    <edge source="COL1" target="WORKFLOW2">
      <data key="d4">1.0</data>
      <data key="d5">In the workflow2 process, col1 is used as an input in the derive step to process data</data>
      <data key="d6">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </edge>
    <edge source="COL2" target="COL_MULTIPLIED">
      <data key="d4">1.0</data>
      <data key="d5">The values in col2 are used as one of the inputs to calculate the values in col_multiplied, which is the result of multiplying col1 and col2</data>
      <data key="d6">f3a07680cbe8ab1f6055369da05f4f38</data>
    </edge>
    <edge source="COL2" target="WORKFLOW2">
      <data key="d4">1.0</data>
      <data key="d5">In the workflow2 process, col2 is used as an input in the derive step to process data</data>
      <data key="d6">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </edge>
    <edge source="RUN_PIPELINE" target="WORKFLOWS">
      <data key="d4">1.0</data>
      <data key="d5">run_pipeline function uses the workflows to determine how to process the dataset</data>
      <data key="d6">f3a07680cbe8ab1f6055369da05f4f38</data>
    </edge>
    <edge source="RUN_PIPELINE" target="OUTPUTS">
      <data key="d4">1.0</data>
      <data key="d5">The results of the run_pipeline function are collected in the outputs list</data>
      <data key="d6">f3a07680cbe8ab1f6055369da05f4f38</data>
    </edge>
    <edge source="WORKFLOWS" target="WORKFLOW NAME">
      <data key="d4">1.0</data>
      <data key="d5">The Workflows property is related to the Workflow Name property, as the workflow name is used to define and reference specific workflows within the workflows section</data>
      <data key="d6">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </edge>
    <edge source="OUTPUTS" target="PIPELINE_RESULT">
      <data key="d4">1.0</data>
      <data key="d5">The last element in the outputs list, pipeline_result, represents the final result of the pipeline execution</data>
      <data key="d6">f3a07680cbe8ab1f6055369da05f4f38</data>
    </edge>
    <edge source="DEFAULT PROMPTS" target="ENTITY/RELATIONSHIP EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Default Prompts include the function of Entity/Relationship Extraction, which is a fundamental step in the creation of a knowledge graph. This relationship indicates that Entity/Relationship Extraction is one of the functions provided by the Default Prompts</data>
      <data key="d6">bdb8f9e797229f596744d9636ab857b0</data>
    </edge>
    <edge source="DEFAULT PROMPTS" target="ENTITY/RELATIONSHIP DESCRIPTION SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Default Prompts include the function of Entity/Relationship Description Summarization, which provides concise summaries of entity and relationship descriptions. This relationship indicates that Entity/Relationship Description Summarization is one of the functions provided by the Default Prompts</data>
      <data key="d6">bdb8f9e797229f596744d9636ab857b0</data>
    </edge>
    <edge source="DEFAULT PROMPTS" target="CLAIM EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Default Prompts include the function of Claim Extraction, which identifies and extracts claims from the input data. This relationship indicates that Claim Extraction is one of the functions provided by the Default Prompts</data>
      <data key="d6">bdb8f9e797229f596744d9636ab857b0</data>
    </edge>
    <edge source="ENTITY/RELATIONSHIP EXTRACTION" target="TOKEN-REPLACEMENTS">
      <data key="d4">1.0</data>
      <data key="d5">Entity/Relationship Extraction utilizes Token-Replacements to process input text and generate tuples representing entities or relationships, enhancing the flexibility and specificity of the extraction process</data>
      <data key="d6">6a7157695d90d434b2625c3f05420916</data>
    </edge>
    <edge source="CLAIM EXTRACTION" target="ENTITY RESOLUTION">
      <data key="d4">1.0</data>
      <data key="d5">Entity Resolution is related to Claim Extraction, as the resolution of entities that refer to the same real-world object or concept is used in the identification of claims or statements about entities in the text</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="CLAIM EXTRACTION" target="GRAPH TABLES">
      <data key="d4">1.0</data>
      <data key="d5">Claim Extraction is related to Graph Tables, as the identification of claims or statements about entities in the text is used in the creation of the graph, which is represented in the Graph Tables</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="CLAIM EXTRACTION" target="TEXT UNITS">
      <data key="d4">1.0</data>
      <data key="d5">Claim Extraction is related to Text Units as the process often operates on Text Units to identify claims or assertions made in the text</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="CLAIM EXTRACTION" target="COVARIATES">
      <data key="d4">1.0</data>
      <data key="d5">Claim Extraction results in the emission of Covariates, which are positive factual statements with an evaluated status and time-bounds.</data>
      <data key="d6">d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </edge>
    <edge source="ROOT" target="DOMAIN">
      <data key="d4">1.0</data>
      <data key="d5">The root property specifies the path to the project directory, which contains the input data for the specified domain. The domain property is used to tailor the prompt generation to a specific subject area, which is determined by the input data located in the project directory specified by the root property.</data>
      <data key="d6">ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </edge>
    <edge source="ROOT" target="LIMIT">
      <data key="d4">1.0</data>
      <data key="d5">The root property specifies the path to the project directory, which contains the input data that is processed using the limit property to control the number of text units selected for template generation. The limit property determines the size of the sample used for prompt generation, which is taken from the input data located in the project directory specified by the root property.</data>
      <data key="d6">ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </edge>
    <edge source="ROOT" target="LANGUAGE">
      <data key="d4">1.0</data>
      <data key="d5">The root property specifies the path to the project directory, which contains the input data that is processed in the language specified by the language property. The language property ensures that the prompt generation is tailored to the language of the input documents, which are located in the project directory specified by the root property.</data>
      <data key="d6">ce9cc3ed2e5f890d02e867ed0b0f8ff9</data>
    </edge>
    <edge source="ROOT" target="STORAGE ACCOUNT BLOB URL">
      <data key="d4">1.0</data>
      <data key="d5">The Root is the base for the Storage Account Blob URL, as the URL is relative to the root directory specified in the configuration</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="ROOT" target="CLI ARGUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">The root argument is a CLI Argument that specifies the data root directory for the GraphRAG Indexer CLI, which contains the input data and environment variables.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="DOMAIN" target="LANGUAGE">
      <data key="d4">1.0</data>
      <data key="d5">DOMAIN and LANGUAGE are related as specifying the domain can help in determining the appropriate language for input processing, especially if the domain is associated with a specific language or if translation is needed</data>
      <data key="d6">9243633f55cccd0885ba553e14fa5e3f</data>
    </edge>
    <edge source="LANGUAGE" target="MAX_TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">The LANGUAGE option can influence the MAX_TOKENS setting, as the language used for input processing might affect the tokenization process and thus the maximum token count for prompt generation</data>
      <data key="d6">9243633f55cccd0885ba553e14fa5e3f</data>
    </edge>
    <edge source="MAX_TOKENS" target="MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The model and max_tokens are related because the model defines the LLM (Language Model) to be used, and the max_tokens sets the limit on the output size of that model</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="CHUNK_SIZE" target="OUTPUT">
      <data key="d4">1.0</data>
      <data key="d5">CHUNK_SIZE and OUTPUT are related as the size of chunks generated from input documents can impact the number and size of prompts saved in the output folder</data>
      <data key="d6">9243633f55cccd0885ba553e14fa5e3f</data>
    </edge>
    <edge source="CHUNK SIZE PARAMETER" target="RANDOM SELECTION METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The Chunk Size Parameter influences the Random Selection Method by determining the size of text units that are randomly selected for template generation</data>
      <data key="d6">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </edge>
    <edge source="CHUNK SIZE PARAMETER" target="TOP SELECTION METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The Chunk Size Parameter influences the Top Selection Method by determining the size of text units from which the head n units are selected for template generation</data>
      <data key="d6">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </edge>
    <edge source="CHUNK SIZE PARAMETER" target="ALL SELECTION METHOD">
      <data key="d4">1.0</data>
      <data key="d5">The Chunk Size Parameter influences the All Selection Method by determining the size of text units that are all used for template generation</data>
      <data key="d6">4f37c0e9c3c9bac4e5c1c6821eea442e</data>
    </edge>
    <edge source="CUSTOM PROMPT FILE" target="TOKEN-REPLACEMENTS">
      <data key="d4">1.0</data>
      <data key="d5">Token-Replacements are a key feature of the Custom Prompt File, enabling the customization of prompts through the use of placeholders that are replaced with actual values</data>
      <data key="d6">6a7157695d90d434b2625c3f05420916</data>
    </edge>
    <edge source="TOKEN-REPLACEMENTS" target="SUMMARIZE ENTITY/RELATIONSHIP DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">Summarize Entity/Relationship Descriptions uses Token-Replacements to process a list of descriptions for an entity or relationship, facilitating the summarization of these descriptions</data>
      <data key="d6">6a7157695d90d434b2625c3f05420916</data>
    </edge>
    <edge source="RECORD_DELIMITER" target="TUPLE_DELIMITER">
      <data key="d4">1.0</data>
      <data key="d5">The record_delimiter is used to separate tuple instances, and the tuple_delimiter is used to separate values within a tuple. These delimiters work together to structure and organize data in a clear and readable format</data>
      <data key="d6">853bfe9a74a916130a20f81506bcaf09</data>
    </edge>
    <edge source="ENTITY_NAME" target="DESCRIPTION_LIST">
      <data key="d4">1.0</data>
      <data key="d5">The entity_name is associated with the description_list, as the descriptions provide detailed information about the entity identified by the entity_name</data>
      <data key="d6">853bfe9a74a916130a20f81506bcaf09</data>
    </edge>
    <edge source="ENTITY_NAME" target="INPUT_TEXT">
      <data key="d4">1.0</data>
      <data key="d5">The input_text contains the entity_name, which is a unique identifier for an entity within the text. The entity_name is extracted from the input_text for further processing and analysis</data>
      <data key="d6">853bfe9a74a916130a20f81506bcaf09</data>
    </edge>
    <edge source="CONFIGURATION DOCUMENTATION" target=".ENV">
      <data key="d4">1.0</data>
      <data key="d5">The Configuration documentation provides guidance on how to properly configure the .env file, which is essential for the operation of GraphRAG.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="CONFIGURATION DOCUMENTATION" target="SETTINGS.YAML">
      <data key="d4">1.0</data>
      <data key="d5">The Configuration documentation provides guidance on how to properly configure the settings.yaml file, which is essential for the operation of GraphRAG.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="CONFIGURATION DOCUMENTATION" target="PROMPTS/">
      <data key="d4">1.0</data>
      <data key="d5">The Configuration documentation provides guidance on how to properly configure the prompts/ directory, including how to modify existing prompts or generate new ones through the Auto Prompt Tuning command.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="GRAPHRAG SYSTEM" target="GRAPHRAG ACCELERATOR SOLUTION">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Accelerator Solution is a recommended way to get started with the GraphRAG system, providing a user-friendly end-to-end experience with Azure resources for setting up and using the system</data>
      <data key="d6">84d24b5db902baca7217b5e3bb6ec462</data>
    </edge>
    <edge source="GRAPHRAG SYSTEM" target="INDEXING PIPELINE OVERVIEW">
      <data key="d4">1.0</data>
      <data key="d5">The Indexing Pipeline Overview is a component of the GraphRAG system that describes the process of indexing text data, which is a key functionality of the system</data>
      <data key="d6">84d24b5db902baca7217b5e3bb6ec462</data>
    </edge>
    <edge source="GRAPHRAG SYSTEM" target="QUERY ENGINE OVERVIEW">
      <data key="d4">1.0</data>
      <data key="d5">The Query Engine Overview is a component of the GraphRAG system that describes the process of querying indexed data, which is another key functionality of the system</data>
      <data key="d6">84d24b5db902baca7217b5e3bb6ec462</data>
    </edge>
    <edge source="GRAPHRAG PIPELINE" target="SETTINGS.YAML">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG pipeline relies on the settings.yaml file for configuration. This file contains settings that can be modified to customize the pipeline's behavior, including the API key and additional settings for Azure OpenAI users.</data>
      <data key="d6">5aaa26fbe97dc7573cd1a56d6fb11213</data>
    </edge>
    <edge source="GRAPHRAG PIPELINE" target="OPENAI API">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG pipeline can be configured to use the OpenAI API for various functionalities, such as language models and embeddings. The pipeline requires an API key for authentication, which is specified in the .env file.</data>
      <data key="d6">5aaa26fbe97dc7573cd1a56d6fb11213</data>
    </edge>
    <edge source="GRAPHRAG PIPELINE" target="AZURE OPENAI">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG pipeline can be configured to use Azure OpenAI for AI capabilities. Azure OpenAI users need to set specific variables in the settings.yaml file, including the API base URL, API version, and deployment name, to integrate with the pipeline.</data>
      <data key="d6">5aaa26fbe97dc7573cd1a56d6fb11213</data>
    </edge>
    <edge source="AZURE OPENAI" target="GRAPHRAG_API_VERSION">
      <data key="d4">1.0</data>
      <data key="d5">The Azure OpenAI service requires the GRAPHRAG_API_VERSION configuration to specify the API version for compatibility and functionality. This relationship is essential for ensuring that the service requests are processed correctly.</data>
      <data key="d6">7b45dafa74553d3899e2291a3c9fb86e</data>
    </edge>
    <edge source="AZURE OPENAI" target="GRAPHRAG_LLM_API_KEY">
      <data key="d4">1.0</data>
      <data key="d5">The Azure OpenAI service uses the GRAPHRAG_LLM_API_KEY for authentication. This relationship is critical for accessing and using the service, as the API key verifies the user's identity and permissions.</data>
      <data key="d6">7b45dafa74553d3899e2291a3c9fb86e</data>
    </edge>
    <edge source="AZURE OPENAI" target="GRAPHRAG_LLM_API_VERSION">
      <data key="d4">1.0</data>
      <data key="d5">The Azure OpenAI service uses the GRAPHRAG_LLM_API_VERSION configuration to specify the API version for language model requests. This relationship is crucial for compatibility and functionality, ensuring that the service requests are processed correctly.</data>
      <data key="d6">7b45dafa74553d3899e2291a3c9fb86e</data>
    </edge>
    <edge source="AZURE OPENAI" target="GRAPHRAG_LLM_MODEL_SUPPORTS_JSON">
      <data key="d4">1.0</data>
      <data key="d5">The Azure OpenAI service's functionality is influenced by the GRAPHRAG_LLM_MODEL_SUPPORTS_JSON configuration, which indicates whether the language model can handle JSON data. This relationship is important for determining the capabilities of the service and how it can be used in various applications.</data>
      <data key="d6">7b45dafa74553d3899e2291a3c9fb86e</data>
    </edge>
    <edge source="SETTINGS.YAML" target="PYTHON -M GRAPHRAG.INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The python -m graphrag.index command creates the settings.yaml file when the --init option is used. This file contains the configuration settings necessary for the operation of GraphRAG.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="SETTINGS.YAML" target=".ENV">
      <data key="d4">1.0</data>
      <data key="d5">.env is referenced in settings.yaml for environment variable settings, allowing for token replacements in the configuration document using ${ENV_VAR} syntax.</data>
      <data key="d6">32e96c66a531ecd0a8edc7414aec0803</data>
    </edge>
    <edge source="API_BASE" target="GRAPHRAG CONFIGURATION DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The api_base is a configuration property that is documented in the GraphRAG configuration documentation, providing guidance on how to set up the base URL for accessing the Azure OpenAI API.</data>
      <data key="d6">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </edge>
    <edge source="API_BASE" target="REQUEST_TIMEOUT">
      <data key="d4">1.0</data>
      <data key="d5">The request_timeout and api_base are related because the request_timeout defines the timeout for requests made to the API specified by api_base</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="API_VERSION" target="GRAPHRAG CONFIGURATION DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The api_version is a configuration property that is documented in the GraphRAG configuration documentation, offering information on how to specify the version of the Azure OpenAI API to be used.</data>
      <data key="d6">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </edge>
    <edge source="API_VERSION" target="ORGANIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The api_version and organization are related because the api_version specifies the version of the API used by the LLM (Language Model) service, which is associated with the organization using the service</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="DEPLOYMENT_NAME" target="GRAPHRAG CONFIGURATION DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The deployment_name is a configuration property that is documented in the GraphRAG configuration documentation, explaining how to set the name of the Azure model deployment for accessing the model.</data>
      <data key="d6">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </edge>
    <edge source="DEPLOYMENT_NAME" target="MODEL_SUPPORTS_JSON">
      <data key="d4">1.0</data>
      <data key="d5">The deployment_name and model_supports_json are related because the deployment_name specifies the deployment to use for Azure-based LLM (Language Model) services, and model_supports_json indicates whether the model deployed supports JSON output</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="DEPLOYMENT_NAME" target="URL ENDPOINT">
      <data key="d4">1.0</data>
      <data key="d5">The url endpoint is related to the deployment_name as the endpoint is specific to the deployment specified by the deployment_name. The deployment_name determines the correct endpoint to use for accessing the model.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="GRAPHRAG CONFIGURATION DOCUMENTATION" target="INITIALIZATION DOCUMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG configuration documentation is related to the Initialization documentation, as both provide information necessary for setting up the environment and preparing for data indexing and querying.</data>
      <data key="d6">7c1bad237a1ef86cb41b6c5dbad4ffc3</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION MODE" target="INIT COMMAND">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Mode supports the use of the Init Command to easily set up the GraphRAG system with the necessary configuration files</data>
      <data key="d6">ccd2de9e2219521fbca779843c65af58</data>
    </edge>
    <edge source="CUSTOM CONFIGURATION MODE" target="ENCODING_MODEL">
      <data key="d4">1.0</data>
      <data key="d5">The encoding_model is related to Custom Configuration Mode as it is a configuration setting that can be customized in the advanced use-case of Custom Configuration Mode</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="CUSTOM CONFIGURATION MODE" target="SKIP_WORKFLOWS">
      <data key="d4">1.0</data>
      <data key="d5">The skip_workflows is related to Custom Configuration Mode as it is a configuration property that can be defined in the advanced use-case of Custom Configuration Mode to skip specific workflows</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="INIT COMMAND" target=".ENV FILE">
      <data key="d4">1.0</data>
      <data key="d5">The init command creates the .env file, which contains environment variables necessary for the configuration of GraphRAG</data>
      <data key="d6">d0f7c236538005bc3056b7daed2401d8</data>
    </edge>
    <edge source="INIT COMMAND" target="SETTINGS.YAML FILE">
      <data key="d4">1.0</data>
      <data key="d5">The init command creates the settings.yaml file, which contains the configuration settings for GraphRAG</data>
      <data key="d6">d0f7c236538005bc3056b7daed2401d8</data>
    </edge>
    <edge source="INIT COMMAND" target="PROMPTS FOLDER">
      <data key="d4">1.0</data>
      <data key="d5">The init command creates the prompts folder, which contains the default prompts used by GraphRAG and can be modified or adapted</data>
      <data key="d6">d0f7c236538005bc3056b7daed2401d8</data>
    </edge>
    <edge source=".ENV" target="PYTHON -M GRAPHRAG.INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The python -m graphrag.index command creates the .env file when the --init option is used. This file is necessary for storing environment variables referenced in the settings.yaml file.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source=".ENV" target="CONFIG.JSON">
      <data key="d4">1.0</data>
      <data key="d5">.env is used alongside config.json for environment variable token replacements, allowing for dynamic configuration settings in GraphRAG.</data>
      <data key="d6">32e96c66a531ecd0a8edc7414aec0803</data>
    </edge>
    <edge source=".ENV" target="CONFIG.YML">
      <data key="d4">1.0</data>
      <data key="d5">.env is used alongside config.yml for environment variable token replacements, allowing for dynamic configuration settings in GraphRAG.</data>
      <data key="d6">32e96c66a531ecd0a8edc7414aec0803</data>
    </edge>
    <edge source="PROMPTS/" target="PYTHON -M GRAPHRAG.INDEX">
      <data key="d4">1.0</data>
      <data key="d5">The python -m graphrag.index command creates the prompts/ directory when the --init option is used. This directory contains default prompts used by GraphRAG and can be modified or new ones generated through the Auto Prompt Tuning command.</data>
      <data key="d6">12294feb07a1d202b27241eaaf64718b</data>
    </edge>
    <edge source="PROMPTS/" target="PROMPT TUNING COMMAND">
      <data key="d4">1.0</data>
      <data key="d5">The prompts/ directory contains default prompts that can be modified or adapted using the Prompt Tuning command, enhancing the performance of GraphRAG for specific data sets.</data>
      <data key="d6">32e96c66a531ecd0a8edc7414aec0803</data>
    </edge>
    <edge source="CONFIG.JSON" target="API KEY">
      <data key="d4">1.0</data>
      <data key="d5">The API Key is a part of the Config.json file, specifically located within the llm section. It is a critical configuration property for accessing the LLM service.</data>
      <data key="d6">f135654a3c057c66b9e5f97a960d302f</data>
    </edge>
    <edge source="CONFIG.JSON" target="INPUT CONFIGURATION">
      <data key="d4">1.0</data>
      <data key="d5">The Input Configuration is a section within the Config.json file that defines how input data should be processed. It includes various fields that are necessary for configuring the input handling process.</data>
      <data key="d6">f135654a3c057c66b9e5f97a960d302f</data>
    </edge>
    <edge source="CONFIG.JSON" target="LLM CONFIGURATION">
      <data key="d4">1.0</data>
      <data key="d5">The LLM Configuration is a section within the Config.json file that specifies settings for the LLM (Language Model) service. It includes the API Key and other parameters that are essential for the operation of the LLM service.</data>
      <data key="d6">f135654a3c057c66b9e5f97a960d302f</data>
    </edge>
    <edge source="API_KEY" target="TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The api_key and type are related because the api_key is required for authentication when using the specified type of LLM (Language Model) service, such as OpenAI or Azure OpenAI</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="INPUT" target="WORKFLOW2">
      <data key="d4">1.0</data>
      <data key="d5">The input data for workflow2 is specified as a file type, with details such as base directory and file pattern, which are necessary for the process to execute</data>
      <data key="d6">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </edge>
    <edge source="INPUT" target="FILE_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The input property is related to the file_type as the file_type specifies the type of file that the input property is configured to read</data>
      <data key="d6">3900b87693f02c43b4294e38647eb7cd</data>
    </edge>
    <edge source="INPUT" target="BASE_DIR">
      <data key="d4">1.0</data>
      <data key="d5">The input property is related to the base_dir as the base_dir specifies the directory from which the input property reads the files</data>
      <data key="d6">3900b87693f02c43b4294e38647eb7cd</data>
    </edge>
    <edge source="INPUT" target="FILE_PATTERN">
      <data key="d4">1.0</data>
      <data key="d5">The input property is related to the file_pattern as the file_pattern is used by the input property to identify and select the files to be read</data>
      <data key="d6">3900b87693f02c43b4294e38647eb7cd</data>
    </edge>
    <edge source="INPUT" target="SOURCE_COLUMN">
      <data key="d4">1.0</data>
      <data key="d5">The input property is related to the source_column as the source_column is a part of the input configuration that specifies which column contains the source or author of the data</data>
      <data key="d6">3900b87693f02c43b4294e38647eb7cd</data>
    </edge>
    <edge source="INPUT" target="TEXT_COLUMN">
      <data key="d4">1.0</data>
      <data key="d5">The input property is related to the text_column as the text_column is a part of the input configuration that specifies which column contains the text of the data</data>
      <data key="d6">3900b87693f02c43b4294e38647eb7cd</data>
    </edge>
    <edge source="INPUT" target="POST PROCESS">
      <data key="d4">1.0</data>
      <data key="d5">The Input is related to the Post Process as the post-process steps are applied to the input data after it is read from the CSV file. The relationship is established through the 'post_process' section in the input that specifies the filtering step based on the 'title' column with the value 'My document'</data>
      <data key="d6">6839baed839d7a5e837af1da93e462e5</data>
    </edge>
    <edge source="TYPE" target="FIELDS">
      <data key="d4">1.0</data>
      <data key="d5">The Fields include the Type property, which specifies the type of storage to use</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="CONNECTION_STRING" target="CONTAINER_NAME">
      <data key="d4">1.0</data>
      <data key="d5">The connection_string and container_name are related because the connection_string provides the necessary details to access the Azure Blob storage, while the container_name specifies the exact container within that storage where data is stored</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="BASE_DIR" target="STORAGE_ACCOUNT_BLOB_URL">
      <data key="d4">1.0</data>
      <data key="d5">The base_dir and storage_account_blob_url are related because the base_dir specifies the directory from which to read input data, and the storage_account_blob_url provides the direct access point to the Azure storage account blob where the data is located</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="PROXY" target="COGNITIVE_SERVICES_ENDPOINT">
      <data key="d4">1.0</data>
      <data key="d5">The proxy and cognitive_services_endpoint are related because the proxy can be used to route requests to the cognitive_services_endpoint, which provides additional AI services</data>
      <data key="d6">647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="MODEL_SUPPORTS_JSON" target="URL ENDPOINT">
      <data key="d4">1.0</data>
      <data key="d5">The model_supports_json is related to the url endpoint as it indicates whether the model at the specified endpoint supports JSON-mode output. This affects how requests are formatted and how responses are interpreted.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="TOKENS_PER_MINUTE" target="REQUESTS_PER_MINUTE">
      <data key="d4">2.0</data>
      <data key="d5">The entities "TOKENS_PER_MINUTE" and "REQUESTS_PER_MINUTE" are intricately related in the context of managing the LLM (Language Model) service. These parameters act in concert to set throttle limits on the service, ensuring a controlled rate of consumption and processing. Specifically, "TOKENS_PER_MINUTE" governs the number of tokens that can be processed by the service, while "REQUESTS_PER_MINUTE" regulates the number of requests that can be made. Together, they ensure that the service can handle requests efficiently without being overwhelmed, maintaining optimal performance and resource allocation.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="REQUESTS_PER_MINUTE" target="SLEEP_ON_RATE_LIMIT_RECOMMENDATION">
      <data key="d4">1.0</data>
      <data key="d5">The sleep_on_rate_limit_recommendation is related to the requests_per_minute as it controls the behavior when the rate limit set by requests_per_minute is reached. It determines whether the system should sleep according to recommendations when the rate limit is exceeded.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="MAX_RETRIES" target="MAX_RETRY_WAIT">
      <data key="d4">2.0</data>
      <data key="d5">The entities MAX_RETRIES and MAX_RETRY_WAIT are integral components of the retry mechanism for handling failed requests to the LLM (Language Model) service. MAX_RETRIES sets the upper limit on the number of times a failed request can be retried, ensuring that the system does not attempt to resend requests indefinitely. Complementing this, MAX_RETRY_WAIT specifies the maximum backoff time before each retry, providing a crucial parameter for managing the frequency and timing of retry attempts. Together, these two entities play a pivotal role in optimizing the efficiency and reliability of the communication between the system and the LLM service.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa,647be47c939b4d72f1c0b29a2e0d2cb2</data>
    </edge>
    <edge source="MAX_RETRIES" target="CONCURRENT_REQUESTS">
      <data key="d4">1.0</data>
      <data key="d5">The concurrent_requests is related to the max_retries as they both manage the handling of requests. The concurrent_requests specifies the number of requests that can be open at once, while the max_retries determines how many times a failed request should be retried.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="TEMPERATURE" target="TOP_P">
      <data key="d4">1.0</data>
      <data key="d5">The temperature is related to the top_p as they both influence the generation of completions. The temperature affects the randomness of the output, while the top_p affects the diversity of the output by controlling the selection of likely completions.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="TEMPERATURE" target="N">
      <data key="d4">1.0</data>
      <data key="d5">The n is related to the temperature as they both determine the output of the model. The n specifies the number of completions to generate, while the temperature influences the randomness of those completions.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="STAGGER" target="NUM_THREADS">
      <data key="d4">1.0</data>
      <data key="d5">The stagger is related to the num_threads as they both influence the parallelization of processing. The stagger specifies the threading stagger value, while the num_threads determines the number of threads used for processing.</data>
      <data key="d6">3c66b7e86b3675fce14fe0047ae731aa</data>
    </edge>
    <edge source="ASYNC_MODE" target="SUMMARIZE_DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The async_mode setting is used in the summarize_descriptions process, indicating how the summarization tasks are handled asynchronously</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="ASYNC_MODE" target="CLAIM_EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The async_mode setting is used in the claim_extraction process, indicating how the claim extraction tasks are handled asynchronously</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="ASYNC_MODE" target="COMMUNITY_REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The async_mode setting is used in the community_reports process, indicating how the report generation tasks are handled asynchronously</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="ASYNC_MODE" target="GRAPHRAG_MAX_CLUSTER_SIZE">
      <data key="d4">1.0</data>
      <data key="d5">ASYNC_MODE and GRAPHRAG_MAX_CLUSTER_SIZE are related as they both are configuration properties that can affect the performance and behavior of the system</data>
      <data key="d6">1b24101de07b1c195448240237b84b37</data>
    </edge>
    <edge source="BATCH_SIZE" target="BATCH_MAX_TOKENS">
      <data key="d4">1.0</data>
      <data key="d5">batch_size and batch_max_tokens are related because they both define parameters for batch processing, with batch_size controlling the number of items and batch_max_tokens controlling the token limit, which together influence the efficiency and resource usage of the system.</data>
      <data key="d6">d27237468a1b9e89110eeeca8080f63c</data>
    </edge>
    <edge source="TARGET" target="STRATEGY">
      <data key="d4">1.0</data>
      <data key="d5">target and strategy are related because the strategy for text-embedding can be fully overridden, which includes how the target set of embeddings is emitted, affecting the output and customization of the text-embedding process.</data>
      <data key="d6">d27237468a1b9e89110eeeca8080f63c</data>
    </edge>
    <edge source="STRATEGY" target="SUMMARIZE_DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The strategy setting is used in the summarize_descriptions process, allowing for the customization of the summarization algorithm</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="STRATEGY" target="CLAIM_EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The strategy setting is used in the claim_extraction process, allowing for the customization of the claim extraction algorithm</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="STRATEGY" target="COMMUNITY_REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The strategy setting is used in the community_reports process, allowing for the customization of the report generation algorithm</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="SIZE" target="OVERLAP">
      <data key="d4">1.0</data>
      <data key="d5">size and overlap are related because they both influence the chunking process, with size defining the maximum chunk size and overlap specifying the overlap between chunks, which together impact the context preservation and processing efficiency of text segments.</data>
      <data key="d6">d27237468a1b9e89110eeeca8080f63c</data>
    </edge>
    <edge source="PARALLELIZATION" target="SUMMARIZE_DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The parallelization setting is used in the summarize_descriptions process, defining how tasks are processed in parallel</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="PARALLELIZATION" target="CLAIM_EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The parallelization setting is used in the claim_extraction process, defining how tasks are processed in parallel</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="PARALLELIZATION" target="COMMUNITY_REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The parallelization setting is used in the community_reports process, defining how tasks are processed in parallel</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="CACHE" target="STORAGE">
      <data key="d4">2.0</data>
      <data key="d5">CACHE and STORAGE are intricately related within the context of data management systems. CACHE primarily focuses on temporary storage solutions designed for quick access, acting as a caching strategy in the pipeline. This role is crucial for enhancing system performance by minimizing latency and improving data retrieval speeds. On the other hand, STORAGE is concerned with long-term data management, concentrating on the output strategy. It ensures the reliability and integrity of data over extended periods, playing a pivotal role in resource management and system efficiency. Both entities are essential components of data management systems, working in tandem to optimize performance, resource allocation, and data reliability.</data>
      <data key="d6">d27237468a1b9e89110eeeca8080f63c,e01c546120a27319dcbdf7a6b89bab26</data>
    </edge>
    <edge source="STORAGE" target="STORAGE ACCOUNT BLOB URL">
      <data key="d4">1.0</data>
      <data key="d5">The Storage section includes the Storage Account Blob URL as a configuration property for blob storage type</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="STORAGE" target="FIELDS">
      <data key="d4">1.0</data>
      <data key="d5">The Storage section contains the Fields, which are specific configuration options for storage settings</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="STORAGE" target="ROOT_DIR">
      <data key="d4">1.0</data>
      <data key="d5">root_dir is related to the storage configuration as it specifies the base directory for storing data, which is a critical component of the storage strategy</data>
      <data key="d6">e01c546120a27319dcbdf7a6b89bab26</data>
    </edge>
    <edge source="REPORTING" target="STORAGE ACCOUNT BLOB URL">
      <data key="d4">1.0</data>
      <data key="d5">The Reporting section includes the Storage Account Blob URL as a configuration property for blob reporting type</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="REPORTING" target="FIELDS">
      <data key="d4">1.0</data>
      <data key="d5">The Reporting section contains the Fields, which are specific configuration options for reporting settings</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="FIELDS" target="CONNECTION STRING">
      <data key="d4">1.0</data>
      <data key="d5">The Fields include the Connection String property, which is used for blob storage type</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="FIELDS" target="CONTAINER NAME">
      <data key="d4">1.0</data>
      <data key="d5">The Fields include the Container Name property, which is used for blob storage type</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="FIELDS" target="BASE DIR">
      <data key="d4">1.0</data>
      <data key="d5">The Fields include the Base Dir property, which specifies the base directory for writing reports</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="FIELDS" target="SUMMARIZE DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The Summarize Descriptions section contains the Fields, which are specific configuration options for summarizing descriptions settings</data>
      <data key="d6">abac77a5673e907cf8d65161c2612784</data>
    </edge>
    <edge source="CONNECTION STRING" target="REPORTING TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The Reporting Type property is related to the Connection String property, as the connection string is only relevant when the reporting type is set to blob</data>
      <data key="d6">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </edge>
    <edge source="CONTAINER NAME" target="REPORTING TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The Reporting Type property is related to the Container Name property, as the container name is only relevant when the reporting type is set to blob</data>
      <data key="d6">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </edge>
    <edge source="PROMPT STR" target="SUMMARIZE_DESCRIPTIONS">
      <data key="d4">1.0</data>
      <data key="d5">The prompt str setting is used in the summarize_descriptions process, specifying the prompt file to guide the summarization</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="PROMPT STR" target="CLAIM_EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The prompt str setting is used in the claim_extraction process, specifying the prompt file to guide the claim extraction</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="PROMPT STR" target="COMMUNITY_REPORTS">
      <data key="d4">1.0</data>
      <data key="d5">The prompt str setting is used in the community_reports process, specifying the prompt file to guide the report generation</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="SUMMARIZE_DESCRIPTIONS" target="MAX_LENGTH">
      <data key="d4">1.0</data>
      <data key="d5">The max_length setting is used in the summarize_descriptions process, controlling the length of the generated summaries</data>
      <data key="d6">9cbd4e21339eeed5e22a638e52a094cb</data>
    </edge>
    <edge source="COMMUNITY_REPORTS" target="STRATEGY DICT">
      <data key="d4">1.0</data>
      <data key="d5">The community_reports section can be fully customized by the strategy dict, allowing for the override of default settings and strategies for community reports</data>
      <data key="d6">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </edge>
    <edge source="STRATEGY DICT" target="CLUSTER_GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The cluster_graph section can be fully customized by the strategy dict, allowing for the override of default settings and strategies for cluster graphs</data>
      <data key="d6">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </edge>
    <edge source="STRATEGY DICT" target="EMBED_GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">The embed_graph section can be fully customized by the strategy dict, allowing for the override of default settings and strategies for graph embeddings</data>
      <data key="d6">53f1b5ad1d2f4ee5dfb53a8f1ff3ec14</data>
    </edge>
    <edge source="STRATEGY DICT" target="NODE2VEC RANDOM SEED">
      <data key="d4">1.0</data>
      <data key="d5">The node2vec random seed is related to the strategy dict as it can be a parameter within the strategy dict that influences the node2vec algorithm's behavior and results</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="ENABLED BOOL" target="UMAP">
      <data key="d4">1.0</data>
      <data key="d5">The UMAP is related to the enabled bool as the enabled bool determines whether UMAP layouts are enabled or disabled in the system</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="SNAPSHOTS" target="GRAPHML BOOL">
      <data key="d4">1.0</data>
      <data key="d5">The snapshots are related to the graphml bool as the graphml bool is a sub-property of snapshots that controls the emission of graphml snapshots</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="SNAPSHOTS" target="RAW_ENTITIES BOOL">
      <data key="d4">1.0</data>
      <data key="d5">The snapshots are related to the raw_entities bool as the raw_entities bool is a sub-property of snapshots that determines if raw entity snapshots are emitted</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="SNAPSHOTS" target="TOP_LEVEL_NODES BOOL">
      <data key="d4">1.0</data>
      <data key="d5">The snapshots are related to the top_level_nodes bool as the top_level_nodes bool is a sub-property of snapshots that specifies whether top-level-node snapshots are emitted</data>
      <data key="d6">b70cb2eda62c6afad9e8d22daafe61cc</data>
    </edge>
    <edge source="EXTENDS" target="ROOT_DIR">
      <data key="d4">1.0</data>
      <data key="d5">extends can influence the root_dir configuration by inheriting settings from base configurations, which may include the root directory path for the pipeline</data>
      <data key="d6">e01c546120a27319dcbdf7a6b89bab26</data>
    </edge>
    <edge source="RUN.PY" target="PYTHONPATH">
      <data key="d4">1.0</data>
      <data key="d5">run.py requires the PYTHONPATH environment variable to be set to the project's root directory in order to find and import the necessary modules and scripts when executing the script</data>
      <data key="d6">e01c546120a27319dcbdf7a6b89bab26</data>
    </edge>
    <edge source="RUN.PY" target="POETRY SHELL">
      <data key="d4">1.0</data>
      <data key="d5">poetry shell is used to activate the virtual environment with the required dependencies before running run.py, ensuring that the correct environment is set up for the execution of the script</data>
      <data key="d6">e01c546120a27319dcbdf7a6b89bab26</data>
    </edge>
    <edge source="REPORTING TYPE" target="BASE DIRECTORY">
      <data key="d4">1.0</data>
      <data key="d5">The Reporting Type property is related to the Base Directory property, as the base directory is only relevant when the reporting type is set to file</data>
      <data key="d6">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </edge>
    <edge source="WORKFLOW NAME" target="STEPS">
      <data key="d4">1.0</data>
      <data key="d5">The Workflow Name property is related to the Steps property, as steps are part of a specific workflow and can establish dependencies on other workflows through the input property</data>
      <data key="d6">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </edge>
    <edge source="INPUT TYPE" target="FILE TYPE">
      <data key="d4">1.0</data>
      <data key="d5">The Input Type property is related to the File Type property, as the file type is only relevant when the input type is set to file</data>
      <data key="d6">a3e5bacdf64bcaf080a04c7dd8218484</data>
    </edge>
    <edge source="WORKFLOW2" target="WORKFLOW1">
      <data key="d4">1.0</data>
      <data key="d5">workflow2 has a dependency on workflow1, as indicated in the derive step's input source</data>
      <data key="d6">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </edge>
    <edge source="WORKFLOW2" target="DERIVE">
      <data key="d4">1.0</data>
      <data key="d5">The derive step is a part of the workflow2 process, used to process data from col1 and col2 with a dependency on workflow1</data>
      <data key="d6">76d9dcb9a27c2caea1f46bb5050851c6</data>
    </edge>
    <edge source="TIMESTAMP_COLUMN" target="TIMESTAMP_FORMAT">
      <data key="d4">1.0</data>
      <data key="d5">The timestamp_column is related to the timestamp_format as the timestamp_format specifies how the timestamps in the timestamp_column are structured and should be interpreted</data>
      <data key="d6">3900b87693f02c43b4294e38647eb7cd</data>
    </edge>
    <edge source="AUTHOR" target="MESSAGE">
      <data key="d4">1.0</data>
      <data key="d5">The Author is related to the Message as the author is the creator or originator of the message. The relationship is established through the 'author' column in the CSV file that links the author to the message</data>
      <data key="d6">6839baed839d7a5e837af1da93e462e5</data>
    </edge>
    <edge source="MESSAGE" target="DATE">
      <data key="d4">1.0</data>
      <data key="d5">The Message is related to the Date as the date indicates when the message was created or recorded. The relationship is established through the 'date(yyyyMMddHHmmss)' column in the CSV file that provides the timestamp for the message</data>
      <data key="d6">6839baed839d7a5e837af1da93e462e5</data>
    </edge>
    <edge source="POST PROCESS" target="FILE FILTER">
      <data key="d4">1.0</data>
      <data key="d5">The File Filter is related to the Post Process as the filtered files are then processed by the post processing steps, which can include further filtering based on specific columns and values</data>
      <data key="d6">765d8a78606fe81a03a0da4f7ff231fa</data>
    </edge>
    <edge source="CSV FILE PATTERN" target="FILE FILTER">
      <data key="d4">1.0</data>
      <data key="d5">The CSV File Pattern is related to the File Filter as the filter uses the named groups from the pattern to apply additional filtering criteria to the files</data>
      <data key="d6">765d8a78606fe81a03a0da4f7ff231fa</data>
    </edge>
    <edge source="GRAPHRAG_API_BASE" target="GRAPHRAG_API_VERSION">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_API_BASE and GRAPHRAG_API_VERSION are related because the API base URL and the API version together determine the endpoint for making API requests. The API version is crucial for ensuring compatibility with the API base. The strength of this relationship is high because both settings are required for successful API interaction.</data>
      <data key="d6">3da10b454f926a257b9fdf5d2487c0a5</data>
    </edge>
    <edge source="GRAPHRAG_INPUT_TYPE" target="GRAPHRAG_INPUT_FILE_TYPE">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_INPUT_TYPE and GRAPHRAG_INPUT_FILE_TYPE are related because GRAPHRAG_INPUT_TYPE specifies the type of input data, and GRAPHRAG_INPUT_FILE_TYPE specifies the type of files to be processed. When GRAPHRAG_INPUT_TYPE is set to "file", GRAPHRAG_INPUT_FILE_TYPE becomes relevant as it determines the format of the input files that the system will handle.</data>
      <data key="d6">8ac79ce92be1254dfda9a10eb54ab703</data>
    </edge>
    <edge source="GRAPHRAG_INPUT_TYPE" target="GRAPHRAG_INPUT_FILE_PATTERN">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_INPUT_TYPE and GRAPHRAG_INPUT_FILE_PATTERN are related because GRAPHRAG_INPUT_TYPE specifies the type of input data, and GRAPHRAG_INPUT_FILE_PATTERN specifies the pattern for matching input files. When GRAPHRAG_INPUT_TYPE is set to "file", GRAPHRAG_INPUT_FILE_PATTERN becomes relevant as it determines which files will be selected for processing based on their names.</data>
      <data key="d6">8ac79ce92be1254dfda9a10eb54ab703</data>
    </edge>
    <edge source="GRAPHRAG_INPUT_TYPE" target="GRAPHRAG_INPUT_SOURCE_COLUMN">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_INPUT_TYPE and GRAPHRAG_INPUT_SOURCE_COLUMN are related because GRAPHRAG_INPUT_TYPE specifies the type of input data, and GRAPHRAG_INPUT_SOURCE_COLUMN specifies the column name in the input data that contains the source information. When GRAPHRAG_INPUT_TYPE is set to "file", GRAPHRAG_INPUT_SOURCE_COLUMN becomes relevant as it determines which column in the input data should be used for source information.</data>
      <data key="d6">8ac79ce92be1254dfda9a10eb54ab703</data>
    </edge>
    <edge source="GRAPHRAG_EMBEDDING_TPM" target="GRAPHRAG_EMBEDDING_RPM">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_EMBEDDING_TPM setting is related to the GRAPHRAG_EMBEDDING_RPM as both represent rate limits for embedding operations, with TPM focusing on transactions and RPM on requests</data>
      <data key="d6">2b777e3d591ce1511a03abd1a6d8dc73</data>
    </edge>
    <edge source="GRAPHRAG_STORAGE_TYPE" target="GRAPHRAG_STORAGE_CONNECTION_STRING">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_STORAGE_TYPE property is related to the GRAPHRAG_STORAGE_CONNECTION_STRING property, as the type of storage system dictates the format and requirements of the connection string</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_STORAGE_TYPE" target="GRAPHRAG_STORAGE_CONTAINER_NAME">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_STORAGE_TYPE property is related to the GRAPHRAG_STORAGE_CONTAINER_NAME property, as the type of storage system may require a specific container name for organization and identification</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_STORAGE_TYPE" target="GRAPHRAG_STORAGE_BASE_DIR">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_STORAGE_TYPE property is related to the GRAPHRAG_STORAGE_BASE_DIR property, as the type of storage system determines the base directory structure for storing data</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_CACHE_TYPE" target="GRAPHRAG_CACHE_CONNECTION_STRING">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_CACHE_TYPE property is related to the GRAPHRAG_CACHE_CONNECTION_STRING property, as the type of cache system dictates the format and requirements of the connection string</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_CACHE_TYPE" target="GRAPHRAG_CACHE_CONTAINER_NAME">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_CACHE_TYPE property is related to the GRAPHRAG_CACHE_CONTAINER_NAME property, as the type of cache system may require a specific container name for organization and identification</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_CACHE_TYPE" target="GRAPHRAG_CACHE_BASE_DIR">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_CACHE_TYPE property is related to the GRAPHRAG_CACHE_BASE_DIR property, as the type of cache system determines the base directory structure for storing cached data</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_REPORTING_TYPE" target="GRAPHRAG_REPORTING_CONNECTION_STRING">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_REPORTING_TYPE property is related to the GRAPHRAG_REPORTING_CONNECTION_STRING property, as the type of reporting system dictates the format and requirements of the connection string</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_REPORTING_TYPE" target="GRAPHRAG_REPORTING_CONTAINER_NAME">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_REPORTING_TYPE property is related to the GRAPHRAG_REPORTING_CONTAINER_NAME property, as the type of reporting system may require a specific container name for organization and identification</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_REPORTING_TYPE" target="GRAPHRAG_REPORTING_BASE_DIR">
      <data key="d4">1.0</data>
      <data key="d5">The GRAPHRAG_REPORTING_TYPE property is related to the GRAPHRAG_REPORTING_BASE_DIR property, as the type of reporting system determines the base directory structure for storing reports</data>
      <data key="d6">cde833db73c46ca28f08e35195134441</data>
    </edge>
    <edge source="GRAPHRAG_ENCODING_MODEL" target="GRAPHRAG_MAX_CLUSTER_SIZE">
      <data key="d4">1.0</data>
      <data key="d5">GRAPHRAG_ENCODING_MODEL and GRAPHRAG_MAX_CLUSTER_SIZE are related as they both influence the structure and efficiency of the graph representation</data>
      <data key="d6">1b24101de07b1c195448240237b84b37</data>
    </edge>
    <edge source="DOCUMENT" target="TEXTUNIT">
      <data key="d4">2.0</data>
      <data key="d5">Documents, in the context of data processing and analysis, are often divided into smaller, more manageable components known as TextUnits. This division facilitates a detailed examination of the document's content, allowing for a more nuanced understanding of the information contained within. The relationship between Documents and TextUnits can be characterized as one-to-many or many-to-many, depending on the nature of the documents and the specific requirements of the analysis. This flexibility in dividing documents into multiple TextUnits enables a thorough exploration of the document's structure and content, enhancing the effectiveness of data processing and analysis tasks. The configuration settings determine the extent of division, creating a 1-to-many relationship between the original documents and the resulting TextUnits, thereby optimizing the analysis process for efficiency and accuracy.</data>
      <data key="d6">81f57cf867ea246ad9a6e794ed613375,85e50a4d70697a2c4420e7a9fc82f22d</data>
    </edge>
    <edge source="TEXTUNIT" target="ENTITY">
      <data key="d4">1.0</data>
      <data key="d5">Entities are extracted from TextUnits, representing people, places, events, or other entity-models</data>
      <data key="d6">85e50a4d70697a2c4420e7a9fc82f22d</data>
    </edge>
    <edge source="TEXTUNIT" target="GRAPH EXTRACTION">
      <data key="d4">2.0</data>
      <data key="d5">During the Graph Extraction phase, TextUnits are meticulously processed to pinpoint and extract key components, including Entities, Relationships, and Claims. This process is fundamental in constructing the graph structure that underpins the analysis. Each TextUnit undergoes a detailed examination by the Graph Extraction process, which adeptly identifies and delineates entities and relationships embedded within the text. This results in the creation of a unique subgraph for every TextUnit, enabling a comprehensive mapping of the information landscape. The Graph Extraction phase plays a pivotal role in transforming raw text into a structured graph format, facilitating a deeper understanding of the relationships and claims within the Motor Control and Drive Systems domain.</data>
      <data key="d6">10d01d36390b307a63fd5bc97d8682c0,81f57cf867ea246ad9a6e794ed613375</data>
    </edge>
    <edge source="ENTITY" target="RELATIONSHIP">
      <data key="d4">1.0</data>
      <data key="d5">Relationships are generated between entities based on the covariates, indicating connections between different entities</data>
      <data key="d6">85e50a4d70697a2c4420e7a9fc82f22d</data>
    </edge>
    <edge source="ENTITY" target="COVARIATE">
      <data key="d4">1.0</data>
      <data key="d5">Covariates contain statements about entities, which may be time-bound, providing context for the entities</data>
      <data key="d6">85e50a4d70697a2c4420e7a9fc82f22d</data>
    </edge>
    <edge source="ENTITY" target="COMMUNITY REPORT">
      <data key="d4">1.0</data>
      <data key="d5">Community Reports are generated by performing hierarchical community detection on entities, providing insights into the structure of the data</data>
      <data key="d6">85e50a4d70697a2c4420e7a9fc82f22d</data>
    </edge>
    <edge source="ENTITY" target="GRAPH EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">During the Graph Extraction phase, Entities are extracted from the text within each TextUnit, contributing to the graph being built</data>
      <data key="d6">81f57cf867ea246ad9a6e794ed613375</data>
    </edge>
    <edge source="RELATIONSHIP" target="GRAPH EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">During the Graph Extraction phase, Relationships are extracted from the text within each TextUnit, contributing to the graph being built</data>
      <data key="d6">81f57cf867ea246ad9a6e794ed613375</data>
    </edge>
    <edge source="COVARIATE" target="COMMUNITY REPORT">
      <data key="d4">1.0</data>
      <data key="d5">Covariates are used in the generation of Community Reports, as they provide context and details about entities which are summarized in the reports</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION WORKFLOW" target="NETWORK VISUALIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Workflow includes Network Visualization as one of its phases, where the network of entities and their relationships are visualized</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION WORKFLOW" target="DOCUMENT PROCESSING">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Workflow includes Document Processing as one of its phases, where documents are processed for further analysis and transformation</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION WORKFLOW" target="COMMUNITY SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Workflow includes Community Summarization as one of its phases, where communities are summarized to provide insights and reports</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION WORKFLOW" target="GRAPH AUGMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Workflow includes Graph Augmentation as one of its phases, where the graph is enhanced with additional information and relationships</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION WORKFLOW" target="GRAPH EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Workflow includes Graph Extraction as one of its phases, where entities and relationships are extracted from the text to form a graph</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DEFAULT CONFIGURATION WORKFLOW" target="COMPOSE TEXTUNITS">
      <data key="d4">1.0</data>
      <data key="d5">The Default Configuration Workflow includes Compose TextUnits as one of its phases, where input documents are transformed into TextUnits for graph analysis</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DOCUMENT PROCESSING" target="COMMUNITY EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Community Embedding is a part of the Document Processing phase, where vector representations of communities are created and used in the processing of documents</data>
      <data key="d6">3e292d936b7efa377ba9530456cfd888</data>
    </edge>
    <edge source="DOCUMENT PROCESSING" target="COMMUNITY TABLES EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Community Tables Emission is a process within the Document Processing phase, where the Communities and CommunityReports tables are generated and emitted</data>
      <data key="d6">3e292d936b7efa377ba9530456cfd888</data>
    </edge>
    <edge source="DOCUMENT PROCESSING" target="DOCUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">The Documents table is created during the Document Processing phase, which involves various steps such as Augment, Link to TextUnits, and Avg. Embedding.</data>
      <data key="d6">827fd80da359cf05b091c24e465dd05d</data>
    </edge>
    <edge source="DOCUMENT PROCESSING" target="LINK TO TEXTUNITS">
      <data key="d4">1.0</data>
      <data key="d5">Link to TextUnits is a step within the Document Processing phase that connects documents to text-units, establishing the relationship between these entities.</data>
      <data key="d6">827fd80da359cf05b091c24e465dd05d</data>
    </edge>
    <edge source="DOCUMENT PROCESSING" target="PHASE 6: NETWORK VISUALIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The Document Processing phase is related to Phase 6: Network Visualization as the processed documents are used to support network visualization of high-dimensional vector spaces within existing graphs.</data>
      <data key="d6">827fd80da359cf05b091c24e465dd05d</data>
    </edge>
    <edge source="COMMUNITY SUMMARIZATION" target="COMMUNITY EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Community Summarization is related to Community Embedding, as the summarization of communities is based on the mapping of communities into a lower-dimensional space for visualization and analysis</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="COMMUNITY SUMMARIZATION" target="GRAPH TABLES EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Community Summarization builds upon the data from Graph Tables Emission, generating reports and summaries for communities at different levels of granularity to provide a high-level understanding of the graph</data>
      <data key="d6">5b2968b8f1c891d47ecbe641c3391663</data>
    </edge>
    <edge source="COMMUNITY SUMMARIZATION" target="NODE2VEC ALGORITHM">
      <data key="d4">1.0</data>
      <data key="d5">Community Summarization utilizes the vector representations generated by the Node2Vec algorithm to summarize communities and understand the graph at various levels of granularity</data>
      <data key="d6">5b2968b8f1c891d47ecbe641c3391663</data>
    </edge>
    <edge source="GRAPH EXTRACTION" target="CLAIM">
      <data key="d4">1.0</data>
      <data key="d5">During the Graph Extraction phase, Claims are extracted from the text within each TextUnit, contributing to the graph being built</data>
      <data key="d6">81f57cf867ea246ad9a6e794ed613375</data>
    </edge>
    <edge source="GRAPH EXTRACTION" target="GRAPH SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The output of Graph Extraction, which are subgraphs for each TextUnit, are combined and processed by Graph Summarization to create a single graph with merged entities and relationships.</data>
      <data key="d6">10d01d36390b307a63fd5bc97d8682c0</data>
    </edge>
    <edge source="UMAP DOCUMENTS" target="DOCUMENT EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Umap Documents is related to Document Embedding, as it maps documents into a lower-dimensional space for visualization and analysis, which is a part of the embedding process</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="UMAP DOCUMENTS" target="PHASE 6: NETWORK VISUALIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Umap Documents is a part of Phase 6: Network Visualization, as it is a process that generates a 2D representation of the document space for visualization.</data>
      <data key="d6">56506e2d064c0732efa3cf418057edfd</data>
    </edge>
    <edge source="UMAP ENTITIES" target="ENTITY &amp; RELATIONSHIP EXTRACTION">
      <data key="d4">1.0</data>
      <data key="d5">Umap Entities is related to Entity &amp; Relationship Extraction, as it maps entities into a lower-dimensional space for visualization and analysis, which is based on the entities and relationships identified in the text</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="UMAP ENTITIES" target="PHASE 6: NETWORK VISUALIZATION">
      <data key="d4">1.0</data>
      <data key="d5">Umap Entities is a part of Phase 6: Network Visualization, as it is a process that generates a 2D representation of the entity space for visualization.</data>
      <data key="d6">56506e2d064c0732efa3cf418057edfd</data>
    </edge>
    <edge source="NODES TABLE" target="LINK TO TEXTUNITS">
      <data key="d4">1.0</data>
      <data key="d5">The Nodes Table is related to the Link to TextUnits, as it contains information about the nodes in the graph, including their layout and relationships, which are connected to the corresponding TextUnits</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DOCUMENT EMBEDDING" target="DOCUMENT GRAPH CREATION">
      <data key="d4">1.0</data>
      <data key="d5">Document Embedding is related to Document Graph Creation, as the embedding process converts documents into numerical vectors for analysis and comparison, which is used in the creation of the document graph</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="DOCUMENT EMBEDDING" target="AVG. EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Avg. Embedding is a part of the Document Embedding process, which generates a vector representation of documents using an average of embeddings of document slices.</data>
      <data key="d6">827fd80da359cf05b091c24e465dd05d</data>
    </edge>
    <edge source="DOCUMENT GRAPH CREATION" target="DOCUMENT TABLES">
      <data key="d4">1.0</data>
      <data key="d5">Document Graph Creation is related to Document Tables, as the creation of the document graph is based on the information about the documents, including metadata and analysis results, which are stored in the Document Tables</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="COMMUNITY EMBEDDING" target="COMMUNITY TABLES">
      <data key="d4">1.0</data>
      <data key="d5">Community Embedding is related to Community Tables, as the mapping of communities into a lower-dimensional space for visualization and analysis is based on the information about the communities, including their members and properties, which are stored in the Community Tables</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="COMMUNITY EMBEDDING" target="GRAPH EMBEDDING">
      <data key="d4">1.0</data>
      <data key="d5">Community Embedding is related to Graph Embedding as both involve the representation of network structures in a vector space, with Community Embedding focusing specifically on community structures</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="GRAPH EMBEDDING" target="AUGMENTED GRAPH TABLES">
      <data key="d4">1.0</data>
      <data key="d5">Graph Embedding is related to Augmented Graph Tables, as the conversion of the graph into a numerical representation for analysis and visualization is based on the information about the graph after augmentation, including additional entities and relationships, which are stored in the Augmented Graph Tables</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="GRAPH EMBEDDING" target="GRAPH TABLES">
      <data key="d4">1.0</data>
      <data key="d5">Graph Tables are related to Graph Embedding as they are often used to store and organize the results of graph embedding techniques, including details about nodes, edges, and attributes</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="GRAPH EMBEDDING" target="PHASE 3: GRAPH AUGMENTATION">
      <data key="d4">1.0</data>
      <data key="d5">Graph Embedding is a subprocess of Phase 3: Graph Augmentation, aimed at understanding the implicit structure of the graph</data>
      <data key="d6">a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </edge>
    <edge source="GRAPH EMBEDDING" target="NODE2VEC ALGORITHM">
      <data key="d4">1.0</data>
      <data key="d5">Graph Embedding uses the Node2Vec algorithm to generate vector representations of the graph, which is essential for understanding the graph's structure and searching for related concepts</data>
      <data key="d6">5b2968b8f1c891d47ecbe641c3391663</data>
    </edge>
    <edge source="GRAPH EMBEDDING" target="GRAPH TABLES EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Graph Tables Emission follows the Graph Embedding step, where the final Entities and Relationships tables are emitted after text embedding, providing a structured representation of the graph data</data>
      <data key="d6">5b2968b8f1c891d47ecbe641c3391663</data>
    </edge>
    <edge source="ENTITY &amp; RELATIONSHIP EXTRACTION" target="ENTITY &amp; RELATIONSHIP SUMMARIZATION">
      <data key="d4">2.0</data>
      <data key="d5">Entity &amp; Relationship Extraction and Entity &amp; Relationship Summarization are closely intertwined processes within the domain of text analysis. Entity &amp; Relationship Extraction involves the identification of entities and their relationships from the text, which serves as a foundational step for Entity &amp; Relationship Summarization. The summarization process, in turn, relies on the results of the extraction to provide a concise overview of the identified entities and relationships, highlighting key connections and patterns within the data. This synergy between the two processes enables a comprehensive understanding of the structure and dynamics of the information, facilitating the identification of collaboration opportunities and knowledge gaps in specialized professional networks.</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20,6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="ENTITY &amp; RELATIONSHIP EXTRACTION" target="TEXT UNITS">
      <data key="d4">1.0</data>
      <data key="d5">Entity &amp; Relationship Extraction is related to Text Units as the process often operates on Text Units to identify entities and relationships within the text</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="ENTITY &amp; RELATIONSHIP SUMMARIZATION" target="ENTITY RESOLUTION">
      <data key="d4">1.0</data>
      <data key="d5">Entity &amp; Relationship Summarization is related to Entity Resolution, as the summarization of the entities and relationships identified in the text is used in the resolution of entities that refer to the same real-world object or concept</data>
      <data key="d6">493f38f41b89e767fc23d84e1fa5ba20</data>
    </edge>
    <edge source="ENTITY RESOLUTION" target="DOCUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">Entity Resolution is related to Documents as the process often operates on data from documents to identify and merge duplicate or similar entities</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="ENTITY RESOLUTION" target="GRAPH SUMMARIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The graph produced by Graph Summarization is further processed by Entity Resolution to identify and merge entities that represent the same real-world entity but have different names.</data>
      <data key="d6">10d01d36390b307a63fd5bc97d8682c0</data>
    </edge>
    <edge source="ENTITY RESOLUTION" target="COVARIATES">
      <data key="d4">1.0</data>
      <data key="d5">Entity Resolution does not directly produce Covariates, but the resolved entities and their relationships can be part of the context for Claim Extraction, which emits Covariates.</data>
      <data key="d6">d44248ff7b7bfd969a7208eb3d6e2a78</data>
    </edge>
    <edge source="DOCUMENTS" target="TEXT UNITS">
      <data key="d4">1.0</data>
      <data key="d5">Text Units are related to Documents as Text Units are derived from documents and used as the basis for further analysis or extraction, with a strict 1-to-many relationship by default</data>
      <data key="d6">6f92ce3fcd05dd5697ded83586f7bc08</data>
    </edge>
    <edge source="COVARIATES" target="CLAIM EXTRACTION &amp; EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Covariates are the primary artifacts emitted from the Claim Extraction &amp; Emission process, representing claims that are positive factual statements with an evaluated status and time-bounds</data>
      <data key="d6">a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </edge>
    <edge source="PHASE 3: GRAPH AUGMENTATION" target="GRAPH TABLES EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Graph Tables Emission is a subprocess of Phase 3: Graph Augmentation, involving emitting the final entities and relationships in a tabular format</data>
      <data key="d6">a6bcb4514cb6de67e3d74ad0ea62452d</data>
    </edge>
    <edge source="AUGMENT" target="AUGMENT WITH COLUMNS (CSV ONLY)">
      <data key="d4">1.0</data>
      <data key="d5">The Augment process is related to Augment with Columns (CSV Only) as it allows for the addition of extra fields to the Documents output when processing CSV data.</data>
      <data key="d6">827fd80da359cf05b091c24e465dd05d</data>
    </edge>
    <edge source="DOCUMENT TABLE EMISSION" target="DOCUMENTS TABLE EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Document Table Emission is the process of emitting the Documents table into the knowledge model, which is a part of the Documents Table Emission function.</data>
      <data key="d6">827fd80da359cf05b091c24e465dd05d</data>
    </edge>
    <edge source="DOCUMENTS TABLE EMISSION" target="PHASE 6: NETWORK VISUALIZATION">
      <data key="d4">1.0</data>
      <data key="d5">The Documents Table Emission is a prerequisite for Phase 6: Network Visualization, as it provides the data necessary for network visualization and understanding the relationships between documents.</data>
      <data key="d6">56506e2d064c0732efa3cf418057edfd</data>
    </edge>
    <edge source="PHASE 6: NETWORK VISUALIZATION" target="ENTITY-RELATIONSHIP GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">Phase 6: Network Visualization involves the Entity-Relationship graph, as it is one of the logical graphs that are visualized to understand the relationships between entities and their attributes.</data>
      <data key="d6">56506e2d064c0732efa3cf418057edfd</data>
    </edge>
    <edge source="PHASE 6: NETWORK VISUALIZATION" target="DOCUMENT GRAPH">
      <data key="d4">1.0</data>
      <data key="d5">Phase 6: Network Visualization involves the Document graph, as it is one of the logical graphs that are visualized to understand the relationships between documents.</data>
      <data key="d6">56506e2d064c0732efa3cf418057edfd</data>
    </edge>
    <edge source="PHASE 6: NETWORK VISUALIZATION" target="NODES TABLE EMISSION">
      <data key="d4">1.0</data>
      <data key="d5">Nodes Table Emission is a part of Phase 6: Network Visualization, as it is a process that emits a table of nodes for visualization, including information about whether the node is a document or an entity and the UMAP coordinates.</data>
      <data key="d6">56506e2d064c0732efa3cf418057edfd</data>
    </edge>
    <edge source="DISCRIMINATOR" target="UMAP COORDINATES">
      <data key="d4">1.0</data>
      <data key="d5">The Discriminator and UMAP Coordinates are related in that the Discriminator can help in interpreting the UMAP Coordinates by providing context on whether the coordinates represent a document or an entity in the data structure</data>
      <data key="d6">2011f03f21e526cf9277c27bf3e68242</data>
    </edge>
    <edge source="GRAPHRAG INDEXER CLI" target="CLI ARGUMENTS">
      <data key="d4">1.0</data>
      <data key="d5">The GraphRAG Indexer CLI utilizes CLI Arguments to customize its functionality and behavior, allowing users to configure various aspects of the indexing process.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="CLI ARGUMENTS" target="VERBOSE">
      <data key="d4">1.0</data>
      <data key="d5">The verbose argument is a CLI Argument that can be used to increase the logging level during the execution of the GraphRAG Indexer CLI.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="CLI ARGUMENTS" target="INIT">
      <data key="d4">1.0</data>
      <data key="d5">The init argument is a CLI Argument that initializes the data project directory for the GraphRAG Indexer CLI, setting up the necessary configuration and overrides.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="CLI ARGUMENTS" target="RESUME">
      <data key="d4">1.0</data>
      <data key="d5">The resume argument is a CLI Argument that enables the GraphRAG Indexer CLI to resume a previous run, loading parquet files as inputs and skipping certain workflows.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="CLI ARGUMENTS" target="CONFIG">
      <data key="d4">1.0</data>
      <data key="d5">The config argument is a CLI Argument that allows the GraphRAG Indexer CLI to execute a custom configuration, overriding the default settings.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="CLI ARGUMENTS" target="REPORTER">
      <data key="d4">1.0</data>
      <data key="d5">The reporter argument is a CLI Argument that specifies the progress reporting mechanism for the GraphRAG Indexer CLI.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
    <edge source="CLI ARGUMENTS" target="EMIT">
      <data key="d4">1.0</data>
      <data key="d5">The emit argument is a CLI Argument that determines the output formats for the tables generated by the GraphRAG Indexer CLI.</data>
      <data key="d6">f239de6498e0f471bf418974c00f1e36</data>
    </edge>
  </graph>
</graphml>