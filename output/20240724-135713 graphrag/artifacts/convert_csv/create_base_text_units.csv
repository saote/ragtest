id,chunk,chunk_id,document_ids,n_tokens
4cf772ca8a1ffad729902e9b630e1ec0,"Developing GraphRAG
Requirements
Name	Installation	Purpose
Python 3.10-3.12	Download	The library is Python-based.
Poetry	Instructions	Poetry is used for package management and virtualenv management in Python codebases
Getting Started
Install Dependencies
# Install Python dependencies.
poetry install

Execute the Indexing Engine
poetry run poe index <...args>

Executing Queries
poetry run poe query <...args>

Azurite
Some unit and smoke tests use Azurite to emulate Azure resources. This can be started by running:

./scripts/start-azurite.sh

or by simply running azurite in the terminal if already installed globally. See the Azurite documentation for more information about how to install and use Azurite.

Lifecycle Scripts
Our Python package utilizes Poetry to manage dependencies and poethepoet to manage build scripts.

Available scripts are:

poetry run poe index - Run the Indexing CLI
poetry run poe query - Run the Query CLI
poetry build - This invokes poetry build, which will build a wheel file and other distributable artifacts.
poetry run poe test - This will execute all tests.
poetry run poe test_unit - This will execute unit tests.
poetry run poe test_integration - This will execute integration tests.
poetry run poe test_smoke - This will execute smoke tests.
poetry run poe check - This will",4cf772ca8a1ffad729902e9b630e1ec0,['1360b438089091252bb82ed740bce5a6'],300
563caa38fe33c495449888d62950b959," CLI
poetry run poe query - Run the Query CLI
poetry build - This invokes poetry build, which will build a wheel file and other distributable artifacts.
poetry run poe test - This will execute all tests.
poetry run poe test_unit - This will execute unit tests.
poetry run poe test_integration - This will execute integration tests.
poetry run poe test_smoke - This will execute smoke tests.
poetry run poe check - This will perform a suite of static checks across the package, including:
formatting
documentation formatting
linting
security patterns
type-checking
poetry run poe fix - This will apply any available auto-fixes to the package. Usually this is just formatting fixes.
poetry run poe fix_unsafe - This will apply any available auto-fixes to the package, including those that may be unsafe.
poetry run poe format - Explicitly run the formatter across the package.
Troubleshooting
""RuntimeError: llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config"" when running poetry install
Make sure llvm-9 and llvm-9-dev are installed:

sudo apt-get install llvm-9 llvm-9-dev

and then in your bashrc, add

export LLVM_CONFIG=/usr/bin/llvm-config-9

""numba/_pymodule.h:6:10: fatal error: Python.h: No such file or directory"" when running",563caa38fe33c495449888d62950b959,['1360b438089091252bb82ed740bce5a6'],300
0dc1f5e4f8fb5903f12acf8e141fb205,"leshooting
""RuntimeError: llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config"" when running poetry install
Make sure llvm-9 and llvm-9-dev are installed:

sudo apt-get install llvm-9 llvm-9-dev

and then in your bashrc, add

export LLVM_CONFIG=/usr/bin/llvm-config-9

""numba/_pymodule.h:6:10: fatal error: Python.h: No such file or directory"" when running poetry install
Make sure you have python3.10-dev installed or more generally python<version>-dev

sudo apt-get install python3.10-dev

LLM call constantly exceeds TPM, RPM or time limits
GRAPHRAG_LLM_THREAD_COUNT and GRAPHRAG_EMBEDDING_THREAD_COUNT are both set to 50 by default. You can modify this values to reduce concurrency. Please refer to the Configuration Documents",0dc1f5e4f8fb5903f12acf8e141fb205,['1360b438089091252bb82ed740bce5a6'],187
e6fa3bdaf65c92df6b3430f02804321a,"Welcome to GraphRAG
üëâ Microsoft Research Blog Post
üëâ GraphRAG Accelerator
üëâ GitHub Repository
üëâ GraphRAG Arxiv

Figure 1: LLM-generated knowledge graph built from a private dataset using GPT-4 Turbo.

Figure 1: An LLM-generated knowledge graph built using GPT-4 Turbo.

GraphRAG is a structured, hierarchical approach to Retrieval Augmented Generation (RAG), as opposed to naive semantic-search approaches using plain text snippets. The GraphRAG process involves extracting a knowledge graph out of raw text, building a community hierarchy, generating summaries for these communities, and then leveraging these structures when perform RAG-based tasks.

To learn more about GraphRAG and how it can be used to enhance your LLMs ability to reason about your private data, please visit the Microsoft Research Blog Post.

Solution Accelerator üöÄ
To quickstart the GraphRAG system we recommend trying the Solution Accelerator package. This provides a user-friendly end-to-end experience with Azure resources.

Get Started with GraphRAG üöÄ
To start using GraphRAG, check out the Get Started guide. For a deeper dive into the main sub-systems, please visit the docpages for the Indexer and Query packages.

GraphRAG vs Baseline RAG üîç
Retrieval-Augmented Generation (RAG) is a technique to improve LLM outputs using real-world information.",e6fa3bdaf65c92df6b3430f02804321a,['150280be206221851ba72cf9f14d5667'],300
32603b739bed06b4695b0cc3915b2c4b," Solution Accelerator package. This provides a user-friendly end-to-end experience with Azure resources.

Get Started with GraphRAG üöÄ
To start using GraphRAG, check out the Get Started guide. For a deeper dive into the main sub-systems, please visit the docpages for the Indexer and Query packages.

GraphRAG vs Baseline RAG üîç
Retrieval-Augmented Generation (RAG) is a technique to improve LLM outputs using real-world information. This technique is an important part of most LLM-based tools and the majority of RAG approaches use vector similarity as the search technique, which we call Baseline RAG. GraphRAG uses knowledge graphs to provide substantial improvements in question-and-answer performance when reasoning about complex information. RAG techniques have shown promise in helping LLMs to reason about private datasets - data that the LLM is not trained on and has never seen before, such as an enterprise‚Äôs proprietary research, business documents, or communications. Baseline RAG was created to help solve this problem, but we observe situations where baseline RAG performs very poorly. For example:

Baseline RAG struggles to connect the dots. This happens when answering a question requires traversing disparate pieces of information through their shared attributes in order to provide new synthesized insights.
Baseline RAG performs poorly when being asked to holistically understand summarized semantic concepts over large data collections or even singular large documents.
To address this, the tech community is working to develop methods",32603b739bed06b4695b0cc3915b2c4b,['150280be206221851ba72cf9f14d5667'],300
d441b136505c273cf3577b6867e872e4," communications. Baseline RAG was created to help solve this problem, but we observe situations where baseline RAG performs very poorly. For example:

Baseline RAG struggles to connect the dots. This happens when answering a question requires traversing disparate pieces of information through their shared attributes in order to provide new synthesized insights.
Baseline RAG performs poorly when being asked to holistically understand summarized semantic concepts over large data collections or even singular large documents.
To address this, the tech community is working to develop methods that extend and enhance RAG. Microsoft Research‚Äôs new approach, GraphRAG, uses LLMs to create a knowledge graph based on an input corpus. This graph, along with community summaries and graph machine learning outputs, are used to augment prompts at query time. GraphRAG shows substantial improvement in answering the two classes of questions described above, demonstrating intelligence or mastery that outperforms other approaches previously applied to private datasets.

The GraphRAG Process ü§ñ
GraphRAG builds upon our prior research and tooling using graph machine learning. The basic steps of the GraphRAG process are as follows:

Index
Slice up an input corpus into a series of TextUnits, which act as analyzable units for the rest of the process, and provide fine-grained references into our outputs.
Extract all entities, relationships, and key claims from the TextUnits using an LLM.
Perform a hierarchical clustering of the graph using the Leiden technique. To see this visually, check out",d441b136505c273cf3577b6867e872e4,['150280be206221851ba72cf9f14d5667'],300
369b39fdfd649d6df32a5d7b4cc559b7," our prior research and tooling using graph machine learning. The basic steps of the GraphRAG process are as follows:

Index
Slice up an input corpus into a series of TextUnits, which act as analyzable units for the rest of the process, and provide fine-grained references into our outputs.
Extract all entities, relationships, and key claims from the TextUnits using an LLM.
Perform a hierarchical clustering of the graph using the Leiden technique. To see this visually, check out Figure 1 above. Each circle is an entity (e.g., a person, place, or organization), with the size representing the degree of the entity, and the color representing its community.
Generate summaries of each community and its constituents from the bottom-up. This aids in holistic understanding of the dataset.
Query
At query time, these structures are used to provide materials for the LLM context window when answering a question. The primary query modes are:

Global Search for reasoning about holistic questions about the corpus by leveraging the community summaries.
Local Search for reasoning about specific entities by fanning-out to their neighbors and associated concepts.
Prompt Tuning
Using GraphRAG with your data out of the box may not yield the best possible results. We strongly recommend to fine-tune your prompts following the Prompt Tuning Guide in our documentation.",369b39fdfd649d6df32a5d7b4cc559b7,['150280be206221851ba72cf9f14d5667'],267
849698743b07680402ff8572b1c6c469," corpus by leveraging the community summaries.
Local Search for reasoning about specific entities by fanning-out to their neighbors and associated concepts.
Prompt Tuning
Using GraphRAG with your data out of the box may not yield the best possible results. We strongly recommend to fine-tune your prompts following the Prompt Tuning Guide in our documentation.",849698743b07680402ff8572b1c6c469,['150280be206221851ba72cf9f14d5667'],67
f8cf53ce98a8bc52581f7907ad98ef70,"Query Engine üîé
The Query Engine is the retrieval module of the Graph RAG Library. It is one of the two main components of the Graph RAG library, the other being the Indexing Pipeline (see Indexing Pipeline). It is responsible for the following tasks:

Local Search
Global Search
Question Generation
Local Search
Local search method generates answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents (e.g. What are the healing properties of chamomile?).

For more details about how Local Search works please refer to the Local Search documentation.

Global Search
Global search method generates answers by searching over all AI-generated community reports in a map-reduce fashion. This is a resource-intensive method, but often gives good responses for questions that require an understanding of the dataset as a whole (e.g. What are the most significant values of the herbs mentioned in this notebook?).

More about this can be checked at the Global Search documentation.

Question Generation
This functionality takes a list of user queries and generates the next candidate questions. This is useful for generating follow-up questions in a conversation or for generating a list of questions for the investigator to dive deeper into the dataset.

Information about how question generation works can be found at the Question Generation documentation page.

Local Search üîé
Entity-based Reasoning
The local search method combines structured data from the knowledge graph",f8cf53ce98a8bc52581f7907ad98ef70,['217bd625ebb21f3343be31530abac558'],300
364624242a84e1859e758069d914d8c8," in this notebook?).

More about this can be checked at the Global Search documentation.

Question Generation
This functionality takes a list of user queries and generates the next candidate questions. This is useful for generating follow-up questions in a conversation or for generating a list of questions for the investigator to dive deeper into the dataset.

Information about how question generation works can be found at the Question Generation documentation page.

Local Search üîé
Entity-based Reasoning
The local search method combines structured data from the knowledge graph with unstructured data from the input documents to augment the LLM context with relevant entity information at query time. It is well-suited for answering questions that require an understanding of specific entities mentioned in the input documents (e.g., ‚ÄúWhat are the healing properties of chamomile?‚Äù).

Methodology
Entity
Description
Embedding
Entity-Text
Unit Mapping
Ranking +
Filtering
Entity-Report
Mapping
Ranking +
Filtering
Entity-Entity
Relationships
Ranking +
Filtering
Entity-Entity
Relationships
Ranking +
Filtering
Entity-Covariate
Mappings
Ranking +
Filtering
User Query
.1
Conversation
History
Extracted Entities
.2
Candidate
Text Units
Prioritized
Text Units
.3
Candidate
Community Reports
Prioritized
Community Reports
Candidate
Entities
Prioritized
Entities
Candidate
Relationships
Prioritized
Relationships
Candidate
Covariates",364624242a84e1859e758069d914d8c8,['217bd625ebb21f3343be31530abac558'],300
3a0742c280217fe600b9af2d06b58eea,"Ranking +
Filtering
Entity-Entity
Relationships
Ranking +
Filtering
Entity-Covariate
Mappings
Ranking +
Filtering
User Query
.1
Conversation
History
Extracted Entities
.2
Candidate
Text Units
Prioritized
Text Units
.3
Candidate
Community Reports
Prioritized
Community Reports
Candidate
Entities
Prioritized
Entities
Candidate
Relationships
Prioritized
Relationships
Candidate
Covariates
Prioritized
Covariates
Conversation History
Response
Local Search Dataflow
Given a user query and, optionally, the conversation history, the local search method identifies a set of entities from the knowledge graph that are semantically-related to the user input. These entities serve as access points into the knowledge graph, enabling the extraction of further relevant details such as connected entities, relationships, entity covariates, and community reports. Additionally, it also extracts relevant text chunks from the raw input documents that are associated with the identified entities. These candidate data sources are then prioritized and filtered to fit within a single context window of pre-defined size, which is used to generate a response to the user query.

Configuration
Below are the key parameters of the LocalSearch class:

llm: OpenAI model object to be used for response generation
context_builder: context builder object to be used for preparing context data from collections of knowledge model objects
system_prompt: prompt template used to generate the search",3a0742c280217fe600b9af2d06b58eea,['217bd625ebb21f3343be31530abac558'],300
25797740f434cc2bf16365fc498791f6," documents that are associated with the identified entities. These candidate data sources are then prioritized and filtered to fit within a single context window of pre-defined size, which is used to generate a response to the user query.

Configuration
Below are the key parameters of the LocalSearch class:

llm: OpenAI model object to be used for response generation
context_builder: context builder object to be used for preparing context data from collections of knowledge model objects
system_prompt: prompt template used to generate the search response. Default template can be found at system_prompt
response_type: free-form text describing the desired response type and format (e.g., Multiple Paragraphs, Multi-Page Report)
llm_params: a dictionary of additional parameters (e.g., temperature, max_tokens) to be passed to the LLM call
context_builder_params: a dictionary of additional parameters to be passed to the context_builder object when building context for the search prompt
callbacks: optional callback functions, can be used to provide custom event handlers for LLM's completion streaming events
How to Use
An example of a local search scenario can be found in the following notebook.

Question Generation ‚ùî
Entity-based Question Generation
The question generation method combines structured data from the knowledge graph with unstructured data from the input documents to generate candidate questions related to specific entities.

Methodology
Given a list of prior user questions, the question generation method uses the same context-building approach employed in local search to extract and prioritize relevant structured and",25797740f434cc2bf16365fc498791f6,['217bd625ebb21f3343be31530abac558'],300
1415949832ba3fee570ea961998a8ac4," custom event handlers for LLM's completion streaming events
How to Use
An example of a local search scenario can be found in the following notebook.

Question Generation ‚ùî
Entity-based Question Generation
The question generation method combines structured data from the knowledge graph with unstructured data from the input documents to generate candidate questions related to specific entities.

Methodology
Given a list of prior user questions, the question generation method uses the same context-building approach employed in local search to extract and prioritize relevant structured and unstructured data, including entities, relationships, covariates, community reports and raw text chunks. These data records are then fitted into a single LLM prompt to generate candidate follow-up questions that represent the most important or urgent information content or themes in the data.

Configuration
Below are the key parameters of the Question Generation class:

llm: OpenAI model object to be used for response generation
context_builder: context builder object to be used for preparing context data from collections of knowledge model objects, using the same context builder class as in local search
system_prompt: prompt template used to generate candidate questions. Default template can be found at system_prompt
llm_params: a dictionary of additional parameters (e.g., temperature, max_tokens) to be passed to the LLM call
context_builder_params: a dictionary of additional parameters to be passed to the context_builder object when building context for the question generation prompt
callbacks: optional callback functions, can be used to provide custom event handlers for L",1415949832ba3fee570ea961998a8ac4,['217bd625ebb21f3343be31530abac558'],300
3e143a60e2aeb57eb418a68d1484bbb3," using the same context builder class as in local search
system_prompt: prompt template used to generate candidate questions. Default template can be found at system_prompt
llm_params: a dictionary of additional parameters (e.g., temperature, max_tokens) to be passed to the LLM call
context_builder_params: a dictionary of additional parameters to be passed to the context_builder object when building context for the question generation prompt
callbacks: optional callback functions, can be used to provide custom event handlers for LLM's completion streaming events
How to Use
An example of the question generation function can be found in the following notebook.

Global Search üîé
Whole Dataset Reasoning
Baseline RAG struggles with queries that require aggregation of information across the dataset to compose an answer. Queries such as ‚ÄúWhat are the top 5 themes in the data?‚Äù perform terribly because baseline RAG relies on a vector search of semantically similar text content within the dataset. There is nothing in the query to direct it to the correct information.

However, with GraphRAG we can answer such questions, because the structure of the LLM-generated knowledge graph tells us about the structure (and thus themes) of the dataset as a whole. This allows the private dataset to be organized into meaningful semantic clusters that are pre-summarized. Using our global search method, the LLM uses these clusters to summarize these themes when responding to a user query.

Methodology
Ranking +
Filtering
Shuffled Community
Report Batch",3e143a60e2aeb57eb418a68d1484bbb3,['217bd625ebb21f3343be31530abac558'],300
812b3414c467da0b62f7932d2adcbad4," the correct information.

However, with GraphRAG we can answer such questions, because the structure of the LLM-generated knowledge graph tells us about the structure (and thus themes) of the dataset as a whole. This allows the private dataset to be organized into meaningful semantic clusters that are pre-summarized. Using our global search method, the LLM uses these clusters to summarize these themes when responding to a user query.

Methodology
Ranking +
Filtering
Shuffled Community
Report Batch 1
Shuffled Community
Report Batch 2
Shuffled Community
Report Batch N
RIR
{1..N}
Rated Intermediate
Response N
Rated Intermediate
Response 1
Rated Intermediate
Response 2
User Query
.1
Conversation History
.2
Aggregated Intermediate
Responses
Response
Global Search Dataflow
Given a user query and, optionally, the conversation history, the global search method uses a collection of LLM-generated community reports from a specified level of the graph's community hierarchy as context data to generate response in a map-reduce manner. At the map step, community reports are segmented into text chunks of pre-defined size. Each text chunk is then used to produce an intermediate response containing a list of point, each of which is accompanied by a numerical rating indicating the importance of the point. At the reduce step, a filtered set of the most important points from the intermediate responses are aggregated and used as the context to generate the final response",812b3414c467da0b62f7932d2adcbad4,['217bd625ebb21f3343be31530abac558'],300
1a4bca0786d529c91073997b63412adc," specified level of the graph's community hierarchy as context data to generate response in a map-reduce manner. At the map step, community reports are segmented into text chunks of pre-defined size. Each text chunk is then used to produce an intermediate response containing a list of point, each of which is accompanied by a numerical rating indicating the importance of the point. At the reduce step, a filtered set of the most important points from the intermediate responses are aggregated and used as the context to generate the final response.

The quality of the global search‚Äôs response can be heavily influenced by the level of the community hierarchy chosen for sourcing community reports. Lower hierarchy levels, with their detailed reports, tend to yield more thorough responses, but may also increase the time and LLM resources needed to generate the final response due to the volume of reports.

Configuration
Below are the key parameters of the GlobalSearch class:

llm: OpenAI model object to be used for response generation
context_builder: context builder object to be used for preparing context data from community reports
map_system_prompt: prompt template used in the map stage. Default template can be found at map_system_prompt
reduce_system_prompt: prompt template used in the reduce stage, default template can be found at reduce_system_prompt
response_type: free-form text describing the desired response type and format (e.g., Multiple Paragraphs, Multi-Page Report)
allow_general_knowledge: setting this to True will include additional instructions to the reduce_system_prompt to prompt the",1a4bca0786d529c91073997b63412adc,['217bd625ebb21f3343be31530abac558'],300
e442fbb7a67e97ebc4de131b25c639e1," used for preparing context data from community reports
map_system_prompt: prompt template used in the map stage. Default template can be found at map_system_prompt
reduce_system_prompt: prompt template used in the reduce stage, default template can be found at reduce_system_prompt
response_type: free-form text describing the desired response type and format (e.g., Multiple Paragraphs, Multi-Page Report)
allow_general_knowledge: setting this to True will include additional instructions to the reduce_system_prompt to prompt the LLM to incorporate relevant real-world knowledge outside of the dataset. Note that this may increase hallucinations, but can be useful for certain scenarios. Default is False *general_knowledge_inclusion_prompt: instruction to add to the reduce_system_prompt if allow_general_knowledge is enabled. Default instruction can be found at general_knowledge_instruction
max_data_tokens: token budget for the context data
map_llm_params: a dictionary of additional parameters (e.g., temperature, max_tokens) to be passed to the LLM call at the map stage
reduce_llm_params: a dictionary of additional parameters (e.g., temperature, max_tokens) to passed to the LLM call at the reduce stage
context_builder_params: a dictionary of additional parameters to be passed to the context_builder object when building context window for the map stage.
concurrent_coroutines: controls the degree of parallelism in the map stage.
callbacks: optional callback functions, can be used to provide custom event handlers for LLM",e442fbb7a67e97ebc4de131b25c639e1,['217bd625ebb21f3343be31530abac558'],300
e0cc1cf05b92456e09100790815186fe," to the LLM call at the map stage
reduce_llm_params: a dictionary of additional parameters (e.g., temperature, max_tokens) to passed to the LLM call at the reduce stage
context_builder_params: a dictionary of additional parameters to be passed to the context_builder object when building context window for the map stage.
concurrent_coroutines: controls the degree of parallelism in the map stage.
callbacks: optional callback functions, can be used to provide custom event handlers for LLM's completion streaming events
How to Use
An example of a global search scenario can be found in the following notebook.

Query CLI
The GraphRAG query CLI allows for no-code usage of the GraphRAG Query engine.

python -m graphrag.query --data <path-to-data> --community_level <comunit-level> --response_type <response-type> --method <""local""|""global""> <query>

CLI Arguments
--data <path-to-data> - Folder containing the .parquet output files from running the Indexer.
--community_level <community-level> - Community level in the Leiden community hierarchy from which we will load the community reports higher value means we use reports on smaller communities. Default: 2
--response_type <response-type> - Free form text describing the response type and format, can be anything, e.g. Multiple Paragraphs, Single Paragraph, Single Sentence, List of 3-7 Points, Single Page, Multi-Page Report",e0cc1cf05b92456e09100790815186fe,['217bd625ebb21f3343be31530abac558'],300
8c70a7321fb0e945054d226a8c69abee," .parquet output files from running the Indexer.
--community_level <community-level> - Community level in the Leiden community hierarchy from which we will load the community reports higher value means we use reports on smaller communities. Default: 2
--response_type <response-type> - Free form text describing the response type and format, can be anything, e.g. Multiple Paragraphs, Single Paragraph, Single Sentence, List of 3-7 Points, Single Page, Multi-Page Report. Default: Multiple Paragraphs.
--method <""local""|""global""> - Method to use to answer the query, one of local or global. For more information check Overview
Env Variables
Required environment variables to execute:

GRAPHRAG_API_KEY - API Key for executing the model, will fallback to OPENAI_API_KEY if one is not provided.
GRAPHRAG_LLM_MODEL - Model to use for Chat Completions.
GRAPHRAG_EMBEDDING_MODEL - Model to use for Embeddings.
You can further customize the execution by providing these environment variables:

GRAPHRAG_LLM_API_BASE - The API Base URL. Default: None
GRAPHRAG_LLM_TYPE - The LLM operation type. Either openai_chat or azure_openai_chat. Default: openai_chat
GRAPHRAG_LLM_MAX_RETRIES - The maximum number of retries to attempt when a request fails. Default: 20
GRAPHRAG_EMBED",8c70a7321fb0e945054d226a8c69abee,['217bd625ebb21f3343be31530abac558'],300
1ef6439b7c457ba43993467ff734eedf," Model to use for Embeddings.
You can further customize the execution by providing these environment variables:

GRAPHRAG_LLM_API_BASE - The API Base URL. Default: None
GRAPHRAG_LLM_TYPE - The LLM operation type. Either openai_chat or azure_openai_chat. Default: openai_chat
GRAPHRAG_LLM_MAX_RETRIES - The maximum number of retries to attempt when a request fails. Default: 20
GRAPHRAG_EMBEDDING_API_BASE - The API Base URL. Default: None
GRAPHRAG_EMBEDDING_TYPE - The embedding client to use. Either openai_embedding or azure_openai_embedding. Default: openai_embedding
GRAPHRAG_EMBEDDING_MAX_RETRIES - The maximum number of retries to attempt when a request fails. Default: 20
GRAPHRAG_LOCAL_SEARCH_TEXT_UNIT_PROP - Proportion of context window dedicated to related text units. Default: 0.5
GRAPHRAG_LOCAL_SEARCH_COMMUNITY_PROP - Proportion of context window dedicated to community reports. Default: 0.1
GRAPHRAG_LOCAL_SEARCH_CONVERSATION_HISTORY_MAX_TURNS - Maximum number of turns to include in the conversation history. Default: 5
GRAPHRAG_LOCAL_SEARCH_TOP_K_ENTITIES - Number of related entities to retrieve from the entity description embedding store. Default: 10
GRAPHRAG_LOCAL_SEARCH_TOP_K_RELATIONSH",1ef6439b7c457ba43993467ff734eedf,['217bd625ebb21f3343be31530abac558'],300
2efb1fec56fe3b0543d395dd541295c3,".5
GRAPHRAG_LOCAL_SEARCH_COMMUNITY_PROP - Proportion of context window dedicated to community reports. Default: 0.1
GRAPHRAG_LOCAL_SEARCH_CONVERSATION_HISTORY_MAX_TURNS - Maximum number of turns to include in the conversation history. Default: 5
GRAPHRAG_LOCAL_SEARCH_TOP_K_ENTITIES - Number of related entities to retrieve from the entity description embedding store. Default: 10
GRAPHRAG_LOCAL_SEARCH_TOP_K_RELATIONSHIPS - Control the number of out-of-network relationships to pull into the context window. Default: 10
GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS - Change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000). Default: 12000
GRAPHRAG_LOCAL_SEARCH_LLM_MAX_TOKENS - Change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000=1500). Default: 2000
GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS - Change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000). Default: 12000
GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS - Change this based on the token limit you have on your model",2efb1fec56fe3b0543d395dd541295c3,['217bd625ebb21f3343be31530abac558'],300
2049798d3000849f8bec3e88c0006807," using a model with 8k limit, a good setting could be 1000=1500). Default: 2000
GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS - Change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000). Default: 12000
GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS - Change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000). Default: 12000
GRAPHRAG_GLOBAL_SEARCH_MAP_MAX_TOKENS - Default: 500
GRAPHRAG_GLOBAL_SEARCH_REDUCE_MAX_TOKENS - Change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000-1500). Default: 2000
GRAPHRAG_GLOBAL_SEARCH_CONCURRENCY - Default: 32
",2049798d3000849f8bec3e88c0006807,['217bd625ebb21f3343be31530abac558'],215
60df16c009594c15c4ead6125e1453ce,"GRAPHRAG_GLOBAL_SEARCH_CONCURRENCY - Default: 32
",60df16c009594c15c4ead6125e1453ce,['217bd625ebb21f3343be31530abac558'],15
25e04f0e9a961dcdc3f6eae6df7807b2,"Indexing Architecture
Key Concepts
Knowledge Model
In order to support the GraphRAG system, the outputs of the indexing engine (in the Default Configuration Mode) are aligned to a knowledge model we call the GraphRAG Knowledge Model. This model is designed to be an abstraction over the underlying data storage technology, and to provide a common interface for the GraphRAG system to interact with. In normal use-cases the outputs of the GraphRAG Indexer would be loaded into a database system, and the GraphRAG's Query Engine would interact with the database using the knowledge model data-store types.

DataShaper Workflows
GraphRAG's Indexing Pipeline is built on top of our open-source library, DataShaper. DataShaper is a data processing library that allows users to declaratively express data pipelines, schemas, and related assets using well-defined schemas. DataShaper has implementations in JavaScript and Python, and is designed to be extensible to other languages.

One of the core resource types within DataShaper is a Workflow. Workflows are expressed as sequences of steps, which we call verbs. Each step has a verb name and a configuration object. In DataShaper, these verbs model relational concepts such as SELECT, DROP, JOIN, etc.. Each verb transforms an input data table, and that table is passed down the pipeline.

Input Table
SELECT
JOIN
BINARIZE
Output Table
Sample Workflow
LLM-based Workflow Steps
GraphRAG",25e04f0e9a961dcdc3f6eae6df7807b2,['6419b98c277c12893cdc15db7eefebca'],300
81031e23c0b000ee60cd9b06950f96cd," the core resource types within DataShaper is a Workflow. Workflows are expressed as sequences of steps, which we call verbs. Each step has a verb name and a configuration object. In DataShaper, these verbs model relational concepts such as SELECT, DROP, JOIN, etc.. Each verb transforms an input data table, and that table is passed down the pipeline.

Input Table
SELECT
JOIN
BINARIZE
Output Table
Sample Workflow
LLM-based Workflow Steps
GraphRAG's Indexing Pipeline implements a handful of custom verbs on top of the standard, relational verbs that our DataShaper library provides. These verbs give us the ability to augment text documents with rich, structured data using the power of LLMs such as GPT-4. We utilize these verbs in our standard workflow to extract entities, relationships, claims, community structures, and community reports and summaries. This behavior is customizable and can be extended to support many kinds of AI-based data enrichment and extraction tasks.

Workflow Graphs
Because of the complexity of our data indexing tasks, we needed to be able to express our data pipeline as series of multiple, interdependent workflows. In the GraphRAG Indexing Pipeline, each workflow may define dependencies on other workflows, effectively forming a directed acyclic graph (DAG) of workflows, which is then used to schedule processing.

Prepare
Chunk
ExtractGraph
EmbedDocuments
GenerateReports
EmbedGraph
EntityResolution
Sample Workflow DAG
Dataframe Message",81031e23c0b000ee60cd9b06950f96cd,['6419b98c277c12893cdc15db7eefebca'],300
d19a57bca2c14fc9c2bf5058958380fd,".

Workflow Graphs
Because of the complexity of our data indexing tasks, we needed to be able to express our data pipeline as series of multiple, interdependent workflows. In the GraphRAG Indexing Pipeline, each workflow may define dependencies on other workflows, effectively forming a directed acyclic graph (DAG) of workflows, which is then used to schedule processing.

Prepare
Chunk
ExtractGraph
EmbedDocuments
GenerateReports
EmbedGraph
EntityResolution
Sample Workflow DAG
Dataframe Message Format
The primary unit of communication between workflows, and between workflow steps is an instance of pandas.DataFrame. Although side-effects are possible, our goal is to be data-centric and table-centric in our approach to data processing. This allows us to easily reason about our data, and to leverage the power of dataframe-based ecosystems. Our underlying dataframe technology may change over time, but our primary goal is to support the DataShaper workflow schema while retaining single-machine ease of use and developer ergonomics.

LLM Caching
The GraphRAG library was designed with LLM interactions in mind, and a common setback when working with LLM APIs is various errors errors due to network latency, throttling, etc.. Because of these potential error cases, we've added a cache layer around LLM interactions. When completion requests are made using the same input set (prompt and tuning parameters), we return a cached result if one exists. This allows our indexer to be more resilient to network issues, to act id",d19a57bca2c14fc9c2bf5058958380fd,['6419b98c277c12893cdc15db7eefebca'],300
6335601c6ec22bd6f15c8b69c26f854b,"M Caching
The GraphRAG library was designed with LLM interactions in mind, and a common setback when working with LLM APIs is various errors errors due to network latency, throttling, etc.. Because of these potential error cases, we've added a cache layer around LLM interactions. When completion requests are made using the same input set (prompt and tuning parameters), we return a cached result if one exists. This allows our indexer to be more resilient to network issues, to act idempotently, and to provide a more efficient end-user experience.",6335601c6ec22bd6f15c8b69c26f854b,['6419b98c277c12893cdc15db7eefebca'],114
f76c18c7582167c3626f8741c2c9374f,"From Local to Global: A Graph RAG Approach to
 Query-Focused Summarization


Abstract
 The use of retrieval-augmented generation (RAG) to retrieve relevant informa
tion from an external knowledge source enables large language models (LLMs)
 to answer questions over private and/or previously unseen document collections.
 However, RAG fails on global questions directed at an entire text corpus, such
 as ‚ÄúWhat are the main themes in the dataset?‚Äù, since this is inherently a query
focused summarization (QFS) task, rather than an explicit retrieval task. Prior
 QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical
 RAGsystems. Tocombine the strengths of these contrasting methods, we propose
 a Graph RAGapproach to question answering over private text corpora that scales
 with both the generality of user questions and the quantity of source text to be in
dexed. Our approach uses an LLM to build a graph-based text index in two stages:
 f
 irst to derive an entity knowledge graph from the source documents, then to pre
generate community summaries for all groups of closely-related entities. Given a
 question, each community summary is used to generate a partial response, before
 all partial responses are again summarized in a final response to the user. For a
 class of global sensemaking questions over datasets in the 1 million token range,
 we show that Graph RAG leads to substantial improvements",f76c18c7582167c3626f8741c2c9374f,['78b4407b9fac94c6e276c9a6a5cd0745'],300
b149708d0b4ac3ff417565739ea6b03b," a graph-based text index in two stages:
 f
 irst to derive an entity knowledge graph from the source documents, then to pre
generate community summaries for all groups of closely-related entities. Given a
 question, each community summary is used to generate a partial response, before
 all partial responses are again summarized in a final response to the user. For a
 class of global sensemaking questions over datasets in the 1 million token range,
 we show that Graph RAG leads to substantial improvements over a na¬® ƒ±ve RAG
 baseline for both the comprehensiveness and diversity of generated answers. An
 open-source, Python-based implementation of both global and local Graph RAG
 approaches is forthcoming at https://aka.ms/graphrag

1 Introduction
 Human endeavors across a range of domains rely on our ability to read and reason about large
 collections of documents, often reaching conclusions that go beyond anything stated in the source
 texts themselves. With the emergence of large language models (LLMs), we are already witnessing
 attempts to automate human-like sensemaking in complex domains like scientific discovery (Mi
crosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as‚Äúa motivated, continuous effort to understand connections (which can be among people, places, and
 events) in order to anticipate their trajectories and act effectively‚Äù (Klein et al., 2006a). Supporting
 human-led sensemaking",b149708d0b4ac3ff417565739ea6b03b,['78b4407b9fac94c6e276c9a6a5cd0745'],300
c7669e6a1add9a2829b09196256b1492,"Ms), we are already witnessing
 attempts to automate human-like sensemaking in complex domains like scientific discovery (Mi
crosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as‚Äúa motivated, continuous effort to understand connections (which can be among people, places, and
 events) in order to anticipate their trajectories and act effectively‚Äù (Klein et al., 2006a). Supporting
 human-led sensemaking over entire text corpora, however, needs a way for people to both apply and
 refine their mental model of the data (Klein et al., 2006b) by asking questions of a global nature.
 Retrieval-augmented generation (RAG, Lewis et al., 2020) is an established approach to answering
 user questions over entire datasets, but it is designed for situations where these answers are contained
 locally within regions of text whose retrieval provides sufficient grounding for the generation task.
 Instead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in
 particular, query-focused abstractive summarization that generates natural language summaries and
 not just concatenated excerpts (Baumel et al., 2018; Laskar et al., 2020; Yao et al., 2017) . In recent
 years, however, such distinctions between summarization tasks that are abstractive versus extractive,
",c7669e6a1add9a2829b09196256b1492,['78b4407b9fac94c6e276c9a6a5cd0745'],300
85eff07c379a9dc24db0edb983acf3c9," task.
 Instead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in
 particular, query-focused abstractive summarization that generates natural language summaries and
 not just concatenated excerpts (Baumel et al., 2018; Laskar et al., 2020; Yao et al., 2017) . In recent
 years, however, such distinctions between summarization tasks that are abstractive versus extractive,
 generic versus query-focused, and single-document versus multi-document, have become less rele
vant. While early applications of the transformer architecture showed substantial improvements on
 the state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu
 and Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam
 et al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,
 all of which can use in-context learning to summarize any content provided in their context window.
 The challenge remains, however, for query-focused abstractive summarization over an entire corpus.
 Such volumes of text can greatly exceed the limits of LLM context windows, and the expansion of
 such windows may not be enough given that",85eff07c379a9dc24db0edb983acf3c9,['78b4407b9fac94c6e276c9a6a5cd0745'],300
3fc3718256cb7f614fcde622af2ed912," Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,
 all of which can use in-context learning to summarize any content provided in their context window.
 The challenge remains, however, for query-focused abstractive summarization over an entire corpus.
 Such volumes of text can greatly exceed the limits of LLM context windows, and the expansion of
 such windows may not be enough given that information can be ‚Äúlost in the middle‚Äù of longer
 contexts (Kuratov et al., 2024; Liu et al., 2023). In addition, although the direct retrieval of text
 chunks in na¬® ƒ±ve RAG is likely inadequate for QFS tasks, it is possible that an alternative form of
 pre-indexing could support a new RAG approach specifically targeting global summarization.
 In this paper, we present a Graph RAG approach based on global summarization of an LLM-derived
 knowledge graph (Figure 1). In contrast with related work that exploits the structured retrieval
 and traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored
 quality of graphs in this context: their inherent modularity (Newman, 2006) and the ability of com
munity detection algorithms to partition graphs into modular communities of closely-related nodes
 (e.g., Louvain, Blondel et al.,",3fc3718256cb7f614fcde622af2ed912,['78b4407b9fac94c6e276c9a6a5cd0745'],300
d39abd5380fb3fe0468ea1e122512091," of an LLM-derived
 knowledge graph (Figure 1). In contrast with related work that exploits the structured retrieval
 and traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored
 quality of graphs in this context: their inherent modularity (Newman, 2006) and the ability of com
munity detection algorithms to partition graphs into modular communities of closely-related nodes
 (e.g., Louvain, Blondel et al., 2008; Leiden, Traag et al., 2019). LLM-generated summaries of these
 community descriptions provide complete coverage of the underlying graph index and the input doc
uments it represents. Query-focused summarization of an entire corpus is then made possible using
 a map-reduce approach: first using each community summary to answer the query independently
 and in parallel, then summarizing all relevant partial answers into a final global answer.
 To evaluate this approach, we used an LLM to generate a diverse set of activity-centered sense
making questions from short descriptions of two representative real-world datasets, containing pod
cast transcripts and news articles respectively. For the target qualities of comprehensiveness, diver
sity, and empowerment (defined in subsection 3.4) that develop understanding of broad issues and
 themes, we both explore the impact of varying the the hierarchical level of community summaries
 used to answer queries, as well as compare to na¬® ƒ±ve RAG and",d39abd5380fb3fe0468ea1e122512091,['78b4407b9fac94c6e276c9a6a5cd0745'],300
d2399fd0aae5bd200639806ca87184f8," to generate a diverse set of activity-centered sense
making questions from short descriptions of two representative real-world datasets, containing pod
cast transcripts and news articles respectively. For the target qualities of comprehensiveness, diver
sity, and empowerment (defined in subsection 3.4) that develop understanding of broad issues and
 themes, we both explore the impact of varying the the hierarchical level of community summaries
 used to answer queries, as well as compare to na¬® ƒ±ve RAG and global map-reduce summarization
 of source texts. We show that all global approaches outperform na¬® ƒ±ve RAG on comprehensiveness
 and diversity, and that Graph RAG with intermediate- and low-level community summaries shows
 favorable performance over source text summarization on these same metrics, at lower token costs.

 2 GraphRAGApproach&Pipeline
 We now unpack the high-level data flow of the Graph RAG approach (Figure 1) and pipeline, de
scribing key design parameters, techniques, and implementation details for each step.
 2.1 Source Documents ‚Üí Text Chunks
 Afundamental design decision is the granularity with which input texts extracted from source doc
uments should be split into text chunks for processing. In the following step, each of these chunks
 will be passed to a set of LLM prompts designed to extract the various elements of a graph index.
 Longer text chunks require fewer LLM calls for such extraction, but suffer from the",d2399fd0aae5bd200639806ca87184f8,['78b4407b9fac94c6e276c9a6a5cd0745'],300
e7caf4256ddea71533af1c4c50444146,"scribing key design parameters, techniques, and implementation details for each step.
 2.1 Source Documents ‚Üí Text Chunks
 Afundamental design decision is the granularity with which input texts extracted from source doc
uments should be split into text chunks for processing. In the following step, each of these chunks
 will be passed to a set of LLM prompts designed to extract the various elements of a graph index.
 Longer text chunks require fewer LLM calls for such extraction, but suffer from the recall degrada
tion of longer LLM context windows (Kuratov et al., 2024; Liu et al., 2023). This behavior can be
 observed in Figure 2 in the case of a single extraction round (i.e., with zero gleanings): on a sample
 dataset (HotPotQA, Yang et al., 2018), using a chunk size of 600 token extracted almost twice as
 many entity references as when using a chunk size of 2400. While more references are generally
 better, any extraction process needs to balance recall and precision for the target activity.
 2.2 Text Chunks ‚ÜíElement Instances
 The baseline requirement for this step is to identify and extract instances of graph nodes and edges
 from each chunk of source text. We do this using a multipart LLM prompt that first identifies all
 entities in the text, including their name, type, and description, before identifying all relationships
 between clearly-related entities, including",e7caf4256ddea71533af1c4c50444146,['78b4407b9fac94c6e276c9a6a5cd0745'],300
e50740c4332fdedb8739773592e2a402,"2400. While more references are generally
 better, any extraction process needs to balance recall and precision for the target activity.
 2.2 Text Chunks ‚ÜíElement Instances
 The baseline requirement for this step is to identify and extract instances of graph nodes and edges
 from each chunk of source text. We do this using a multipart LLM prompt that first identifies all
 entities in the text, including their name, type, and description, before identifying all relationships
 between clearly-related entities, including the source and target entities and a description of their
 relationship. Both kinds of element instance are output in a single list of delimited tuples.
 The primary opportunity to tailor this prompt to the domain of the document corpus lies in the
 choice of few-shot examples provided to the LLM for in-context learning (Brown et al., 2020).
For example, while our default prompt extracting the broad class of ‚Äúnamed entities‚Äù like people,
 places, and organizations is generally applicable, domains with specialized knowledge (e.g., science,
 medicine, law) will benefit from few-shot examples specialized to those domains. We also support
 a secondary extraction prompt for any additional covariates we would like to associate with the
 extracted node instances. Our default covariate prompt aims to extract claims linked to detected
 entities, including the subject, object, type, description, source text span, and start and end dates.
 To balance the needs of efficiency and quality, we use multiple rounds of ‚Äúgleanings",e50740c4332fdedb8739773592e2a402,['78b4407b9fac94c6e276c9a6a5cd0745'],300
805a07a8f9c2ed5da2d9a61356aafa77," (e.g., science,
 medicine, law) will benefit from few-shot examples specialized to those domains. We also support
 a secondary extraction prompt for any additional covariates we would like to associate with the
 extracted node instances. Our default covariate prompt aims to extract claims linked to detected
 entities, including the subject, object, type, description, source text span, and start and end dates.
 To balance the needs of efficiency and quality, we use multiple rounds of ‚Äúgleanings‚Äù, up to a
 specified maximum, to encourage the LLM to detect any additional entities it may have missed
 on prior extraction rounds. This is a multi-stage process in which we first ask the LLM to assess
 whether all entities were extracted, using a logit bias of 100 to force a yes/no decision. If the LLM
 responds that entities were missed, then a continuation indicating that ‚ÄúMANY entities were missed
 in the last extraction‚Äù encourages the LLM to glean these missing entities. This approach allows us
 to use larger chunk sizes without a drop in quality (Figure 2) or the forced introduction of noise.
 2.3 Element Instances ‚Üí Element Summaries
 The use of an LLM to ‚Äúextract‚Äù descriptions of entities, relationships, and claims represented in
 source texts is already a form of abstractive summarization, relying on the LLM to create inde
pendently meaningful summaries of concepts that may be implied but not stated by the",805a07a8f9c2ed5da2d9a61356aafa77,['78b4407b9fac94c6e276c9a6a5cd0745'],300
a73d3e7b661743b7583d8a0fd412b6a7," these missing entities. This approach allows us
 to use larger chunk sizes without a drop in quality (Figure 2) or the forced introduction of noise.
 2.3 Element Instances ‚Üí Element Summaries
 The use of an LLM to ‚Äúextract‚Äù descriptions of entities, relationships, and claims represented in
 source texts is already a form of abstractive summarization, relying on the LLM to create inde
pendently meaningful summaries of concepts that may be implied but not stated by the text itself
 (e.g., the presence of implied relationships). To convert all such instance-level summaries into sin
gle blocks of descriptive text for each graph element (i.e., entity node, relationship edge, and claim
 covariate) requires a further round of LLM summarization over matching groups of instances.
 A potential concern at this stage is that the LLM may not consistently extract references to the
 same entity in the same text format, resulting in duplicate entity elements and thus duplicate nodes
 in the entity graph. However, since all closely-related ‚Äúcommunities‚Äù of entities will be detected
 and summarized in the following step, and given that LLMs can understand the common entity
 behind multiple name variations, our overall approach is resilient to such variations provided there
 is sufficient connectivity from all variations to a shared set of closely-related entities.
 Overall, our use of rich descriptive text for homogeneous nodes in a potentially noisy graph structure
 is aligned with both the capabilities of LLMs and",a73d3e7b661743b7583d8a0fd412b6a7,['78b4407b9fac94c6e276c9a6a5cd0745'],300
6dace8e490674ac8e031aed987a63789," in the entity graph. However, since all closely-related ‚Äúcommunities‚Äù of entities will be detected
 and summarized in the following step, and given that LLMs can understand the common entity
 behind multiple name variations, our overall approach is resilient to such variations provided there
 is sufficient connectivity from all variations to a shared set of closely-related entities.
 Overall, our use of rich descriptive text for homogeneous nodes in a potentially noisy graph structure
 is aligned with both the capabilities of LLMs and the needs of global, query-focused summarization.
 These qualities also differentiate our graph index from typical knowledge graphs, which rely on
 concise and consistent knowledge triples (subject, predicate, object) for downstream reasoning tasks.
 2.4 Element Summaries ‚Üí Graph Communities
 The index created in the previous step can be modelled as an homogeneous undirected weighted
 graph in which entity nodes are connected by relationship edges, with edge weights representing the
 normalized counts of detected relationship instances. Given such a graph, a variety of community
 detection algorithms may be used to partition the graph into communities of nodes with stronger
 connections to one another than to the other nodes in the graph (e.g., see the surveys by Fortu
nato, 2010 and Jin et al., 2021). In our pipeline, we use Leiden (Traag et al., 2019) on account of
 its ability to recover hierarchical community structure of large-scale graphs efficiently (Figure 3).
 Each",6dace8e490674ac8e031aed987a63789,['78b4407b9fac94c6e276c9a6a5cd0745'],300
a660289d2bf43f25d3524d35cd2d9a96,", a variety of community
 detection algorithms may be used to partition the graph into communities of nodes with stronger
 connections to one another than to the other nodes in the graph (e.g., see the surveys by Fortu
nato, 2010 and Jin et al., 2021). In our pipeline, we use Leiden (Traag et al., 2019) on account of
 its ability to recover hierarchical community structure of large-scale graphs efficiently (Figure 3).
 Each level of this hierarchy provides a community partition that covers the nodes of the graph in a
 mutually-exclusive, collective-exhaustive way, enabling divide-and-conquer global summarization.
 2.5 GraphCommunities ‚Üí Community Summaries
 The next step is to create report-like summaries of each community in the Leiden hierarchy, using
 a method designed to scale to very large datasets. These summaries are independently useful in
 their own right as a way to understand the global structure and semantics of the dataset, and may
 themselves be used to make sense of a corpus in the absence of a question. For example, a user
 may scan through community summaries at one level looking for general themes of interest, then
 follow links to the reports at the lower level that provide more details for each of the subtopics. Here,
 however, we focus on their utility as part of a graph-based index used for answering global queries.
 Community summaries are generated in the following way:
 ‚Ä¢ Leaf-level communities.",a660289d2bf43f25d3524d35cd2d9a96,['78b4407b9fac94c6e276c9a6a5cd0745'],300
93d4d4effbf989e6ef1c4c3b4f42494e," dataset, and may
 themselves be used to make sense of a corpus in the absence of a question. For example, a user
 may scan through community summaries at one level looking for general themes of interest, then
 follow links to the reports at the lower level that provide more details for each of the subtopics. Here,
 however, we focus on their utility as part of a graph-based index used for answering global queries.
 Community summaries are generated in the following way:
 ‚Ä¢ Leaf-level communities. The element summaries of a leaf-level community (nodes, edges,
 covariates) are prioritized and then iteratively added to the LLM context window until
 the token limit is reached. The prioritization is as follows: for each community edge in
 decreasing order of combined source and target node degree (i.e., overall prominance), add
 descriptions of the source node, target node, linked covariates, and the edge itself.
 ‚Ä¢ Higher-level communities. If all element summaries fit within the token limit of the con
text window, proceed as for leaf-level communities and summarize all element summaries
 within the community. Otherwise, rank sub-communities in decreasing order of element
 summary tokens and iteratively substitute sub-community summaries (shorter) for their
 associated element summaries (longer) until fit within the context window is achieved.
2.6 Community Summaries ‚Üí Community Answers ‚Üí Global Answer
 Given a user query, the community summaries generated in the previous step can be used",93d4d4effbf989e6ef1c4c3b4f42494e,['78b4407b9fac94c6e276c9a6a5cd0745'],300
9b52298451f8936974ab08a129b0b92e," within the token limit of the con
text window, proceed as for leaf-level communities and summarize all element summaries
 within the community. Otherwise, rank sub-communities in decreasing order of element
 summary tokens and iteratively substitute sub-community summaries (shorter) for their
 associated element summaries (longer) until fit within the context window is achieved.
2.6 Community Summaries ‚Üí Community Answers ‚Üí Global Answer
 Given a user query, the community summaries generated in the previous step can be used to generate
 a final answer in a multi-stage process. The hierarchical nature of the community structure also
 means that questions can be answered using the community summaries from different levels, raising
 the question of whether a particular level in the hierarchical community structure offers the best
 balance of summary detail and scope for general sensemaking questions (evaluated in section 3).
 For a given community level, the global answer to any user query is generated as follows:
 ‚Ä¢ Prepare community summaries. Community summaries are randomly shuffled and divided
 into chunks of pre-specified token size. This ensures relevant information is distributed
 across chunks, rather than concentrated (and potentially lost) in a single context window.
 ‚Ä¢ Map community answers. Generate intermediate answers in parallel, one for each chunk.
 The LLM is also asked to generate a score between 0-100 indicating how helpful the gen
erated answer is in answering the target question. Answers with score 0 are filtered out.
 ‚Ä¢ Reduce to global answer. Intermediate community",9b52298451f8936974ab08a129b0b92e,['78b4407b9fac94c6e276c9a6a5cd0745'],300
aed2ea39de8a027cc818c7f4557f0514," and divided
 into chunks of pre-specified token size. This ensures relevant information is distributed
 across chunks, rather than concentrated (and potentially lost) in a single context window.
 ‚Ä¢ Map community answers. Generate intermediate answers in parallel, one for each chunk.
 The LLM is also asked to generate a score between 0-100 indicating how helpful the gen
erated answer is in answering the target question. Answers with score 0 are filtered out.
 ‚Ä¢ Reduce to global answer. Intermediate community answers are sorted in descending order
 of helpfulness score and iteratively added into a new context window until the token limit
 is reached. This final context is used to generate the global answer returned to the user.
Dataset   Example activity framing and generation of global sensemaking questions
 Podcast
 transcripts

 User: A tech journalist looking for insights and trends in the tech industry
 Task: Understanding how tech leaders view the role of policy and regulation
 Questions:
 1. Which episodes deal primarily with tech policy and government regulation?
 2. How do guests perceive the impact of privacy laws on technology development?
 3. Do any guests discuss the balance between innovation and ethical considerations?
 4. What are the suggested changes to current policies mentioned by the guests?
 5. Are collaborations between tech companies and governments discussed and how?

 News
 articles

 User: Educator incorporating current affairs into curricula
 Task: Teaching about health and wellness
 Questions:
 1. What current topics in health can",aed2ea39de8a027cc818c7f4557f0514,['78b4407b9fac94c6e276c9a6a5cd0745'],300
5a5a94f85dfc4d119ebb87f3037fd1cc," policy and government regulation?
 2. How do guests perceive the impact of privacy laws on technology development?
 3. Do any guests discuss the balance between innovation and ethical considerations?
 4. What are the suggested changes to current policies mentioned by the guests?
 5. Are collaborations between tech companies and governments discussed and how?

 News
 articles

 User: Educator incorporating current affairs into curricula
 Task: Teaching about health and wellness
 Questions:
 1. What current topics in health can be integrated into health education curricula?
 2. How do news articles address the concepts of preventive medicine and wellness?
 3. Are there examples of health articles that contradict each other, and if so, why?
 4. What insights can be gleaned about public health priorities based on news coverage?
 5. How can educators use the dataset to highlight the importance of health literacy?
 Table 1: Examples of potential users, tasks, and questions generated by the LLM based on short
 descriptions of the target datasets. Questions target global understanding rather than specific details.
 3 Evaluation
 3.1 Datasets
 We selected two datasets in the one million token range, each equivalent to about 10 novels of text
 and representative of the kind of corpora that users may encounter in their real world activities:
 ‚Ä¢ Podcast transcripts. Compiled transcripts of podcast conversations between Kevin Scott,
 Microsoft CTO, and other technology leaders (Behind the Tech, Scott, 2024). Size:",5a5a94f85dfc4d119ebb87f3037fd1cc,['78b4407b9fac94c6e276c9a6a5cd0745'],300
5d04129d46662571f635a4e63cb4d6b7,"
 descriptions of the target datasets. Questions target global understanding rather than specific details.
 3 Evaluation
 3.1 Datasets
 We selected two datasets in the one million token range, each equivalent to about 10 novels of text
 and representative of the kind of corpora that users may encounter in their real world activities:
 ‚Ä¢ Podcast transcripts. Compiled transcripts of podcast conversations between Kevin Scott,
 Microsoft CTO, and other technology leaders (Behind the Tech, Scott, 2024). Size: 1669
 √ó600-token text chunks, with 100-token overlaps between chunks (‚àº1 million tokens).
 ‚Ä¢ News articles. Benchmark dataset comprising news articles published from September
 2013 to December 2023 in a range of categories, including entertainment, business, sports,
 technology, health, and science (MultiHop-RAG; Tang and Yang, 2024). Size: 3197 √ó
 600-token text chunks, with 100-token overlaps between chunks (‚àº1.7 million tokens).
 3.2 Queries
 Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang
 et al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,
 the associated question sets target explicit fact retrieval rather than summarization for the purpose
 of data sensemaking, i.e., the process though which people inspect, engage with, and",5d04129d46662571f635a4e63cb4d6b7,['78b4407b9fac94c6e276c9a6a5cd0745'],300
8e69f04648f5fc24c299591365f1aa68,".7 million tokens).
 3.2 Queries
 Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang
 et al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,
 the associated question sets target explicit fact retrieval rather than summarization for the purpose
 of data sensemaking, i.e., the process though which people inspect, engage with, and contextualize
 data within the broader scope of real-world activities (Koesten et al., 2021). Similarly, methods for
 extracting latent summarization queries from source texts also exist (Xu and Lapata, 2021), but such
 extracted questions can target details that betray prior knowledge of the texts.
 To evaluate the effectiveness of RAG systems for more global sensemaking tasks, we need questions
 that convey only a high-level understanding of dataset contents, and not the details of specific texts.
 We used an activity-centered approach to automate the generation of such questions: given a short
 description of a dataset, we asked the LLM to identify N potential users and N tasks per user,
 then for each (user, task) combination, we asked the LLM to generate N questions that require
 understanding of the entire corpus. For our evaluation, a value of N = 5 resulted in 125 test questions
 per dataset. Table 1 shows example questions for each",8e69f04648f5fc24c299591365f1aa68,['78b4407b9fac94c6e276c9a6a5cd0745'],300
a739018eb63cbb6c26b779bd37afc233," of specific texts.
 We used an activity-centered approach to automate the generation of such questions: given a short
 description of a dataset, we asked the LLM to identify N potential users and N tasks per user,
 then for each (user, task) combination, we asked the LLM to generate N questions that require
 understanding of the entire corpus. For our evaluation, a value of N = 5 resulted in 125 test questions
 per dataset. Table 1 shows example questions for each of the two evaluation datasets.
3.3 Conditions
 Wecomparesixdifferent conditions in our analysis, including Graph RAG using four levels of graph
 communities (C0, C1, C2, C3), a text summarization method applying our map-reduce approach
 directly to source texts (TS), and a na¬® ƒ±ve ‚Äúsemantic search‚Äù RAG approach (SS):
 ‚Ä¢ CO.Uses root-level community summaries (fewest in number) to answer user queries.
 ‚Ä¢ C1. Uses high-level community summaries to answer queries. These are sub-communities
 of C0, if present, otherwise C0 communities projected down.
 ‚Ä¢ C2. Uses intermediate-level community summaries to answer queries. These are sub
communities of C1, if present, otherwise C1 communities projected down.
 ‚Ä¢ C3. Uses low-level community summaries (greatest in number) to answer queries. These
 are sub-communities of C2, if present, otherwise",a739018eb63cbb6c26b779bd37afc233,['78b4407b9fac94c6e276c9a6a5cd0745'],300
88847c4d3e6c5a64a5b44d9d99d06237,".
 ‚Ä¢ C1. Uses high-level community summaries to answer queries. These are sub-communities
 of C0, if present, otherwise C0 communities projected down.
 ‚Ä¢ C2. Uses intermediate-level community summaries to answer queries. These are sub
communities of C1, if present, otherwise C1 communities projected down.
 ‚Ä¢ C3. Uses low-level community summaries (greatest in number) to answer queries. These
 are sub-communities of C2, if present, otherwise C2 communities projected down.
 ‚Ä¢ TS. The same method as in subsection 2.6, except source texts (rather than community
 summaries) are shuffled and chunked for the map-reduce summarization stages.
 ‚Ä¢ SS. An implementation of na¬® ƒ±ve RAG in which text chunks are retrieved and added to the
 available context window until the specified token limit is reached.
 The size of the context window and the prompts used for answer generation are the same across
 all six conditions (except for minor modifications to reference styles to match the types of context
 information used). Conditions only differ in how the contents of the context window are created.
 The graph index supporting conditions C0-C3 was created using our generic prompts for entity and
 relationship extraction only, with entity types and few-shot examples tailored to the domain of the
 data. The graph indexing process used a context window size of 600 tokens with 1 gleaning for the
 Podcast dataset and 0 gleanings for",88847c4d3e6c5a64a5b44d9d99d06237,['78b4407b9fac94c6e276c9a6a5cd0745'],300
53455f8552b0787cb13c5a03eb550842," (except for minor modifications to reference styles to match the types of context
 information used). Conditions only differ in how the contents of the context window are created.
 The graph index supporting conditions C0-C3 was created using our generic prompts for entity and
 relationship extraction only, with entity types and few-shot examples tailored to the domain of the
 data. The graph indexing process used a context window size of 600 tokens with 1 gleaning for the
 Podcast dataset and 0 gleanings for the News dataset.
 3.4 Metrics
 LLMs have been shown to be good evaluators of natural language generation, achieving state-of
the-art or competitive results compared against human judgements (Wang et al., 2023a; Zheng et al.,
 2024). While this approach can generate reference-based metrics when gold standard answers are
 known, it is also capable of measuring the qualities of generated texts (e.g., fluency) in a reference
free style (Wang et al., 2023a) as well as in head-to-head comparison of competing outputs (LLM
as-a-judge, Zheng et al., 2024). LLMs have also shown promise at evaluating the performance of
 conventional RAG systems, automatically evaluating qualities like context relevance, faithfulness,
 and answer relevance (RAGAS, Es et al., 2023).
 Given the multi-stage nature of our Graph RAG mechanism, the multiple conditions we wanted to",53455f8552b0787cb13c5a03eb550842,['78b4407b9fac94c6e276c9a6a5cd0745'],300
cbfd4a09b266218f64dc6e6d80f8a77e,"Wang et al., 2023a) as well as in head-to-head comparison of competing outputs (LLM
as-a-judge, Zheng et al., 2024). LLMs have also shown promise at evaluating the performance of
 conventional RAG systems, automatically evaluating qualities like context relevance, faithfulness,
 and answer relevance (RAGAS, Es et al., 2023).
 Given the multi-stage nature of our Graph RAG mechanism, the multiple conditions we wanted to
 compare, and the lack of gold standard answers to our activity-based sensemaking questions, we
 decided to adopt a head-to-head comparison approach using an LLM evaluator. We selected three
 target metrics capturing qualities that are desirable for sensemaking activities, as well as a control
 metric (directness) used as a indicator of validity. Since directness is effectively in opposition to
 comprehensiveness and diversity, we would not expect any method to win across all four metrics.
 Our head-to-head measures computed using an LLM evaluator are as follows:
 ‚Ä¢ Comprehensiveness. How much detail does the answer provide to cover all aspects and
 details of the question?
 ‚Ä¢ Diversity. Howvaried andrich is the answerin providing different perspectives and insights
 on the question?
 ‚Ä¢ Empowerment. How well does the answer help the reader understand and make informed
 judgements about the topic?
 ‚Ä¢ Directness. How specifically and clearly does the answer address the question?
 For our evaluation",cbfd4a09b266218f64dc6e6d80f8a77e,['78b4407b9fac94c6e276c9a6a5cd0745'],300
2a5e1212b351d63d059ba1a1dec2811f,"-head measures computed using an LLM evaluator are as follows:
 ‚Ä¢ Comprehensiveness. How much detail does the answer provide to cover all aspects and
 details of the question?
 ‚Ä¢ Diversity. Howvaried andrich is the answerin providing different perspectives and insights
 on the question?
 ‚Ä¢ Empowerment. How well does the answer help the reader understand and make informed
 judgements about the topic?
 ‚Ä¢ Directness. How specifically and clearly does the answer address the question?
 For our evaluation, the LLM is provided with the question, target metric, and a pair of answers, and
 asked to assess which answer is better according to the metric, as well as why. It returns the winner
 if one exists, otherwise a tie if they are fundamentally similar and the differences are negligible.
 To account for the stochasticity of LLMs, we run each comparison five times and use mean scores.
 Table 2 shows an example of LLM-generated assessment.

Figure4:Head-to-headwinratepercentagesof(rowcondition)over(columncondition)acrosstwo
 datasets, fourmetrics,and125questionspercomparison(eachrepeatedfivetimesandaveraged).
 Theoverallwinnerperdatasetandmetricisshowninbold. Self-winrateswerenotcomputedbut
 areshownastheexpected50%forreference.AllGraphRAGconditionsoutperformedna¬® ƒ±veRAG
 oncomprehensivenessanddiversity.ConditionsC1-C3als",2a5e1212b351d63d059ba1a1dec2811f,['78b4407b9fac94c6e276c9a6a5cd0745'],300
b83d819b03401fb8332316960610e5d6,"-headwinratepercentagesof(rowcondition)over(columncondition)acrosstwo
 datasets, fourmetrics,and125questionspercomparison(eachrepeatedfivetimesandaveraged).
 Theoverallwinnerperdatasetandmetricisshowninbold. Self-winrateswerenotcomputedbut
 areshownastheexpected50%forreference.AllGraphRAGconditionsoutperformedna¬® ƒ±veRAG
 oncomprehensivenessanddiversity.ConditionsC1-C3alsoshowedslightimprovementsinanswer
 comprehensivenessanddiversityoverTS(globaltextsummarizationwithoutagraphindex).
 3.5 Configuration
 Theeffect of contextwindowsizeonanyparticular task isunclear, especiallyformodels like
 gpt-4-turbowitha largecontext sizeof128k tokens. Given thepotential for information to
 be‚Äúlostinthemiddle‚Äùoflongercontexts(Kuratovetal.,2024;Liuetal.,2023),wewantedtoex
ploretheeffectsofvaryingthecontextwindowsizeforourcombinationsofdatasets,questions,and
 metrics. Inparticular,ourgoalwastodeterminetheoptimumcontextsizeforourbaselinecondition
 (SS)andthenusethisuniformlyforallquery-timeLLMuse. Tothatend,wetestedfourcontext
 windowsizes: 8k,16k,32kand64k. Surprisingly",b83d819b03401fb8332316960610e5d6,['78b4407b9fac94c6e276c9a6a5cd0745'],300
3900d15a5f3ace358fc06038c34cdf79,"uetal.,2023),wewantedtoex
ploretheeffectsofvaryingthecontextwindowsizeforourcombinationsofdatasets,questions,and
 metrics. Inparticular,ourgoalwastodeterminetheoptimumcontextsizeforourbaselinecondition
 (SS)andthenusethisuniformlyforallquery-timeLLMuse. Tothatend,wetestedfourcontext
 windowsizes: 8k,16k,32kand64k. Surprisingly, thesmallestcontextwindowsizetested(8k)
 wasuniversallybetterforallcomparisonsoncomprehensiveness(averagewinrateof58.1%),while
 performingcomparablywithlargercontextsizesondiversity(averagewinrate=52.4%),andem
powerment(averagewinrate=51.3%).Givenourpreferenceformorecomprehensiveanddiverse
 answers,wethereforeusedafixedcontextwindowsizeof8ktokensforthefinalevaluation.
 3.6 Results
 Theindexingprocessresultedinagraphconsistingof8564nodesand20691edgesforthePodcast
 dataset,andalargergraphof15754nodesand19520edgesfortheNewsdataset.Table3showsthe
 numberofcommunitysummariesatdifferentlevelsofeachgraphcommunityhierarchy.
 Global approachesvs. na¬®ƒ±veRAG.As shown inFigure4, global approachesconsistently",3900d15a5f3ace358fc06038c34cdf79,['78b4407b9fac94c6e276c9a6a5cd0745'],300
d08fc91bbfe9749abab38a99a1a88dc6,"windowsizeof8ktokensforthefinalevaluation.
 3.6 Results
 Theindexingprocessresultedinagraphconsistingof8564nodesand20691edgesforthePodcast
 dataset,andalargergraphof15754nodesand19520edgesfortheNewsdataset.Table3showsthe
 numberofcommunitysummariesatdifferentlevelsofeachgraphcommunityhierarchy.
 Global approachesvs. na¬®ƒ±veRAG.As shown inFigure4, global approachesconsistentlyout
performedthena¬® ƒ±veRAG(SS)approachinbothcomprehensivenessanddiversitymetricsacross
 datasets. Specifically,globalapproachesachievedcomprehensivenesswinratesbetween72-83%
 forPodcasttranscriptsand72-80%forNewsarticles,whilediversitywinratesrangedfrom75-82%
 and62-71%respectively.Ouruseofdirectnessasavaliditytestalsoachievedtheexpectedresults,
 i.e.,thatna¬® ƒ±veRAGproducesthemostdirectresponsesacrossallcomparisons.

 Table 3: Number of context units (community summaries for C0-C3 and text chunks for TS), corre
sponding token counts, and percentage of the maximum token count. Map-reduce summarization of
 source texts is the most resource-intensive approach requiring the highest number of context tokens.
 Root-level community summaries (C0) require dramatically fewer tokens per query (",d08fc91bbfe9749abab38a99a1a88dc6,['78b4407b9fac94c6e276c9a6a5cd0745'],300
71f14506a6b15dfabd93fd1606a67b73," i.e.,thatna¬® ƒ±veRAGproducesthemostdirectresponsesacrossallcomparisons.

 Table 3: Number of context units (community summaries for C0-C3 and text chunks for TS), corre
sponding token counts, and percentage of the maximum token count. Map-reduce summarization of
 source texts is the most resource-intensive approach requiring the highest number of context tokens.
 Root-level community summaries (C0) require dramatically fewer tokens per query (9x-43x).
 Community summaries vs. source texts. When comparing community summaries to source texts
 using Graph RAG, community summaries generally provided a small but consistent improvement
 in answer comprehensiveness and diversity, except for root-level summaries. Intermediate-level
 summaries in the Podcast dataset and low-level community summaries in the News dataset achieved
 comprehensiveness win rates of 57% and 64%, respectively. Diversity win rates were 57% for
 Podcast intermediate-level summaries and 60% for News low-level community summaries. Table 3
 also illustrates the scalability advantages of Graph RAG compared to source text summarization: for
 low-level community summaries (C3), Graph RAG required 26-33% fewer context tokens, while
 for root-level community summaries (C0), it required over 97% fewer tokens. For a modest drop in
 performance compared with other global methods, root-level Graph RAG offers a highly efficient
 method for the iterative question answering",71f14506a6b15dfabd93fd1606a67b73,['78b4407b9fac94c6e276c9a6a5cd0745'],300
ed433e2f5d5387b47376eb0e45ca1c99,"% for News low-level community summaries. Table 3
 also illustrates the scalability advantages of Graph RAG compared to source text summarization: for
 low-level community summaries (C3), Graph RAG required 26-33% fewer context tokens, while
 for root-level community summaries (C0), it required over 97% fewer tokens. For a modest drop in
 performance compared with other global methods, root-level Graph RAG offers a highly efficient
 method for the iterative question answering that characterizes sensemaking activity, while retaining
 advantages in comprehensiveness (72% win rate) and diversity (62% win rate) over na¬® ƒ±ve RAG.
 Empowerment. Empowermentcomparisons showed mixed results for both global approaches versus
 na¬® ƒ±ve RAG (SS) and Graph RAG approaches versus source text summarization (TS). Ad-hoc LLM
 use to analyze LLM reasoning for this measure indicated that the ability to provide specific exam
ples, quotes, and citations was judged to be key to helping users reach an informed understanding.
 Tuning element extraction prompts may help to retain more of these details in the Graph RAG index.
 4 Related Work
 4.1 RAGApproaches and Systems
 When using LLMs, RAG involves first retrieving relevant information from external data sources,
 then adding this information to the context window of the LLM along with the original query (Ram
 et al",ed433e2f5d5387b47376eb0e45ca1c99,['78b4407b9fac94c6e276c9a6a5cd0745'],300
38feec52b8bfbd3fd8e03635acdaec97," provide specific exam
ples, quotes, and citations was judged to be key to helping users reach an informed understanding.
 Tuning element extraction prompts may help to retain more of these details in the Graph RAG index.
 4 Related Work
 4.1 RAGApproaches and Systems
 When using LLMs, RAG involves first retrieving relevant information from external data sources,
 then adding this information to the context window of the LLM along with the original query (Ram
 et al., 2023). Na¬® ƒ±ve RAG approaches (Gao et al., 2023) do this by converting documents to text,
 splitting text into chunks, and embedding these chunks into a vector space in which similar positions
 represent similar semantics. Queries are then embedded into the same vector space, with the text
 chunks of the nearest k vectors used as context. More advanced variations exist, but all solve the
 problem of what to do when an external dataset of interest exceeds the LLM‚Äôs context window.
 Advanced RAG systems include pre-retrieval, retrieval, post-retrieval strategies designed to over
come the drawbacks of Na¬® ƒ±ve RAG, while Modular RAG systems include patterns for iterative and
 dynamic cycles of interleaved retrieval and generation (Gao et al., 2023). Our implementation of
 Graph RAG incorporates multiple concepts related to other systems. For example, our community
 summaries are a kind of self-memory (Selfmem,",38feec52b8bfbd3fd8e03635acdaec97,['78b4407b9fac94c6e276c9a6a5cd0745'],300
7da3d8d244b67f09425a4a7783e4bb55," LLM‚Äôs context window.
 Advanced RAG systems include pre-retrieval, retrieval, post-retrieval strategies designed to over
come the drawbacks of Na¬® ƒ±ve RAG, while Modular RAG systems include patterns for iterative and
 dynamic cycles of interleaved retrieval and generation (Gao et al., 2023). Our implementation of
 Graph RAG incorporates multiple concepts related to other systems. For example, our community
 summaries are a kind of self-memory (Selfmem, Cheng et al., 2024) for generation-augmented re
trieval (GAR, Mao et al., 2020) that facilitates future generation cycles, while our parallel generation
 of community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)
 or federated (FeB4RAG, Wang et al., 2024) retrieval-generation strategy. Other systems have also
 combined these concepts for multi-document summarization (CAiRE-COVID, Su et al., 2020) and
 multi-hop question answering (ITRG, Feng et al., 2023; IR-CoT, Trivedi et al., 2022; DSP, Khattab
 et al., 2022). Our use of a hierarchical index and summarization also bears resemblance to further
 approaches, such as generating a hierarchical index of text chunks by clustering the vectors of text
 embeddings (RAPTOR,",7da3d8d244b67f09425a4a7783e4bb55,['78b4407b9fac94c6e276c9a6a5cd0745'],300
40f2d6a0270e54743e7ace239369da96,"ization (CAiRE-COVID, Su et al., 2020) and
 multi-hop question answering (ITRG, Feng et al., 2023; IR-CoT, Trivedi et al., 2022; DSP, Khattab
 et al., 2022). Our use of a hierarchical index and summarization also bears resemblance to further
 approaches, such as generating a hierarchical index of text chunks by clustering the vectors of text
 embeddings (RAPTOR, Sarthi et al., 2024) or generating a ‚Äútree of clarifications‚Äù to answer mul
tiple interpretations of ambiguous questions (Kim et al., 2023). However, none of these iterative or
 hierarchical approaches use the kind of self-generated graph index that enables Graph RAG.
4.2 Graphs and LLMs
 Use of graphs in connection with LLMs and RAG is a developing research area, with multiple
 directions already established. These include using LLMs for knowledge graph creation (Tra
janoska et al., 2023) and completion (Yao et al., 2023), as well as for the extraction of causal
 graphs (Ban et al., 2023; Zhang et al., 2024) from source texts. They also include forms of ad
vanced RAG (Gao et al., 2023) where the index is a knowledge graph (KAPING, Baek et al.,",40f2d6a0270e54743e7ace239369da96,['78b4407b9fac94c6e276c9a6a5cd0745'],300
7383c69e93bb8c8648181f5355d2c9a7," LLMs for knowledge graph creation (Tra
janoska et al., 2023) and completion (Yao et al., 2023), as well as for the extraction of causal
 graphs (Ban et al., 2023; Zhang et al., 2024) from source texts. They also include forms of ad
vanced RAG (Gao et al., 2023) where the index is a knowledge graph (KAPING, Baek et al., 2023),
 where subsets of the graph structure (G-Retriever, He et al., 2024) or derived graph metrics (Graph
ToolFormer, Zhang, 2023) are the objects of enquiry, where narrative outputs are strongly grounded
 in the facts of retrieved subgraphs (SURGE, Kang et al., 2023), where retrieved event-plot sub
graphs are serialized using narrative templates (FABULA, Ranade and Joshi, 2023), and where the
 system supports both creation and traversal of text-relationship graphs for multi-hop question an
swering (Wang et al., 2023b). In terms of open-source software, a variety a graph databases are
 supported by both the LangChain (LangChain, 2024) and LlamaIndex (LlamaIndex, 2024) libraries,
 while a moregeneral class of graph-based RAG applications is also emerging, including systems that
 can create and reason",7383c69e93bb8c8648181f5355d2c9a7,['78b4407b9fac94c6e276c9a6a5cd0745'],300
e015335cdcae20e6546fe7cbdef56c1a," and where the
 system supports both creation and traversal of text-relationship graphs for multi-hop question an
swering (Wang et al., 2023b). In terms of open-source software, a variety a graph databases are
 supported by both the LangChain (LangChain, 2024) and LlamaIndex (LlamaIndex, 2024) libraries,
 while a moregeneral class of graph-based RAG applications is also emerging, including systems that
 can create and reason over knowledge graphs in both Neo4J (NaLLM, Neo4J, 2024) and Nebula
Graph (GraphRAG, NebulaGraph, 2024) formats. Unlike our Graph RAG approach, however, none
 of these systems use the natural modularity of graphs to partition data for global summarization.
 5 Discussion
 Limitations of evaluation approach. Our evaluation to date has only examined a certain class of
 sensemaking questions for two corpora in the region of 1 million tokens. More work is needed
 to understand how performance varies across different ranges of question types, data types, and
 dataset sizes, as well as to validate our sensemaking questions and target metrics with end users.
 Comparison of fabrication rates, e.g., using approaches like SelfCheckGPT (Manakul et al., 2023),
 would also improve on the current analysis.
 Trade-offs of building a graph index. We consistently observed Graph RAG achieve the best",e015335cdcae20e6546fe7cbdef56c1a,['78b4407b9fac94c6e276c9a6a5cd0745'],300
7040ba36a7c09899a355d14a30d65375," the region of 1 million tokens. More work is needed
 to understand how performance varies across different ranges of question types, data types, and
 dataset sizes, as well as to validate our sensemaking questions and target metrics with end users.
 Comparison of fabrication rates, e.g., using approaches like SelfCheckGPT (Manakul et al., 2023),
 would also improve on the current analysis.
 Trade-offs of building a graph index. We consistently observed Graph RAG achieve the best head
to-head results against other methods, but in many cases the graph-free approach to global summa
rization of source texts performed competitively. The real-world decision about whether to invest in
 building a graph index depends on multiple factors, including the compute budget, expected number
 of lifetime queries per dataset, and value obtained from other aspects of the graph index (including
 the generic community summaries and the use of other graph-related RAG approaches).
 Future work. The graph index, rich text annotations, and hierarchical community structure support
ing the current Graph RAG approach offer many possibilities for refinement and adaptation. This
 includes RAG approaches that operate in a more local manner, via embedding-based matching of
 user queries and graph annotations, as well as the possibility of hybrid RAG schemes that combine
 embedding-based matching against community reports before employing our map-reduce summa
rization mechanisms. This ‚Äúroll-up‚Äù operation could also be extended across more levels of the
 community hierarchy",7040ba36a7c09899a355d14a30d65375,['78b4407b9fac94c6e276c9a6a5cd0745'],300
5e2933c9646c751e6a60c9de12a255f2," text annotations, and hierarchical community structure support
ing the current Graph RAG approach offer many possibilities for refinement and adaptation. This
 includes RAG approaches that operate in a more local manner, via embedding-based matching of
 user queries and graph annotations, as well as the possibility of hybrid RAG schemes that combine
 embedding-based matching against community reports before employing our map-reduce summa
rization mechanisms. This ‚Äúroll-up‚Äù operation could also be extended across more levels of the
 community hierarchy, as well as implemented as a more exploratory ‚Äúdrill down‚Äù mechanism that
 follows the information scent contained in higher-level community summaries.
 6 Conclusion
 We have presented a global approach to Graph RAG, combining knowledge graph generation,
 retrieval-augmented generation (RAG), and query-focused summarization (QFS) to support human
 sensemaking over entire text corpora. Initial evaluations show substantial improvements over a
 na¬® ƒ±ve RAG baseline for both the comprehensiveness and diversity of answers, as well as favorable
 comparisons to a global but graph-free approach using map-reduce source text summarization. For
 situations requiring many global queries over the same dataset, summaries of root-level communi
ties in the entity-based graph index provide a data index that is both superior to na¬® ƒ±ve RAG and
 achieves competitive performance to other global methods at a fraction of the token cost.
 An open-source, Python-based implementation of both global and local Graph",5e2933c9646c751e6a60c9de12a255f2,['78b4407b9fac94c6e276c9a6a5cd0745'],300
e31d2d134cf501c93f9445914d7350f9,"iveness and diversity of answers, as well as favorable
 comparisons to a global but graph-free approach using map-reduce source text summarization. For
 situations requiring many global queries over the same dataset, summaries of root-level communi
ties in the entity-based graph index provide a data index that is both superior to na¬® ƒ±ve RAG and
 achieves competitive performance to other global methods at a fraction of the token cost.
 An open-source, Python-based implementation of both global and local Graph RAG approaches is
 forthcoming at https://aka.ms/graphrag.


",e31d2d134cf501c93f9445914d7350f9,['78b4407b9fac94c6e276c9a6a5cd0745'],114
251e8d332b451d900df961cbe215bca0,"GraphRAG Indexing ü§ñ
The GraphRAG indexing package is a data pipeline and transformation suite that is designed to extract meaningful, structured data from unstructured text using LLMs.

Indexing Pipelines are configurable. They are composed of workflows, standard and custom steps, prompt templates, and input/output adapters. Our standard pipeline is designed to:

extract entities, relationships and claims from raw text
perform community detection in entities
generate community summaries and reports at multiple levels of granularity
embed entities into a graph vector space
embed text chunks into a textual vector space
The outputs of the pipeline can be stored in a variety of formats, including JSON and Parquet - or they can be handled manually via the Python API.

Getting Started
Requirements
See the requirements section in Get Started for details on setting up a development environment.

The Indexing Engine can be used in either a default configuration mode or with a custom pipeline. To configure GraphRAG, see the configuration documentation. After you have a config file you can run the pipeline using the CLI or the Python API.

Usage
CLI
# Via Poetry
poetry run poe cli --root <data_root> # default config mode
poetry run poe cli --config your_pipeline.yml # custom config mode

# Via Node
yarn run:index --root <data_root> # default config mode
yarn run:index --config your_pipeline.yml # custom config mode

Python API
from graphrag.index import",251e8d332b451d900df961cbe215bca0,['a75a18ec511e6735c719807c25a3b3d7'],300
b0505e11596cadd9890fef049c29473c," you have a config file you can run the pipeline using the CLI or the Python API.

Usage
CLI
# Via Poetry
poetry run poe cli --root <data_root> # default config mode
poetry run poe cli --config your_pipeline.yml # custom config mode

# Via Node
yarn run:index --root <data_root> # default config mode
yarn run:index --config your_pipeline.yml # custom config mode

Python API
from graphrag.index import run_pipeline
from graphrag.index.config import PipelineWorkflowReference

workflows: list[PipelineWorkflowReference] = [
    PipelineWorkflowReference(
        steps=[
            {
                # built-in verb
                ""verb"": ""derive"",  # https://github.com/microsoft/datashaper/blob/main/python/datashaper/datashaper/engine/verbs/derive.py
                ""args"": {
                    ""column1"": ""col1"",  # from above
                    ""column2"": ""col2"",  # from above
                    ""to"": ""col_multiplied"",  # new column name
                    ""operator"": ""*"",  # multiply the two columns
                },
                # Since we're trying to act on the default input, we don't need explicitly to specify an input
            }
        ]
    ),
]

dataset = pd.DataFrame([{""col1"": 2, ""col2"": 4}, {""col1"": 5, ""col2"": 10}])
",b0505e11596cadd9890fef049c29473c,['a75a18ec511e6735c719807c25a3b3d7'],300
f3a07680cbe8ab1f6055369da05f4f38,"2"",  # from above
                    ""to"": ""col_multiplied"",  # new column name
                    ""operator"": ""*"",  # multiply the two columns
                },
                # Since we're trying to act on the default input, we don't need explicitly to specify an input
            }
        ]
    ),
]

dataset = pd.DataFrame([{""col1"": 2, ""col2"": 4}, {""col1"": 5, ""col2"": 10}])
outputs = []
async for output in await run_pipeline(dataset=dataset, workflows=workflows):
    outputs.append(output)
pipeline_result = outputs[-1]
print(pipeline_result)

Further Reading
To start developing within the GraphRAG project, see getting started
To understand the underlying concepts and execution model of the indexing library, see the architecture documentation
To get running with a series of examples, see the examples documentation
To read more about configuring the indexing engine, see the configuration documentation",f3a07680cbe8ab1f6055369da05f4f38,['a75a18ec511e6735c719807c25a3b3d7'],198
bdb8f9e797229f596744d9636ab857b0,"Prompt Tuning ‚öôÔ∏è
This page provides an overview of the prompt tuning options available for the GraphRAG indexing engine.

Default Prompts
The default prompts are the simplest way to get started with the GraphRAG system. It is designed to work out-of-the-box with minimal configuration. You can find more detail about these prompts in the following links:

Entity/Relationship Extraction
Entity/Relationship Description Summarization
Claim Extraction
Community Reports
Auto Templating
Auto Templating leverages your input data and LLM interactions to create domain adaptive templates for the generation of the knowledge graph. It is highly encouraged to run it as it will yield better results when executing an Index Run. For more details about how to use it, please refer to the Auto Templating documentation.

Manual Configuration
Manual configuration is an advanced use-case. Most users will want to use the Auto Templating feature instead. Details about how to use manual configuration are available in the Manual Prompt Configuration documentation.

Prompt Tuning ‚öôÔ∏è
GraphRAG provides the ability to create domain adaptive templates for the generation of the knowledge graph. This step is optional, though it is highly encouraged to run it as it will yield better results when executing an Index Run.

The templates are generated by loading the inputs, splitting them into chunks (text units) and then running a series of LLM invocations and template substitutions to generate the final prompts. We suggest using the default values provided by the script, but in",bdb8f9e797229f596744d9636ab857b0,['a9acbe32a842fc986348d830aeecab88'],300
9b364093aeecfc789c70fc5bd9503487," Tuning ‚öôÔ∏è
GraphRAG provides the ability to create domain adaptive templates for the generation of the knowledge graph. This step is optional, though it is highly encouraged to run it as it will yield better results when executing an Index Run.

The templates are generated by loading the inputs, splitting them into chunks (text units) and then running a series of LLM invocations and template substitutions to generate the final prompts. We suggest using the default values provided by the script, but in this page you'll find the detail of each in case you want to further explore and tweak the template generation algorithm.

Prerequisites
Before running the automatic template generation make sure you have already initialized your workspace with the graphrag.index --init command. This will create the necessary configuration files and the default prompts. Refer to the Init Documentation for more information about the initialization process.

Usage
You can run the main script from the command line with various options:

python -m graphrag.prompt_tune [--root ROOT] [--domain DOMAIN]  [--method METHOD] [--limit LIMIT] [--language LANGUAGE] [--max-tokens MAX_TOKENS] [--chunk-size CHUNK_SIZE] [--no-entity-types] [--output OUTPUT]

Command-Line Options
--root (optional): The data project root directory, including the config files (YML, JSON, or .env). Defaults to the current directory.

--domain (optional): The domain related to your input data, such as 'space science', 'microbiology',",9b364093aeecfc789c70fc5bd9503487,['a9acbe32a842fc986348d830aeecab88'],300
9243633f55cccd0885ba553e14fa5e3f," ROOT] [--domain DOMAIN]  [--method METHOD] [--limit LIMIT] [--language LANGUAGE] [--max-tokens MAX_TOKENS] [--chunk-size CHUNK_SIZE] [--no-entity-types] [--output OUTPUT]

Command-Line Options
--root (optional): The data project root directory, including the config files (YML, JSON, or .env). Defaults to the current directory.

--domain (optional): The domain related to your input data, such as 'space science', 'microbiology', or 'environmental news'. If left empty, the domain will be inferred from the input data.

--method (optional): The method to select documents. Options are all, random, or top. Default is random.

--limit (optional): The limit of text units to load when using random or top selection. Default is 15.

--language (optional): The language to use for input processing. If it is different from the inputs' language, the LLM will translate. Default is """" meaning it will be automatically detected from the inputs.

--max-tokens (optional): Maximum token count for prompt generation. Default is 2000.

--chunk-size (optional): The size in tokens to use for generating text units from input documents. Default is 200.

--no-entity-types (optional): Use untyped entity extraction generation. We recommend using this when your data covers a lot of topics or it is highly randomized.

--output (optional): The folder to save the generated prompts.",9243633f55cccd0885ba553e14fa5e3f,['a9acbe32a842fc986348d830aeecab88'],300
ce9cc3ed2e5f890d02e867ed0b0f8ff9," meaning it will be automatically detected from the inputs.

--max-tokens (optional): Maximum token count for prompt generation. Default is 2000.

--chunk-size (optional): The size in tokens to use for generating text units from input documents. Default is 200.

--no-entity-types (optional): Use untyped entity extraction generation. We recommend using this when your data covers a lot of topics or it is highly randomized.

--output (optional): The folder to save the generated prompts. Default is ""prompts"".

Example Usage
python -m graphrag.prompt_tune --root /path/to/project --domain ""environmental news"" --method random --limit 10 --language English --max-tokens 2048 --chunk-size 256 --no-entity-types --output /path/to/output

or, with minimal configuration (suggested):

python -m graphrag.prompt_tune --root /path/to/project --no-entity-types

Document Selection Methods
The auto template feature ingests the input data and then divides it into text units the size of the chunk size parameter. After that, it uses one of the following selection methods to pick a sample to work with for template generation:

random: Select text units randomly. This is the default and recommended option.
top: Select the head n text units.
all: Use all text units for the generation. Use only with small datasets; this option is not usually recommended.
Modify Env Vars
After running auto-templating,",ce9cc3ed2e5f890d02e867ed0b0f8ff9,['a9acbe32a842fc986348d830aeecab88'],300
4f37c0e9c3c9bac4e5c1c6821eea442e,"ests the input data and then divides it into text units the size of the chunk size parameter. After that, it uses one of the following selection methods to pick a sample to work with for template generation:

random: Select text units randomly. This is the default and recommended option.
top: Select the head n text units.
all: Use all text units for the generation. Use only with small datasets; this option is not usually recommended.
Modify Env Vars
After running auto-templating, you should modify the following environment variables (or config variables) to pick up the new prompts on your index run. Note: Please make sure to update the correct path to the generated prompts, in this example we are using the default ""prompts"" path.

GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE = ""prompts/entity_extraction.txt""

GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE = ""prompts/community_report.txt""

GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE = ""prompts/summarize_descriptions.txt""

Prompt Tuning‚öôÔ∏è
The GraphRAG indexer, by default, will run with a handful of prompts that are designed to work well in the broad context of knowledge discovery. However, it is quite common to want to tune the prompts to better suit your specific use case. We provide a means for you to do this by allowing you to specify a custom prompt file, which will each use a series of token-replacements internally",4f37c0e9c3c9bac4e5c1c6821eea442e,['a9acbe32a842fc986348d830aeecab88'],300
6a7157695d90d434b2625c3f05420916," = ""prompts/summarize_descriptions.txt""

Prompt Tuning‚öôÔ∏è
The GraphRAG indexer, by default, will run with a handful of prompts that are designed to work well in the broad context of knowledge discovery. However, it is quite common to want to tune the prompts to better suit your specific use case. We provide a means for you to do this by allowing you to specify a custom prompt file, which will each use a series of token-replacements internally.

Each of these prompts may be overridden by writing a custom prompt file in plaintext. We use token-replacements in the form of {token_name}, and the descriptions for the available tokens can be found below.

Entity/Relationship Extraction
Prompt Source

Tokens (values provided by extractor)
{input_text} - The input text to be processed.
{entity_types} - A list of entity types
{tuple_delimiter} - A delimiter for separating values within a tuple. A single tuple is used to represent an individual entity or relationship.
{record_delimiter} - A delimiter for separating tuple instances.
{completion_delimiter} - An indicator for when generation is complete.
Summarize Entity/Relationship Descriptions
Prompt Source

Tokens (values provided by extractor)
{entity_name} - The name of the entity or the source/target pair of the relationship.
{description_list} - A list of descriptions for the entity or relationship.
Claim Extraction
Prompt Source

Tokens (values provided by extractor",6a7157695d90d434b2625c3f05420916,['a9acbe32a842fc986348d830aeecab88'],300
853bfe9a74a916130a20f81506bcaf09," to represent an individual entity or relationship.
{record_delimiter} - A delimiter for separating tuple instances.
{completion_delimiter} - An indicator for when generation is complete.
Summarize Entity/Relationship Descriptions
Prompt Source

Tokens (values provided by extractor)
{entity_name} - The name of the entity or the source/target pair of the relationship.
{description_list} - A list of descriptions for the entity or relationship.
Claim Extraction
Prompt Source

Tokens (values provided by extractor)
{input_text} - The input text to be processed.
{tuple_delimiter} - A delimiter for separating values within a tuple. A single tuple is used to represent an individual entity or relationship.
{record_delimiter} - A delimiter for separating tuple instances.
{completion_delimiter} - An indicator for when generation is complete.
Note: there is additional paramater for the Claim Description that is used in claim extraction. The default value is

""Any claims or facts that could be relevant to information discovery.""

See the configuration documentation for details on how to change this.

Generate Community Reports
Prompt Source

Tokens (values provided by extractor)
{input_text} - The input text to generate the report with. This will contain tables of entities and relationships.",853bfe9a74a916130a20f81506bcaf09,['a9acbe32a842fc986348d830aeecab88'],253
21cdf11c58927ae505d3d375d1b75c82," to information discovery.""

See the configuration documentation for details on how to change this.

Generate Community Reports
Prompt Source

Tokens (values provided by extractor)
{input_text} - The input text to generate the report with. This will contain tables of entities and relationships.",21cdf11c58927ae505d3d375d1b75c82,['a9acbe32a842fc986348d830aeecab88'],53
84d24b5db902baca7217b5e3bb6ec462,"Get Started
Requirements
Python 3.10-3.12

To get started with the GraphRAG system, you have a few options:

üëâ Use the GraphRAG Accelerator solution
üëâ Install from pypi.
üëâ Use it from source

Quickstart
To get started with the GraphRAG system we recommend trying the Solution Accelerator package. This provides a user-friendly end-to-end experience with Azure resources.

Top-Level Modules
Indexing Pipeline Overview
Query Engine Overview

Overview
The following is a simple end-to-end example for using the GraphRAG system. It shows how to use the system to index some text, and then use the indexed data to answer questions about the documents.

Install GraphRAG
pip install graphrag

Running the Indexer
Now we need to set up a data project and some initial configuration. Let's set that up. We're using the default configuration mode, which you can customize as needed using a config file, which we recommend, or environment variables.

First let's get a sample dataset ready:

mkdir -p ./ragtest/input

Now let's get a copy of A Christmas Carol by Charles Dickens from a trusted source

curl https://www.gutenberg.org/cache/epub/24022/pg24022.txt > ./ragtest/input/book.txt

Next we'll inject some required config variables:

Set Up Your Workspace Variables
First let's make sure to setup the required environment variables.",84d24b5db902baca7217b5e3bb6ec462,['b60327cd0428bd92d6c875c7541e3c6e'],300
9f2cd3d789fd49f220d4cda6b9e8048c," a config file, which we recommend, or environment variables.

First let's get a sample dataset ready:

mkdir -p ./ragtest/input

Now let's get a copy of A Christmas Carol by Charles Dickens from a trusted source

curl https://www.gutenberg.org/cache/epub/24022/pg24022.txt > ./ragtest/input/book.txt

Next we'll inject some required config variables:

Set Up Your Workspace Variables
First let's make sure to setup the required environment variables. For details on these environment variables, and what environment variables are available, see the variables documentation.

To initialize your workspace, let's first run the graphrag.index --init command. Since we have already configured a directory named .ragtest` in the previous step, we can run the following command:

python -m graphrag.index --init --root ./ragtest

This will create two files: .env and settings.yaml in the ./ragtest directory.

.env contains the environment variables required to run the GraphRAG pipeline. If you inspect the file, you'll see a single environment variable defined, GRAPHRAG_API_KEY=<API_KEY>. This is the API key for the OpenAI API or Azure OpenAI endpoint. You can replace this with your own API key.
settings.yaml contains the settings for the pipeline. You can modify this file to change the settings for the pipeline.
OpenAI and Azure OpenAI
To run in OpenAI mode, just make sure to update the value",9f2cd3d789fd49f220d4cda6b9e8048c,['b60327cd0428bd92d6c875c7541e3c6e'],300
5aaa26fbe97dc7573cd1a56d6fb11213," the GraphRAG pipeline. If you inspect the file, you'll see a single environment variable defined, GRAPHRAG_API_KEY=<API_KEY>. This is the API key for the OpenAI API or Azure OpenAI endpoint. You can replace this with your own API key.
settings.yaml contains the settings for the pipeline. You can modify this file to change the settings for the pipeline.
OpenAI and Azure OpenAI
To run in OpenAI mode, just make sure to update the value of GRAPHRAG_API_KEY in the .env file with your OpenAI API key.

Azure OpenAI
In addition, Azure OpenAI users should set the following variables in the settings.yaml file. To find the appropriate sections, just search for the llm: configuration, you should see two sections, one for the chat endpoint and one for the embeddings endpoint. Here is an example of how to configure the chat endpoint:

type: azure_openai_chat # Or azure_openai_embedding for embeddings
api_base: https://<instance>.openai.azure.com
api_version: 2024-02-15-preview # You can customize this for other versions
deployment_name: <azure_model_deployment_name>

For more details about configuring GraphRAG, see the configuration documentation.
To learn more about Initialization, refer to the Initialization documentation.
For more details about using the CLI, refer to the CLI documentation.
Running the Indexing pipeline
Finally we'll run the pipeline!

python -m",5aaa26fbe97dc7573cd1a56d6fb11213,['b60327cd0428bd92d6c875c7541e3c6e'],300
7c1bad237a1ef86cb41b6c5dbad4ffc3,"
api_base: https://<instance>.openai.azure.com
api_version: 2024-02-15-preview # You can customize this for other versions
deployment_name: <azure_model_deployment_name>

For more details about configuring GraphRAG, see the configuration documentation.
To learn more about Initialization, refer to the Initialization documentation.
For more details about using the CLI, refer to the CLI documentation.
Running the Indexing pipeline
Finally we'll run the pipeline!

python -m graphrag.index --root ./ragtest

pipeline executing from the CLI

This process will take some time to run. This depends on the size of your input data, what model you're using, and the text chunk size being used (these can be configured in your .env file). Once the pipeline is complete, you should see a new folder called ./ragtest/output/<timestamp>/artifacts with a series of parquet files.

Using the Query Engine
Running the Query Engine
Now let's ask some questions using this dataset.

Here is an example using Global search to ask a high-level question:

python -m graphrag.query \
--root ./ragtest \
--method global \
""What are the top themes in this story?""

Here is an example using Local search to ask a more specific question about a particular character:

python -m graphrag.query \
--root ./ragtest \
--method local \
""Who is Scrooge, and what are his main relationships?""

Please refer to",7c1bad237a1ef86cb41b6c5dbad4ffc3,['b60327cd0428bd92d6c875c7541e3c6e'],300
ae6e91a8cc5773dbd4789773c9ef5a30,"'s ask some questions using this dataset.

Here is an example using Global search to ask a high-level question:

python -m graphrag.query \
--root ./ragtest \
--method global \
""What are the top themes in this story?""

Here is an example using Local search to ask a more specific question about a particular character:

python -m graphrag.query \
--root ./ragtest \
--method local \
""Who is Scrooge, and what are his main relationships?""

Please refer to Query Engine docs for detailed information about how to leverage our Local and Global search mechanisms for extracting meaningful insights from data after the Indexer has wrapped up execution.",ae6e91a8cc5773dbd4789773c9ef5a30,['b60327cd0428bd92d6c875c7541e3c6e'],131
ccd2de9e2219521fbca779843c65af58,"Configuring GraphRAG Indexing
The GraphRAG system is highly configurable. This page provides an overview of the configuration options available for the GraphRAG indexing engine.

Default Configuration Mode
The default configuration mode is the simplest way to get started with the GraphRAG system. It is designed to work out-of-the-box with minimal configuration. The primary configuration sections for the Indexing Engine pipelines are described below. The main ways to set up GraphRAG in Default Configuration mode are via:

Init command (recommended)
Purely using environment variables
Using JSON or YAML for deeper control
Custom Configuration Mode
Custom configuration mode is an advanced use-case. Most users will want to use the Default Configuration instead. The primary configuration sections for Indexing Engine pipelines are described below. Details about how to use custom configuration are available in the Custom Configuration Mode documentation.

Configuring GraphRAG Indexing
To start using GraphRAG, you need to configure the system. The init command is the easiest way to get started. It will create a .env and settings.yaml files in the specified directory with the necessary configuration settings. It will also output the default LLM prompts used by GraphRAG.

Usage
python -m graphrag.index [--init] [--root PATH]

Options
--init - Initialize the directory with the necessary configuration files.
--root PATH - The root directory to initialize. Default is the current directory.
Example
python -m graphrag.index --init --root ./ragtest",ccd2de9e2219521fbca779843c65af58,['bbe409e13dd4d7bbd284976287a4feba'],300
12294feb07a1d202b27241eaaf64718b," way to get started. It will create a .env and settings.yaml files in the specified directory with the necessary configuration settings. It will also output the default LLM prompts used by GraphRAG.

Usage
python -m graphrag.index [--init] [--root PATH]

Options
--init - Initialize the directory with the necessary configuration files.
--root PATH - The root directory to initialize. Default is the current directory.
Example
python -m graphrag.index --init --root ./ragtest

Output
The init command will create the following files in the specified directory:

settings.yaml - The configuration settings file. This file contains the configuration settings for GraphRAG.
.env - The environment variables file. These are referenced in the settings.yaml file.
prompts/ - The LLM prompts folder. This contains the default prompts used by GraphRAG, you can modify them or run the Auto Prompt Tuning command to generate new prompts adapted to your data.
Next Steps
After initializing your workspace, you can either run the Prompt Tuning command to adapt the prompts to your data or even start running the Indexing Pipeline to index your data. For more information on configuring GraphRAG, see the Configuration documentation.


Configuring GraphRAG Indexing
To start using GraphRAG, you need to configure the system. The init command is the easiest way to get started. It will create a .env and settings.yaml files in the specified directory with the necessary configuration settings. It will also",12294feb07a1d202b27241eaaf64718b,['bbe409e13dd4d7bbd284976287a4feba'],300
d0f7c236538005bc3056b7daed2401d8,", you can either run the Prompt Tuning command to adapt the prompts to your data or even start running the Indexing Pipeline to index your data. For more information on configuring GraphRAG, see the Configuration documentation.


Configuring GraphRAG Indexing
To start using GraphRAG, you need to configure the system. The init command is the easiest way to get started. It will create a .env and settings.yaml files in the specified directory with the necessary configuration settings. It will also output the default LLM prompts used by GraphRAG.

Usage
python -m graphrag.index [--init] [--root PATH]

Options
--init - Initialize the directory with the necessary configuration files.
--root PATH - The root directory to initialize. Default is the current directory.
Example
python -m graphrag.index --init --root ./ragtest

Output
The init command will create the following files in the specified directory:

settings.yaml - The configuration settings file. This file contains the configuration settings for GraphRAG.
.env - The environment variables file. These are referenced in the settings.yaml file.
prompts/ - The LLM prompts folder. This contains the default prompts used by GraphRAG, you can modify them or run the Auto Prompt Tuning command to generate new prompts adapted to your data.
Next Steps
After initializing your workspace, you can either run the Prompt Tuning command to adapt the prompts to your data or even start running the Indexing Pipeline to index your",d0f7c236538005bc3056b7daed2401d8,['bbe409e13dd4d7bbd284976287a4feba'],300
32e96c66a531ecd0a8edc7414aec0803," configuration settings for GraphRAG.
.env - The environment variables file. These are referenced in the settings.yaml file.
prompts/ - The LLM prompts folder. This contains the default prompts used by GraphRAG, you can modify them or run the Auto Prompt Tuning command to generate new prompts adapted to your data.
Next Steps
After initializing your workspace, you can either run the Prompt Tuning command to adapt the prompts to your data or even start running the Indexing Pipeline to index your data. For more information on configuring GraphRAG, see the Configuration documentation.

Default Configuration Mode (using JSON/YAML)
The default configuration mode may be configured by using a config.json or config.yml file in the data project root. If a .env file is present along with this config file, then it will be loaded, and the environment variables defined therein will be available for token replacements in your configuration document using ${ENV_VAR} syntax.

For example:

# .env
API_KEY=some_api_key

# config.json
{
    ""llm"": {
        ""api_key"": ""${API_KEY}""
    }
}
Config Sections
input
Fields
type file|blob - The input type to use. Default=file
file_type text|csv - The type of input data to load. Either text or csv. Default is text
file_encoding str - The encoding of the input file. Default is utf-8
file_pattern str - A regex to match input files. Default is",32e96c66a531ecd0a8edc7414aec0803,['bbe409e13dd4d7bbd284976287a4feba'],300
f135654a3c057c66b9e5f97a960d302f,"_api_key

# config.json
{
    ""llm"": {
        ""api_key"": ""${API_KEY}""
    }
}
Config Sections
input
Fields
type file|blob - The input type to use. Default=file
file_type text|csv - The type of input data to load. Either text or csv. Default is text
file_encoding str - The encoding of the input file. Default is utf-8
file_pattern str - A regex to match input files. Default is .*\.csv$ if in csv mode and .*\.txt$ if in text mode.
source_column str - (CSV Mode Only) The source column name.
timestamp_column str - (CSV Mode Only) The timestamp column name.
timestamp_format str - (CSV Mode Only) The source format.
text_column str - (CSV Mode Only) The text column name.
title_column str - (CSV Mode Only) The title column name.
document_attribute_columns list[str] - (CSV Mode Only) The additional document attributes to include.
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to read input from, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
llm
This is the base LLM configuration section. Other steps may override this configuration with their own LLM configuration.

Fields
api_key str - The OpenAI API",f135654a3c057c66b9e5f97a960d302f,['bbe409e13dd4d7bbd284976287a4feba'],300
647be47c939b4d72f1c0b29a2e0d2cb2," additional document attributes to include.
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to read input from, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
llm
This is the base LLM configuration section. Other steps may override this configuration with their own LLM configuration.

Fields
api_key str - The OpenAI API key to use.
type openai_chat|azure_openai_chat|openai_embedding|azure_openai_embedding - The type of LLM to use.
model str - The model name.
max_tokens int - The maximum number of output tokens.
request_timeout float - The per-request timeout.
api_base str - The API base url to use.
api_version str - The API version
organization str - The client organization.
proxy str - The proxy URL to use.
cognitive_services_endpoint str - The url endpoint for cognitive services.
deployment_name str - The deployment name to use (Azure).
model_supports_json bool - Whether the model supports JSON-mode output.
tokens_per_minute int - Set a leaky-bucket throttle on tokens-per-minute.
requests_per_minute int - Set a leaky-bucket throttle on requests-per-minute.
max_retries int - The maximum number of retries to use.
max_retry_wait float - The maximum backoff time.
sleep_on_rate_limit_recommendation bool - Whether to",647be47c939b4d72f1c0b29a2e0d2cb2,['bbe409e13dd4d7bbd284976287a4feba'],300
3c66b7e86b3675fce14fe0047ae731aa," url endpoint for cognitive services.
deployment_name str - The deployment name to use (Azure).
model_supports_json bool - Whether the model supports JSON-mode output.
tokens_per_minute int - Set a leaky-bucket throttle on tokens-per-minute.
requests_per_minute int - Set a leaky-bucket throttle on requests-per-minute.
max_retries int - The maximum number of retries to use.
max_retry_wait float - The maximum backoff time.
sleep_on_rate_limit_recommendation bool - Whether to adhere to sleep recommendations (Azure).
concurrent_requests int The number of open requests to allow at once.
temperature float - The temperature to use.
top_p float - The top-p value to use.
n int - The number of completions to generate.
parallelization
Fields
stagger float - The threading stagger value.
num_threads int - The maximum number of work threads.
async_mode
asyncio|threaded The async mode to use. Either asyncio or `threaded.

embeddings
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
batch_size int - The maximum batch size to use.
batch_max_tokens int - The maximum batch #-tokens.
target required|all - Determines which set of embeddings to emit.
skip list[str] - Which embeddings to skip.
strategy dict - Fully override the text-embedding strategy.
chunks
Fields
size int - The max chunk size in",3c66b7e86b3675fce14fe0047ae731aa,['bbe409e13dd4d7bbd284976287a4feba'],300
d27237468a1b9e89110eeeca8080f63c,"Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
batch_size int - The maximum batch size to use.
batch_max_tokens int - The maximum batch #-tokens.
target required|all - Determines which set of embeddings to emit.
skip list[str] - Which embeddings to skip.
strategy dict - Fully override the text-embedding strategy.
chunks
Fields
size int - The max chunk size in tokens.
overlap int - The chunk overlap in tokens.
group_by_columns list[str] - group documents by fields before chunking.
strategy dict - Fully override the chunking strategy.
cache
Fields
type file|memory|none|blob - The cache type to use. Default=file
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to write cache to, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
storage
Fields
type file|memory|blob - The storage type to use. Default=file
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to write reports to, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
reporting
Fields",d27237468a1b9e89110eeeca8080f63c,['bbe409e13dd4d7bbd284976287a4feba'],300
abac77a5673e907cf8d65161c2612784," the root.
storage_account_blob_url str - The storage account blob URL to use.
storage
Fields
type file|memory|blob - The storage type to use. Default=file
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to write reports to, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
reporting
Fields
type file|console|blob - The reporting type to use. Default=file
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to write reports to, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
entity_extraction
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
entity_types list[str] - The entity types to identify.
max_gleanings int - The maximum number of gleaning cycles to use.
strategy dict - Fully override the entity extraction strategy.
summarize_descriptions
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The",abac77a5673e907cf8d65161c2612784,['bbe409e13dd4d7bbd284976287a4feba'],300
9cbd4e21339eeed5e22a638e52a094cb,")
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
entity_types list[str] - The entity types to identify.
max_gleanings int - The maximum number of gleaning cycles to use.
strategy dict - Fully override the entity extraction strategy.
summarize_descriptions
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
max_length int - The maximum number of output tokens per summarization.
strategy dict - Fully override the summarize description strategy.
claim_extraction
Fields
enabled bool - Whether to enable claim extraction. default=False
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
description str - Describes the types of claims we want to extract.
max_gleanings int - The maximum number of gleaning cycles to use.
strategy dict - Fully override the claim extraction strategy.
community_reports
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
max_length int - The maximum number of output tokens per report.
max_input_length int - The maximum number of input tokens to use when generating reports.
strategy",9cbd4e21339eeed5e22a638e52a094cb,['bbe409e13dd4d7bbd284976287a4feba'],300
53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,"_gleanings int - The maximum number of gleaning cycles to use.
strategy dict - Fully override the claim extraction strategy.
community_reports
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
max_length int - The maximum number of output tokens per report.
max_input_length int - The maximum number of input tokens to use when generating reports.
strategy dict - Fully override the community reports strategy.
cluster_graph
Fields
max_cluster_size int - The maximum cluster size to emit.
strategy dict - Fully override the cluster_graph strategy.
embed_graph
Fields
enabled bool - Whether to enable graph embeddings.
num_walks int - The node2vec number of walks.
walk_length int - The node2vec walk length.
window_size int - The node2vec window size.
iterations int - The node2vec number of iterations.
random_seed int - The node2vec random seed.
strategy dict - Fully override the embed graph strategy.
umap
Fields
enabled bool - Whether to enable UMAP layouts.
snapshots
Fields
graphml bool - Emit graphml snapshots.
raw_entities bool - Emit raw entity snapshots.
top_level_nodes bool - Emit top-level-node snapshots.
encoding_model
str - The text encoding model to use. Default is cl100k_base.

skip_workflows
list[str] - Which workflow names to skip.

Custom",53f1b5ad1d2f4ee5dfb53a8f1ff3ec14,['bbe409e13dd4d7bbd284976287a4feba'],300
b70cb2eda62c6afad9e8d22daafe61cc," The node2vec random seed.
strategy dict - Fully override the embed graph strategy.
umap
Fields
enabled bool - Whether to enable UMAP layouts.
snapshots
Fields
graphml bool - Emit graphml snapshots.
raw_entities bool - Emit raw entity snapshots.
top_level_nodes bool - Emit top-level-node snapshots.
encoding_model
str - The text encoding model to use. Default is cl100k_base.

skip_workflows
list[str] - Which workflow names to skip.

Custom Configuration Mode
The primary configuration sections for Indexing Engine pipelines are described below. Each configuration section can be expressed in Python (for use in Python API mode) as well as YAML, but YAML is show here for brevity.

Using custom configuration is an advanced use-case. Most users will want to use the Default Configuration instead.

Indexing Engine Examples
The examples directory contains several examples of how to use the indexing engine with custom configuration.

Most examples include two different forms of running the pipeline, both are contained in the examples run.py

Using mostly the Python API
Using mostly the a pipeline configuration file
To run an example:

Run poetry shell to activate a virtual environment with the required dependencies.
Run PYTHONPATH=""$(pwd)"" python examples/path_to_example/run.py from the root directory.
For example to run the single_verb example, you would run the following commands:

poetry shell

PYTHONPATH=""$(pwd)"" python examples/single_verb/run.py

Configuration Sections
> extends
This",b70cb2eda62c6afad9e8d22daafe61cc,['bbe409e13dd4d7bbd284976287a4feba'],300
e01c546120a27319dcbdf7a6b89bab26," both are contained in the examples run.py

Using mostly the Python API
Using mostly the a pipeline configuration file
To run an example:

Run poetry shell to activate a virtual environment with the required dependencies.
Run PYTHONPATH=""$(pwd)"" python examples/path_to_example/run.py from the root directory.
For example to run the single_verb example, you would run the following commands:

poetry shell

PYTHONPATH=""$(pwd)"" python examples/single_verb/run.py

Configuration Sections
> extends
This configuration allows you to extend a base configuration file or files.

# single base
extends: ../base_config.yml

# multiple bases
extends:
  - ../base_config.yml
  - ../base_config2.yml

> root_dir
This configuration allows you to set the root directory for the pipeline. All data inputs and outputs are assumed to be relative to this path.

root_dir: /workspace/data_project

> storage
This configuration allows you define the output strategy for the pipeline.

type: The type of storage to use. Options are file, memory, and blob
base_dir (type: file only): The base directory to store the data in. This is relative to the config root.
connection_string (type: blob only): The connection string to use for blob storage.
container_name (type: blob only): The container to use for blob storage.
> cache
This configuration allows you define the cache strategy for the pipeline.

type: The type of cache to use. Options are",e01c546120a27319dcbdf7a6b89bab26,['bbe409e13dd4d7bbd284976287a4feba'],300
763b51b68ecc9b69bc8014cf6f59fd33," The type of storage to use. Options are file, memory, and blob
base_dir (type: file only): The base directory to store the data in. This is relative to the config root.
connection_string (type: blob only): The connection string to use for blob storage.
container_name (type: blob only): The container to use for blob storage.
> cache
This configuration allows you define the cache strategy for the pipeline.

type: The type of cache to use. Options are file and memory, and blob.
base_dir (type: file only): The base directory to store the cache in. This is relative to the config root.
connection_string (type: blob only): The connection string to use for blob storage.
container_name (type: blob only): The container to use for blob storage.
> reporting
This configuration allows you define the reporting strategy for the pipeline. Report files are generated artifacts that summarize the performance metrics of the pipeline and emit any error messages.

type: The type of reporting to use. Options are file, memory, and blob
base_dir (type: file only): The base directory to store the reports in. This is relative to the config root.
connection_string (type: blob only): The connection string to use for blob storage.
container_name (type: blob only): The container to use for blob storage.
> workflows
This configuration section defines the workflow DAG for the pipeline. Here we define an array of workflows and express their inter",763b51b68ecc9b69bc8014cf6f59fd33,['bbe409e13dd4d7bbd284976287a4feba'],300
a3e5bacdf64bcaf080a04c7dd8218484,": The type of reporting to use. Options are file, memory, and blob
base_dir (type: file only): The base directory to store the reports in. This is relative to the config root.
connection_string (type: blob only): The connection string to use for blob storage.
container_name (type: blob only): The container to use for blob storage.
> workflows
This configuration section defines the workflow DAG for the pipeline. Here we define an array of workflows and express their inter-dependencies in steps:

name: The name of the workflow. This is used to reference the workflow in other parts of the config.
steps: The DataShaper steps that this workflow comprises. If a step defines an input in the form of workflow:<workflow_name>, then it is assumed to have a dependency on the output of that workflow.
workflows:
  - name: workflow1
    steps:
      - verb: derive
        args:
          column1: ""col1""
          column2: ""col2""
  - name: workflow2
    steps:
      - verb: derive
        args:
          column1: ""col1""
          column2: ""col2""
        input:
          # dependency established here
          source: workflow:workflow1

> input
type: The type of input to use. Options are file or blob.
file_type: The file type field discriminates between the different input types. Options are csv and text.
base_dir",a3e5bacdf64bcaf080a04c7dd8218484,['bbe409e13dd4d7bbd284976287a4feba'],300
76d9dcb9a27c2caea1f46bb5050851c6," column2: ""col2""
  - name: workflow2
    steps:
      - verb: derive
        args:
          column1: ""col1""
          column2: ""col2""
        input:
          # dependency established here
          source: workflow:workflow1

> input
type: The type of input to use. Options are file or blob.
file_type: The file type field discriminates between the different input types. Options are csv and text.
base_dir: The base directory to read the input files from. This is relative to the config file.
file_pattern: A regex to match the input files. The regex must have named groups for each of the fields in the file_filter.
post_process: A DataShaper workflow definition to apply to the input before executing the primary workflow.
source_column (type: csv only): The column containing the source/author of the data
text_column (type: csv only): The column containing the text of the data
timestamp_column (type: csv only): The column containing the timestamp of the data
timestamp_format (type: csv only): The format of the timestamp
input:
  type: file
  file_type: csv
  base_dir: ../data/csv # the directory containing the CSV files, this is relative to the config file
  file_pattern: '.*[\/](?P<source>[^\/]+)[\/](?P<year>\d{4})-(?P",76d9dcb9a27c2caea1f46bb5050851c6,['bbe409e13dd4d7bbd284976287a4feba'],300
3900b87693f02c43b4294e38647eb7cd," data
timestamp_column (type: csv only): The column containing the timestamp of the data
timestamp_format (type: csv only): The format of the timestamp
input:
  type: file
  file_type: csv
  base_dir: ../data/csv # the directory containing the CSV files, this is relative to the config file
  file_pattern: '.*[\/](?P<source>[^\/]+)[\/](?P<year>\d{4})-(?P<month>\d{2})-(?P<day>\d{2})_(?P<author>[^_]+)_\d+\.csv$' # a regex to match the CSV files
  # An additional file filter which uses the named groups from the file_pattern to further filter the files
  # file_filter:
  #   # source: (source_filter)
  #   year: (2023)
  #   month: (06)
  #   # day: (22)
  source_column: ""author"" # the column containing the source/author of the data
  text_column: ""message"" # the column containing the text of the data
  timestamp_column: ""date(yyyyMMddHHmmss)"" # optional, the column containing the timestamp of the data
  timestamp_format: ""%Y%m%d%H%M%S"" # optional,  the format of the timestamp
  post_process: # Optional, set of steps to process the",3900b87693f02c43b4294e38647eb7cd,['bbe409e13dd4d7bbd284976287a4feba'],300
6839baed839d7a5e837af1da93e462e5," day: (22)
  source_column: ""author"" # the column containing the source/author of the data
  text_column: ""message"" # the column containing the text of the data
  timestamp_column: ""date(yyyyMMddHHmmss)"" # optional, the column containing the timestamp of the data
  timestamp_format: ""%Y%m%d%H%M%S"" # optional,  the format of the timestamp
  post_process: # Optional, set of steps to process the data before going into the workflow
    - verb: filter
      args:
        column: ""title"",
        value: ""My document""

input:
  type: file
  file_type: csv
  base_dir: ../data/csv # the directory containing the CSV files, this is relative to the config file
  file_pattern: '.*[\/](?P<source>[^\/]+)[\/](?P<year>\d{4})-(?P<month>\d{2})-(?P<day>\d{2})_(?P<author>[^_]+)_\d+\.csv$' # a regex to match the CSV files
  # An additional file filter which uses the named groups from the file_pattern to further filter the files
  # file_filter:
  #   # source: (source_filter)
  #   year: (2023)
  #   month: (06)
  #   # day: (22",6839baed839d7a5e837af1da93e462e5,['bbe409e13dd4d7bbd284976287a4feba'],300
765d8a78606fe81a03a0da4f7ff231fa,"{2})-(?P<day>\d{2})_(?P<author>[^_]+)_\d+\.csv$' # a regex to match the CSV files
  # An additional file filter which uses the named groups from the file_pattern to further filter the files
  # file_filter:
  #   # source: (source_filter)
  #   year: (2023)
  #   month: (06)
  #   # day: (22)
  post_process: # Optional, set of steps to process the data before going into the workflow
    - verb: filter
      args:
        column: ""title"",
        value: ""My document""

Configuration Template
The following template can be used and stored as a .env in the the directory where you're are pointing the --root parameter on your Indexing Pipeline execution.

For details about how to run the Indexing Pipeline, refer to the Index CLI documentation.

.env File Template
Required variables are uncommented. All the optional configuration can be turned on or off as needed.

Minimal Configuration
# Base LLM Settings
GRAPHRAG_API_KEY=""your_api_key""
GRAPHRAG_API_BASE=""http://<domain>.openai.azure.com"" # For Azure OpenAI Users
GRAPHRAG_API_VERSION=""api_version"" # For Azure OpenAI Users

# Text Generation Settings
GRAPHRAG_LLM_TYPE=""azure_openai_chat"" # or",765d8a78606fe81a03a0da4f7ff231fa,['bbe409e13dd4d7bbd284976287a4feba'],300
3da10b454f926a257b9fdf5d2487c0a5,"Required variables are uncommented. All the optional configuration can be turned on or off as needed.

Minimal Configuration
# Base LLM Settings
GRAPHRAG_API_KEY=""your_api_key""
GRAPHRAG_API_BASE=""http://<domain>.openai.azure.com"" # For Azure OpenAI Users
GRAPHRAG_API_VERSION=""api_version"" # For Azure OpenAI Users

# Text Generation Settings
GRAPHRAG_LLM_TYPE=""azure_openai_chat"" # or openai_chat
GRAPHRAG_LLM_DEPLOYMENT_NAME=""gpt-4-turbo-preview""
GRAPHRAG_LLM_MODEL_SUPPORTS_JSON=True

# Text Embedding Settings
GRAPHRAG_EMBEDDING_TYPE=""azure_openai_embedding"" # or openai_embedding
GRAPHRAG_LLM_DEPLOYMENT_NAME=""text-embedding-3-small""

# Data Mapping Settings
GRAPHRAG_INPUT_TYPE=""text""

Full Configuration

# Required LLM Config

# Input Data Configuration
GRAPHRAG_INPUT_TYPE=""file""

# Plaintext Input Data Configuration
# GRAPHRAG_INPUT_FILE_PATTERN=.*\.txt

# Text Input Data Configuration
GRAPHRAG_INPUT_FILE_TYPE=""text""
GRAPHRAG_INPUT_FILE_PATTERN="".*\.txt$""
GRAPHRAG_INPUT_SOURCE_COLUMN=source
# GRAPHRAG_INPUT_TIMESTAMP_COLUMN=None
# GRAPHRAG_INPUT_TIMESTAMP_FORMAT=None
# GRAP",3da10b454f926a257b9fdf5d2487c0a5,['bbe409e13dd4d7bbd284976287a4feba'],300
8ac79ce92be1254dfda9a10eb54ab703," LLM Config

# Input Data Configuration
GRAPHRAG_INPUT_TYPE=""file""

# Plaintext Input Data Configuration
# GRAPHRAG_INPUT_FILE_PATTERN=.*\.txt

# Text Input Data Configuration
GRAPHRAG_INPUT_FILE_TYPE=""text""
GRAPHRAG_INPUT_FILE_PATTERN="".*\.txt$""
GRAPHRAG_INPUT_SOURCE_COLUMN=source
# GRAPHRAG_INPUT_TIMESTAMP_COLUMN=None
# GRAPHRAG_INPUT_TIMESTAMP_FORMAT=None
# GRAPHRAG_INPUT_TEXT_COLUMN=""text""
# GRAPHRAG_INPUT_ATTRIBUTE_COLUMNS=id
# GRAPHRAG_INPUT_TITLE_COLUMN=""title""
# GRAPHRAG_INPUT_TYPE=""file""
# GRAPHRAG_INPUT_CONNECTION_STRING=None
# GRAPHRAG_INPUT_CONTAINER_NAME=None
# GRAPHRAG_INPUT_BASE_DIR=None

# Base LLM Settings
GRAPHRAG_API_KEY=""your_api_key""
GRAPHRAG_API_BASE=""http://<domain>.openai.azure.com"" # For Azure OpenAI Users
GRAPHRAG_API_VERSION=""api_version"" # For Azure OpenAI Users
# GRAPHRAG_API_ORGANIZATION=None
# GRAPHRAG_API_PROXY=None

# Text Generation Settings
# GRAPHRAG_LLM_TYPE=openai_chat
GRAPHRAG_LLM_API_KEY=""your_api_key"" # If GRAPHRAG_API_KEY is not set
GRAPHRAG_LLM_API_BASE=""http://",8ac79ce92be1254dfda9a10eb54ab703,['bbe409e13dd4d7bbd284976287a4feba'],300
7b45dafa74553d3899e2291a3c9fb86e,".azure.com"" # For Azure OpenAI Users
GRAPHRAG_API_VERSION=""api_version"" # For Azure OpenAI Users
# GRAPHRAG_API_ORGANIZATION=None
# GRAPHRAG_API_PROXY=None

# Text Generation Settings
# GRAPHRAG_LLM_TYPE=openai_chat
GRAPHRAG_LLM_API_KEY=""your_api_key"" # If GRAPHRAG_API_KEY is not set
GRAPHRAG_LLM_API_BASE=""http://<domain>.openai.azure.com"" # For Azure OpenAI Users and if GRAPHRAG_API_BASE is not set
GRAPHRAG_LLM_API_VERSION=""api_version"" # For Azure OpenAI Users and if GRAPHRAG_API_VERSION is not set
GRAPHRAG_LLM_MODEL_SUPPORTS_JSON=True # Suggested by default
# GRAPHRAG_LLM_API_ORGANIZATION=None
# GRAPHRAG_LLM_API_PROXY=None
# GRAPHRAG_LLM_DEPLOYMENT_NAME=None
# GRAPHRAG_LLM_MODEL=gpt-4-turbo-preview
# GRAPHRAG_LLM_MAX_TOKENS=4000
# GRAPHRAG_LLM_REQUEST_TIMEOUT=180
# GRAPHRAG_LLM_THREAD_COUNT=50
# GRAPHRAG_LLM_THREAD_STAGGER=0.3
# GRAPHRAG_LLM_CONCURRENT_REQUESTS=25
# GRAPHRAG",7b45dafa74553d3899e2291a3c9fb86e,['bbe409e13dd4d7bbd284976287a4feba'],300
9aff9243c57cabca574b35438bf31a50,"HRAG_LLM_DEPLOYMENT_NAME=None
# GRAPHRAG_LLM_MODEL=gpt-4-turbo-preview
# GRAPHRAG_LLM_MAX_TOKENS=4000
# GRAPHRAG_LLM_REQUEST_TIMEOUT=180
# GRAPHRAG_LLM_THREAD_COUNT=50
# GRAPHRAG_LLM_THREAD_STAGGER=0.3
# GRAPHRAG_LLM_CONCURRENT_REQUESTS=25
# GRAPHRAG_LLM_TPM=0
# GRAPHRAG_LLM_RPM=0
# GRAPHRAG_LLM_MAX_RETRIES=10
# GRAPHRAG_LLM_MAX_RETRY_WAIT=10
# GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION=True

# Text Embedding Settings
# GRAPHRAG_EMBEDDING_TYPE=openai_embedding
GRAPHRAG_EMBEDDING_API_KEY=""your_api_key"" # If GRAPHRAG_API_KEY is not set
GRAPHRAG_EMBEDDING_API_BASE=""http://<domain>.openai.azure.com""  # For Azure OpenAI Users and if GRAPHRAG_API_BASE is not set
GRAPHRAG_EMBEDDING_API_VERSION=""api_version"" # For Azure OpenAI Users and if GRAPHRAG_API_VERSION is not set
# GRAPHRAG_EMBEDDING_API_ORGANIZATION=None
# GRAPHR",9aff9243c57cabca574b35438bf31a50,['bbe409e13dd4d7bbd284976287a4feba'],300
485c17007ccb3102887eaa47d6a6100f,"APHRAG_API_KEY is not set
GRAPHRAG_EMBEDDING_API_BASE=""http://<domain>.openai.azure.com""  # For Azure OpenAI Users and if GRAPHRAG_API_BASE is not set
GRAPHRAG_EMBEDDING_API_VERSION=""api_version"" # For Azure OpenAI Users and if GRAPHRAG_API_VERSION is not set
# GRAPHRAG_EMBEDDING_API_ORGANIZATION=None
# GRAPHRAG_EMBEDDING_API_PROXY=None
# GRAPHRAG_EMBEDDING_DEPLOYMENT_NAME=None
# GRAPHRAG_EMBEDDING_MODEL=text-embedding-3-small
# GRAPHRAG_EMBEDDING_BATCH_SIZE=16
# GRAPHRAG_EMBEDDING_BATCH_MAX_TOKENS=8191
# GRAPHRAG_EMBEDDING_TARGET=required
# GRAPHRAG_EMBEDDING_SKIP=None
# GRAPHRAG_EMBEDDING_THREAD_COUNT=None
# GRAPHRAG_EMBEDDING_THREAD_STAGGER=50
# GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS=25
# GRAPHRAG_EMBEDDING_TPM=0
# GRAPHRAG_EMBEDDING_RPM=0
# GRAPHRAG_EMBEDDING_MAX_RETRIES=10
# GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT=10
",485c17007ccb3102887eaa47d6a6100f,['bbe409e13dd4d7bbd284976287a4feba'],300
2b777e3d591ce1511a03abd1a6d8dc73,"HRAG_EMBEDDING_THREAD_COUNT=None
# GRAPHRAG_EMBEDDING_THREAD_STAGGER=50
# GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS=25
# GRAPHRAG_EMBEDDING_TPM=0
# GRAPHRAG_EMBEDDING_RPM=0
# GRAPHRAG_EMBEDDING_MAX_RETRIES=10
# GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT=10
# GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION=True

# Data Mapping Settings
# GRAPHRAG_INPUT_ENCODING=utf-8

# Data Chunking
# GRAPHRAG_CHUNK_SIZE=1200
# GRAPHRAG_CHUNK_OVERLAP=100
# GRAPHRAG_CHUNK_BY_COLUMNS=id

# Prompting Overrides
# GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE=None
# GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS=1
# GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES=organization,person,event,geo
# GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE=None
# GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH=500
# GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION=""Any claims or facts that could be relevant to threat analysis.""
# GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE=None
# GRAPHRAG_CLAIM_EXTRACTION",2b777e3d591ce1511a03abd1a6d8dc73,['bbe409e13dd4d7bbd284976287a4feba'],300
cde833db73c46ca28f08e35195134441,"_GLEANINGS=1
# GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES=organization,person,event,geo
# GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE=None
# GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH=500
# GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION=""Any claims or facts that could be relevant to threat analysis.""
# GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE=None
# GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS=1
# GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE=None
# GRAPHRAG_COMMUNITY_REPORT_MAX_LENGTH=1500

# Storage
# GRAPHRAG_STORAGE_TYPE=file
# GRAPHRAG_STORAGE_CONNECTION_STRING=None
# GRAPHRAG_STORAGE_CONTAINER_NAME=None
# GRAPHRAG_STORAGE_BASE_DIR=None

# Cache
# GRAPHRAG_CACHE_TYPE=file
# GRAPHRAG_CACHE_CONNECTION_STRING=None
# GRAPHRAG_CACHE_CONTAINER_NAME=None
# GRAPHRAG_CACHE_BASE_DIR=None

# Reporting
# GRAPHRAG_REPORTING_TYPE=file
# GRAPHRAG_REPORTING_CONNECTION_STRING=None
# GRAPHRAG_REPORTING_CONTAINER_NAME=None
# GRAPHRAG_REPORTING_BASE_DIR=None

# Node2Vec Parameters
# GRAPHRAG_NODE2VEC_ENABLED=False
# GRAPHRAG_NODE2VEC_NUM_WALKS=10
# GRAPHR",cde833db73c46ca28f08e35195134441,['bbe409e13dd4d7bbd284976287a4feba'],300
79d4b8574baf3734b60b969b66326b2e,"APHRAG_CACHE_CONTAINER_NAME=None
# GRAPHRAG_CACHE_BASE_DIR=None

# Reporting
# GRAPHRAG_REPORTING_TYPE=file
# GRAPHRAG_REPORTING_CONNECTION_STRING=None
# GRAPHRAG_REPORTING_CONTAINER_NAME=None
# GRAPHRAG_REPORTING_BASE_DIR=None

# Node2Vec Parameters
# GRAPHRAG_NODE2VEC_ENABLED=False
# GRAPHRAG_NODE2VEC_NUM_WALKS=10
# GRAPHRAG_NODE2VEC_WALK_LENGTH=40
# GRAPHRAG_NODE2VEC_WINDOW_SIZE=2
# GRAPHRAG_NODE2VEC_ITERATIONS=3
# GRAPHRAG_NODE2VEC_RANDOM_SEED=597832

# Data Snapshotting
# GRAPHRAG_SNAPSHOT_GRAPHML=False
# GRAPHRAG_SNAPSHOT_RAW_ENTITIES=False
# GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES=False

# Miscellaneous Settings
# GRAPHRAG_ASYNC_MODE=asyncio
# GRAPHRAG_ENCODING_MODEL=cl100k_base
# GRAPHRAG_MAX_CLUSTER_SIZE=10
# GRAPHRAG_ENTITY_RESOLUTION_ENABLED=False
# GRAPHRAG_SKIP_WORKFLOWS=None
# GRAPHRAG_UMAP_ENABLED=False



",79d4b8574baf3734b60b969b66326b2e,['bbe409e13dd4d7bbd284976287a4feba'],263
1b24101de07b1c195448240237b84b37,"_ASYNC_MODE=asyncio
# GRAPHRAG_ENCODING_MODEL=cl100k_base
# GRAPHRAG_MAX_CLUSTER_SIZE=10
# GRAPHRAG_ENTITY_RESOLUTION_ENABLED=False
# GRAPHRAG_SKIP_WORKFLOWS=None
# GRAPHRAG_UMAP_ENABLED=False



",1b24101de07b1c195448240237b84b37,['bbe409e13dd4d7bbd284976287a4feba'],63
85e50a4d70697a2c4420e7a9fc82f22d,"Indexing Dataflow
The GraphRAG Knowledge Model
The knowledge model is a specification for data outputs that conform to our data-model definition. You can find these definitions in the python/graphrag/graphrag/model folder within the GraphRAG repository. The following entity types are provided. The fields here represent the fields that are text-embedded by default.

Document - An input document into the system. These either represent individual rows in a CSV or individual .txt file.
TextUnit - A chunk of text to analyze. The size of these chunks, their overlap, and whether they adhere to any data boundaries may be configured below. A common use case is to set CHUNK_BY_COLUMNS to id so that there is a 1-to-many relationship between documents and TextUnits instead of a many-to-many.
Entity - An entity extracted from a TextUnit. These represent people, places, events, or some other entity-model that you provide.
Relationship - A relationship between two entities. These are generated from the covariates.
Covariate - Extracted claim information, which contains statements about entities which may be time-bound.
Community Report - Once entities are generated, we perform hierarchical community detection on them and generate reports for each community in this hierarchy.
Node - This table contains layout information for rendered graph-views of the Entities and Documents which have been embedded and clustered.
The Default Configuration Workflow
Let's take a look at how the default-configuration workflow transforms text documents into the GraphRAG Knowledge Model",85e50a4d70697a2c4420e7a9fc82f22d,['d3bd620a40d6cd4be5e0136357eb62da'],300
493f38f41b89e767fc23d84e1fa5ba20," covariates.
Covariate - Extracted claim information, which contains statements about entities which may be time-bound.
Community Report - Once entities are generated, we perform hierarchical community detection on them and generate reports for each community in this hierarchy.
Node - This table contains layout information for rendered graph-views of the Entities and Documents which have been embedded and clustered.
The Default Configuration Workflow
Let's take a look at how the default-configuration workflow transforms text documents into the GraphRAG Knowledge Model. This page gives a general overview of the major steps in this process. To fully configure this workflow, check out the configuration documentation.

Phase 6: Network Visualization
Phase 5: Document Processing
Phase 4: Community Summarization
Phase 3: Graph Augmentation
Phase 2: Graph Extraction
Phase 1: Compose TextUnits
Umap Documents
Umap Entities
Nodes Table
Link to TextUnits
Document Embedding
Document Graph Creation
Document Tables
Community Summarization
Community Embedding
Community Tables
Community Detection
Graph Embedding
Augmented Graph Tables
Entity & Relationship Extraction
Entity & Relationship Summarization
Entity Resolution
Claim Extraction
Graph Tables
Chunk
Documents
Text Units
Embed
Dataflow Overview
Phase 1: Compose TextUnits
The first phase of the default-configuration workflow is to transform input documents into TextUnits. A TextUnit is a chunk of text that is used for our graph",493f38f41b89e767fc23d84e1fa5ba20,['d3bd620a40d6cd4be5e0136357eb62da'],300
6f92ce3fcd05dd5697ded83586f7bc08,"
Community Summarization
Community Embedding
Community Tables
Community Detection
Graph Embedding
Augmented Graph Tables
Entity & Relationship Extraction
Entity & Relationship Summarization
Entity Resolution
Claim Extraction
Graph Tables
Chunk
Documents
Text Units
Embed
Dataflow Overview
Phase 1: Compose TextUnits
The first phase of the default-configuration workflow is to transform input documents into TextUnits. A TextUnit is a chunk of text that is used for our graph extraction techniques. They are also used as source-references by extracted knowledge items in order to empower breadcrumbs and provenance by concepts back to their original source tex.

The chunk size (counted in tokens), is user-configurable. By default this is set to 300 tokens, although we've had positive experience with 1200-token chunks using a single ""glean"" step. (A ""glean"" step is a follow-on extraction). Larger chunks result in lower-fidelity output and less meaningful reference texts; however, using larger chunks can result in much faster processing time.

The group-by configuration is also user-configurable. By default, we align our chunks to document boundaries, meaning that there is a strict 1-to-many relationship between Documents and TextUnits. In rare cases, this can be turned into a many-to-many relationship. This is useful when the documents are very short and we need several of them to compose a meaningful analysis unit (e.g. Tweets or a chat log",6f92ce3fcd05dd5697ded83586f7bc08,['d3bd620a40d6cd4be5e0136357eb62da'],300
81f57cf867ea246ad9a6e794ed613375," meaningful reference texts; however, using larger chunks can result in much faster processing time.

The group-by configuration is also user-configurable. By default, we align our chunks to document boundaries, meaning that there is a strict 1-to-many relationship between Documents and TextUnits. In rare cases, this can be turned into a many-to-many relationship. This is useful when the documents are very short and we need several of them to compose a meaningful analysis unit (e.g. Tweets or a chat log)

Each of these text-units are text-embedded and passed into the next phase of the pipeline.

Document 1
TextUnit 1
TextUnit 2
Document 2
TextUnit 3
TextUnit 4
Documents into Text Chunks
Phase 2: Graph Extraction
In this phase, we analyze each text unit and extract our graph primitives: Entities, Relationships, and Claims. Entities and Relationships are extracted at once in our entity_extract verb, and claims are extracted in our claim_extract verb. Results are then combined and passed into following phases of the pipeline.

TextUnit
Graph Extraction
Graph Summarization
Entity Resolution
Claim Extraction
Graph Extraction
Entity & Relationship Extraction
In this first step of graph extraction, we process each text-unit in order to extract entities and relationships out of the raw text using the LLM. The output of this step is a subgraph-per-TextUnit containing a list of entities with a name, type, and",81f57cf867ea246ad9a6e794ed613375,['d3bd620a40d6cd4be5e0136357eb62da'],300
10d01d36390b307a63fd5bc97d8682c0," extracted in our claim_extract verb. Results are then combined and passed into following phases of the pipeline.

TextUnit
Graph Extraction
Graph Summarization
Entity Resolution
Claim Extraction
Graph Extraction
Entity & Relationship Extraction
In this first step of graph extraction, we process each text-unit in order to extract entities and relationships out of the raw text using the LLM. The output of this step is a subgraph-per-TextUnit containing a list of entities with a name, type, and description, and a list of relationships with a source, target, and description.

These subgraphs are merged together - any entities with the same name and type are merged by creating an array of their descriptions. Similarly, any relationships with the same source and target are merged by creating an array of their descriptions.

Entity & Relationship Summarization
Now that we have a graph of entities and relationships, each with a list of descriptions, we can summarize these lists into a single description per entity and relationship. This is done by asking the LLM for a short summary that captures all of the distinct information from each description. This allows all of our entities and relationships to have a single concise description.

Entity Resolution (Not Enabled by Default)
The final step of graph extraction is to resolve any entities that represent the same real-world entity but but have different names. Since this is done via LLM, and we don't want to lose information, we want to take a conservative, non-destructive approach to this",10d01d36390b307a63fd5bc97d8682c0,['d3bd620a40d6cd4be5e0136357eb62da'],300
d44248ff7b7bfd969a7208eb3d6e2a78," This is done by asking the LLM for a short summary that captures all of the distinct information from each description. This allows all of our entities and relationships to have a single concise description.

Entity Resolution (Not Enabled by Default)
The final step of graph extraction is to resolve any entities that represent the same real-world entity but but have different names. Since this is done via LLM, and we don't want to lose information, we want to take a conservative, non-destructive approach to this.

Our current implementation of Entity Resolution, however, is destructive. It will provide the LLM with a series of entities and ask it to determine which ones should be merged. Those entities are then merged together into a single entity and their relationships are updated.

We are currently exploring other entity resolution techniques. In the near future, entity resolution will be executed by creating an edge between entity variants indicating that the entities have been resolved by the indexing engine. This will allow for end-users to undo indexing-side resolutions, and add their own non-destructive resolutions using a similar process.

Claim Extraction & Emission
Finally, as an independent workflow, we extract claims from the source TextUnits. These claims represent positive factual statements with an evaluated status and time-bounds. These are emitted as a primary artifact called Covariates.

Phase 3: Graph Augmentation
Now that we have a usable graph of entities and relationships, we want to understand their community structure and augment the graph with additional information. This is",d44248ff7b7bfd969a7208eb3d6e2a78,['d3bd620a40d6cd4be5e0136357eb62da'],300
a6bcb4514cb6de67e3d74ad0ea62452d,", and add their own non-destructive resolutions using a similar process.

Claim Extraction & Emission
Finally, as an independent workflow, we extract claims from the source TextUnits. These claims represent positive factual statements with an evaluated status and time-bounds. These are emitted as a primary artifact called Covariates.

Phase 3: Graph Augmentation
Now that we have a usable graph of entities and relationships, we want to understand their community structure and augment the graph with additional information. This is done in two steps: Community Detection and Graph Embedding. These give us explicit (communities) and implicit (embeddings) ways of understanding the topological structure of our graph.

Leiden Hierarchical Community Detection
Node2Vec Graph Embedding
Graph Table Emission
Graph Augmentation
Community Detection
In this step, we generate a hierarchy of entity communities using the Hierarchical Leiden Algorithm. This method will apply a recursive community-clustering to our graph until we reach a community-size threshold. This will allow us to understand the community structure of our graph and provide a way to navigate and summarize the graph at different levels of granularity.

Graph Embedding
In this step, we generate a vector representation of our graph using the Node2Vec algorithm. This will allow us to understand the implicit structure of our graph and provide an additional vector-space in which to search for related concepts during our query phase.

Graph Tables Emission
Once our graph augmentation steps are complete, the final Entities and",a6bcb4514cb6de67e3d74ad0ea62452d,['d3bd620a40d6cd4be5e0136357eb62da'],300
5b2968b8f1c891d47ecbe641c3391663," threshold. This will allow us to understand the community structure of our graph and provide a way to navigate and summarize the graph at different levels of granularity.

Graph Embedding
In this step, we generate a vector representation of our graph using the Node2Vec algorithm. This will allow us to understand the implicit structure of our graph and provide an additional vector-space in which to search for related concepts during our query phase.

Graph Tables Emission
Once our graph augmentation steps are complete, the final Entities and Relationships tables are emitted after their text fields are text-embedded.

Phase 4: Community Summarization
Generate Community Reports
Summarize Community Reports
Community Embedding
Community Tables Emission
Community Summarization
At this point, we have a functional graph of entities and relationships, a hierarchy of communities for the entities, as well as node2vec embeddings.

Now we want to build on the communities data and generate reports for each community. This gives us a high-level understanding of the graph at several points of graph granularity. For example, if community A is the top-level community, we'll get a report about the entire graph. If the community is lower-level, we'll get a report about a local cluster.

Generate Community Reports
In this step, we generate a summary of each community using the LLM. This will allow us to understand the distinct information contained within each community and provide a scoped understanding of the graph, from either a high-level or a low-level perspective",5b2968b8f1c891d47ecbe641c3391663,['d3bd620a40d6cd4be5e0136357eb62da'],300
3e292d936b7efa377ba9530456cfd888," the graph at several points of graph granularity. For example, if community A is the top-level community, we'll get a report about the entire graph. If the community is lower-level, we'll get a report about a local cluster.

Generate Community Reports
In this step, we generate a summary of each community using the LLM. This will allow us to understand the distinct information contained within each community and provide a scoped understanding of the graph, from either a high-level or a low-level perspective. These reports contain an executive overview and reference the key entities, relationships, and claims within the community sub-structure.

Summarize Community Reports
In this step, each community report is then summarized via the LLM for shorthand use.

Community Embedding
In this step, we generate a vector representation of our communities by generating text embeddings of the community report, the community report summary, and the title of the community report.

Community Tables Emission
At this point, some bookkeeping work is performed and we emit the Communities and CommunityReports tables.

Phase 5: Document Processing
In this phase of the workflow, we create the Documents table for the knowledge model.

Augment
Link to TextUnits
Avg. Embedding
Document Table Emission
Document Processing
Augment with Columns (CSV Only)
If the workflow is operating on CSV data, you may configure your workflow to add additional fields to Documents output. These fields should exist on the incoming CSV tables. Details about configuring this",3e292d936b7efa377ba9530456cfd888,['d3bd620a40d6cd4be5e0136357eb62da'],300
827fd80da359cf05b091c24e465dd05d," performed and we emit the Communities and CommunityReports tables.

Phase 5: Document Processing
In this phase of the workflow, we create the Documents table for the knowledge model.

Augment
Link to TextUnits
Avg. Embedding
Document Table Emission
Document Processing
Augment with Columns (CSV Only)
If the workflow is operating on CSV data, you may configure your workflow to add additional fields to Documents output. These fields should exist on the incoming CSV tables. Details about configuring this can be found in the configuration documentation.

Link to TextUnits
In this step, we link each document to the text-units that were created in the first phase. This allows us to understand which documents are related to which text-units and vice-versa.

Document Embedding
In this step, we generate a vector representation of our documents using an average embedding of document slices. We re-chunk documents without overlapping chunks, and then generate an embedding for each chunk. We create an average of these chunks weighted by token-count and use this as the document embedding. This will allow us to understand the implicit relationship between documents, and will help us generate a network representation of our documents.

Documents Table Emission
At this point, we can emit the Documents table into the knowledge Model.

Phase 6: Network Visualization
In this phase of the workflow, we perform some steps to support network visualization of our high-dimensional vector spaces within our existing graphs. At this point there are two logical graphs at",827fd80da359cf05b091c24e465dd05d,['d3bd620a40d6cd4be5e0136357eb62da'],300
56506e2d064c0732efa3cf418057edfd," these chunks weighted by token-count and use this as the document embedding. This will allow us to understand the implicit relationship between documents, and will help us generate a network representation of our documents.

Documents Table Emission
At this point, we can emit the Documents table into the knowledge Model.

Phase 6: Network Visualization
In this phase of the workflow, we perform some steps to support network visualization of our high-dimensional vector spaces within our existing graphs. At this point there are two logical graphs at play: the Entity-Relationship graph and the Document graph.

Umap Documents
Umap Entities
Nodes Table Emission
Network Visualization Workflows
For each of the logical graphs, we perform a UMAP dimensionality reduction to generate a 2D representation of the graph. This will allow us to visualize the graph in a 2D space and understand the relationships between the nodes in the graph. The UMAP embeddings are then emitted as a table of Nodes. The rows of this table include a discriminator indicating whether the node is a document or an entity, and the UMAP coordinates.",56506e2d064c0732efa3cf418057edfd,['d3bd620a40d6cd4be5e0136357eb62da'],219
2011f03f21e526cf9277c27bf3e68242," a discriminator indicating whether the node is a document or an entity, and the UMAP coordinates.",2011f03f21e526cf9277c27bf3e68242,['d3bd620a40d6cd4be5e0136357eb62da'],19
f239de6498e0f471bf418974c00f1e36,"Indexer CLI
The GraphRAG indexer CLI allows for no-code usage of the GraphRAG Indexer.

python -m graphrag.index --verbose --root </workspace/project/root> --config <custom_config.yml>
--resume <timestamp> --reporter <rich|print|none> --emit json,csv,parquet
--nocache

CLI Arguments
--verbose - Adds extra logging information during the run.
--root <data-project-dir> - the data root directory. This should contain an input directory with the input data, and an .env file with environment variables. These are described below.
--init - This will initialize the data project directory at the specified root with bootstrap configuration and prompt-overrides.
--resume <output-timestamp> - if specified, the pipeline will attempt to resume a prior run. The parquet files from the prior run will be loaded into the system as inputs, and the workflows that generated those files will be skipped. The input value should be the timestamped output folder, e.g. ""20240105-143721"".
--config <config_file.yml> - This will opt-out of the Default Configuration mode and execute a custom configuration. If this is used, then none of the environment-variables below will apply.
--reporter <reporter> - This will specify the progress reporter to use. The default is rich. Valid values are rich, print, and none.
--emit <types> - This specifies the table output",f239de6498e0f471bf418974c00f1e36,['e9ffa90c42f0891629fcbd2a857da299'],300
919cb44d9688a14bf48fa7c98163ed81," be the timestamped output folder, e.g. ""20240105-143721"".
--config <config_file.yml> - This will opt-out of the Default Configuration mode and execute a custom configuration. If this is used, then none of the environment-variables below will apply.
--reporter <reporter> - This will specify the progress reporter to use. The default is rich. Valid values are rich, print, and none.
--emit <types> - This specifies the table output formats the pipeline should emit. The default is parquet. Valid values are parquet, csv, and json, comma-separated.
--nocache - This will disable the caching mechanism. This is useful for debugging and development, but should not be used in production.",919cb44d9688a14bf48fa7c98163ed81,['e9ffa90c42f0891629fcbd2a857da299'],153
